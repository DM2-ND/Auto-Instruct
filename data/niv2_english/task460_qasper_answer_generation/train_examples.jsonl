{"id": "task460-1f9aaae91ac04306a943bce36d325023", "input": "The task, as framed above, requires to detect the semantic change between two corpora. The two corpora used in the shared task correspond to the diachronic corpus pair from BIBREF0: DTA18 and DTA19. They consist of subparts of DTA corpus BIBREF11 which is a freely available lemmatized, POS-tagged and spelling-normalized diachronic corpus of German containing texts from the 16th to the 20th century. DTA18 contains 26 million sentences published between 1750-1799 and DTA19 40 million between 1850-1899. \n Question: What is the corpus used for the task?", "target": "DTA18 DTA19"}
{"id": "task460-63d9983c7ffe463e8af35a703be9ba77", "input": "WAS: The architecture used in BIBREF3 without the audio input. The decoder output Chinese character at each timestep. Others keep unchanged to the original implementation. LipCH-Net-seq: For a fair comparison, we use sequence-to-sequence with attention framework to replace the Connectionist temporal classification (CTC) loss BIBREF14 used in LipCH-Net BIBREF5 when converting picture to pinyin. CSSMCM-w/o video: To evaluate the necessity of video information when predicting tone, the video stream is removed when predicting tone and Chinese characters. In other word, video is only used when predicting the pinyin sequence. The tone is predicted from the pinyin sequence. Tone information and pinyin information work together to predict Chinese character. \n Question: What was the previous state of the art model for this task?", "target": "WAS LipCH-Net-seq CSSMCM-w/o video"}
{"id": "task460-4a7cf4560f464b37aaaec1ca14bf9539", "input": "We used fastText and SVM BIBREF16 for preliminary experiments.  \n Question: What classification models were used?", "target": "fastText and SVM BIBREF16"}
{"id": "task460-7950909759854e86a72daad46aab8746", "input": "Four datasets are used in our work, including CoNLL 2003 German BIBREF9 , CoNLL 2002 Spanish BIBREF10 , OntoNotes 4 BIBREF11 and Weibo NER BIBREF12 .  Table TABREF22 shows the results on Chinese OntoNotes 4.0.  \n Question: Which languages do they work with?", "target": "German Spanish Chinese"}
{"id": "task460-a2ae1145fbea447883c1bc2b71a0aba3", "input": "Our work is further connected to the topic of interpretability and transparency of machine learning models BIBREF11, BIBREF35, BIBREF12, for which humans are increasingly involved, for instance for post-hoc evaluations of the model's interpretability.  \n Question: How are the interpretability merits of the approach demonstrated?", "target": "By involving humans for post-hoc evaluation of model's interpretability"}
{"id": "task460-09d3024fa1b544788becc32c63698ffd", "input": "Despite its usefulness, linked entities extracted from ELS's have issues because of low precision rates BIBREF11 and design challenges in training datasets BIBREF12 . First, the extracted entities may be ambiguous. Second, the linked entities may also be too common to be considered an entity.  \n Question: Why are current ELS's not sufficiently effective?", "target": "Linked entities may be ambiguous or too common"}
{"id": "task460-e06c2781b8554a798166f918245bdd07", "input": "Given that non-content words are distinctive enough for a classifier to determine style, we propose a suite of low-level linguistic feature counts (henceforth, controls) as our formal, content-blind definition of style. The style of a sentence is represented as a vector of counts of closed word classes (like personal pronouns) as well as counts of syntactic features like the number of SBAR non-terminals in its constituency parse, since clause structure has been shown to be indicative of style BIBREF20. Controls are extracted heuristically, and almost all rely on counts of pre-defined word lists. For constituency parses we use the Stanford Parser BIBREF21. table:controlexamples lists all the controls along with examples. \n Question: How they model style as a suite of low-level linguistic controls, such as frequency of pronouns, prepositions, and subordinate clause constructions?", "target": "style of a sentence is represented as a vector of counts of closed word classes (like personal pronouns) as well as counts of syntactic features like the number of SBAR non-terminals in its constituency parse, since clause structure has been shown to be indicative of style"}
{"id": "task460-9c9d0000f57f40808898bc046ec51641", "input": "Our data consists of two sets used to train and evaluate our automatic speech recognition system. Four major evaluation campaigns have enabled the creation of wide corpora of French broadcast speech: ESTER1 BIBREF13, ESTER2 BIBREF14, ETAPE BIBREF15 and REPERE BIBREF16. \n Question: Which corpora does this paper analyse?", "target": "ESTER1 ESTER2 ETAPE REPERE"}
{"id": "task460-07180827e89746c5aaa7663c9fbd0b57", "input": "We conduct experiments on our self-collected CAIS to evaluate the generalizability in different language. We apply two baseline models for comparison, one is the popular BiLSTMs + CRF architecture BIBREF36 for sequence labeling task, and the other one is the more powerful sententce-state LSTM BIBREF21. The results listed in Table TABREF50 demonstrate the generalizability and effectiveness of our CM-Net when handling various domains and different languages. \n Question: What were the baselines models?", "target": "BiLSTMs + CRF architecture BIBREF36 sententce-state LSTM BIBREF21"}
{"id": "task460-79c57bfdb49f4a5999de1414981a6fa4", "input": "Trinomials are likely to appear in exactly one order, and when they appear in more than one order the last word is almost always the same across all instances.  \n Question: Are there any new finding in analasys of trinomials that was not present binomials?", "target": "Trinomials are likely to appear in exactly one order"}
{"id": "task460-107d6134282e4cecaf5468aaadaa44f3", "input": "Depechemood is a lexicon-based emotion detection method gathered from crowd-annotated news BIBREF24. Drawing on approximately 23.5K documents with average of 500 words per document from rappler.com, researchers asked subjects to report their emotions after reading each article. They then multiplied the document-emotion matrix and word-document matrix to derive emotion-word matrix for these words.  \n Question: How id Depechemood trained?", "target": "By multiplying crowd-annotated document-emotion matrix with emotion-word matrix. "}
{"id": "task460-27d354d894194bb5a8ccf1c98a9e15d4", "input": "We also measure the usage of words related to people's core values as reported by Boyd et al. boyd2015. The sets of words, or themes, were excavated using the Meaning Extraction Method (MEM) BIBREF10 . \n Question: How do they obtain psychological dimensions of people?", "target": "using the Meaning Extraction Method"}
{"id": "task460-198f4950b3e243f088a8439846d026d4", "input": "The improved performance of our attention models that actively select their optimal context, over a model with the complete thread as context, hLSTM, shows that the context inference improves intervention prediction over using the default full context. \n Question: What aspects of discussion are relevant to instructor intervention, according to the attention mechanism?", "target": "context inference"}
{"id": "task460-3bcd9d8eb2794ffe9acd470754f716af", "input": "Deeply moved readers shed tears or get chills and goosebumps even in lab settings BIBREF4. In cases like these, the emotional response actually implies an aesthetic evaluation: narratives that have the capacity to move readers are evaluated as good and powerful texts for this very reason. Similarly, feelings of suspense experienced in narratives not only respond to the trajectory of the plot's content, but are also directly predictive of aesthetic liking (or disliking). Emotions that exhibit this dual capacity have been defined as \u201caesthetic emotions\u201d BIBREF2. \n Question: What are the aesthetic emotions formalized?", "target": "feelings of suspense experienced in narratives not only respond to the trajectory of the plot's content, but are also directly predictive of aesthetic liking (or disliking) Emotions that exhibit this dual capacity have been defined as \u201caesthetic emotions\u201d"}
{"id": "task460-80280839e3e34ecf82430c73c810bcd7", "input": "In our method, we take an image as input and generate a natural question as output. The architecture for our model is shown in Figure FIGREF4 . Our model contains three main modules, (a) Representation Module that extracts multimodal features (b) Mixture Module that fuses the multimodal representation and (c) Decoder that generates question using an LSTM-based language model. \n Question: How/where are the natural question generated?", "target": "Decoder that generates question using an LSTM-based language model"}
{"id": "task460-3dc316c542b14c34b1447e494444f679", "input": "This effect of context on human ratings is very similar to the one reported in BIBREF5 . They find that sentences rated as ill formed out of context are improved when they are presented in their document contexts. BIBREF5 suggest that adding context causes speakers to focus on broader semantic and pragmatic issues of discourse coherence, rather than simply judging syntactic well formedness (measured as naturalness) when a sentence is considered in isolation. \n Question: What provisional explanation do the authors give for the impact of document context?", "target": "adding context causes speakers to focus on broader semantic and pragmatic issues of discourse coherence"}
{"id": "task460-9c00fffda58b48aa872b83cfcc0160e0", "input": "We manually reviewed 1,177 pairs of entities and referring expressions generated by the system. Overall, 73.3% of the non-first mentions of entities were replaced with suitable shorter and more fluent expressions. \n Question: How is fluency of generated text evaluated?", "target": "manually reviewed"}
{"id": "task460-be3b9c83756e4c8ea63fb68800c98bcb", "input": "We create three additional training datasets by adding sentences involving object RCs to the original Wikipedia corpus (Section lm). To this end, we randomly pick up 30 million sentences from Wikipedia (not overlapped to any sentences in the original corpus), parse by the same parser, and filter sentences containing an object RC, amounting to 680,000 sentences.  \n Question: How do they perform data augmentation?", "target": "They randomly sample sentences from Wikipedia that contains an object RC and add them to training data"}
{"id": "task460-19df51d4ea064395a1749abbdd4bd34e", "input": "We collect HLA data from TV Tropes BIBREF3, a knowledge-based website dedicated to pop culture, containing information on a plethora of characters from a variety of sources. Similar to Wikipedia, its content is provided and edited collaboratively by a massive user-base. These attributes are determined by human viewers and their impressions of the characters, and are correlated with human-like characteristics. We believe that TV Tropes is better for our purpose of fictional character modeling than data sources used in works such as BIBREF25 shuster2019engaging because TV Tropes' content providers are rewarded for correctly providing content through community acknowledgement. \n Question: How does dataset model character's profiles?", "target": "attributes are determined by human viewers and their impressions of the characters, and are correlated with human-like characteristics"}
{"id": "task460-9758c89e52c44aa4b0673d17bda088ac", "input": "For generating a poem from images we use an existing actor-critic architecture BIBREF1. For Shakespearizing modern English texts, we experimented with various types of sequence to sequence models. We use a sequence-to-sequence model which consists of a single layer unidrectional LSTM encoder and a single layer LSTM decoder and pre-trained retrofitted word embeddings shared between source and target sentences. Since a pair of corresponding Shakespeare and modern English sentences have significant vocabulary overlap we extend the sequence-to-sequence model mentioned above using pointer networks BIBREF11 that provide location based attention and have been used to enable copying of tokens directly from the input. \n Question: What models are used for painting embedding and what for language style transfer?", "target": "generating a poem from images we use an existing actor-critic architecture various types of sequence to sequence models"}
{"id": "task460-a82219cfb76b45eeab3d579775a3f4f1", "input": "We carried out a reliability study for the proposed scheme using two pairs of expert annotators, P1 and P2.  Inter-rater reliability was assessed using Cohen's kappa: unweighted for argumentation and knowledge domain, but quadratic-weighted for specificity given its ordered labels. \n Question: what experiments are conducted?", "target": "a reliability study for the proposed scheme "}
{"id": "task460-b23a6f232e1544bf8cd25371cd318d3b", "input": "Baseline Experiments\nIn this section, we briefly describe a baseline and evaluation scripts that we release, with a detailed documentation, along with the corpus. \n Question: What baseline system is proposed?", "target": "Answer with content missing: (Baseline Method section) We implemented a simple approach inspired by previous work on concept map generation and keyphrase extraction."}
{"id": "task460-938919dcae3f48999ecb52f01496ecd5", "input": "The memory mechanism is adopted in order to enable the model to look beyond localized features and have access to the entire sequence. \n Question: Which features do they use?", "target": "beyond localized features and have access to the entire sequence"}
{"id": "task460-20de68d0b3714f2caeb405567a9972e9", "input": "The clinical notes we used for the experiment are provided by domain experts, consisting of 1,160 physician logs of Medical ICU admission requests at a tertiary care center affiliated to Mount Sanai. The enriched corpus contains 42,506 Wikipedia articles, each of which corresponds to one candidate, 6 research papers and 2 critical care medicine textbooks, besides our raw ICU data. \n Question: Which dataset do they use to build their model?", "target": "1,160 physician logs of Medical ICU admission requests 42,506 Wikipedia articles 6 research papers and 2 critical care medicine textbooks"}
{"id": "task460-bf366b6c165b4387b130fcc7fc407bb4", "input": "Conditional Random Fields\nConditional Random Fields (CRF) BIBREF10 are a standard approach when dealing with sequential data in the context of sequence labeling. BiLSTM-CRF\nPrior to the emergence of deep neural language models, BiLSTM-CRF models BIBREF11 had achieved state-of-the-art results for the task of sequence labeling. Multi-Task Learning\nMulti-Task Learning (MTL) BIBREF15 has become popular with the progress in deep learning. BioBERT\nDeep neural language models have recently evolved to a successful method for representing text. In particular, Bidirectional Encoder Representations from Transformers (BERT) outperformed previous state-of-the-art methods by a large margin on various NLP tasks BIBREF17. \n Question: What baseline systems are proposed?", "target": "Conditional Random Fields BiLSTM-CRF Multi-Task Learning BioBERT"}
{"id": "task460-d38377382f234cc4a620762109c6dd68", "input": "Figure FIGREF23 shows the proposed ontology, which, in our evaluation procedure, was populated with 3121 events entries from 51 documents. \n Question: How is the effectiveness of this pipeline approach evaluated?", "target": "proposed ontology, which, in our evaluation procedure, was populated with 3121 events entries from 51 documents."}
{"id": "task460-d5095102b31d4140a1014c2bfa83cd39", "input": "Among the GLUE tasks, these state-of-the-art systems make their greatest gains on the acceptability task with the Corpus of Linguistic Acceptability BIBREF0 . CoLA contains example sentences from linguistics publications labeled by experts for grammatical acceptability, and written to show subtle grammatical features. Because minimal syntactic differences can separate acceptable sentences from unacceptable ones (What did Bo write a book about? / *What was a book about written by Bo?), and acceptability classifiers are more reliable when trained on GPT and BERT than on recurrent models, it stands to reason that GPT and BERT have better implicit knowledge of syntactic features relevant to acceptability. \n Question: How is the CoLA grammatically annotated?", "target": "labeled by experts"}
{"id": "task460-b244233ec5d146d4b018c4738c291ad6", "input": "Machine-machine Interaction A related line of work explores simulation-based dialogue generation, where the user and system roles are simulated to generate a complete conversation flow, which can then be converted to natural language using crowd workers BIBREF1. Such a framework may be cost-effective and error-resistant since the underlying crowd worker task is simpler, and semantic annotations are obtained automatically. It is often argued that simulation-based data collection does not yield natural dialogues or sufficient coverage, when compared to other approaches such as Wizard-of-Oz. We argue that simulation-based collection is a better alternative for collecting datasets like this owing to the factors below. \n Question: How did they gather the data?", "target": "simulation-based dialogue generation, where the user and system roles are simulated to generate a complete conversation flow, which can then be converted to natural language using crowd workers "}
{"id": "task460-6335633e15964bc6b4eac62b8edcc447", "input": "The input for both tasks consists of news articles in free-text format, collected from 36 propagandist and 12 non-propagandist news outlets and then annotated by professional annotators.  \n Question: What dataset was used?", "target": " news articles in free-text format"}
{"id": "task460-b994c6e21cae41d9b441c330bacb8b2e", "input": "We adopt the DSTC2 BIBREF20 dataset and Maluuba BIBREF21 dataset to evaluate our proposed model. \n Question: What two benchmark datasets are used?", "target": "DSTC2 Maluuba"}
{"id": "task460-a9e67f269f3f4b66bde884475747e837", "input": "Our system learns Perceptron models BIBREF37 using the Machine Learning machinery provided by the Apache OpenNLP project with our own customized (local and clustering) features.  The local features constitute our baseline system on top of which the clustering features are added. \n Question: what are the baselines?", "target": "Perceptron model using the local features."}
{"id": "task460-d477bfdd4e0e4a04a880438343b5bd68", "input": " For the embeddings, we relied on $AraVec$ BIBREF30 for Arabic, FastText BIBREF31 for French, and Word2vec Google News BIBREF32 for English .  \n Question: What monolingual word representations are used?", "target": "AraVec for Arabic, FastText for French, and Word2vec Google News for English."}
{"id": "task460-1566c4e4bd494243b67f7596b784f487", "input": "n this work, we apply an automated neural ensemble annotation process for dialogue act labeling. Several neural models are trained with the Switchboard Dialogue Act (SwDA) Corpus BIBREF9, BIBREF10 and used for inferring dialogue acts on the emotion datasets. We ensemble five model output labels by checking majority occurrences (most of the model labels are the same) and ranking confidence values of the models. We adopted the neural architectures based on Bothe et al. bothe2018discourse where two variants are: non-context model (classifying at utterance level) and context model (recognizing the dialogue act of the current utterance given a few preceding utterances). \n Question: How many models were used?", "target": "five"}
{"id": "task460-f0b5667a5c0f4fa5aa75fbdb043c21aa", "input": "Category II and III errors are harder to avoid and could be improved by applying reasoning BIBREF36 or irony detection methods BIBREF37. \n Question: What recommendations are made to improve the performance in future?", "target": "applying reasoning BIBREF36 or irony detection methods BIBREF37"}
{"id": "task460-6b78560efa3d406f88dea083b2f077f3", "input": "We use the additional INLINEFORM0 training articles labeled by publisher as an unsupervised data set to further train the BERT model. We first investigate the impact of pre-training on BERT-BASE's performance.  On the same computer, fine tuning the model on the small training set took only about 35 minutes for sequence length 100.  \n Question: How are the two different models trained?", "target": "They pre-train the models using 600000 articles as an unsupervised dataset and then fine-tune the models on small training set."}
{"id": "task460-6d0142a86770422fa5cad2cca95d26f2", "input": "We operationalized the question by submitting the chosen test data to the same vendor-based transcription pipeline that is used at Microsoft for production data (for model training and internal evaluation purposes), and then comparing the results to ASR system output under the NIST scoring protocol.  \n Question: what standard speech transcription pipeline was used?", "target": "pipeline that is used at Microsoft for production data"}
{"id": "task460-50ad4d5fc94b4e72a8e0f60fed0d06b8", "input": "For Turkish, as the first dataset, we utilised the movie reviews which are collected from a popular website. The number of reviews in this movie corpus is 20,244 and the average number of words in reviews is 39. Each of these reviews has a star-rating score which is indicative of sentiment. These polarity scores are between the values 0.5 and 5, at intervals of 0.5. We consider a review to be negative it the score is equal to or lower than 2.5. On the other hand, if it is equal to or higher than 4, it is assumed to be positive. We have randomly selected 7,020 negative and 7,020 positive reviews and processed only them.\n\n \n Question: What details are given about the movie domain dataset?", "target": "there are 20,244 reviews divided into positive and negative with an average 39 words per review, each one having a star-rating score"}
{"id": "task460-b53cd9ba4a1b4fbbbb8e76eac6768e36", "input": "In our dataset, we store the full tweet object including the id of the tweet, username, hashtags, and geolocation of the tweet. We created a list of the most common Arabic keywords associated with COVID-19. Using Twitter\u2019s streaming API, we searched for any tweet containing the keyword(s) in the text of the tweet. Table TABREF1 shows the list of keywords used along with the starting date of tracking each keyword. Furthermore, Table TABREF2 shows the list of hashtags we have been tracking along with the number of tweets collected from each hashtag.  \n Question: What additional information is found in the dataset?", "target": "the full tweet object including the id of the tweet, username, hashtags, and geolocation of the tweet"}
{"id": "task460-f3a01ab636ed45898b29e097cf980680", "input": "Although we create new models for tasks such as sentiment analysis and gender detection as part of AraNet, our focus is more on putting together the toolkit itself and providing strong baselines that can be compared to. Hence, although we provide some baseline models for some of the tasks, we do not explicitly compare to previous research since most existing works either exploit smaller data (and so it will not be a fair comparison), use methods pre-dating BERT (and so will likely be outperformed by our models) . For many of the tasks we model, there have not been standard benchmarks for comparisons across models. This makes it difficult to measure progress and identify areas worthy of allocating efforts and budgets. \n Question: What models did they compare to?", "target": " we do not explicitly compare to previous research since most existing works either exploit smaller data (and so it will not be a fair comparison), use methods pre-dating BERT (and so will likely be outperformed by our models)"}
{"id": "task460-38ade7c2f4fa497297d59617c21b6658", "input": "We tested the natural language representation against the visual-based and feature representations on several tasks, with varying difficulty. The tasks included a basic scenario, a health gathering scenario, a scenario in which the agent must take cover from fireballs, a scenario in which the agent must defend itself from charging enemies, and a super scenario, where a mixture of the above scenarios was designed to challenge the agent. \n Question: What experiments authors perform?", "target": "a basic scenario, a health gathering scenario, a scenario in which the agent must take cover from fireballs, a scenario in which the agent must defend itself from charging enemies, and a super scenario, where a mixture of the above scenarios"}
{"id": "task460-f22537301a174accadd3575199aa16ef", "input": "Throughout this article, we use two different embedding spaces. The first is the widely used representation built on GoogleNews BIBREF8 . The second is taken from BIBREF2 , and was trained on a Reddit dataset BIBREF9 . \n Question: Which embeddings do they detect biases in?", "target": "Word embeddings trained on GoogleNews and Word embeddings trained on Reddit dataset"}
{"id": "task460-8670ca13550a4dc1901d8754d71cb843", "input": "Text Representation.\nThe model, implemented as a deep neural network, learns to respond by training on hundreds of millions context-reply $(c,r)$ pairs. First, similar to Henderson:2017arxiv, raw text from both $c$ and $r$ is converted to unigrams and bigrams. Photo Representation.\nPhotos are represented using convolutional neural net (CNN) models pretrained on ImageNet BIBREF17. We use a MobileNet model with a depth multiplier of 1.4, and an input dimension of $224 \\times 224$ pixels as in BIBREF18. \n Question: How does PolyResponse architecture look like?", "target": "Henderson:2017 MobileNet model"}
{"id": "task460-cd51b3b371794174ace84e83497e845a", "input": "To further improve the performance of our models, we introduce in-house labeled data that we use to fine-tune BERT. For the gender classification task, we manually label an in-house dataset of 1,100 users with gender tags, including 550 female users, 550 male users. We obtain 162,829 tweets by crawling the 1,100 users' timelines. For the dialect identification task, we randomly sample 20,000 tweets for each class from an in-house dataset gold labeled with the same 15 classes as the shared task. \n Question: What are the in-house data employed?", "target": "we manually label an in-house dataset of 1,100 users with gender tags we randomly sample 20,000 tweets for each class from an in-house dataset gold labeled with the same 15 classes as the shared task"}
{"id": "task460-9c274abddacb42acbfedb4cb7df62165", "input": "We now consider the following problem: given two BabelNet categories $A$ and $B$, predict whether they are likely to be conceptual neighbors based on the sentences from a text corpus in which they are both mentioned. To train such a classifier, we use the distant supervision labels from Section SECREF8 as training data. Once this classifier has been trained, we can then use it to predict conceptual neighborhood for categories for which only few instances are known. To find sentences in which both $A$ and $B$ are mentioned, we rely on a disambiguated text corpus in which mentions of BabelNet categories are explicitly tagged. Such a disambiguated corpus can be automatically constructed, using methods such as the one proposed by BIBREF30 mancini-etal-2017-embedding, for instance. For each pair of candidate categories, we thus retrieve all sentences where they co-occur. Next, we represent each extracted sentence as a vector.  \n Question: How they indentify conceptual neighbours?", "target": "Once this classifier has been trained, we can then use it to predict conceptual neighborhood for categories for which only few instances are known."}
{"id": "task460-495820c332354b1994c0e56788b08b5c", "input": "For instance, BIBREF16 and BIBREF17 showed that a hypothesis-only baseline performs better than chance due to cues on their lexical choice and sentence length. Similarly, BIBREF18 showed that NLI models tend to predict entailment for sentence pairs with a high lexical overlap. \n Question: What are examples of these artifacts?", "target": "hypothesis-only baseline performs better than chance due to cues on their lexical choice and sentence length NLI models tend to predict entailment for sentence pairs with a high lexical overlap"}
{"id": "task460-a65a8f448b394439b4a0cfc47cbb59d6", "input": "Table TABREF19 displays the performance of the 4 baselines on the ReviewQA's test set. These results are the performance achieved by our own implementation of these 4 models. \n Question: What tasks were evaluated?", "target": "ReviewQA's test set"}
{"id": "task460-82bd7e50c9664422bfbb6094284df069", "input": "The final result submitted in public leaderboard is 0.73019 and in private leaderboard is 0.58455. It is quite different in bad way. That maybe is the result of the model too overfit on train set tuning on public test set. \n Question: What is public dashboard?", "target": "Public dashboard where competitors can see their results during competition, on part of the test set (public test set)."}
{"id": "task460-e8485906dab64cf68202a8d4889f6f98", "input": "With this approach we improve the standard BERT models by up to four percentage points in accuracy. \n Question: By how much do they outperform standard BERT?", "target": "up to four percentage points in accuracy"}
{"id": "task460-4a23508b043443508b6c6ec6989666ca", "input": "Davidson et al. BIBREF2 showed that there were systematic and substantial racial biases in five benchmark Twitter datasets annotated for offensive language detection. By examining more samples and with respect to recently studies BIBREF2, BIBREF24, BIBREF19, it is clear that many errors are due to biases from data collection BIBREF19 and rules of annotation BIBREF24 and not the classifier itself.  \n Question: What are the existing biases?", "target": "systematic and substantial racial biases biases from data collection rules of annotation"}
{"id": "task460-499a9b64434e4ee5939504616b989fb7", "input": "In our profile verification process, we observed that most gang member profiles portray a context representative of gang culture. Some examples of these profile pictures are shown in Figure FIGREF32 , where the user holds or points weapons, is seen in a group fashion which displays a gangster culture, or is showing off graffiti, hand signs, tattoos and bulk cash. \n Question: What are the differences in the use of images between gang member and the rest of the Twitter population?", "target": "user holds or points weapons, is seen in a group fashion which displays a gangster culture, or is showing off graffiti, hand signs, tattoos and bulk cash"}
{"id": "task460-680fc1186b12450aa81be773739b4605", "input": "Using the LDA model, each person in the dataset is with a topic probability vector $X_i$ . Assume $x_{ik}\\in X_{i}$ denotes the likelihood that the $\\emph {i}^{th}$ tweet account favors $\\emph {k}^{th}$ topic in the dataset. Our topic based features can be calculated as below. Global Outlier Standard Score measures the degree that a user's tweet content is related to a certain topic compared to the other users. Local Outlier Standard Score measures the degree of interest someone shows to a certain topic by considering his own homepage content only. Three baseline classification methods: Support Vector Machines (SVM), Adaboost, and Random Forests are adopted to evaluate our extracted features. \n Question: How do they detect spammers?", "target": "Extract features from the LDA model and use them in a binary classification task"}
{"id": "task460-e71bddab6aa64d90bb726ebd45afe1fe", "input": "Using an encoder-decoder backbone, our model may be regarded as an extension of the constituent parsing model of BIBREF18 as shown in Figure FIGREF4. The difference is that in our model both constituent and dependency parsing share the same token representation and shared self-attention layers and each has its own individual Self-Attention Layers and subsequent processing layers. Our model includes four modules: token representation, self-attention encoder, constituent and dependency parsing decoder. Constituent Parsing Decoder Dependency Parsing Decoder \n Question: What are the models used to perform constituency and dependency parsing?", "target": "token representation self-attention encoder, Constituent Parsing Decoder  Dependency Parsing Decoder"}
{"id": "task460-35b3dc4219294a66b7ecaff6b092e685", "input": "Various automated evaluation approaches are proposed to facilitate the development and evaluation of NLG models. We summarize these evaluation approaches below. Text Overlap Metrics, including BLEU BIBREF5, METEOR BIBREF6 and ROUGE BIBREF7, are the most popular metrics employed in the evaluation of NLG models. Perplexity is commonly used to evaluate the quality of a language model. Parameterized Metrics learn a parameterized model to evaluate generated text. \n Question: What previous automated evalution approaches authors mention?", "target": "Text Overlap Metrics, including BLEU Perplexity Parameterized Metrics"}
{"id": "task460-2d907fd283ba4fa38f100ba4f3ae9b88", "input": "Table TABREF46 shows that our Open model achieves more than 3 points of f1-score than the state-of-the-art result, and RelAwe with DepPath&RelPath achieves the best in both Closed and Open settings. \n Question: How big is improvement over the old  state-of-the-art performance on CoNLL-2009 dataset?", "target": "our Open model achieves more than 3 points of f1-score than the state-of-the-art result"}
{"id": "task460-f15f9ce8183c4d1f8c577fd45fb1caed", "input": "WikiTableQuestions contains 22,033 questions and is an order of magnitude larger than previous state-of-the-art datasets. Its questions were not designed by predefined templates but were hand crafted by users, demonstrating high linguistic variance. \n Question: How do they gather data for the query explanation problem?", "target": "hand crafted by users"}
{"id": "task460-7f6eb54f3b074dc0b28f84987acc950c", "input": "A simple, yet recurrent text model is the well-known word co-occurrence network. After optional textual pre-processing steps, in a co-occurrence network each different word becomes a node and edges are established via co-occurrence in a desired window. A common strategy connects only adjacent words in the so called word adjacency networks.\n\nWhile the co-occurrence representation yields good results in classification scenarios, some important features are not considered in the model. For example, long-range syntactical links, though less frequent than adjacent syntactical relationships, might be disregarded from a simple word adjacency approach BIBREF12. \n Question: What is the reason that traditional co-occurrence networks fail in establishing links between similar words whenever they appear distant in the text?", "target": "long-range syntactical links, though less frequent than adjacent syntactical relationships, might be disregarded from a simple word adjacency approach"}
{"id": "task460-f680bfdf4d6a444d92800e42c13ad5d3", "input": "In this paper, we present an extraction-then-synthesis framework for machine reading comprehension shown in Figure 1 , in which the answer is synthesized from the extraction results. We build an evidence extraction model to predict the most important sub-spans from the passages as evidence, and then develop an answer synthesis model which takes the evidence as additional features along with the question and passage to further elaborate the final answers. \n Question: Which framework they propose in this paper?", "target": " extraction-then-synthesis framework"}
{"id": "task460-b4f65a0738b74391a2a9f2c528e8f80d", "input": "The dataset consists of a set of polysemous words: 20 nouns, 20 verbs, and 10 adjectives and specifies 20 to 100 contexts per word, with the total of 4,664 contexts, drawn from the Open American National Corpus. Given a set of contexts of a polysemous word, the participants of the competition had to divide them into clusters by sense of the word. The contexts are manually labelled with WordNet senses of the target words, the gold standard clustering is generated from this labelling.\n\nThe task allows two setups: graded WSI where participants can submit multiple senses per word and provide the probability of each sense in a particular context, and non-graded WSI where a model determines a single sense for a word in context. In our experiments we performed non-graded WSI. We considered the most suitable sense as the one with the highest cosine similarity with embeddings of the context, as described in Section SECREF9. \n Question: How are the different senses annotated/labeled? ", "target": "The contexts are manually labelled with WordNet senses of the target words"}
{"id": "task460-5df9bd3220f44526bbf7a23d55e3ae5d", "input": "We observe interesting hidden correlation in data. Fig. FIGREF24 has Topic 2 as selected topic. Topic 2 contains top-4 co-occurring keywords \"vegan\", \"yoga\", \"job\", \"every_woman\" having the highest term frequency. We can infer different things from the topic that \"women usually practice yoga more than men\", \"women teach yoga and take it as a job\", \"Yogi follow vegan diet\". We would say there are noticeable correlation in data i.e. `Yoga-Veganism', `Women-Yoga'. Women-Yoga \n Question: What other interesting correlations are observed?", "target": "Women-Yoga"}
{"id": "task460-3ae3a95379c24708aa2437146220e550", "input": "The results in Table TABREF38 confirm the results of BIBREF13 and suggest that we successfully replicated a large proportion of their features. The results for all three prediction settings (one outgoing edge: INLINEFORM0 , support/attack: INLINEFORM1 and support/attack/neither: INLINEFORM2 ) across all type variables ( INLINEFORM3 , INLINEFORM4 and INLINEFORM5 ) are displayed in Table TABREF39 . All models significantly outperform the majority baseline with respect to macro F1. \n Question: What baseline and classification systems are used in experiments?", "target": "BIBREF13 majority baseline"}
{"id": "task460-22db3c942eb049dab81b940714a18db1", "input": "We pre-trained our model on the Dutch section of the OSCAR corpus, a large multilingual corpus which was obtained by language classification in the Common Crawl corpus BIBREF16. \n Question: What data did they use?", "target": "the Dutch section of the OSCAR corpus"}
{"id": "task460-10017b5da7404786aa82917459fc5041", "input": "We can see that how 1.10intelligence is used varies by field: in computer science the most similar words include 1.10artificial and 1.10ai; in finance, similar words include 1.10abilities and 1.10consciousness. \n Question: Which words are used differently across ArXiv?", "target": "intelligence"}
{"id": "task460-afe5119bccbb4dac95bbaeea95789a78", "input": "In case of polysemous words, only the first word sense (usually the most common) is taken into account. \n Question: How do they handle polysemous words in their entity library?", "target": "only the first word sense (usually the most common) is taken into account"}
{"id": "task460-a9f76a831ae94f60bf6418309e7cc9ac", "input": "To train our model, we generated a dataset of 20,000 demonstrated 7 DOF trajectories (6 robot joints and 1 gripper dimension) in our simulated environment together with a sentence generator capable of creating natural task descriptions for each scenario. In order to create the language generator, we conducted an human-subject study to collect sentence templates of a placement task as well as common words and synonyms for each of the used features. By utilising these data, we are able to generate over 180,000 unique sentences, depending on the generated scenario. To test our model, we generated 500 new scenario testing each of the three features to identify the correct target among other bowls.  \n Question: Does proposed end-to-end approach learn in reinforcement or supervised learning manner?", "target": "supervised learning"}
{"id": "task460-10b16a19659349b692959fe8b1fe5736", "input": "Bag-of-words feature vectors were used to train a multinomial logistic regression model. Let INLINEFORM0 be the true label, where INLINEFORM1 is the total number of labels and INLINEFORM2 is the concatenation of the weight vectors INLINEFORM3 associated with the INLINEFORM4 th party then DISPLAYFORM0 \n Question: What model are the text features used in to provide predictions?", "target": " multinomial logistic regression"}
{"id": "task460-59d141a9fc97410eb832f40e135e5808", "input": "Building Extractive CNN/Daily Mail \n Question: What is the problem with existing metrics that they are trying to address?", "target": "Answer with content missing: (whole introduction) However, recent\nstudies observe the limits of ROUGE and find in\nsome cases, it fails to reach consensus with human.\njudgment (Paulus et al., 2017; Schluter, 2017)."}
{"id": "task460-f1e7e9fc6ea54b1f9833d5ff59e40e95", "input": "Datasets\nFor training English-Hindi NMT systems, we use the IITB English-Hindi parallel corpus BIBREF22 ( INLINEFORM0 sentences from the training set) and the ILCI English-Hindi parallel corpus ( INLINEFORM1 sentences). The ILCI (Indian Language Corpora Initiative) multilingual parallel corpus BIBREF23 spans multiple Indian languages from the health and tourism domains. We use the 520-sentence dev-set of the IITB parallel corpus for validation. For each child task, we use INLINEFORM2 sentences from ILCI corpus as the test set. \n Question: Which dataset(s) do they experiment with?", "target": "IITB English-Hindi parallel corpus BIBREF22 ILCI English-Hindi parallel corpus"}
{"id": "task460-9d39b3ca94d746378887b6a28740909b", "input": "The first type of classifier we evaluate in this work are feedforward neural networks (DNNs) consisting of 3 hidden layers, each with 512 rectified linear units (ReLUs) with a softmax activation function.  As a second classifier to evaluate, we use convolutional neural networks (CNNs) with 2 convolutional and max pooling layers, followed by 2 fully-connected ReLU layers with 512 nodes.  \n Question: What model do they use to classify phonetic segments? ", "target": "feedforward neural networks convolutional neural networks"}
{"id": "task460-1876e1ea7d6a43209918ab05d5bb14b2", "input": "Without ELMo (the same setting as 4-th row in Table 3 ), our data settings is the same as qin-EtAl:2017:Long whose performance was state-of-the-art and will be compared directly. We see that even without the pre-trained ELMo encoder, our performance is better, which is mostly attributed to our better sentence pair representations. \n Question: Why does their model do better than prior models?", "target": "better sentence pair representations"}
{"id": "task460-63aa348857384876961e5db613e36298", "input": "To assess the predictive capability of this and other models, we require some method by which we can compare the models. For that purpose, we use receiver operating characteristic (ROC) curves as a visual representation of predictive effectiveness. ROC curves compare the true positive rate (TPR) and false positive rate (FPR) of a model's predictions at different threshold levels. The area under the curve (AUC) (between 0 and 1) is a numerical measure, where the higher the AUC is, the better the model performs. We cross-validate our model by first randomly splitting the corpus into a training set (95% of the corpus) and test set (5% of the corpus). We then fit the model to the training set, and use it to predict the response of the documents in the test set. We repeat this process 100 times. The threshold-averaged ROC curve BIBREF13 is found from these predictions, and shown in Figure 3 . Table 1 shows the AUC for each model considered. \n Question: How is performance measured?", "target": "they use ROC curves and cross-validation"}
{"id": "task460-15f0ce89a8664b2a9d5613e465664c0c", "input": "In this work, we introduce IndoSum, a new benchmark dataset for Indonesian text summarization, and evaluated several well-known extractive single-document summarization methods on the dataset. The dataset consists of online news articles and has almost 200 times more documents than the next largest one of the same domain BIBREF2 We used a dataset provided by Shortir, an Indonesian news aggregator and summarizer company. The dataset contains roughly 20K news articles.  \n Question: What is the size of the dataset?", "target": "20K"}
{"id": "task460-f6f56436ed274925b70d84ecc26310eb", "input": "Although such an approach has been used in different studies during feature engineering, the selection of word vectors and the number of clusters remain a trial-end-error procedure.  \n Question: Which other hyperparameters, other than number of clusters are typically evaluated in this type of research?", "target": "selection of word vectors"}
{"id": "task460-22b91a5c03f3486cbfd22400723ede67", "input": "As Garimella et al. BIBREF23 have made their code public , we reproduced their best method Randomwalk on our datasets and measured the AUC ROC, obtaining a score of 0.935. An interesting finding was that their method had a poor performance over their own datasets. This was due to the fact (already explained in Section SECREF4) that it was not possible to retrieve the complete discussions, moreover, in no case could we restore more than 50% of the tweets. So we decided to remove these discussions and measure again the AUC ROC of this method, obtaining a 0.99 value. Our hypothesis is that the performance of that method was seriously hurt by the incompleteness of the data. We also tested our method on these datasets, obtaining a 0.99 AUC ROC with Walktrap and 0.989 with Louvain clustering. \n Question: What are the state of the art measures?", "target": "Randomwalk Walktrap Louvain clustering"}
{"id": "task460-476774172ce84bfea11090ed2e6d1cd7", "input": "Only the information concerned with the dictionary definitions are used there, discarding the polarity scores. However, when we utilise the supervised score (+1 or -1), words of opposite polarities (e.g. \u201chappy\" and \u201cunhappy\") get far away from each other as they are translated across coordinate regions. \n Question: How are the supervised scores of the words calculated?", "target": "(+1 or -1), words of opposite polarities (e.g. \u201chappy\" and \u201cunhappy\") get far away from each other"}
{"id": "task460-29f8f0dddc024b24b6b0d8d5acd8db80", "input": "In our experiments, we used TF.IDF-based features over the title and over the content of the article we wanted to classify. We had these features twice \u2013 once for the title and once for the the content of the article, as we wanted to have two different representations of the same article. Thus, we used a total of 1,100 TF.IDF-weighted features (800 content + 300 title), limiting the vocabulary to the top 800 and 300 words, respectively (which occurred in more than five articles). \n Question: what lexical features did they experiment with?", "target": "TF.IDF-based features"}
{"id": "task460-1f191582515a4f73866c67d58e355b4e", "input": "We conduct all our experiments on 635hrs of audio data for 7 Indian languages collected from $\\textbf {All India Radio}$ news channel.  We collected and curated around 635Hrs of audio data for 7 Indian languages, namely Kannada, Hindi, Telugu, Malayalam, Bengali, and English. We collected the data from the All India Radio news channel where an actor will be reading news for about 5-10 mins. To cover many speakers for the dataset, we crawled data from 2010 to 2019. Since the audio is very long to train any deep neural network directly, we segment the audio clips into smaller chunks using Voice activity detector. Since the audio clips will have music embedded during the news, we use Inhouse music detection model to remove the music segments from the dataset to make the dataset clean and our dataset contains 635Hrs of clean audio which is divided into 520Hrs of training data containing 165K utterances and 115Hrs of testing data containing 35K utterances.  \n Question: How was the audio data gathered?", "target": "Through the All India Radio new channel where actors read news."}
{"id": "task460-634df917c96645b2bef967f76762e010", "input": "The Style baseline achieves an Accuracy of 83%, LDA features 89% and a combination of the two gets 90%. However, training on full poems reduces this to 42\u201452%. \n Question: Is the outcome of the LDA analysis evaluated in any way?", "target": "Yes"}
{"id": "task460-0db117908def4066b25ef069c63c9f69", "input": "Human Judgments\nFollowing BIBREF11 , BIBREF12 and the vast amount of previous work on semantic similarity, we ask nine undergraduate subjects to assess the similarity of 360 pairs of relations from a subset of Wikidata BIBREF8 that are chosen to cover from high to low levels of similarity. In our experiment, subjects were asked to rate an integer similarity score from 0 (no similarity) to 4 (perfectly the same) for each pair.  \n Question: How do they gather human judgements for similarity between relations?", "target": "By assessing similarity of 360 pairs of relations from a subset of Wikidata using an integer similarity score from 0 to 4"}
{"id": "task460-fbf7cb8aebab483eb63df65c5974b543", "input": "Our method is purely text-based, and ignores the publication date and the source of the article. It combines task-specific embeddings, produced by a two-level attention-based deep neural network model, with manually crafted features (stylometric, lexical, grammatical, and semantic), into a kernel-based SVM classifier. \n Question: what types of features were used?", "target": "stylometric, lexical, grammatical, and semantic"}
{"id": "task460-a71f7c650d8d48c9942a75da2cc1ba52", "input": "Especially for Davidson-dataset, some tweets with specific language (written within the African American Vernacular English) and geographic restriction (United States of America) are oversampled such as tweets containing disparage words \u201cnigga\", \u201cfaggot\", \u201ccoon\", or \u201cqueer\", result in high rates of misclassification. However, these misclassifications do not confirm the low performance of our classifier because annotators tended to annotate many samples containing disrespectful words as hate or offensive without any presumption about the social context of tweeters such as the speaker\u2019s identity or dialect, whereas they were just offensive or even neither tweets.  \n Question: What biases does their model capture?", "target": "Data annotation biases where tweet containing disrespectful words are annotated as hate or offensive without any presumption about the social context of tweeters"}
{"id": "task460-9c9d858cf456471b8fcff8978e38d439", "input": "We downloaded 76 videos from a tutorial website about an image editing program .  \n Question: What is the source of the triples?", "target": "a tutorial website about an image editing program "}
{"id": "task460-7a2f6be715b9485e927d563d6112c186", "input": "FastTextBIBREF4: It uses bag of words and bag of n-grams as features for text classification, capturing partial information about the local word order efficiently. BiLSTM: Unlike feedforward neural networks, recurrent neural networks like BiLSTMs use memory based on history information to learn long-distance features and then predict the output. We use a two-layer BiLSTM architecture with GloVe word embeddings as a strong RNN baseline. BERT BIBREF5: It is a contextualized word representation model that uses bidirectional transformers, pretrained on a large $3.3B$ word corpus. We use the $BERT_{large}$ model finetuned on the training dataset. \n Question: What is the baseline for the experiments?", "target": "FastText BiLSTM BERT"}
{"id": "task460-a69e705a58394ca38bdd91b426f3919f", "input": "It has been shown that one can significantly increase the semantic information carried by a NER system when we successfully linking entities from a deep learning method to the related entities from a knowledge base BIBREF26 , BIBREF27 . Redirection: For the Wikidata linking element, we recognize that the lookup will be constrained by the most common lookup name for each entity.  \n Question: How do they combine a deep learning model with a knowledge base?", "target": "Entities from a deep learning model are linked to the related entities from a knowledge base by a lookup."}
{"id": "task460-ba6e97ccbe3b4988a3545ed16ee875c9", "input": "Each classifier is implemented with the following specifications:\n\nNa\u00efve Bayes (NB): Multinomial NB with additive smoothing constant 1\n\nLogistic Regression (LR): Linear LR with L2 regularization constant 1 and limited-memory BFGS optimization\n\nSupport Vector Machine (SVM): Linear SVM with L2 regularization constant 1 and logistic loss function\n\nRandom Forests (RF): Averaging probabilistic predictions of 10 randomized decision trees\n\nGradient Boosted Trees (GBT): Tree boosting with learning rate 1 and logistic loss function Along with traditional machine learning approaches, we investigate neural network based models to evaluate their efficacy within a larger dataset. In particular, we explore Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), and their variant models. \n Question: What learning models are used on the dataset?", "target": "Na\u00efve Bayes (NB) Logistic Regression (LR) Support Vector Machine (SVM) Random Forests (RF) Gradient Boosted Trees (GBT)  Convolutional Neural Networks (CNN) Recurrent Neural Networks (RNN)"}
{"id": "task460-0749199a07214ae2848487fc237776b0", "input": "We evaluate our proposed model on a Twitter dataset obtained from the authors of BIBREF12. Our final dataset consists of 11,576 users (i.e, fact-checkers), 4,732 fact-checking URLs and 63,429 interactions. The dataset also contains each user's social network information. Note that each user's social relationship is restricted within available users in the dataset. \n Question: What dataset is used?", "target": "Twitter dataset obtained from the authors of BIBREF12"}
{"id": "task460-20b91d704bc34238b23dcf8c630da859", "input": "We compare our HR-VAE model with three strong baselines using VAE for text modelling:\n\nVAE-LSTM-base: A variational autoencoder model which uses LSTM for both encoder and decoder. KL annealing is used to tackled the latent variable collapse issue BIBREF0;\n\nVAE-CNN: A variational autoencoder model with a LSTM encoder and a dilated CNN decoder BIBREF7;\n\nvMF-VAE: A variational autoencoder model using LSTM for both encoder and decoder where the prior distribution is the von Mises-Fisher (vMF) distribution rather than a Gaussian distribution BIBREF5. \n Question: Do they compare against state of the art text generation?", "target": "Yes"}
{"id": "task460-7619b0068eed43e59f6a119a6bc0a1aa", "input": "We used dataset, HEOT obtained from one of the past studies done by Mathur et al. where they annotated a set of cleaned tweets obtained from twitter for the conversations happening in Indian subcontinent. A labelled dataset for a corresponding english tweets were also obtained from a study conducted by Davidson et al. \n Question: What dataset is used?", "target": "HEOT  A labelled dataset for a corresponding english tweets "}
{"id": "task460-074cc7654af342a2821c0202900fa169", "input": "Our experiments on a goal-oriented dialog corpus, the personalized bAbI dialog dataset, show that leveraging personal information can significantly improve the performance of dialog systems.  \n Question: What datasets did they use?", "target": "the personalized bAbI dialog dataset"}
{"id": "task460-d831836c4cab46c6962cac2162d286a0", "input": "At the same time, some information about inflected word forms in the context can be useful, but it is lost during lemmatization, and this leads to the decreased score. Arguably, this means that lemmatization brings along both advantages and disadvantages for WSD with ELMo.  \n Question: Do the authors mention any downside of lemmatizing input before training ELMo?", "target": "Yes"}
{"id": "task460-3aba94a5e095461f8c61cc4d2144edb3", "input": "The evaluation results are quite favorable for both targets and particularly higher for Target-1, considering the fact that they are the initial experiments on the data set. \n Question: Which SVM approach resulted in the best performance?", "target": "Target-1"}
{"id": "task460-4119c5b6feb546fe9b10225472c32fe5", "input": "The Swissmetro dataset consists of survey data collected on the trains between St. Gallen and Geneva, Switzerland, during March 1998.  \n Question: What datasets are used for evaluation?", "target": "Swissmetro dataset"}
{"id": "task460-4d6bc8b1db6c4c90b9a51421b67275de", "input": "We use the Yelp Challenge dataset BIBREF2 for our fake review generation.  \n Question: Which dataset do they use a starting point in generating fake reviews?", "target": "the Yelp Challenge dataset"}
{"id": "task460-629a98541ed7488f8c60a12f15fb8004", "input": "Our goal was to generate questions without templates and with minimal human involvement using machine learning transformers that have been demonstrated to train faster and better than RNNs. Such a system would benefit educators by saving time to generate quizzes and tests. \n Question: What is the motivation behind the work? Why question generation is an important task?", "target": "Such a system would benefit educators by saving time to generate quizzes and tests."}
{"id": "task460-6123cf29bd324b9a98ace18de8d535a6", "input": "According to the categories mentioned in BIBREF5 , irony can be classified into 3 classes: verbal irony by means of a polarity contrast, the sentences containing expression whose polarity is inverted between the intended and the literal evaluation; other types of verbal irony, the sentences that show no polarity contrast between the literal and intended meaning but are still ironic; and situational irony, the sentences that describe situations that fail to meet some expectations. As ironies in the latter two categories are obscure and hard to understand, we decide to only focus on ironies in the first category in this work.   Because of the lack of previous work and baselines on irony generation, we implement our model based on style transfer.  \n Question: What are the difficulties in modelling the ironic pattern?", "target": "obscure and hard to understand  lack of previous work and baselines on irony generation"}
{"id": "task460-3dc8c7aec0774e2ca6c2825581ebbf2d", "input": "Notably, automatic translations of TED talks contain more words than the corresponding reference translation, which means that machine-translated texts of this type have also more potential tokens to enter in a coreference relation, and potentially indicating a shining through effect. The same does not happen with the news test set. We see how NMT translations increase the number of mentions about $30\\%$ with respect to human references showing even a more marked explicitation effect than human translations do. \n Question: What translationese effects are seen in the analysis?", "target": "potentially indicating a shining through effect explicitation effect"}
{"id": "task460-8d918fb2b8304582b85f25d83f438a0c", "input": "Each game's video ranges from 30 to 50 minutes in length which contains image and chat data linked to the specific timestamp of the game. \n Question: What is the average length of the recordings?", "target": "40 minutes"}
{"id": "task460-594d2737a4d44d9aac2def8664f463e4", "input": "As a first experiment, we compare the quality of fastText embeddings trained on (high-quality) curated data and (low-quality) massively extracted data for Twi and Yor\u00f9b\u00e1 languages. The huge ambiguity in the written Twi language motivates the exploration of different approaches to word embedding estimations. In this work, we compare the standard fastText methodology to include sub-word information with the character-enhanced approach with position-based clustered embeddings (CWE-LP as introduced in Section SECREF17). \n Question: What two architectures are used?", "target": "fastText CWE-LP"}
{"id": "task460-de07077716e644038b4d675c07e65444", "input": "We evaluate our model in a simulated binning task in which the robot is tasked to place a cube into a bowl as outlined by the verbal command. Each environment contains between three and five objects differentiated by their size (small, large), shape (round, square) and color (red, green, blue, yellow, pink), totalling in 20 different objects. Depending on the generated scenario, combinations of these three features are necessary to distinguish the targets from each other, allowing for tasks of varying complexity. \n Question: What simulations are performed by the authors to validate their approach?", "target": "a simulated binning task in which the robot is tasked to place a cube into a bowl as outlined by the verbal command"}
{"id": "task460-0ad51eabc5c849c2805ab50d05686476", "input": " Moreover, due to their noisy nature, they are also processed using some Twitter-specific techniques such as substitution/removal of URLs, of user mentions, of hashtags, and of emoticons, spelling correction, elongation normalization, abbreviation lookup, punctuation removal, detection of amplifiers and diminishers, negation scope detection, etc. That language proved to be quite challenging with its use of creative spelling and punctuation, misspellings, slang, new words, URLs, and genre-specific terminology and abbreviations, e.g., RT for re-tweet and #hashtags. In addition to the genre difference, there is also a difference in length: social media messages are generally short, often length-limited by design as in Twitter, i.e., a sentence or a headline rather than a full document \n Question: What difficulties does sentiment analysis on Twitter have, compared to sentiment analysis in other domains?", "target": "Tweets noisy nature, use of creative spelling and punctuation, misspellings, slang, new words, URLs, and genre-specific terminology and abbreviations, short (length limited) text"}
{"id": "task460-f670a51ebfe04506aacdbc4b91ceb9d5", "input": "Five attributes, specifying certain details of clinical significance, are defined to characterize the answer types of INLINEFORM4 : (1) the time the patient has been experiencing the symptom, (2) activities that trigger the symptom (to occur or worsen), (3) the extent of seriousness, (4) the frequency occurrence of the symptom, and (5) the location of symptom. For each symptom/attribute, it can take on different linguistic expressions, defined as entities. Note that if the queried symptom or attribute is not mentioned in the dialogue, the groundtruth output is \u201cNo Answer\u201d, as in BIBREF6 . \n Question: What labels do they create on their dataset?", "target": "(1) the time the patient has been experiencing the symptom, (2) activities that trigger the symptom (to occur or worsen), (3) the extent of seriousness, (4) the frequency occurrence of the symptom, and (5) the location of symptom No Answer"}
{"id": "task460-cfae5422681648ec98afab8c810c45c8", "input": "The total F-1 score on the OntoNotes dataset is 88%, and the total F-1 cross-validation score on the 112 class Wiki(gold) dataset is 53%.  \n Question: What results do they achieve using their proposed approach?", "target": "F-1 score on the OntoNotes is 88%, and it is 53% on Wiki (gold)."}
{"id": "task460-047bf5f3518a466e812a82941128ee77", "input": "Our current feature set for training a machine learning classifier is relatively small, consisting of paragraph domain scores, bag-of-words, length of stay, and number of previous admissions, but we intend to factor in many additional features that extend beyond the scope of the present study. These include a deeper analysis of clinical narratives in EHRs: our next task will be to extend our EHR data pipeline by distinguishing between clinically positive and negative phenomena within each risk factor domain. This will involve a series of annotation tasks that will allow us to generate lexicon-based and corpus-based sentiment analysis tools. We can then use these clinical sentiment scores to generate a gradient of patient improvement or deterioration over time. \n Question: What additional features are proposed for future work?", "target": "distinguishing between clinically positive and negative phenomena within each risk factor domain and accounting for structured data collected on the target cohort"}
{"id": "task460-6f38425be1ff43cc93dd04f8433cc508", "input": "Here we describe the components of our probabilistic model of question generation.  The details of optimization are as follows. First, a large set of 150,000 questions is sampled in order to approximate the gradient at each step via importance sampling. Second, to run the procedure for a given model and training set, we ran 100,000 iterations of gradient ascent at a learning rate of 0.1. \n Question: Is it a neural model? How is it trained?", "target": "No, it is a probabilistic model trained by finding feature weights through gradient ascent"}
{"id": "task460-21554b5929844d9aab9749a9cb10ba85", "input": "Moreover, because English does not mark grammatical gender, approaches developed for English are not transferable to morphologically rich languages that exhibit gender agreement BIBREF8 . \n Question: Why does not the approach from English work on other languages?", "target": "Because, unlike other languages, English does not mark grammatical genders"}
{"id": "task460-c6f43f12538644d28e7945d60967a9a0", "input": "BioASQ organizers provide the training and testing data. The training data consists of questions, gold standard documents, snippets, concepts, and ideal answers (which we did not use in this paper, but we used last year BIBREF2). The test data is split between phases A and B. The Phase A dataset consists of the questions, unique ids, question types. The Phase B dataset consists of the questions, golden standard documents, snippets, unique ids and question types. \n Question: What dataset did they use?", "target": "BioASQ  dataset"}
{"id": "task460-e6af7b6c971f4be1aea44556d54efedf", "input": "Therefore, in order to create fair systems it is necessary to take into account the representation problems in society that are going to be encapsulated in the data. \n Question: What is the goal of investigating NLP gender bias specifically in the news broadcast domain and Anchor role?", "target": "create fair systems"}
{"id": "task460-194e3fa34e87418a8099bd40174826e7", "input": "Our user study compares the correctness of three scenarios:\n\nParser correctness - our baseline is the percentage of examples where the top query returned by the semantic parser was correct.\n\nUser correctness - the percentage of examples where the user selected a correct query from the top-7 generated by the parser.\n\nHybrid correctness - correctness of queries returned by a combination of the previous two scenarios. The system returns the query marked by the user as correct; if the user marks all queries as incorrect it will return the parser's top candidate. \n Question: Do they conduct a user study where they show an NL interface with and without their explanation?", "target": "No"}
{"id": "task460-d0e82415765b4a49a24a3465a11f1ee8", "input": "The annotator carried out all annotation. \n Question: How many annotators tagged each tweet?", "target": "One"}
{"id": "task460-6830b99d79b541fe810f954c5de917f2", "input": "Machine translation finds use in cheminformatics in \u201ctranslation\" from one language (e.g. reactants) to another (e.g. products). The variational Auto-encoder (VAE) is another widely adopted text generation architecture BIBREF101. Generative Adversarial Network (GAN) models generate novel molecules by using two components: the generator network generates novel molecules, and the discriminator network aims to distinguish between the generated molecules and real molecules BIBREF107. \n Question: Are this models usually semi/supervised or unsupervised?", "target": "Both supervised and unsupervised, depending on the task that needs to be solved."}
{"id": "task460-92e61a9221f1453c818fe42a04a8bda3", "input": "We build and test our MMT models on the Multi30K dataset BIBREF21 . Each image in Multi30K contains one English (EN) description taken from Flickr30K BIBREF22 and human translations into German (DE), French (FR) and Czech BIBREF23 , BIBREF24 , BIBREF25 . The dataset contains 29,000 instances for training, 1,014 for development, and 1,000 for test. We only experiment with German and French, which are languages for which we have in-house expertise for the type of analysis we present. In addition to the official Multi30K test set (test 2016), we also use the test set from the latest WMT evaluation competition, test 2018 BIBREF25 . \n Question: Do they report results only on English dataset?", "target": "No"}
{"id": "task460-2ba65ce11fc24a219a2eee093361edf5", "input": "Experimental Studies ::: Comparison with State-of-the-art Methods\nSince BERT has already achieved the state-of-the-art performance of question-answering, in this section we compare our proposed model with state-of-the-art question answering models (i.e. QANet BIBREF39) and BERT-Base BIBREF26. As BERT has two versions: BERT-Base and BERT-Large, due to the lack of computational resource, we can only compare with BERT-Base model instead of BERT-Large. \n Question: What baselines is the proposed model compared against?", "target": "BERT-Base QANet"}
{"id": "task460-c2d52ff7398546fe8d993d56ed7f2c12", "input": "The top performing approaches Caravel, COAV and NNCD deserve closer attention. \n Question: Which is the best performing method?", "target": "Caravel, COAV and NNCD"}
{"id": "task460-141486a18004455580c69ba039b1eb2b", "input": "The overall Total Accuracy score reported in table TABREF19 using the entire feature set is 549.  \n Question: Do they experiment with the dataset?", "target": "Yes"}
{"id": "task460-129f535a44a146cdaa6ff710874727d4", "input": "The end-to-end system (prefix E2E) uses the DNN topology depicted in Figure FIGREF8 . We present results with 3 distinct size configurations (infixes 700K, 318K, and 40K) each representing the number of approximate parameters, and 2 types of training recipes (suffixes 1stage and 2stage) corresponding to end-to-end and encoder+decoder respectively, as described in UID7 . \n Question: How many parameters does the presented model have?", "target": "(infixes 700K, 318K, and 40K) each representing the number of approximate parameters"}
{"id": "task460-ed952729d813414ca062b3ff54b0548f", "input": "We are using the dataset of the competition, which includes text from tweets having the aforementioned categories.  \n Question: What dataset is used for this work?", "target": "Twitter dataset provided by the organizers"}
{"id": "task460-7f30be8dd8e247089a6febff22693a2b", "input": "Using our annotation software, we automatically extracted landmarks of 2000 images from the UOttawa database BIBREF14 had been annotated for image segmentation tasks.  \n Question: How big are datasets used in experiments?", "target": "2000 images"}
{"id": "task460-6890274b649d4f708983674a3ad3d949", "input": "We obtained 9,892 stories of sexual harassment incidents that was reported on Safecity. Those stories include a text description, along with tags of the forms of harassment, e.g. commenting, ogling and groping. A dataset of these stories was published by Karlekar and Bansal karlekar2018safecity. In addition to the forms of harassment, we manually annotated each story with the key elements (i.e. \u201charasser\", \u201ctime\", \u201clocation\", \u201ctrigger\"), because they are essential to uncover the harassment patterns. An example is shown in Figure FIGREF3. Furthermore, we also assigned each story classification labels in five dimensions (Table TABREF4). The detailed definitions of classifications in all dimensions are explained below. \n Question: What is the size of the dataset?", "target": " 9,892 stories of sexual harassment incidents"}
{"id": "task460-43b0a107bf4740d8806b7ec8458dc3c7", "input": "As shown in Table TABREF3 , we collect Amazon review keywords for 2,896 e-books (publishers: Kiwi, Rowohlt, Fischer, and Droemer), which leads to 33,663 distinct review keywords and on average 30 keyword assignments per e-book. \n Question: how large is the vocabulary?", "target": "33,663"}
{"id": "task460-842ac5bbadcd436dab785f2dbced3cdd", "input": "Our training data consists of 2.09M sentence pairs extracted from LDC corpus. To test different approaches on Chinese-to-English translation task, we use NIST 2003(MT03) dataset as the validation set, and NIST 2004(MT04), NIST 2005(MT05), NIST 2006(MT06) datasets as our test sets. For English-to-Chinese translation task, we also use NIST 2003(MT03) dataset as the validation set, and NIST 2008(MT08) will be used as test set. \n Question: What dataset did they use?", "target": "LDC corpus NIST 2003(MT03) NIST 2004(MT04), NIST 2005(MT05), NIST 2006(MT06) NIST 2008(MT08)"}
{"id": "task460-e01dc32addad4b9686e19cd23b290d7c", "input": "Word2vec representation is far better, advanced and a recent technique which functions by mapping words to a 300 dimensional vector representations. \n Question: What is the dimension of the embeddings?", "target": "300"}
{"id": "task460-b7ebaeffc6bf4556a26222f4a2b5301d", "input": "We use the Universal Dependencies' Hindi-English codemixed data set BIBREF9 to test the model's ability to label code-mixed data. This dataset is based on code-switching tweets of Hindi and English multilingual speakers. We use the Devanagari script provided by the data set as input tokens. \n Question: What codemixed language pairs are evaluated?", "target": "Hindi-English"}
{"id": "task460-aa5ddb45d68641c69ab8082ca63bad16", "input": "In this experiment set, our BERT implementation is compared to several systems that participated in the MEDDOCAN challenge: a CRF classifier BIBREF18, a spaCy entity recogniser BIBREF18, and NLNDE BIBREF12, the winner of the shared task and current state of the art for sensitive information detection and classification in Spanish clinical text.  However, attending to the obtained results, BERT remains only 0.3 F1-score points behind, and would have achieved the second position among all the MEDDOCAN shared task competitors. Taking into account that only 3% of the gold labels remain incorrectly annotated, the task can be considered almost solved, and it is not clear if the differences among the systems are actually significant, or whether they stem from minor variations in initialisation or a long-tail of minor labelling inconsistencies. \n Question: Does BERT reach the best performance among all the algorithms compared?", "target": "No"}
{"id": "task460-22b1da5f49974c03abc87137fae910fc", "input": "The log-likelihood of a Plackett-Luce model is not a strict upper bound of the BLEU score, however, it correlates with BLEU well in the case of rich features. The concept of \u201crich\u201d is actually qualitative, and obscure to define in different applications. We empirically provide a formula to measure the richness in the scenario of machine translation. The greater, the richer. In practice, we find a rough threshold of r is 5 \n Question: How they measure robustness in experiments?", "target": "We empirically provide a formula to measure the richness in the scenario of machine translation."}
{"id": "task460-6e2b557c2de547cfa4b24df2e2b13e1a", "input": "Accordingly, marcheggiani2017 presented a neural model putting syntax aside for dependency-based SRL and obtain favorable results, which overturns the inherent belief that syntax is indispensable in SRL task BIBREF11 . \n Question: Are there syntax-agnostic SRL models before?", "target": "Yes"}
{"id": "task460-e5367e2f4be84f5e8a6e9f023621c63c", "input": "The dataset is composed of 90 annotated documents with 5 citations each ranked 1 to 5, where 1 is least relevant and 5 is most relevant for a total of 450 annotated citations. \n Question: what is the size of this built corpus?", "target": "90 annotated documents with 5 citations each ranked 1 to 5, where 1 is least relevant and 5 is most relevant for a total of 450 annotated citations"}
{"id": "task460-cd3228cda29e47548ebeb71762722520", "input": " In particular, the best-performing systems at SemEval-2015 and SemEval-2016 used deep convolutional networks BIBREF53 , BIBREF54  \n Question: What is the current SOTA for sentiment analysis on Twitter at the time of writing?", "target": "deep convolutional networks BIBREF53 , BIBREF54"}
{"id": "task460-c68496699c75446a86284c6fac6c7152", "input": "To demonstrate SPNet's effectiveness, we compare it with two state-of-the-art methods, Pointer-Generator BIBREF5 and Transformer BIBREF6. \n Question: What are previous state-of-the-art document summarization methods used?", "target": "Pointer-Generator Transformer"}
{"id": "task460-7fe309f2671d4836a7af5f349396fe01", "input": "We use the unshuffled version of the French OSCAR corpus, which amounts to 138GB of uncompressed text and 32.7B SentencePiece tokens. \n Question: What data is used for training CamemBERT?", "target": "unshuffled version of the French OSCAR corpus"}
{"id": "task460-41cae39abcce4f18a43666cee5a4a929", "input": "In order to evaluate the models trained on generated data, we manually annotated a named entities dataset comprising 53453 tokens and 2566 sentences selected from over 250 news texts from ilur.am. During annotation, we generally relied on categories and guidelines assembled by BBN Technologies for TREC 2002 question answering track. \n Question: did they use a crowdsourcing platform for manual annotations?", "target": "No"}
{"id": "task460-af4871b60c4b4ee5803fa580d441f09d", "input": "We evalute Bertram on the WNLaMPro dataset of BIBREF0. \n Question: What is dataset for word probing task?", "target": "WNLaMPro dataset"}
{"id": "task460-c10b922fe3b049b3aad4d7ef71c20fbb", "input": "We extract data from the WMT'14 English-French (En-Fr) and English-German (En-De) datasets. To create a larger discrepancy between the tasks, so that there is a clear dataset size imbalance, the En-De data is artificially restricted to only 1 million parallel sentences, while the full En-Fr dataset, comprising almost 40 million parallel sentences, is used entirely.  \n Question: What datasets are used for experiments?", "target": "the WMT'14 English-French (En-Fr) and English-German (En-De) datasets."}
{"id": "task460-759e1af75097488cbacafb2f37a02e15", "input": " To remain consistent with experiments performed with LSTM's we use the morfessor for the subword tokenization in the Finnish Language. \n Question: Is the LSTM baseline a sub-word model?", "target": "Yes"}
