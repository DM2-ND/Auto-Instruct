{"id": "task1540-c9643e4c45b449ac847b8dc399bae933", "input": "Sentiment prediction of contemporary music can have a wide-range of applications in modern society, for instance, selecting music for public institutions such as hospitals or restaurants to potentially improve the emotional well-being of personnel, patients, and customers, respectively. In this project, music recommendation system built upon on a naive Bayes classifier, trained to predict the sentiment of songs based on song lyrics alone. The experimental results show that music corresponding to a happy mood can be detected with high precision based on text features obtained from song lyrics.", "target": "MusicMood: Predicting the mood of music from song lyrics using machine learning"}
{"id": "task1540-9b2db386825e479aa263566f26bf762c", "input": "The fastest known exact algorithms for scorebased structure discovery in Bayesian networks on n nodes run in time and space 2n. The usage of these algorithms is limited to networks on at most around 25 nodes mainly due to the space requirement. Here, we study space\u2013time tradeoffs for finding an optimal network structure. When little space is available, we apply the Gurevich\u2013 Shelah recurrence\u2014originally proposed for the Hamiltonian path problem\u2014and obtain time 2n in space 2n for any s = n/2, n/4, n/8, . . .; we assume the indegree of each node is bounded by a constant. For the more practical setting with moderate amounts of space, we present a novel scheme. It yields running time 2(3/2)n in space 2(3/4)n for any p = 0, 1, . . . , n/2; these bounds hold as long as the indegrees are at most 0.238n. Furthermore, the latter scheme allows easy and efficient parallelization beyond previous algorithms. We also explore empirically the potential of the presented techniques.", "target": "Exact Structure Discovery in Bayesian Networks with Less Space"}
{"id": "task1540-82d257b3f09c46059757522b0277400f", "input": "We investigate adversarial attacks for autoencoders. We propose a procedure that distorts the input image to mislead the autoencoder in reconstructing a completely different target image. We attack the internal latent representations, attempting to make the adversarial input produce an internal representation as similar as possible as the target\u2019s. We find that autoencoders are much more robust to the attack than classifiers: while some examples have tolerably small input distortion, and reasonable similarity to the target image, there is a quasi-linear trade-off between those aims. We report results on MNIST and SVHN datasets, and also test regular deterministic autoencoders, reaching similar conclusions in all cases. Finally, we show that the usual adversarial attack for classifiers, while being much easier, also presents a direct proportion between distortion on the input, and misdirection on the output. That proportionality however is hidden by the normalization of the output, which maps a linear layer into non-linear probabilities.", "target": "Adversarial Images for Variational Autoencoders"}
{"id": "task1540-0aeb2cf4ed674f648bbfb475fa3013cc", "input": "We explore the problem of binary classification in machine learning, with a twist the classifier is allowed to abstain on any datum, professing ignorance about the true class label without committing to any prediction. This is directly motivated by applications like medical diagnosis and fraud risk assessment, in which incorrect predictions have potentially calamitous consequences. We focus on a recent spate of theoretically driven work in this area that characterizes how allowing abstentions can lead to fewer errors in very general settings. Two areas are highlighted: the surprising possibility of zero-error learning, and the fundamental tradeoff between predicting sufficiently often and avoiding incorrect predictions. We review efficient algorithms with provable guarantees for each of these areas. We also discuss connections to other scenarios, notably active learning, as they suggest promising directions of further inquiry in this emerging field.", "target": "The Utility of Abstaining in Binary Classification"}
{"id": "task1540-d2211bc024ed4575a32c808b018e4614", "input": "We present a new theoretical framework for analyzing and learning artificial neural networks. Our approach simultaneously and adaptively learns both the structure of the network as well as its weights. The methodology is based upon and accompanied by strong data-dependent theoretical learning guarantees, so that the final network architecture provably adapts to the complexity of any given problem.", "target": "AdaNet: Adaptive Structural Learning of Artificial Neural Networks"}
{"id": "task1540-71ffe28cc07d4531b01a7d8ae3ba4cb4", "input": "In machine learning, there is a fundamental trade-off between ease of optimization and expressive power. Neural Networks, in particular, have enormous expressive power and yet are notoriously challenging to train. The nature of that optimization challenge changes over the course of learning. Traditionally in deep learning, one makes a static trade-off between the needs of early and late optimization. In this paper, we investigate a novel framework, GradNets, for dynamically adapting architectures during training to get the benefits of both. For example, we can gradually transition from linear to non-linear networks, deterministic to stochastic computation, shallow to deep architectures, or even simple downsampling to fully differentiable attention mechanisms. Benefits include increased accuracy, easier convergence with more complex architectures, solutions to test-time execution of batch normalization, and the ability to train networks of up to 200 layers.", "target": "GRADNETS: DYNAMIC INTERPOLATION BETWEEN NEURAL ARCHITECTURES"}
{"id": "task1540-2f049a0091594e0086b7d5485ee49a7b", "input": "A Dialogue System is a system which interacts with human in natural language. At present many universities are developing the dialogue system in their regional language. This paper will discuss about dialogue system, its components, challenges and its evaluation. This paper helps the researchers for getting info regarding dialogues system.", "target": "Dialogue System: A Brief Review"}
{"id": "task1540-cfb784ba4dd84834a85d64d547f55501", "input": "Online learning constitutes a mathematical and compelling framework to analyze sequential decision making problems in adversarial environments. The learner repeatedly chooses an action, the environment responds with an outcome, and then the learner receives a reward for the played action. The goal of the learner is to maximize his total reward. However, there are situations in which, in addition to maximizing the cumulative reward, there are some additional constraints on the sequence of decisions that must be satisfied on average by the learner. In this paper we study an extension to the online learning where the learner aims to maximize the total reward given that some additional constraints need to be satisfied. By leveraging on the theory of Lagrangian method in constrained optimization, we propose Lagrangian exponentially weighted average (LEWA) algorithm, which is a primal-dual variant of the well known exponentially weighted average algorithm, to efficiently solve constrained online decision making problems. Using novel theoretical analysis, we establish the regret and the violation of the constraint bounds in full information and bandit feedback models.", "target": "Efficient Constrained Regret Minimization"}
{"id": "task1540-0be266d0e7c74c33b17537a370f00a2c", "input": "Understanding entailment and contradiction is fundamental to understanding natural language, and inference about entailment and contradiction is a valuable testing ground for the development of semantic representations. However, machine learning research in this area has been dramatically limited by the lack of large-scale resources. To address this, we introduce the Stanford Natural Language Inference corpus, a new, freely available collection of labeled sentence pairs, written by humans doing a novel grounded task based on image captioning. At 570K pairs, it is two orders of magnitude larger than all other resources of its type. This increase in scale allows lexicalized classifiers to outperform some sophisticated existing entailment models, and it allows a neural network-based model to perform competitively on natural language inference benchmarks for the first time.", "target": "A large annotated corpus for learning natural language inference"}
{"id": "task1540-c690ab333133453fa074fbb1fdc1c076", "input": "A prediction market is a useful means of aggregating information about a future event. To function, the market needs a trusted entity who will verify the true outcome in the end. Motivated by the recent introduction of decentralized prediction markets, we introduce a mechanism that allows for the outcome to be determined by the votes of a group of arbiters who may themselves hold stakes in the market. Despite the potential conflict of interest, we derive conditions under which we can incentivize arbiters to vote truthfully by using funds raised from market fees to implement a peer prediction mechanism. Finally, we investigate what parameter values could be used in a real-world implementation of our mechanism.", "target": "Crowdsourced Outcome Determination in Prediction Markets"}
{"id": "task1540-1a112749a48144f590a87bb1a9019034", "input": "Distilling from a knowledge base only the part that is relevant to a subset of alphabet, which is recognized as forgetting, has attracted extensive interests in AI community. In standard propositional logic, a general algorithm of forgetting and its computation-oriented investigation in various fragments whose satisfiability are tractable are still lacking. The paper aims at filling the gap. After exploring some basic properties of forgetting in propositional logic, we present a resolution-based algorithm of forgetting for CNF fragment, and some complexity results about forgetting in Horn, renamable Horn, q-Horn, Krom, DNF and CNF fragments of propositional logic.", "target": "On Forgetting in Tractable Propositional Fragments"}
{"id": "task1540-f096ca5851c8494d8be685db25445449", "input": "1 College of Computer, National University of Defense Technology, 410073 Changsha, Hunan, CHINA. plliu@nudt.edu.cn Abstract: Protein-protein interaction extraction is the key precondition of the construction of protein knowledge network, and it is very important for the research in the biomedicine. This paper extracted directional protein-protein interaction from the biological text, using the SVM-based method. Experiments were evaluated on the LLL05 corpus with good results. The results show that dependency features are import for the protein-protein interaction extraction and features related to the interaction word are effective for the interaction direction judgment. At last, we analyzed the effects of different features and planed for the next step.", "target": "Automatic Extraction of Protein-Protein Interaction in Literature"}
{"id": "task1540-8505e94d534f40e6a607584271666583", "input": "Distantly supervised relation extraction has been widely used to find novel relational facts from plain text. To predict the relation between a pair of two target entities, existing methods solely rely on those direct sentences containing both entities. In fact, there are also many sentences containing only one of the target entities, which provide rich and useful information for relation extraction. To address this issue, we build inference chains between two target entities via intermediate entities, and propose a path-based neural relation extraction model to encode the relational semantics from both direct sentences and inference chains. Experimental results on realworld datasets show that, our model can make full use of those sentences containing only one target entity, and achieves significant and consistent improvements on relation extraction as compared with baselines.", "target": "Incorporating Relation Paths in Neural Relation Extraction"}
{"id": "task1540-da2488501a174357a159e4a2453d085f", "input": "Medical image analysis and computer-assisted intervention problems are increasingly being addressed with deep-learning-based solutions. This has resulted is substantial duplication of effort and incompatible infrastructure across many research groups. This work presents the open-source NiftyNet platform for deep learning in medical imaging. The ambition of NiftyNet is to accelerate and simplify the development of these solutions, and to provide a common mechanism for disseminating research outputs for the community to use, adapt and build upon. This TensorFlow-based infrastructure provides a complete modular deep learning pipeline for a range of medical imaging applications including segmentation, regression, image generation and representation learning applications with data loading, data augmentation, network architectures, loss functions and evaluation metrics that are tailored to, and take advantage of, the idiosyncracies of medical image analysis and computer-assisted interventions.", "target": "NiftyNet: a deep-learning platform for medical imaging"}
{"id": "task1540-3acec2391779416282a4d5ba85b3dde6", "input": "The validation of any database mining methodology goes through an evaluation process where benchmarks availability is essential. In this paper, we aim to randomly generate relational database benchmarks that allow to check probabilistic dependencies among the attributes. We are particularly interested in Probabilistic Relational Models (PRMs), which extend Bayesian Networks (BNs) to a relational data mining context and enable effective and robust reasoning over relational data. Even though a panoply of works have focused, separately, on the generation of random Bayesian networks and relational databases, no work has been identified for PRMs on that track. This paper provides an algorithmic approach for generating random PRMs from scratch to fill this gap. The proposed method allows to generate PRMs as well as synthetic relational data from a randomly generated relational schema and a random set of probabilistic dependencies. This can be of interest not only for machine learning researchers to evaluate their proposals in a common framework, but also for databases designers to evaluate the effectiveness of the components of a database management system.", "target": "Probabilistic Relational Model Benchmark Generation"}
{"id": "task1540-d510f8f7d6ec49f19ef67f769dbc568f", "input": "In this paper, we implicitly incorporate morpheme information into word embedding. Based on the strategy we utilize the morpheme information, three models are proposed. To test the performances of our models, we conduct the word similarity and syntactic analogy. The results demonstrate the effectiveness of our methods. Our models beat the comparative baselines on both tasks to a great extent. On the golden standard Wordsim-353 and RG-65, our models approximately outperform CBOW for 5 and 7 percent, respectively. In addition, 7 percent advantage is also achieved by our models on syntactic analysis. According to parameter analysis, our models can increase the semantic information in the corpus and our performances on the smallest corpus are similar to the performance of CBOW on the corpus which is five times ours. This property of our methods may have some positive effects on NLP researches about the corpus-limited languages.", "target": "Implicitly Incorporating Morphological Information into Word Embedding"}
{"id": "task1540-1bb90619b64c4516a365a36a6e4d47e3", "input": "Most existing Neural Machine Translation models use groups of characters or whole words as their unit of input and output. We propose a model with a hierarchical char2word encoder, that takes individual characters both as input and output. We first argue that this hierarchical representation of the character encoder reduces computational complexity, and show that it improves translation performance. Secondly, by qualitatively studying attention plots from the decoder we find that the model learns to compress common words into a single embedding whereas rare words, such as names and places, are represented character by character.", "target": "NEURAL MACHINE TRANSLATION WITH CHARACTERS AND HIERARCHICAL ENCODING"}
{"id": "task1540-deedf3c9342f4c3d9d5485e227c1a787", "input": "Formal languages for probabilistic modeling enable re-use, modularity, and descriptive clarity, and can foster generic inference techniques. We introduce Church, a universal language for describing stochastic generative processes. Church is based on the Lisp model of lambda calculus, containing a pure Lisp as its deterministic subset. The semantics of Church is defined in terms of evaluation histories and conditional distributions on such histories. Church also includes a novel language construct, the stochastic memoizer, which enables simple description of many complex non-parametric models. We illustrate language features through several examples, including: a generalized Bayes net in which parameters cluster over trials, infinite PCFGs, planning by inference, and various non-parametric clustering models. Finally, we show how to implement query on any Church program, exactly and approximately, using Monte Carlo techniques.", "target": "Church: a language for generative models"}
{"id": "task1540-849f075d54a1476a815744f9ac7653f3", "input": "We address a challenging fine-grain classification problem: recognizing a font style from an image of text. In this task, it is very easy to generate lots of rendered font examples but very hard to obtain real-world labeled images. This realto-synthetic domain gap caused poor generalization to new real data in previous methods (Chen et al. (2014)). In this paper, we refer to Convolutional Neural Networks, and use an adaptation technique based on a Stacked Convolutional AutoEncoder that exploits unlabeled real-world images combined with synthetic data. The proposed method achieves an accuracy of higher than 80% (top-5) on a realworld dataset.", "target": "REAL-WORLD FONT RECOGNITION USING DEEP NET-"}
{"id": "task1540-1418a5f1ac3e4b6faaa5d76d49f69908", "input": "We introduce a new interpretation of two re\u00ad lated notions conditional utility and utility independence. Unlike the traditional inter\u00ad pretation, the new interpretation render the notions the direct analogues of their prob\u00ad abilistic counterparts. To capture these no\u00ad tions formally, we appeal to the notion of util\u00ad ity distribution, introduced in previous paper. We show that utility distributions, which have a structure that is identical to that of probability distributions, can be viewed as a special case of an additive multiattribute utility functions, and show how this special case permits us to capture the novel senses of conditional utility and utility independence. Finally, we present the notion of utility net\u00ad works, which do for utilities what Bayesian networks do for probabilities. Specifically, utility networks exploit the new interpreta\u00ad tion of conditional utility and utility indepen\u00ad dence to compactly represent a utility distri\u00ad bution.", "target": "Conditional Utility, Utility Independence, and Utility Networks"}
{"id": "task1540-d7528a61f13a49ffaab0d374de6a0808", "input": "Understanding open-domain text is one of the primary challenges in natural language processing (NLP). Machine comprehension benchmarks evaluate the system\u2019s ability to understand text based on the text content only. In this work, we investigate machine comprehension on MCTest, a question answering (QA) benchmark. Prior work is mainly based on feature engineering approaches. We come up with a neural network framework, named hierarchical attention-based convolutional neural network (HABCNN), to address this task without any manually designed features. Specifically, we explore HABCNN for this task by two routes, one is through traditional joint modeling of document, question and answer, one is through textual entailment. HABCNN employs an attention mechanism to detect key phrases, key sentences and key snippets that are relevant to answering the question. Experiments show that HABCNN outperforms prior deep learning approaches by a big margin.", "target": "Attention-Based Convolutional Neural Network for Machine Comprehension"}
{"id": "task1540-42de88dd07cb453f8712e99ddb0b8281", "input": "Until recently, research on artificial neural networks was largely restricted to systems with only two types of variable: Neural activities that represent the current or recent input and weights that learn to capture regularities among inputs, outputs and payoffs. There is no good reason for this restriction. Synapses have dynamics at many different time-scales and this suggests that artificial neural networks might benefit from variables that change slower than activities but much faster than the standard weights. These \u201cfast weights\u201d can be used to store temporary memories of the recent past and they provide a neurally plausible way of implementing the type of attention to the past that has recently proved very helpful in sequence-to-sequence models. By using fast weights we can avoid the need to store copies of neural activity patterns.", "target": "Using Fast Weights to Attend to the Recent Past"}
{"id": "task1540-c5d252f16b004578beddc91829478a55", "input": "Many natural language understanding (NLU) tasks, such as shallow parsing (i.e., text chunking) and semantic slot filling, require the assignment of representative labels to the meaningful chunks in a sentence. Most of the current deep neural network (DNN) based methods consider these tasks as a sequence labeling problem, in which a word, rather than a chunk, is treated as the basic unit for labeling. These chunks are then inferred by the standard IOB (Inside-OutsideBeginning) labels. In this paper, we propose an alternative approach by investigating the use of DNN for sequence chunking, and propose three neural models so that each chunk can be treated as a complete unit for labeling. Experimental results show that the proposed neural sequence chunking models can achieve start-of-the-art performance on both the text chunking and slot filling tasks.", "target": "Neural Models for Sequence Chunking"}
{"id": "task1540-677ef07a21a54ff08f0a1a13b9b53c38", "input": "This paper studies single-image depth perception in the wild, i.e., recovering depth from a single image taken in unconstrained settings. We introduce a new dataset \u201cDepth in the Wild\u201d consisting of images in the wild annotated with relative depth between pairs of random points. We also propose a new algorithm that learns to estimate metric depth using annotations of relative depth. Compared to the state of the art, our algorithm is simpler and performs better. Experiments show that our algorithm, combined with existing RGB-D data and our new relative depth annotations, significantly improves single-image depth perception in the wild. Deep Network with Pixel-wise Prediction Metric Depth RGB-D Data Relative Depth Annotations", "target": "Single-Image Depth Perception in the Wild"}
{"id": "task1540-85dd973c254c4ea7bdd3387bdd81e8f1", "input": "We extend the theory of boosting for regression problems to the online learning setting. Generalizing from the batch setting for boosting, the notion of a weak learning algorithm is modeled as an online learning algorithm with linear loss functions that competes with a base class of regression functions, while a strong learning algorithm is an online learning algorithm with smooth convex loss functions that competes with a larger class of regression functions. Our main result is an online gradient boosting algorithm that converts a weak online learning algorithm into a strong one where the larger class of functions is the linear span of the base class. We also give a simpler boosting algorithm that converts a weak online learning algorithm into a strong one where the larger class of functions is the convex hull of the base class, and prove its optimality.", "target": "Online Gradient Boosting"}
{"id": "task1540-b7b491c6027f489a980427cfffdcdc94", "input": "In this paper, we consider several types of in\u00ad formation and methods of combination asso\u00ad ciated with incomplete probabilistic systems. \\Ve discriminate between 'a priori' and evi\u00ad dential information. The former one is a de\u00ad scription of the whole population, the latest is a restriction based on observations for a particular case. Then, we proposse different combination methods for each one of them. 'We also consider conditioning as the hetero\u00ad geneous combination of 'a priori' and eviden\u00ad tial information. The evidential information is represented as a convex set of likelihood func\u00ad tions. These will have an associated possi\u00ad bility distribution with behavior according to classical Possibility Theory.", "target": "COMBINATION OF UPPER AND LOWER PROBABILITIES"}
{"id": "task1540-815a766001d94f48be64310c5c33ae31", "input": "The impact of culture in visual emotion perception has recently captured the attention of multimedia research. In this study, we provide powerful computational linguistics tools to explore, retrieve and browse a dataset of 16K multilingual affective visual concepts and 7.3M Flickr images. First, we design an effective crowdsourcing experiment to collect human judgements of sentiment connected to the visual concepts. We then use word embeddings to represent these concepts in a low dimensional vector space, allowing us to expand the meaning around concepts, and thus enabling insight about commonalities and differences among different languages. We compare a variety of concept representations through a novel evaluation task based on the notion of visual semantic relatedness. Based on these representations, we design clustering schemes to group multilingual visual concepts, and evaluate them with novel metrics based on the crowdsourced sentiment annotations as well as visual semantic relatedness. The proposed clustering framework enables us to analyze the full multilingual dataset in-depth and also show an application on a facial data subset, exploring cultural insights of portrait-related affective visual concepts.", "target": "Multilingual Visual Sentiment Concept Matching"}
{"id": "task1540-8423d65fd0fa44f8ac6763fa819b7d5e", "input": "In this correspondence, we will point out a problem with testing adaptive classifiers on autocorrelated data. In such a case random change alarms may boost the accuracy figures. Hence, we cannot be sure if the adaptation is working well.", "target": "How good is the Electricity benchmark for evaluating concept drift adaptation"}
{"id": "task1540-ca1dbd031a854b5f8612ecbe84ed958d", "input": "This manuscript shows that AdaBoost and its immediate variants can produce approximate maximum margin classifiers simply by scaling step size choices with a fixed small constant. In this way, when the unscaled step size is an optimal choice, these results provide guarantees for Friedman\u2019s empirically successful \u201cshrinkage\u201d procedure for gradient boosting (Friedman, 2000). Guarantees are also provided for a variety of other step sizes, affirming the intuition that increasingly regularized line searches provide improved margin guarantees. The results hold for the exponential loss and similar losses, most notably the logistic loss.", "target": "Margins, Shrinkage, and Boosting"}
{"id": "task1540-99f3a75dad0f4615b593b326b0daeaf1", "input": "Many real-world decision-theoretic planning problems can be naturally modeled with discrete and continuous state Markov decision processes (DC-MDPs). While previous work has addressed automated decision-theoretic planning for DCMDPs, optimal solutions have only been defined so far for limited settings, e.g., DC-MDPs having hyper-rectangular piecewise linear value functions. In this work, we extend symbolic dynamic programming (SDP) techniques to provide optimal solutions for a vastly expanded class of DCMDPs. To address the inherent combinatorial aspects of SDP, we introduce the XADD \u2014 a continuous variable extension of the algebraic decision diagram (ADD) \u2014 that maintains compact representations of the exact value function. Empirically, we demonstrate an implementation of SDP with XADDs on various DC-MDPs, showing the first optimal automated solutions to DCMDPs with linear and nonlinear piecewise partitioned value functions and showing the advantages of constraint-based pruning for XADDs.", "target": "Symbolic Dynamic Programming for Discrete and Continuous State MDPs"}
{"id": "task1540-2b12330eda9f4dc9b86a68f38d00b47b", "input": "In this paper, we present a novel approach for Human Computer Interaction (HCI) where, we control cursor movement using a real-time camera. Current methods involve changing mouse parts such as adding more buttons or changing the position of the tracking ball. Instead, our method is to use a camera and computer vision technology, such as image segmentation and gesture recognition, to control mouse tasks (left and right clicking, double-clicking, and scrolling) and we show how it can perform everything as current mouse devices can. The software will be developed in JAVA language. Recognition and pose estimation in this system are user independent and robust as we will be using colour tapes on our finger to perform actions. The software can be used as an intuitive input interface to applications that require multi-dimensional control e.g. computer games etc.", "target": "Mouse Simulation Using Two Coloured Tapes"}
{"id": "task1540-3114dcccaa3646fcb79535600ced2486", "input": "A fundamental challenge in developing semantic parsers is the paucity of strong supervision in the form of language utterances annotated with logical form. In this paper, we propose to exploit structural regularities in language in different domains, and train semantic parsers over multiple knowledge-bases (KBs), while sharing information across datasets. We find that we can substantially improve parsing accuracy by training a single sequence-tosequence model over multiple KBs, when providing an encoding of the domain at decoding time. Our model achieves state-ofthe-art performance on the OVERNIGHT dataset (containing eight domains), improves performance over a single KB baseline from 75.6% to 79.6%, while obtaining a 7x reduction in the number of model parameters.", "target": "Neural Semantic Parsing over Multiple Knowledge-bases"}
{"id": "task1540-35ad305fda3e49bfabf724820cee4c36", "input": "In recent years we have seen rapid and significant progress in automatic image description but what are the open problems in this area? Most work has been evaluated using text-based similarity metrics, which only indicate that there have been improvements, without explaining what has improved. In this paper, we present a detailed error analysis of the descriptions generated by a state-of-the-art attentionbased model. Our analysis operates on two levels: first we check the descriptions for accuracy, and then we categorize the types of errors we observe in the inaccurate descriptions. We find only 20% of the descriptions are free from errors, and surprisingly that 26% are unrelated to the image. Finally, we manually correct the most frequently occurring error types (e.g. gender identification) to estimate the performance reward for addressing these errors, observing gains of 0.2\u20131 BLEU point per type.", "target": "Room for improvement in automatic image description: an error analysis"}
{"id": "task1540-02260778e634461fab5ee207358e9e05", "input": "This paper investigates a new method for improving the learning algorithm of Mixture of Experts (ME) model using a hybrid of Modified Cuckoo Search (MCS) and Conjugate Gradient (CG) as a second order optimization technique. The CG technique is combined with Back-Propagation (BP) algorithm to yield a much more efficient learning algorithm for ME structure. In addition, the experts and gating networks in enhanced model are replaced by CG based Multi-Layer Perceptrons (MLPs) to provide faster and more accurate learning. The CG is considerably depends on initial weights of connections of Artificial Neural Network (ANN), so, a metaheuristic algorithm, the so-called Modified Cuckoo Search is applied in order to select the optimal weights. The performance of proposed method is compared with Gradient Decent Based ME (GDME) and Conjugate Gradient Based ME (CGME) in classification and regression problems. The experimental results show that hybrid MSC and CG based ME (MCS-CGME) has faster convergence and better performance in utilized benchmark data sets.", "target": "Extended Mixture of MLP Experts by Hybrid of Conjugate Gradient Method and Modified Cuckoo Search"}
{"id": "task1540-56e45135790543ce9a5911507f42c3fc", "input": "In typical neural machine translation (NMT), the decoder generates a sentence word by word, packing all linguistic granularities in the same timescale of RNN. In this paper, we propose a new type of decoder for NMT, which splits the decode state into two parts and updates them in two different time-scales. Specifically, we first predict a chunk time-scale state for phrasal modeling, on top of which multiple word time-scale states are generated. In this way, the target sentence is translated hierarchically from chunks to words, with information in different granularities being leveraged. Experiments show that our proposed model significantly improves the translation performance over the state-of-the-art NMT model.", "target": "Chunk-Based Bi-Scale Decoder for Neural Machine Translation"}
{"id": "task1540-eaef42dfa1c6457b96e757cbc69e208e", "input": "Clustering evaluation measures are frequently used to evaluate the performance of algorithms. However, most measures are not properly normalized and ignore some information in the inherent structure of clusterings. We model the relation between two clusterings as a bipartite graph and propose a general component-based decomposition formula based on the components of the graph. Most existing measures are examples of this formula. In order to satisfy consistency in the component, we further propose a split-merge framework for comparing clusterings of different data sets. Our framework gives measures that are conditionally normalized, and it can make use of data point information, such as feature vectors and pairwise distances. We use an entropy-based instance of the framework and a coreference resolution data set to demonstrate empirically the utility of our framework over other measures.", "target": "A Split-Merge Framework for Comparing Clusterings "}
{"id": "task1540-921d9509f4904984b1e10daaab677fde", "input": "The Gumbel trick is a method to sample from a discrete probability distribution, or to estimate its normalizing partition function. The method relies on repeatedly applying a random perturbation to the distribution in a particular way, each time solving for the most likely configuration. We derive an entire family of related methods, of which the Gumbel trick is one member, and show that the new methods have superior properties in several settings with minimal additional computational cost. In particular, for the Gumbel trick to yield computational benefits for discrete graphical models, Gumbel perturbations on all configurations are typically replaced with socalled low-rank perturbations. We show how a subfamily of our new methods adapts to this setting, proving new upper and lower bounds on the log partition function and deriving a family of sequential samplers for the Gibbs distribution. Finally, we balance the discussion by showing how the simpler analytical form of the Gumbel trick enables additional theoretical results.", "target": "Lost Relatives of the Gumbel Trick"}
{"id": "task1540-b6a97545c4d942edaa42de2ce8630b7e", "input": "The ability to monitor the progress of students\u2019 academic performance is a critical issue to the academic community of higher learning. A system for analyzing students\u2019 results based on cluster analysis and uses standard statistical algorithms to arrange their scores data according to the level of their performance is described. In this paper, we also implemented k-mean clustering algorithm for analyzing students\u2019 result data. The model was combined with the deterministic model to analyze the students\u2019 results of a private Institution in %igeria which is a good benchmark to monitor the progression of academic performance of students in higher Institution for the purpose of making an effective decision by the academic planners. Keywordsk \u2013 mean, clustering, academic performance, algorithm.", "target": "Application of k-Means Clustering algorithm for prediction of Students\u2019 Academic Performance"}
{"id": "task1540-54d43d9ed9504905bdb3059064037a36", "input": "Pattern-based methods of IS-A relation extraction rely heavily on so called Hearst patterns. These are ways of expressing instance enumerations of a class in natural language. While these lexico-syntactic patterns prove quite useful, they may not capture all taxonomical relations expressed in text. Therefore in this paper we describe a novel method of IS-A relation extraction from patterns, which uses morpho-syntactical annotations along with grammatical case of noun phrases that constitute entities participating in IS-A relation. We also describe a method for increasing the number of extracted relations that we call pseudo-subclass boosting which has potential application in any pattern-based relation extraction method. Experiments were conducted on a corpus of about 0.5 billion web documents in Polish language.", "target": "Grammatical Case Based IS-A Relation Extraction with Boosting for Polish"}
{"id": "task1540-85ceee0ee2774ac38d797d75724788ac", "input": "Given a point cloud, we consider inferring kinematic models of 3D articulated objects such as boxes for the purpose of manipulating them. While previous work has shown how to extract a planar kinematic model (often represented as a linear chain), such planar models do not apply to 3D objects that are composed of segments often linked to the other segments in cyclic configurations. We present an approach for building a model that captures the relation between the input point cloud features and the object segment as well as the relation between the neighboring object segments. We use a conditional random field that allows us to model the dependencies between different segments of the object. We test our approach on inferring the kinematic structure from partial and noisy point cloud data for a wide variety of boxes including cake boxes, pizza boxes, and cardboard cartons of several sizes. The inferred structure enables our robot to successfully close these boxes by manipulating the flaps.", "target": "Inferring 3D Articulated Models for Box Packaging Robot"}
{"id": "task1540-fa2a664b9a204dfe983af43c75866877", "input": "Object recognition and localization are important tasks in computer vision. The focus of this work is the incorporation of contextual information in order to improve object recognition and localization. For instance, it is natural to expect not to see an elephant to appear in the middle of an ocean. We consider a simple approach to encapsulate such common sense knowledge using co-occurrence statistics from web documents. By merely counting the number of times nouns (such as elephants, sharks, oceans, etc.) co-occur in web documents, we obtain a good estimate of expected co-occurrences in visual data. We then cast the problem of combining textual co-occurrence statistics with the predictions of image-based classifiers as an optimization problem. The resulting optimization problem serves as a surrogate for our inference procedure. Albeit the simplicity of the resulting optimization problem, it is effective in improving both recognition and localization accuracy. Concretely, we observe significant improvements in recognition and localization rates for both ImageNet Detection 2012 and Sun 2012 datasets.", "target": "Using Web Co-occurrence Statistics for Improving Image Categorization"}
{"id": "task1540-aa0ec363cb3f439f942f02153e2d4e09", "input": "We consider the problem of sequentially choosing between a set of unbiased Monte Carlo estimators to minimize the mean-squared-error (MSE) of a final combined estimate. By reducing this task to a stochastic multi-armed bandit problem, we show that well developed allocation strategies can be used to achieve an MSE that approaches that of the best estimator chosen in retrospect. We then extend these developments to a scenario where alternative estimators have different, possibly stochastic costs. The outcome is a new set of adaptive Monte Carlo strategies that provide stronger guarantees than previous approaches while offering practical advantages.", "target": "Adaptive Monte Carlo via Bandit Allocation"}
{"id": "task1540-818062b38f8d4ed8a44f4e94ce967580", "input": "The goal of our research was to enhance local search heuristics used to construct Latin Hypercube Designs. First, we introduce the 1D-move perturbation to improve the space exploration performed by these algorithms. Second, we propose a new evaluation function \u03c8p,\u03c3 specifically targeting the Maximin criterion. Exhaustive series of experiments with Simulated Annealing, which we used as a typically well-behaving local search heuristics, confirm that our goal was reached as the result we obtained surpasses the best scores reported in the literature. Furthermore, the \u03c8p,\u03c3 function seems very promising for a wide spectrum of optimization problems through the Maximin criterion.", "target": "On Simulated Annealing Dedicated to Maximin Latin Hypercube Designs"}
{"id": "task1540-6392682802ed4ea7ad0bb432132166f3", "input": "A frequent object of study in linguistic typology is the order of elements {demonstrative, adjective, numeral, noun} in the noun phrase. The goal is to predict the relative frequencies of these orders across languages. Here we use Poisson regression to statistically compare some prominent accounts of this variation. We compare feature systems derived from Cinque (2005) to feature systems given in Cysouw (2010) and Dryer (in prep). In this setting, we do not find clear reasons to prefer the model of Cinque (2005) or Dryer (in prep), but we find both of these models have substantially better fit to the typological data than the model from Cysouw (2010).", "target": "A Statistical Comparison of Some Theories of NP Word Order"}
{"id": "task1540-958c4ee39e39427f99948b00ce6d5db8", "input": "Search engines are the most important tools for web data acquisition. Web pages are crawled and indexed by search Engines. Users typically locate useful web pages by querying a search engine. One of the challenges in search engines administration is spam pages which waste search engine resources. These pages by deception of search engine ranking algorithms try to be showed in the first page of results. There are many approaches to web spam pages detection such as measurement of HTML code style similarity, pages linguistic pattern analysis and machine learning algorithm on page content features. One of the famous algorithms has been used in machine learning approach is Support Vector Machine (SVM) classifier. Recently basic structure of SVM has been changed by new extensions to increase robustness and classification accuracy. In this paper we improved accuracy of web spam detection by using two nonlinear kernels into Twin SVM (TSVM) as an improved extension of SVM. The classifier ability to data separation has been increased by using two separated kernels for each class of data. Effectiveness of new proposed method has been experimented with two publicly used spam datasets called UK-2007 and UK-2006. Results show the effectiveness of proposed kernelized version of TSVM in web spam page detection.", "target": "Web Spam Detection Using Multiple Kernels in Twin Support Vector Machine"}
{"id": "task1540-edc0a00aaaec421bb6b9c842e680e4cb", "input": "Instant messaging is one of the major channels of computer mediated communication. However, humans are known to be very limited in understanding others\u2019 emotions via textbased communication. Aiming on introducing emotion sensing technologies to instant messaging, we developed EmotionPush, a system that automatically detects the emotions of the messages end-users received on Facebook Messenger and provides colored cues on their smartphones accordingly. We conducted a deployment study with 20 participants during a time span of two weeks. In this paper, we revealed five challenges, along with examples, that we observed in our study based on both user\u2019s feedback and chat logs, including (i) the continuum of emotions, (ii) multi-user conversations, (iii) different dynamics between different users, (iv) misclassification of emotions, and (v) unconventional content. We believe this discussion will benefit the future exploration of affective computing for instant messaging, and also shed light on research of conversational emotion sensing.", "target": "Challenges in Providing Automatic Affective Feedback in Instant Messaging Applications"}
{"id": "task1540-75f7590ac1734547a947724dfb5744ea", "input": "The alternating direction method of multipliers (ADMM) is a versatile tool for solving a wide range of constrained optimization problems, with differentiable or non-differentiable objective functions. Unfortunately, its performance is highly sensitive to a penalty parameter, which makes ADMM often unreliable and hard to automate for a non-expert user. We tackle this weakness of ADMM by proposing a method to adaptively tune the penalty parameters to achieve fast convergence. The resulting adaptive ADMM (AADMM) algorithm, inspired by the successful Barzilai-Borwein spectral method for gradient descent, yields fast convergence and relative insensitivity to the initial stepsize and problem scaling.", "target": "Adaptive ADMM with Spectral Penalty Parameter Selection"}
{"id": "task1540-fc0f3072f3584ca2a1e75040cb6ca50b", "input": "In recent years, total variation (TV) and Euler\u2019s elastica (EE) have been successfully applied to image processing tasks such as denoising and inpainting. This paper investigates how to extend TV and EE to the supervised learning settings on high dimensional data. The supervised learning problem can be formulated as an energy functional minimization under Tikhonov regularization scheme, where the energy is composed of a squared loss and a total variation smoothing (or Euler\u2019s elastica smoothing). Its solution via variational principles leads to an Euler-Lagrange PDE. However, the PDE is always high-dimensional and cannot be directly solved by common methods. Instead, radial basis functions are utilized to approximate the target function, reducing the problem to finding the linear coefficients of basis functions. We apply the proposed methods to supervised learning tasks (including binary classification, multi-class classification, and regression) on benchmark data sets. Extensive experiments have demonstrated promising results of the proposed methods.", "target": "Total Variation and Euler\u2019s Elastica for Supervised Learning"}
{"id": "task1540-8ebb9f28ed2d4de7838557d3ae0e0357", "input": "We propose a StochAstic Fault diagnosis AlgoRIthm, called Safari, which trades off guarantees of computing minimal diagnoses for computational efficiency. We empirically demonstrate, using the 74XXX and ISCAS85 suites of benchmark combinatorial circuits, that Safari achieves several orders-of-magnitude speedup over two well-known deterministic algorithms, CDA\u2217 and HA\u2217, for multiple-fault diagnoses; further, Safari can compute a range of multiple-fault diagnoses that CDA\u2217 and HA\u2217 cannot. We also prove that Safari is optimal for a range of propositional fault models, such as the widely-used weak-fault models (models with ignorance of abnormal behavior). We discuss the optimality of Safari in a class of strong-fault circuit models with stuck-at failure modes. By modeling the algorithm itself as a Markov chain, we provide exact bounds on the minimality of the diagnosis computed. Safari also displays strong anytime behavior, and will return a diagnosis after any non-trivial inference time.", "target": "Approximate Model-Based Diagnosis Using Greedy Stochastic Search"}
{"id": "task1540-942af0746a104c288058776305105d29", "input": "Feature squeezing is a recently-introduced framework for mitigating and detecting adversarial examples. In previous work, we showed that it is effective against several earlier methods for generating adversarial examples. In this short note, we report on recent results showing that simple feature squeezing techniques also make deep learning models significantly more robust against the Carlini/Wagner attacks, which are the best known adversarial methods discovered to date.", "target": "Feature Squeezing Mitigates and Detects Carlini/Wagner Adversarial Examples"}
{"id": "task1540-082dd847a0c048f997da6441c7bd0cbe", "input": "This paper addresses how a recursive neural network model can automatically leave out useless information and emphasize important evidence, in other words, to perform \u201cweight tuning\u201d for higher-level representation acquisition. We propose two models, Weighted Neural Network (WNN) and Binary-Expectation Neural Network (BENN), which automatically control how much one specific unit contributes to the higher-level representation. The proposed model can be viewed as incorporating a more powerful compositional function for embedding acquisition in recursive neural networks. Experimental results demonstrate the significant improvement over standard neural models.", "target": "Feature Weight Tuning for Recursive Neural Networks"}
{"id": "task1540-c9e2f1b5bef34808b062a415ca422ae3", "input": "Case-based reasoning (CBR) based on description logics (DLs) has gained a lot of attention lately. Adaptation is a basic task in CBR that can be modeled as a knowledge base revision problem which has been solved in propositional logic. However, in DLs, adaptation is still a challenge problem since existing revision operators only work well for DLs of the DL-Lite family. It is difficult to design revision algorithms that are syntax-independent and fine-grained. In this paper, we present a new method for adaptation based on the tractable DL EL\u22a5. Following the idea of adaptation as revision, we firstly extend the logical basis for describing cases from propositional logic to the DL EL\u22a5, and then present a formalism for adaptation based on EL\u22a5. With this formalism, we show that existing revision operators and algorithms in DLs do not work for it, and then present our adaptation algorithm. Our algorithm is syntax-independent and fine-grained, and satisfies the requirements on revision operators.", "target": "Algorithm for Adapting Cases Represented in a Tractable Description Logic"}
{"id": "task1540-7b86ddbcbe9f43fcaa414b0756a886c1", "input": "In this paper, we propose several improvements on the block-coordinate Frank-Wolfe (BCFW) algorithm from Lacoste-Julien et al. (2013) recently used to optimize the structured support vector machine (SSVM) objective in the context of structured prediction, though it has wider applications. The key intuition behind our improvements is that the estimates of block gaps maintained by BCFW reveal the block suboptimality that can be used as an adaptive criterion. First, we sample objects at each iteration of BCFW in an adaptive non-uniform way via gapbased sampling. Second, we incorporate pairwise and away-step variants of Frank-Wolfe into the block-coordinate setting. Third, we cache oracle calls with a cache-hit criterion based on the block gaps. Fourth, we provide the first method to compute an approximate regularization path for SSVM. Finally, we provide an exhaustive empirical evaluation of all our methods on four structured prediction datasets.", "target": "Minding the Gaps for Block Frank-Wolfe Optimization of Structured SVMs"}
{"id": "task1540-e982f94045b24c3787f76b986c50f49a", "input": "We formulate a phonetic-prosodic space based on attributes as perceptual observables, rather than articulatory specifications. We propose an alphabet as markers in the phonetic subspace, aiming for a resolution sufficient to support recognition of all spoken languages. The prosodic subspace is made up of directly measurable physical variables. With the proposed alphabet, traditional diphthongs naturally generalize to a broader class of language-neutral phonotactic constraints, indicating a correlation structure similar to that of the traditional sonority-based syllable. We define a stochastic structure on the phone strings based on this diphthongal constraint, and show how a specific spoken language can be defined as a specific set of probability distributions of this stochastic structure. Furthermore, phonological variations within a spoken language can be modeled as varying probability distributions restricted to the phonetic subspace, conditioned on different values in the prosodic subspace.", "target": "A 10-dimensional Phonetic-prosodic Space and its Stochastic Structure A framework for probabilistic modeling of spoken languages and their phonology"}
{"id": "task1540-2b61157550624df485f6f68e47a16599", "input": "Biological neural networks are systems of extraordinary computational capabilities shaped by evolution, development, and lifetime learning. The interplay of these elements leads to the emergence of adaptive behavior and intelligence, but the complexity of the whole system of interactions is an obstacle to the understanding of the key factors at play. Inspired by such intricate natural phenomena, Evolved Plastic Artificial Neural Networks (EPANNs) use simulated evolution in-silico to breed plastic neural networks, artificial systems composed of sensors, outputs, and plastic components that change in response to sensory-output experiences in an environment. These systems may reveal key algorithmic ingredients of adaptation, autonomously discover novel adaptive algorithms, and lead to hypotheses on the emergence of biological adaptation. EPANNs have seen considerable progress over the last two decades. Current scientific and technological advances in artificial neural networks are now setting the conditions for radically new approaches and results. In particular, the limitations of hand-designed structures and algorithms currently used in most deep neural networks could be overcome by more flexible and innovative solutions. This paper brings together a variety of inspiring ideas that define the field of EPANNs. The main computational methods and results are reviewed. Finally, new opportunities and developments are presented.", "target": "Born to Learn: the Inspiration, Progress, and Future of Evolved Plastic Artificial Neural Networks"}
{"id": "task1540-bfbe5e52973c42cebf7832675021615d", "input": "Recursive neural models, which use syntactic parse trees to recursively generate representations bottom-up from parse children, are a popular new architecture, promising to capture structural properties like the scope of negation or long-distance semantic dependencies. But understanding exactly which tasks this parse-based method is appropriate for remains an open question. In this paper we benchmark recursive neural models against sequential recurrent neural models, which are structured solely on word sequences. We investigate 5 tasks: sentiment classification on (1) sentences and (2) syntactic phrases; (3) question answering; (4) discourse parsing; (5) semantic relations (e.g., component-whole between nouns); We find that recurrent models have equal or superior performance to recursive models on all tasks except one: semantic relations between nominals. Our analysis suggests that tasks relying on the scope of negation (like sentiment) are well-handled by sequential models. Recursive models help only with tasks that require representing long-distance relations between words. Our results offer insights on the design of neural architectures for representation learning.", "target": "When Are Tree Structures Necessary for Deep Learning of Representations?"}
{"id": "task1540-7919cb3648d2442cb7fe29d816cc0e86", "input": "A task of clustering data given on the ordinal scale under conditions of overlapping clusters has been considered. It\u2019s proposed to use an approach based on membership and likelihood functions sharing. A number of performed experiments proved effectiveness of the proposed method. The proposed method is characterized by robustness to outliers due to a way of ordering values while constructing membership functions.", "target": "Fuzzy Clustering Data Given on the Ordinal Scale Based on Membership and Likelihood Functions Sharing"}
{"id": "task1540-be75d7112fd44d87958fe3d8aa34f558", "input": "This manuscript presents a trust computation for international relations and its calculus, which related to Bayesian inference, Dempster-Shafer theory and subjective logic. We proposed a method that allows a trust computation which is previously subjective and incomputable. An example of case study for the trust computation is the United States of America\u2013Great Britain relations. The method supports decision makers in a government such as foreign ministry, defense ministry, presidential or prime minister office. The Department of Defense (DoD) may use our method to determine a nation that can be known as a friendly, neutral or hostile nation.", "target": "A Mathematical Trust Algebra for International Nation Relations Computation and Evaluation"}
{"id": "task1540-9e19a2759d544edfa00174a283fc0477", "input": "Vulnerability of state-of-the-art deep neural networks to adversarial attacks has been attracting a lot of attention recently. In this work we propose a new algorithm for constructing universal adversarial perturbations. Our approach is based on computing the so called (p, q)-singular vectors of the Jacobian matrices of hidden layers of a network. Resulting perturbations present interesting visual patterns and by using a batch of just 64 images we can construct adversarial perturbations with relatively high fooling rate. We also investigate a correlation between the singular values of the Jacobian matrices and the fooling rate of a corresponding singular vector.", "target": "Art of singular vectors and universal adversarial perturbations"}
{"id": "task1540-d26eebc31a6c41f3b5340943043b4b7e", "input": "In this paper we consider the problem of multi-task learning, in which a learner is given a collection of prediction tasks that need to be solved. In contrast to previous work, we give up on the assumption that labeled training data is available for all tasks. Instead, we propose an active task selection framework, where based only on the unlabeled data, the learner can choose a, typically small, subset of tasks for which he gets some labeled examples. For the remaining tasks, which have no available annotation, solutions are found by transferring information from the selected tasks. We analyze two transfer strategies and develop generalization bounds for each of them. Based on this theoretical analysis we propose two algorithms for making the choice of labeled tasks in a principled way and show their effectiveness on synthetic and real data.", "target": "Active Task Selection for Multi-Task Learning"}
{"id": "task1540-b1077466d0f14ff38ff0d025289f7288", "input": "This paper develops automated testing and debugging techniques for answer set solver development. We describe a flexible grammar-based black-box ASP fuzz testing tool which is able to reveal various defects such as unsound and incomplete behavior, i.e. invalid answer sets and inability to find existing solutions, in state-of-the-art answer set solver implementations. Moreover, we develop delta debugging techniques for shrinking failureinducing inputs on which solvers exhibit defective behavior. In particular, we develop a delta debugging algorithm in the context of answer set solving, and evaluate two different elimination strategies for the algorithm.", "target": "Testing and Debugging Techniques for Answer Set Solver Development"}
{"id": "task1540-777a258d102241c19ab151d07b8ad472", "input": "This article constructs a Turing Machine which can solve for \u03b2 \u2032 which is REcomplete. Such a machine is only possible if there is something wrong with the foundations of computer science and mathematics. We therefore check our work by looking very closely at Cantor\u2019s diagonalization and construct a novel formal language as an Abelian group which allows us, through equivalence relations, to provide a non-trivial counterexample to Cantor\u2019s argument. As if that wasn\u2019t enough, we then discover that the impredicative nature of G\u00f6del\u2019s diagonalization lemma leads to logical tautology, invalidating any meaning behind the method, leaving no doubt that diagonalization is flawed. Our discovery in regards to these foundational arguments opens the door to solving the P vs NP problem. 1 Turing\u2019s Proof on the Entscheidungsproblem has", "target": "A Stronger Foundation for Computer Science and P=NP"}
{"id": "task1540-d7b9d88e372d4a82b071a2c4852eb9ac", "input": "Truth discovery is to resolve conflicts and find the truth from multiple-source statements. Conventional methods mostly research based on the mutual effect between the reliability of sources and the credibility of statements, however, pay no attention to the mutual effect among the credibility of statements about the same object. We propose memory network based models to incorporate these two ideas to do the truth discovery. We use feedforward memory network and feedback memory network to learn the representation of the credibility of statements which are about the same object. Specially, we adopt memory mechanism to learn source reliability and use it through truth prediction. During learning models, we use multiple types of data (categorical data and continuous data) by assigning different weights automatically in the loss function based on their own effect on truth discovery prediction. The experiment results show that the memory network based models much outperform the state-of-the-art method and other baseline methods.", "target": "Truth Discovery with Memory Network"}
{"id": "task1540-5272c010cc804af48e93b3a5aa7fd0e9", "input": "We study multi-turn response generation in chatbots where a response is generated according to a conversation context. Existing work has modeled the hierarchy of the context, but does not pay enough attention to the fact that words and utterances in the context are differentially important. As a result, they may lose important information in context and generate irrelevant responses. We propose a hierarchical recurrent attention network (HRAN) to model both aspects in a unified framework. In HRAN, a hierarchical attention mechanism attends to important parts within and among utterances with word level attention and utterance level attention respectively. With the word level attention, hidden vectors of a word level encoder are synthesized as utterance vectors and fed to an utterance level encoder to construct hidden representations of the context. The hidden vectors of the context are then processed by the utterance level attention and formed as context vectors for decoding the response. Empirical studies on both automatic evaluation and human judgment show that HRAN can significantly outperform state-of-the-art models for multi-turn response generation.", "target": "Hierarchical Recurrent Attention Network for Response Generation"}
{"id": "task1540-e5968256dc044770829983e29290c642", "input": "Answer sentence selection is the task of identifying sentences that contain the answer to a given question. This is an important problem in its own right as well as in the larger context of open domain question answering. We propose a novel approach to solving this task via means of distributed representations, and learn to match questions with answers by considering their semantic encoding. This contrasts prior work on this task, which typically relies on classifiers with large numbers of hand-crafted syntactic and semantic features and various external resources. Our approach does not require any feature engineering nor does it involve specialist linguistic data, making this model easily applicable to a wide range of domains and languages. Experimental results on a standard benchmark dataset from TREC demonstrate that\u2014despite its simplicity\u2014our model matches state of the art performance on the answer sentence selection task.", "target": "Deep Learning for Answer Sentence Selection"}
{"id": "task1540-cc06c796a2f44c6a93f26c236b6eb012", "input": "Generating texts from structured data (e.g., a table) is important for various natural language processing tasks such as question answering and dialog systems. In recent studies, researchers use neural language models and encoder-decoder frameworks for table-to-text generation. However, these neural network-based approaches do not model the order of contents during text generation. When a human writes a summary based on a given table, he or she would probably consider the content order before wording. In a biography, for example, the nationality of a person is typically mentioned before occupation in a biography. In this paper, we propose an order-planning text generation model to capture the relationship between different fields and use such relationship to make the generated text more fluent and smooth. We conducted experiments on the WIKIBIO dataset and achieve significantly higher performance than previous methods in terms of BLEU, ROUGE, and NIST scores.", "target": "Order-Planning Neural Text Generation From Structured Data"}
{"id": "task1540-7be024277db640d4b317a3344e4ed300", "input": "We give solutions to two fundamental computational problems in ontologybased data access with the W3C standard ontology language OWL2QL: the succinctness problem for first-order rewritings of ontology-mediated queries (OMQs), and the complexity problem for OMQ answering. We classify OMQs according to the shape of their conjunctive queries (treewidth, the number of leaves) and the existential depth of their ontologies. For each of these classes, we determine the combined complexity of OMQ answering, and whether all OMQs in the class have polynomial-size first-order, positive existential, and nonrecursive datalog rewritings. We obtain the succinctness results using hypergraph programs, a new computational model for Boolean functions, which makes it possible to connect the size of OMQ rewritings and circuit complexity.", "target": "Ontology-Mediated Queries: Combined Complexity and Succinctness of Rewritings via Circuit Complexity"}
{"id": "task1540-8e43407e0d4641fb96127adc6ed8f10b", "input": "Multiagent planning and coordination problems are common and known to be computationally hard. We show that a wide range of two-agent problems can be formulated as bilinear programs. We present a successive approximation algorithm that significantly outperforms the coverage set algorithm, which is the state-of-the-art method for this class of multiagent problems. Because the algorithm is formulated for bilinear programs, it is more general and simpler to implement. The new algorithm can be terminated at any time and\u2013unlike the coverage set algorithm\u2013it facilitates the derivation of a useful online performance bound. It is also much more efficient, on average reducing the computation time of the optimal solution by about four orders of magnitude. Finally, we introduce an automatic dimensionality reduction method that improves the effectiveness of the algorithm, extending its applicability to new domains and providing a new way to analyze a subclass of bilinear programs.", "target": "A Bilinear Programming Approach for Multiagent Planning"}
{"id": "task1540-0484eae34a064d6baaa91e850d568519", "input": "Online learning algorithms are designed to learn even when their input is generated by an adversary. The widely-accepted formal definition of an online algorithm\u2019s ability to learn is the game-theoretic notion of regret. We argue that the standard definition of regret becomes inadequate if the adversary is allowed to adapt to the online algorithm\u2019s actions. We define the alternative notion of policy regret, which attempts to provide a more meaningful way to measure an online algorithm\u2019s performance against adaptive adversaries. Focusing on the online bandit setting, we show that no bandit algorithm can guarantee a sublinear policy regret against an adaptive adversary with unbounded memory. On the other hand, if the adversary\u2019s memory is bounded, we present a general technique that converts any bandit algorithm with a sublinear regret bound into an algorithm with a sublinear policy regret bound. We extend this result to other variants of regret, such as switching regret, internal regret, and swap regret.", "target": "Online Bandit Learning against an Adaptive Adversary: from Regret to Policy Regret"}
{"id": "task1540-8df66577435449d2870367fd2e1c857c", "input": "Logic-based event recognition systems infer occurrences of events in time using a set of event definitions in the form of first-order rules. The Event Calculus is a temporal logic that has been used as a basis in event recognition applications, providing among others, direct connections to machine learning, via Inductive Logic Programming (ILP). OLED is a recently proposed ILP system that learns event definitions in the form of Event Calculus theories, in a single pass over a data stream. In this work we present a version of OLED that allows for distributed, online learning. We evaluate our approach on a benchmark activity recognition dataset and show that we can significantly reduce training times, exchanging minimal information between processing nodes.", "target": "Distributed Online Learning of Event Definitions"}
{"id": "task1540-82662a1b1fec42e1bebc84ac5a1aad8b", "input": "We propose a new encoder-decoder approach to learn distributed sentence representations from unlabeled sentences. The word-to-vector representation is used, and convolutional neural networks are employed as sentence encoders, mapping an input sentence into a fixed-length vector. This representation is decoded using long short-term memory recurrent neural networks, considering several tasks, such as reconstructing the input sentence, or predicting the future sentence. We further describe a hierarchical encoder-decoder model to encode a sentence to predict multiple future sentences. By training our models on a large collection of novels, we obtain a highly generic convolutional sentence encoder that performs well in practice. Experimental results on several benchmark datasets, and across a broad range of applications, demonstrate the superiority of the proposed model over competing methods.", "target": "Unsupervised Learning of Sentence Representations using Convolutional Neural Networks"}
{"id": "task1540-5772c558155c4056b307657d7aac9712", "input": "In budget\u2013limited multi\u2013armed bandit (MAB) problems, the learner\u2019s actions are costly and constrained by a fixed budget. Consequently, an optimal exploitation policy may not be to pull the optimal arm repeatedly, as is the case in other variants of MAB, but rather to pull the sequence of different arms that maximises the agent\u2019s total reward within the budget. This difference from existing MABs means that new approaches to maximising the total reward are required. Given this, we develop two pulling policies, namely: (i) KUBE; and (ii) fractional KUBE. Whereas the former provides better performance up to 40% in our experimental settings, the latter is computationally less expensive. We also prove logarithmic upper bounds for the regret of both policies, and show that these bounds are asymptotically optimal (i.e. they only differ from the best possible regret by a constant factor).", "target": "Knapsack based Optimal Policies for Budget\u2013Limited Multi\u2013Armed Bandits"}
{"id": "task1540-5a9fd9524bf848bebe1b7a7703f9b266", "input": "The rise of graph-structured data such as social networks, regulatory networks, citation graphs, and functional brain networks, in combination with resounding success of deep learning in various applications, has brought the interest in generalizing deep learning models to non-Euclidean domains. In this paper, we introduce a new spectral domain convolutional architecture for deep learning on graphs. The core ingredient of our model is a new class of parametric rational complex functions (Cayley polynomials) allowing to efficiently compute localized regular filters on graphs that specialize on frequency bands of interest. Our model scales linearly with the size of the input data for sparsely-connected graphs, can handle different constructions of Laplacian operators, and typically requires less parameters than previous models. Extensive experimental results show the superior performance of our approach on various graph learning problems.", "target": "CayleyNets: Graph Convolutional Neural Networks with Complex Rational Spectral Filters"}
{"id": "task1540-91a5e27770c74f8c9a61964568635f47", "input": "We introduce a powerful recurrent neural network based method for novelty detection to the application of detecting radio anomalies. This approach holds promise in significantly increasing the ability of naive anomaly detection to detect small anomalies in highly complex complexity multi-user radio bands. We demonstrate the efficacy of this approach on a number of common real over the air radio communications bands of interest and quantify detection performance in terms of probability of detection an false alarm rates across a range of interference to band power ratios and compare to baseline methods.", "target": "Recurrent Neural Radio Anomaly Detection"}
{"id": "task1540-f2407b330ffd4e9eb0390bf59579ed38", "input": "Many studies on the cost-sensitive learning assumed that a unique cost matrix is known for a problem. However, this assumption may not hold for many real-world problems. For example, a classifier might need to be applied in several circumstances, each of which associates with a different cost matrix. Or, different human experts have different opinions about the costs for a given problem. Motivated by these facts, this study aims to seek the minimax classifier over multiple cost matrices. In summary, we theoretically proved that, no matter how many cost matrices are involved, the minimax problem can be tackled by solving a number of standard cost-sensitive problems and sub-problems that involve only two cost matrices. As a result, a general framework for achieving minimax classifier over multiple cost matrices is suggested and justified by preliminary empirical studies.", "target": "Minimax Classifier for Uncertain Costs"}
{"id": "task1540-9849dc7845a544b39fc118d55ecff9c8", "input": "We present a computational analysis of three language varieties: native, advanced non-native, and translation. Our goal is to investigate the similarities and differences between non-native language productions and translations, contrasting both with native language. Using a collection of computational methods we establish three main results: (1) the three types of texts are easily distinguishable; (2) nonnative language and translations are closer to each other than each of them is to native language; and (3) some of these characteristics depend on the source or native language, while others do not, reflecting, perhaps, unified principles that similarly affect translations and non-native language.", "target": "On the Similarities Between Native, Non-native and Translated Texts"}
{"id": "task1540-5a6338f3e42d44d09f5dedcdbe6c55bc", "input": "This essay investigates the question of how the naive Bayes classifier and the support vector machine compare in their ability to forecast the Stock Exchange of Thailand. The theory behind the SVM and the naive Bayes classifier is explored. The algorithms are trained using data from the month of January 2010, extracted from the MarketWatch.com website. Input features are selected based on previous studies of the SET100 Index. The Weka 3 software is used to create models from the labeled training data. Mean squared error and proportion of correctly classified instances, and a number of other error measurements are the used to compare the two algorithms. This essay shows that these two algorithms are currently not advanced enough to accurately model the stock exchange. Nevertheless, the naive Bayes is better than the support vector machine at predicting the Stock Exchange of Thailand.", "target": "How do the naive Bayes classifier and the Support Vector Machine compare in their ability to forecast the Stock Exchange of Thailand?"}
{"id": "task1540-3e55ebf394984d258d40694798aeece2", "input": "We present a method for implementing an Efficient Unitary Neural Network (EUNN) whose computational complexity is merelyO(1) per parameter and has full tunability, from spanning part of unitary space to all of it. We apply the EUNN in Recurrent Neural Networks, and test its performance on the standard copying task and the MNIST digit recognition benchmark, finding that it significantly outperforms a non-unitary RNN, an LSTM network, an exclusively partial space URNN and a projective URNN with comparable parameter numbers.", "target": "Tunable Efficient Unitary Neural Networks (EUNN) and their application to RNN"}
{"id": "task1540-08857948a17541cda6bd12cd280ebf7b", "input": "We consider the problem of translating high-level textual descriptions to formal representations in technical documentation as part of an effort to model the meaning of such documentation. We focus specifically on the problem of learning translational correspondences between text descriptions and grounded representations in the target documentation, such as formal representation of functions or code templates. Our approach exploits the parallel nature of such documentation, or the tight coupling between high-level text and the low-level representations we aim to learn. Data is collected by mining technical documents for such parallel text-representation pairs, which we use to train a simple semantic parsing model. We report new baseline results on sixteen novel datasets, including the standard library documentation for nine popular programming languages across seven natural languages, and a small collection of Unix utility manuals.", "target": "Learning Semantic Correspondences in Technical Documentation"}
{"id": "task1540-da3b7c005f214116acc59ad82c9a9c51", "input": "The efficiency of inference in both the Hugin and, most notably, the Shafer-Shenoy archi\u00ad tectures can be improved by exploiting the independence relations induced by the incom\u00ad ing messages of a clique. That is, the mes\u00ad sage to be sent from a clique can be com\u00ad puted via a factorization of the clique poten\u00ad tial in the form of a junction tree. In this pa\u00ad per we show that by exploiting such nested junction trees in the computation of messages both space and time costs of the conventional propagation methods may be reduced. The paper presents a structured way of exploit\u00ad ing the nested junction trees technique to achieve such reductions. The usefulness of the method is emphasized through a thor\u00ad ough empirical evaluation involving ten large real-world Bayesian networks and the Hugin inference algorithm.", "target": "Nested Junction Trees"}
{"id": "task1540-14e651433693406c9e71bdf2c475ebdc", "input": "The holy Quran is the holy book of the Muslims. It contains information about many domains. Often people search for particular concepts of holy Quran based on the relations among concepts. An ontological modeling of holy Quran can be useful in such a scenario. In this paper, we have modeled nature related concepts of holy Quran using OWL (Web Ontology Language) / RDF (Resource Description Framework). Our methodology involves identifying nature related concepts mentioned in holy Quran and identifying relations among those concepts. These concepts and relations are represented as classes/instances and properties of an OWL ontology. Later, in the result section it is shown that, using the Ontological model, SPARQL queries can retrieve verses and concepts of interest. Thus, this modeling helps semantic search and query on the holy Quran. In this work, we have used English translation of the holy Quran by Sahih International, Protege OWL Editor and for querying we have used SPARQL. Keywords\u2014 Quranic Ontology; Semantic Quran; Quranic Knowledge Representation.", "target": "Applying Ontological Modeling on Quranic \u201cNature\u201d Domain"}
{"id": "task1540-72b397ec33804b3894c088ea22a7935b", "input": "Feature selection plays an important role in the data mining process. It is needed to deal with the excessive number of features, which can become a computational burden on the learning algorithms. It is also necessary, even when computational resources are not scarce, since it improves the accuracy of the machine learning tasks, as we will see in the upcoming sections. In this review, we discuss the different feature selection approaches, and the relation between them and the various machine learning algorithms.", "target": "Survey on Feature Selection"}
{"id": "task1540-e9e46627b5fc468aab27f2d347080ad6", "input": "We describe a method to produce a network where current methods such as DeepFool have great difficulty producing adversarial samples. Our construction suggests some insights into how deep networks work. We provide a reasonable analyses that our construction is difficult to defeat, and show experimentally that our method is hard to defeat using several standard networks and datasets. We use our method to produce a system that can reliably detect whether an image is a picture of a real scene or not. Our system applies to images captured with depth maps (RGBD images) and checks if a pair of image and depth map is consistent. It relies on the relative difficulty of producing naturalistic depth maps for images in post processing. We demonstrate that our system is robust to adversarial examples built from currently known attacking approaches.", "target": "SafetyNet: Detecting and Rejecting Adversarial Examples Robustly"}
{"id": "task1540-0faeab4be6c347adbe0b3dae4bf2b5e2", "input": "We present a method for inducing new dialogue systems from very small amounts of unannotated dialogue data, showing how word-level exploration using Reinforcement Learning (RL), combined with an incremental and semantic grammar Dynamic Syntax (DS) allows systems to discover, generate, and understand many new dialogue variants. The method avoids the use of expensive and time-consuming dialogue act annotations, and supports more natural (incremental) dialogues than turn-based systems. Here, language generation and dialogue management are treated as a joint decision/optimisation problem, and the MDP model for RL is constructed automatically. With an implemented system, we show that this method enables a wide range of dialogue variations to be automatically captured, even when the system is trained from only a single dialogue. The variants include questionanswer pairs, overand under-answering, selfand other-corrections, clarification interaction, split-utterances, and ellipsis. This generalisation property results from the structural knowledge and constraints present within the DS grammar, and highlights some limitations of recent systems built using machine learning techniques only.", "target": "Bootstrapping incremental dialogue systems: using linguistic knowledge to learn from minimal data"}
{"id": "task1540-a5468a537a1240ad9b69a97f8b5d0f06", "input": "We present a probabilistic generative model for inferring a description of coordinated, recursively structured group activities at multiple levels of temporal granularity based on observations of individuals\u2019 trajectories. The model accommodates: (1) hierarchically structured groups, (2) activities that are temporally and compositionally recursive, (3) component roles assigning different subactivity dynamics to subgroups of participants, and (4) a nonparametric Gaussian Process model of trajectories. We present an MCMC sampling framework for performing joint inference over recursive activity descriptions and assignment of trajectories to groups, integrating out continuous parameters. We demonstrate the model\u2019s expressive power in several simulated and complex real-world scenarios from the VIRAT and UCLA Aerial Event video data sets.", "target": "Bayesian Inference of Recursive Sequences of Group Activities from Tracks"}
{"id": "task1540-b9161eee080a4c909e58179186e485de", "input": "We propose a supervised machine learning approach for boosting existing signal and image recovery methods and demonstrate its efficacy on example of image reconstruction in computed tomography. Our technique is based on a local nonlinear fusion of several image estimates, all obtained by applying a chosen reconstruction algorithm with different values of its control parameters. Usually such output images have different bias/variance trade-off. The fusion of the images is performed by feed-forward neural network trained on a set of known examples. Numerical experiments show an improvement in reconstruction quality relatively to existing direct and iterative reconstruction methods.", "target": "Spatially-Adaptive Reconstruction in Computed Tomography using Neural Networks"}
{"id": "task1540-5a1ace342209481eb47dfb80f7901e39", "input": "Latent state space models are a fundamental and widely used tool for modeling dynamical systems. However, they are difficult to learn from data and learned models often lack performance guarantees on inference tasks such as filtering and prediction. In this work, we present the PREDICTIVE STATE INFERENCE MACHINE (PSIM), a data-driven method that considers the inference procedure on a dynamical system as a composition of predictors. The key idea is that rather than first learning a latent state space model, and then using the learned model for inference, PSIM directly learns predictors for inference in predictive state space. We provide theoretical guarantees for inference, in both realizable and agnostic settings, and showcase practical performance on a variety of simulated and real world robotics benchmarks.", "target": "Learning to Filter with Predictive State Inference Machines"}
{"id": "task1540-517cb47827054753846e90628b496441", "input": "Measuring the effectiveness of corporate environmental reports, it being highly qualitative and less regulated, is often considered as a daunting task. The task becomes more complex if comparisons are to be performed. This study is undertaken to overcome the physical verification problems by implementing data mining technique. It further explores on the effectiveness by performing exploratory analysis and structural equation model to bring out the significant linkages between the selected 10 variables. Samples of 539 reports across various countries are used from an international directory to perform the statistical analysis like \u2013 One way ANOVA (Analysis of Variance), MDA (Multivariate Discriminant Analysis) and SEM (Structural Equation Modeling). The results indicate the significant differences among the various types of industries in their environmental reporting, and the exploratory factors like stakeholder, organization strategy and industrial oriented factors, proved significant. The major accomplishment is that the findings correlate with the conceptual frame work of GRI.", "target": "Analysis of Corporate Environmental Reports using Statistical Techniques and Data Mining"}
{"id": "task1540-a8f1e045fa7e47d48dd2275182833a99", "input": "This paper proposes a framework dedicated to the construction of what we call discrete elastic inner product allowing one to embed sets of non-uniformly sampled multivariate time series or sequences of varying lengths into inner product space structures. This framework is based on a recursive definition that covers the case of multiple embedded time elastic dimensions. We prove that such inner products exist in our general framework and show how a simple instance of this inner product class operates on some prospective applications, while generalizing the Euclidean inner product. Classification experimentations on time series and symbolic sequences datasets demonstrate the benefits that we can expect by embedding time series or sequences into elastic inner spaces rather than into classical Euclidean spaces. These experiments show good accuracy when compared to the euclidean distance or even dynamic programming algorithms while maintaining a linear algorithmic complexity at exploitation stage, although a quadratic indexing phase beforehand is required.", "target": "Discrete Elastic Inner Vector Spaces with Application to Time Series and Sequence Mining"}
{"id": "task1540-f4c2914553324dfc908a92c64c5839cd", "input": "We propose randomized least-squares value iteration (RLSVI) \u2013 a new reinforcement learning algorithm designed to explore and generalize efficiently via linearly parameterized value functions. We explain why versions of least-squares value iteration that use Boltzmann or -greedy exploration can be highly inefficient, and we present computational results that demonstrate dramatic efficiency gains enjoyed by RLSVI. Further, we establish an upper bound on the expected regret of RLSVI that demonstrates nearoptimality in a tabula rasa learning context. More broadly, our results suggest that randomized value functions offer a promising approach to tackling a critical challenge in reinforcement learning: synthesizing efficient exploration and effective generalization.", "target": "Generalization and Exploration via Randomized Value Functions"}
{"id": "task1540-5010c819e23f4cc496d46d199012c769", "input": "We study the problem of learning in the presence of a drifting target concept. Specifically, we provide bounds on the error rate at a given time, given a learner with access to a history of independent samples labeled according to a target concept that can change on each round. One of our main contributions is a refinement of the best previous results for polynomial-time algorithms for the space of linear separators under a uniform distribution. We also provide general results for an algorithm capable of adapting to a variable rate of drift of the target concept. Some of the results also describe an active learning variant of this setting, and provide bounds on the number of queries for the labels of points in the sequence sufficient to obtain the stated bounds on the error rates.", "target": "Learning with a Drifting Target Concept"}
{"id": "task1540-e2ca987482dd43e7938c8e9bb86fa857", "input": "As a general and thus popular model for autonomous systems, partially observable Markov decision process (POMDP) can capture uncertainties from different sources like sensing noises, actuation errors, and uncertain environments. However, its comprehensiveness makes the planning and control in POMDP difficult. Traditional POMDP planning problems target to find the optimal policy to maximize the expectation of accumulated rewards. But for safety critical applications, guarantees of system performance described by formal specifications are desired, which motivates us to consider formal methods to synthesize supervisor for POMDP. With system specifications given by Probabilistic Computation Tree Logic (PCTL), we propose a supervisory control framework with a type of deterministic finite automata (DFA), za-DFA, as the controller form. While the existing work mainly relies on optimization techniques to learn fixed-size finite state controllers (FSCs), we develop an L\u2217 learning based algorithm to determine both space and transitions of za-DFA. Membership queries and different oracles for conjectures are defined. The learning algorithm is sound and complete. An example is given in detailed steps to illustrate the supervisor synthesis algorithm.", "target": "Supervisor Synthesis of POMDP based on Automata Learning"}
{"id": "task1540-06f2a986f5a14b5a91112e82b83ffe51", "input": "This paper explores an incremental training strategy for the skip-gram model with negative sampling (SGNS) from both empirical and theoretical perspectives. Existing methods of neural word embeddings, including SNGS, are multi-pass algorithms and thus cannot perform incremental model update. To address this problem, we present a simple incremental extension of SNGS and provide a thorough theoretical analysis to demonstrate its validity. Empirical experiments demonstrated the correctness of the theoretical analysis as well as the practical usefulness of the incremental algorithm.", "target": "Incremental Skip-gram Model with Negative Sampling"}
{"id": "task1540-26231e7f5c044d2295b11f66c6fd0782", "input": "Sequence-to-sequence models have shown strong performance across a broad range of applications. However, their application to parsing and generating text using Abstract Meaning Representation (AMR) has been limited, due to the relatively limited amount of labeled data and the nonsequential nature of the AMR graphs. We present a novel training procedure that can lift this limitation using millions of unlabeled sentences and careful preprocessing of the AMR graphs. For AMR parsing, our model achieves competitive results of 62.1 SMATCH, the current best score reported without significant use of external semantic resources. For AMR generation, our model establishes a new state-of-the-art performance of BLEU 33.8. We present extensive ablative and qualitative analysis including strong evidence that sequencebased AMR models are robust against ordering variations of graph-to-sequence conversions.", "target": "Neural AMR: Sequence-to-Sequence Models for Parsing and Generation"}
{"id": "task1540-0529fda1394c40b4b4f1b1e0b5d4ba0a", "input": "We show a Talagrand-type of concentration inequality for MTL, using which we establish sharp excess risk bounds for Multi-Task Learning (MTL) in terms of distributionand data-dependent versions of the Local Rademacher Complexity (LRC). We also give a new bound on the LRC for strongly convex hypothesis classes, which applies not only to MTL but also to the standard i.i.d. setting. Combining both results, one can now easily derive fast-rate bounds on the excess risk for many prominent MTL methods, including\u2014as we demonstrate\u2014Schatten-norm, group-norm, and graph-regularized MTL. The derived bounds reflect a relationship akeen to a conservation law of asymptotic convergence rates. This very relationship allows for trading off slower rates w.r.t. the number of tasks for faster rates with respect to the number of available samples per task, when compared to the rates obtained via a traditional, global Rademacher analysis.", "target": "Local Rademacher Complexity-based Learning Guarantees for Multi-Task Learning"}
{"id": "task1540-42b05617d9d244a39444452af6e46a0a", "input": "Most provably-efficient reinforcement learning algorithms introduce optimism about poorly-understood states and actions to encourage exploration. We study an alternative approach for efficient exploration: posterior sampling for reinforcement learning (PSRL). This algorithm proceeds in repeated episodes of known duration. At the start of each episode, PSRL updates a prior distribution over Markov decision processes and takes one sample from this posterior. PSRL then follows the policy that is optimal for this sample during the episode. The algorithm is conceptually simple, computationally efficient and allows an agent to encode prior knowledge in a natural way. We establish an \u00d5(\u03c4S \u221a AT ) bound on expected regret, where T is time, \u03c4 is the episode length and S and A are the cardinalities of the state and action spaces. This bound is one of the first for an algorithm not based on optimism, and close to the state of the art for any reinforcement learning algorithm. We show through simulation that PSRL significantly outperforms existing algorithms with similar regret bounds.", "target": "(More) Efficient Reinforcement Learning via Posterior Sampling"}
{"id": "task1540-4df1c491c41346db94ad787bb7ffb119", "input": "The paper analyzes the interaction between humans and computers in terms of response time in solving the image-based CAPTCHA. In particular, the analysis focuses on the attitude of the different Internet users in easily solving four different types of image-based CAPTCHAs which include facial expressions like: animated character, old woman, surprised face, worried face. To pursue this goal, an experiment is realized involving 100 Internet users in solving the four types of CAPTCHAs, differentiated by age, Internet experience, and education level. The response times are collected for each user. Then, association rules are extracted from user data, for evaluating the dependence of the response time in solving the CAPTCHA from age, education level and experience in internet usage by statistical analysis. The results implicitly capture the users\u2019 psychological states showing in what states the users are more sensible. It reveals to be a novelty and a meaningful analysis in the state-of-the-art.", "target": "Analysis of the Human-Computer Interaction on the Example of Image-based CAPTCHA by Association Rule Mining"}
{"id": "task1540-202c077790794c83a955566676982a75", "input": "We propose the Gaussian Error Linear Unit (GELU), a high-performing neural network activation function. The GELU nonlinearity is the expected transformation of a stochastic process which randomly applies the identity or zero map, combining the intuitions of dropout and zoneout while respecting neuron values. This connection suggests a new probabilistic understanding of nonlinearities. We perform an empirical evaluation of the GELU nonlinearity against the ReLU and ELU activations and find performance improvements across all tasks.", "target": "Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units"}
{"id": "task1540-10c9071f442f48dfb69993492584097c", "input": "Detecting a small number of outliers from a set of data observations is always challenging. This problem is more difficult in the setting of multiple network samples, where computing the anomalous degree of a network sample is generally not sufficient. In fact, explaining why the network is exceptional, expressed in the form of subnetwork, is also equally important. In this paper, we develop a novel algorithm to address these two key problems. We treat each network sample as a potential outlier and identify subnetworks that mostly discriminate it from nearby regular samples. The algorithm is developed in the framework of network regression combined with the constraints on both network topology and L1-norm shrinkage to perform subnetwork discovery. Our method thus goes beyond subspace /subgraph discovery and we show that it converges to a global optimum. Evaluation on various real-world network datasets demonstrates that our algorithm not only outperforms baselines in both network and high dimensional setting, but also discovers highly relevant and interpretable local subnetworks, further enhancing our understanding of anomalous networks.", "target": "Outlier Detection from Network Data with Subnetwork Interpretation"}
{"id": "task1540-7abd84e21bee417e8ff545e16c072eea", "input": "In this article, an agent-based negotiation model for negotiation teams that negotiate a deal with an opponent is presented. Agent-based negotiation teams are groups of agents that join together as a single negotiation party because they share an interest that is related to the negotiation process. The model relies on a trusted mediator that coordinates and helps team members in the decisions that they have to take during the negotiation process: which offer is sent to the opponent, and whether or not the offers received from the opponent are accepted. The main strength of the proposed negotiation model is the fact that it guarantees unanimity within team decisions since decisions report a utility to team members that is greater than or equal to their aspiration levels at each negotiation round. This work analyzes how unanimous decisions are taken within the team and the robustness of the model against different types of manipulations. An empirical evaluation is also performed to study the impact of the different parameters of the model.", "target": "Reaching Unanimous Agreements within Agent-Based Negotiation Teams with Linear and Monotonic Utility Functions"}
{"id": "task1540-862e88a5b496413aa3ab5dc42a8ae7a5", "input": "Based on NFL game data we try to predict the outcome of a play in multiple different ways including Decision and Classification Trees, Nearest Neighbors, Naive Bayes, Linear Discriminant Analysis, Support Vector Machines and Regression, and Artificial Neural Networks. An application of this is the following: by plugging in various play options one could determine the best play for a given situation in real time. While the outcome of a play can be described in many ways we had the most promising results with a newly defined measure that we call progress. We see this work as a first step to include predictive analysis into NFL playcalling.", "target": "NFL Play Prediction"}
{"id": "task1540-e43d3178a1884f01bfd2bb2befc53206", "input": "We study a novel architecture and training procedure for locomotion tasks. A high-frequency, low-level \u201cspinal\u201d network with access to proprioceptive sensors learns sensorimotor primitives by training on simple tasks. This pre-trained module is fixed and connected to a low-frequency, high-level \u201ccortical\u201d network, with access to all sensors, which drives behavior by modulating the inputs to the spinal network. Where a monolithic end-to-end architecture fails completely, learning with a pre-trained spinal module succeeds at multiple high-level tasks, and enables the effective exploration required to learn from sparse rewards. We test our proposed architecture on three simulated bodies: a 16-dimensional swimming snake, a 20-dimensional quadruped, and a 54-dimensional humanoid (see attached video).", "target": "Learning and Transfer of Modulated Locomotor Controllers"}
{"id": "task1540-339c225e4fb748f0989d9822979d037f", "input": "Most conventional sentence similarity methods only focus on similar parts of two input sentences, and simply ignore the dissimilar parts, which usually give us some clues and semantic meanings about the sentences. In this work, we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences. The model represents each word as a vector, and calculates a semantic matching vector for each word based on all words in the other sentence. Then, each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector. After this, a twochannel CNN model is employed to capture features by composing the similar and dissimilar components. Finally, a similarity score is estimated over the composed feature vectors. Experimental results show that our model gets the state-of-the-art performance on the answer sentence selection task, and achieves a comparable result on the paraphrase identification task.", "target": "Sentence Similarity Learning by Lexical Decomposition and Composition"}
{"id": "task1540-c1a848e9b9424fca9031c18fa6b7e4cb", "input": "A modification of the neo-fuzzy neuron is proposed (an extended neo-fuzzy neuron (ENFN)) that is characterized by improved approximating properties. An adaptive learning algorithm is proposed that has both tracking and smoothing properties and solves prediction, filtering and smoothing tasks of non-stationary \u201cnoisy\u201d stochastic and chaotic signals. An ENFN distinctive feature is its computational simplicity compared to other artificial neural networks and neuro-fuzzy systems.", "target": "An Extended Neo-Fuzzy Neuron and its Adaptive Learning Algorithm"}
{"id": "task1540-d2092c1e985f42a6840c8d4cf5536274", "input": "We consider the problem of learning for planning, where knowledge acquired while planning is reused to plan faster in new problem instances. For robotic tasks, among others, plan execution can be captured as a sequence of visual images. For such domains, we propose to use deep neural networks in learning for planning, based on learning a reactive policy that imitates execution traces produced by a planner. We investigate architectural properties of deep networks that are suitable for learning long-horizon planning behavior, and explore how to learn, in addition to the policy, a heuristic function that can be used with classical planners or search algorithms such as A\u2217. Our results on the challenging Sokoban domain show that, with a suitable network design, complex decision making policies and powerful heuristic functions can be learned through imitation. Videos available at https://sites.google.com/site/learn2plannips/.", "target": "Learning Generalized Reactive Policies using Deep Neural Networks"}
{"id": "task1540-3590b7e23f904c33a74a6fd7ed27b574", "input": "Abstract. Random Forest (RF) is a powerful ensemble method for classification and regression tasks. It consists of decision trees set. Although, a single tree is well interpretable for human, the ensemble of trees is a black-box model. The popular technique to look inside the RF model is to visualize a RF proximity matrix obtained on data samples with Multidimensional Scaling (MDS) method. Herein, we present a novel method based on Self-Organising Maps (SOM) for revealing intrinsic relationships in data that lay inside the RF used for classification tasks. We propose an algorithm to learn the SOM with the proximity matrix obtained from the RF. The visualization of RF proximity matrix with MDS and SOM is compared. What is more, the SOM learned with the RF proximity matrix has better classification accuracy in comparison to SOM learned with Euclidean distance. Presented approach enables better understanding of the RF and additionally improves accuracy of the SOM.", "target": "Visualizing Random Forest with Self-Organising Map"}
{"id": "task1540-14015d1e6ae441a2aea01c8737208f12", "input": "To mitigate the uncertainty of variable renewable<lb>resources, two off-the-shelf machine learning tools are deployed<lb>to forecast the solar power output of a solar photovoltaic system.<lb>The support vector machines generate the forecasts and the<lb>random forest acts as an ensemble learning method to combine<lb>the forecasts. The common ensemble technique in wind and solar<lb>power forecasting is the blending of meteorological data from<lb>several sources. In this study though, the present and the past<lb>solar power forecasts from several models, as well as the<lb>associated meteorological data, are incorporated into the random<lb>forest to combine and improve the accuracy of the day-ahead<lb>solar power forecasts. The performance of the combined model is<lb>evaluated over the entire year and compared with other<lb>combining techniques. Keywords\u2014Ensemble learning, post-processing, random forest,<lb>solar power, support vector regression.", "target": "Random Forest Ensemble of Support Vector Regression Models for Solar Power Forecasting"}
{"id": "task1540-539dd761427941c7a107447e0936c217", "input": "The way experts manage uncertainty usually changes depending on the task they are performing. This fact has lead us to consider the problem of communicating modules (task implementations) in a large and structured knowledge based system when modules have different uncertainty calculi. In this paper, the analysis of the communication problem is made assuming that (i) each uncertainty calculus is an inference mechanism defining an entailment relation, and therefore the communication is considered to be inference-preserving, and (ii) we restrict ourselves to the case which the different uncertainty calculi are given by a class of truth\u00ad functional Multiple-valued Logics.", "target": "Combining Multiple-valued Logics in Modular Expert Systems"}
{"id": "task1540-d6b9052a731b40c7ae878109d93b63dc", "input": "Slot Filling (SF) aims to extract the values of certain types of attributes (or slots, such as person:cities of residence) for a given entity from a large collection of source documents. In this paper we propose an effective DNN architecture for SF with the following new strategies: (1). Take a regularized dependency graph instead of a raw sentence as input to DNN, to compress the wide contexts between query and candidate filler; (2). Incorporate two attention mechanisms: local attention learned from query and candidate filler, and global attention learned from external knowledge bases, to guide the model to better select indicative contexts to determine slot type. Experiments show that this framework outperforms state-of-the-art on both relation extraction (16% absolute F-score gain) and slot filling validation for each individual system (up to 8.5% absolute Fscore gain).", "target": "Improving Slot Filling Performance with Attentive Neural Networks on Dependency Structures"}
{"id": "task1540-28aa1afad79841b692739f8a5c131464", "input": "In this paper, we propose a novel technique for direct recognition of multiple speech streams given the single channel of mixed speech, without first separating them. Our technique is based on permutation invariant training (PIT) for automatic speech recognition (ASR). In PIT-ASR, we compute the average cross entropy (CE) over all frames in the whole utterance for each possible output-target assignment, pick the one with the minimum CE, and optimize for that assignment. PIT-ASR forces all the frames of the same speaker to be aligned with the same output layer. This strategy elegantly solves the label permutation problem and speaker tracing problem in one shot. Our experiments on artificially mixed AMI data showed that the proposed approach is very promising.", "target": "Recognizing Multi-talker Speech with Permutation Invariant Training"}
{"id": "task1540-b50d8b468ef8482681fe921192f64df6", "input": "Nearest neighbor methods are a popular class of nonparametric estimators with several<lb>desirable properties, such as adaptivity to different distance scales in different regions of<lb>space. Prior work on convergence rates for nearest neighbor classification has not fully<lb>reflected these subtle properties. We analyze the behavior of these estimators in metric<lb>spaces and provide finite-sample, distribution-dependent rates of convergence under min-<lb>imal assumptions. As a by-product, we are able to establish the universal consistency of<lb>nearest neighbor in a broader range of data spaces than was previously known. We illus-<lb>trate our upper and lower bounds by introducing smoothness classes that are customized<lb>for nearest neighbor classification.", "target": "Rates of Convergence for Nearest Neighbor Classification"}
{"id": "task1540-e49a16cd82774abf878ebe0ce612df1e", "input": "We examine an important setting for engineered systems in which low-power distributed sensors are each making highly noisy measurements of some unknown target function. A center wants to accurately learn this function by querying a small number of sensors, which ordinarily would be impossible due to the high noise rate. The question we address is whether local communication among sensors, together with natural best-response dynamics in an appropriately-defined game, can denoise the system without destroying the true signal and allow the center to succeed from only a small number of active queries. By using techniques from game theory and empirical processes, we prove positive (and negative) results on the denoising power of several natural dynamics. We then show experimentally that when combined with recent agnostic active learning algorithms, this process can achieve low error from very few queries, performing substantially better than active or passive learning without these denoising dynamics as well as passive learning with denoising.", "target": "Active Learning and Best-Response Dynamics"}
{"id": "task1540-3aa96c7df9684eea84e49358d6803a0e", "input": "In principle, reinforcement learning and policy search methods can enable robots to learn highly complex and general skills that may allow them to function amid the complexity and diversity of the real world. However, training a policy that generalizes well across a wide range of realworld conditions requires far greater quantity and diversity of experience than is practical to collect with a single robot. Fortunately, it is possible for multiple robots to share their experience with one another, and thereby, learn a policy collectively. In this work, we explore distributed and asynchronous policy learning as a means to achieve generalization and improved training times on challenging, real-world manipulation tasks. We propose a distributed and asynchronous version of Guided Policy Search and use it to demonstrate collective policy learning on a vision-based door opening task using four robots. We show that it achieves better generalization, utilization, and training times than the single robot alternative.", "target": "Collective Robot Reinforcement Learning with Distributed Asynchronous Guided Policy Search"}
{"id": "task1540-a93685b35a0b4bc1922f66816476bf4c", "input": "A Long Short-Term Memory (LSTM) network is a type of recurrent neural network architecture which has recently obtained strong results on a variety of sequence modeling tasks. The only underlying LSTM structure that has been explored so far is a linear chain. However, natural language exhibits syntactic properties that would naturally combine words to phrases. We introduce the Tree-LSTM, a generalization of LSTMs to tree-structured network topologies. TreeLSTMs outperform all existing systems and strong LSTM baselines on two tasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task 1) and sentiment classification (Stanford Sentiment Treebank).", "target": "Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks"}
{"id": "task1540-fb56327fe07741aebcf9d35b06f02a18", "input": "The era of Big Data has spawned unprecedented interests in developing hashing algorithms for efficient storage and fast nearest neighbor search. Most existing work learn hash functions that are numeric quantizations of feature values in projected feature space. In this work, we propose a novel hash learning framework that encodes feature\u2019s rank orders instead of numeric values in a number of optimal low-dimensional ranking subspaces. We formulate the ranking subspace learning problem as the optimization of a piecewise linear convex-concave function and present two versions of our algorithm: one with independent optimization of each hash bit and the other exploiting a sequential learning framework. Our work is a generalization of the Winner-TakeAll (WTA) hash family and naturally enjoys all the numeric stability benefits of rank correlation measures while being optimized to achieve high precision at very short code length. We compare with several state-of-the-art hashing algorithms in both supervised and unsupervised domain, showing superior performance in a number of data sets.", "target": "Rank Subspace Learning for Compact Hash Codes"}
{"id": "task1540-7d015deaaa1147d39a8d5b412dcd3cbe", "input": "Reasoning and inference are central to human and artificial intelligence. Modeling inference in human language is very challenging. With the availability of large annotated data (Bowman et al., 2015), it has recently become feasible to train neural network based inference models, which have shown to be very effective. In this paper, we present a new state-of-the-art result, achieving the accuracy of 88.6% on the Stanford natural language inference dataset. Unlike the previous top models that use very complicated network architectures, we first demonstrate that carefully designing sequential inference models based on chain LSTMs can outperform all previous models. Based on this, we further show that by explicitly considering recursive architectures in both local inference modeling and inference composition, we achieve additional improvement. Particularly, incorporating syntactic parsing information contributes to our best result\u2014it further improves the performance even when added to the already very strong model.", "target": "Enhanced LSTM for Natural Language Inference"}
{"id": "task1540-38ea0b341c2d455687ad1a5b911aeaf7", "input": "Applications and systems are constantly faced with decisions to make, often using a policy to pick from a set of actions based on some contextual information. We create a service that uses machine learning to accomplish this goal. The service uses exploration, logging, and online learning to create a counterfactually sound system supporting a full data lifecycle. The system is general: it works for any discrete choices, with respect to any reward metric, and can work with many learning algorithms and feature representations. The service has a simple API, and was designed to be modular and reproducible to ease deployment and debugging, respectively. We demonstrate how these properties enable learning systems that are robust and safe. Our evaluation shows that the Decision Service makes decisions in real time and incorporates new data quickly into learned policies. A large-scale deployment for a personalized news website has been handling all traffic since Jan. 2016, resulting in a 25% relative lift in clicks. By making the Decision Service externally available, we hope to make optimal decision making available to all.", "target": "A Multiworld Testing Decision Service"}
{"id": "task1540-7fb93f4307bb4aab99c4495acdb4f030", "input": "We introduce a new type of graphical model that we call a \u201cmemory factor network\u201d (MFN). We show how to use MFNs to model the structure inherent in many types of data sets. We also introduce an associated message-passing style algorithm called \u201cproactive message passing\u201d (PMP) that performs inference on MFNs. PMP comes with convergence guarantees and is efficient in comparison to competing algorithms such as variants of belief propagation. We specialize MFNs and PMP to a number of distinct types of data (discrete, continuous, labelled) and inference problems (interpolation, hypothesis testing), provide examples, and discuss approaches for efficient implementation.", "target": "Proactive Message Passing on Memory Factor Networks Proactive Message Passing on Memory Factor Networks"}
{"id": "task1540-9b5569d70cb94134924e90d24dbc5723", "input": "Zero-sum stochastic games are easy to solve as they can be cast as simple Markov decision processes. This is however not the case with general-sum stochastic games. A fairly general optimization problem formulation is available for general-sum stochastic games by Filar and Vrieze [2004]. However, the optimization problem there has a non-linear objective and non-linear constraints with special structure. Since gradients of both the objective as well as constraints of this optimization problem are well defined, gradient based schemes seem to be a natural choice. We discuss a gradient scheme tuned for two-player stochastic games. We show in simulations that this scheme indeed converges to a Nash equilibrium, for a simple terrain exploration problem modelled as a general-sum stochastic game. However, it turns out that only global minima of the optimization problem correspond to Nash equilibria of the underlying general-sum stochastic game, while gradient schemes only guarantee convergence to local minima. We then provide important necessary conditions for gradient schemes to converge to Nash equilibria in general-sum stochastic games.", "target": "A Study of Gradient Descent Schemes for General-Sum Stochastic Games"}
{"id": "task1540-1aa0336b774c44bda5ed521fa5d60257", "input": "We study the problem of learning domain invariant representations for time series data while transferring the complex temporal latent dependencies between domains. Our model termed as Variational Recurrent Adversarial Deep Domain Adaptation (VRADA) is built atop a variational recurrent neural network (VRNN) and trains adversarially to capture complex temporal relationships that are domain-invariant. This is (as far as we know) the first to capture and transfer temporal latent dependencies of multivariate time-series data. Through experiments on real-world multivariate healthcare time-series datasets, we empirically demonstrate that learning temporal dependencies helps our model\u2019s ability to create domain-invariant representations, allowing our model to outperform current state-of-the-art deep domain adaptation approaches.", "target": "VARIATIONAL RECURRENT ADVERSARIAL DEEP DOMAIN ADAPTATION"}
{"id": "task1540-4bc7fe6a832342f191e9aacad2de650e", "input": "In this article, how word embeddings can be used as features in Chinese sentiment classification is presented. Firstly, a Chinese opinion corpus is built with a million comments from hotel review websites. Then the word embeddings which represent each comment are used as input in different machine learning methods for sentiment classification, including SVM, Logistic Regression, Convolutional Neural Network (CNN) and ensemble methods. These methods get better performance compared with N-gram models using Naive Bayes (NB) and Maximum Entropy (ME). Finally, a combination of machine learning methods is proposed which presents an outstanding performance in precision, recall and F1 score. After selecting the most useful methods to construct the combinational model and testing over the corpus, the final F1 score is 0.920.", "target": "An Empirical Study on Sentiment Classification of Chinese Review using Word Embedding"}
{"id": "task1540-86fe51e8f61240519b6c55867aa6fce2", "input": "In this paper, we correct an upper bound, presented in [4], on the generalisation error of classifiers learned through multiple kernel learning. The bound in [4] uses Rademacher complexity and has anadditive dependence on the logarithm of the number of kernels and the margin achieved by the classifier. However, there are some errors in parts of the proof which are corrected in this paper. Unfortunately, the final result turns out to be a risk bound which has a multiplicative dependence on the logarithm of the number of kernels and the margin achieved by the classifier.", "target": "A Note on Improved Loss Bounds for Multiple Kernel Learning"}
{"id": "task1540-0dfaf8cfa9eb490bb7d63509975b46d3", "input": "Combining abstract, symbolic reasoning with continuous neural reasoning is a grand challenge of representation learning. As a step in this direction, we propose a new architecture, called neural equivalence networks, for the problem of learning continuous semantic representations of algebraic and logical expressions. These networks are trained to represent semantic equivalence, even of expressions that are syntactically very different. The challenge is that semantic representations must be computed in a syntax-directed manner, because semantics is compositional, but at the same time, small changes in syntax can lead to very large changes in semantics, which can be difficult for continuous neural architectures. We perform an exhaustive evaluation on the task of checking equivalence on a highly diverse class of symbolic algebraic and boolean expression types, showing that our model significantly outperforms existing architectures.", "target": "Learning Continuous Semantic Representations of Symbolic Expressions"}
{"id": "task1540-82f28f45e8594c379420ba4a8898ade5", "input": "Distributed knowledge based applications in open domain rely on common sense infor\u00ad mation which is bound to be uncertain and incomplete. To draw the useful conclusions from ambiguous data, one must address un\u00ad certainties and conflicts incurred in a holis\u00ad tic view. No integrated frameworks are vi\u00ad able without an in-depth analysis of con\u00ad flicts incurred by uncertainties. In this pa\u00ad per, we give such an analysis and based on the result, propose an integrated framework. Our framework extends definite argumenta\u00ad tion theory to model uncertainty. It sup\u00ad ports three views over conflicting and uncer\u00ad tain knowledge. Thus, knowledge engineers can draw different conclusions depending on the application context (i.e. view). We also give an illustrative example on strategical de\u00ad cision support to show the practical useful\u00ad ness of our framework.", "target": "Resolving Conflicting Arguments under Uncertainties"}
{"id": "task1540-46ff322a0e45486aba4df648da006575", "input": "We introduce a model for bidirectional retrieval of images and sentences through a multi-modal embedding of visual and natural language data. Unlike previous models that directly map images or sentences into a common embedding space, our model works on a finer level and embeds fragments of images (objects) and fragments of sentences (typed dependency tree relations) into a common space. In addition to a ranking objective seen in previous work, this allows us to add a new fragment alignment objective that learns to directly associate these fragments across modalities. Extensive experimental evaluation shows that reasoning on both the global level of images and sentences and the finer level of their respective fragments significantly improves performance on image-sentence retrieval tasks. Additionally, our model provides interpretable predictions since the inferred intermodal fragment alignment is explicit.", "target": "Deep Fragment Embeddings for Bidirectional Image Sentence Mapping"}
{"id": "task1540-91e7695ed6e44b679523e589dc59ea44", "input": "There is a brief description of the probabilistic causal graph model for representing, reasoning with, and learn\u00ad ing causal structure using Bayesian networks. It is then argued that this model is closely related to how humans reason with and learn causal structure. It is shown that studies in psychology on discounting (reasoning concern\u00ad ing how the presence of one cause of an effect makes an\u00ad other cause less probable) support the hypothesis that humans reach the same judgments as algorithms for do\u00ad ing inference in Bayesian networks. Next, it is shown how studies by Piaget indicate that humans learn causal structure by observing the same independencies and de\u00ad pendencies as those used by certain algorithms for learn\u00ad ing the structure of a Bayesian network. Based on this indication, a subjective definition of causality is for\u00ad warded. Finally, methods for further testing the accu\u00ad racy of these claims are discussed.", "target": "THE CoGNITIVE PROCESSING OF CAUSAL KNOWLEDGE"}
{"id": "task1540-a1e9097d07e6490fba9c69ad3ced1dc6", "input": "In this paper, we consider the problem of predicting demographics of geographic units given geotagged Tweets that are composed within these units. Traditional survey methods that offer demographics estimates are usually limited in terms of geographic resolution, geographic boundaries, and time intervals. Thus, it would be highly useful to develop computational methods that can complement traditional survey methods by offering demographics estimates at finer geographic resolutions, with flexible geographic boundaries (i.e. not confined to administrative boundaries), and at different time intervals. While prior work has focused on predicting demographics and health statistics at relatively coarse geographic resolutions such as the county-level or state-level, we introduce an approach to predict demographics at finer geographic resolutions such as the blockgroup-level. For the task of predicting gender and race/ethnicity counts at the blockgrouplevel, an approach adapted from prior work to our problem achieves an average correlation of 0.389 (gender) and 0.569 (race) on a held-out test dataset. Our approach outperforms this prior approach with an average correlation of 0.671 (gender) and 0.692 (race).", "target": "Predicting Demographics of High-Resolution Geographies with Geotagged Tweets"}
{"id": "task1540-042b4e172cda4d179df8d06d83fe76c9", "input": "A totally semantic measure is presented which is able to calculate a similarity value between concept descriptions and also between concept description and individual or between individuals expressed in an expressive description logic. It is applicable on symbolic descriptions although it uses a numeric approach for the calculus. Considering that Description Logics stand as the theoretic framework for the ontological knowledge representation and reasoning, the proposed measure can be effectively used for agglomerative and divisional clustering task applied to the semantic web domain.", "target": "A Semantic Similarity Measure for Expressive Description Logics"}
{"id": "task1540-d1156ec2f3a344fba8fdf79b3329c7a3", "input": "We present LTLS, a technique for multiclass and multilabel prediction that can perform training and inference in logarithmic time and space. LTLS embeds large classification problems into simple structured prediction problems and relies on efficient dynamic programming algorithms for inference. We train LTLS with stochastic gradient descent on a number of multiclass and multilabel datasets and show that despite its small memory footprint it is often competitive with existing approaches.", "target": "Log-time and Log-space Extreme Classification"}
{"id": "task1540-edd2af67a0bd46c0bca09ffb1422b8a8", "input": "We study response selection for multi-turn conversation in retrieval based chatbots. Existing works either ignores relationships among utterances, or misses important information in context when matching a response with a highly abstract context vector finally. We propose a new session based matching model to address both problems. The model first matches a response with each utterance on multiple granularities, and distills important matching information from each pair as a vector with convolution and pooling operations. The vectors are then accumulated in a chronological order through a recurrent neural network (RNN) which models the relationships among the utterances. The final matching score is calculated with the hidden states of the RNN. Empirical study on two public data sets shows that our model can significantly outperform the state-of-the-art methods for response selection in multi-turn conversation.", "target": "Sequential Match Network: A New Architecture for Multi-turn Response Selection in Retrieval-based Chatbots"}
{"id": "task1540-ded30079206a41cfbc4389ffaf5c159a", "input": "We propose a simple, scalable, and fast gradient descent algorithm to optimize a nonconvex objective for the rank minimization problem and a closely related family of semidefinite programs. WithO(r3\u03ba2n log n) random measurements of a positive semidefinite n\u00d7nmatrix of rank r and condition number \u03ba, our method is guaranteed to converge linearly to the global optimum.", "target": "A Convergent Gradient Descent Algorithm for Rank Minimization and Semidefinite Programming from Random Linear Measurements"}
{"id": "task1540-e940f2b987894d45984597d183160b16", "input": "Novel research in the field of Linked Data focuses on the problem of entity summarization. This field addresses the problem of ranking features according to their importance for the task of identifying a particular entity. Next to a more human friendly presentation, these summarizations can play a central role for semantic search engines and semantic recommender systems. In current approaches, it has been tried to apply entity summarization based on patterns that are inherent to the regarded data. The proposed approach of this paper focuses on the movie domain. It utilizes usage data in order to support measuring the similarity between movie entities. Using this similarity it is possible to determine the k-nearest neighbors of an entity. This leads to the idea that features that entities share with their nearest neighbors can be considered as significant or important for these entities. Additionally, we introduce a downgrading factor (similar to TF-IDF) in order to overcome the high number of commonly occurring features. We exemplify the approach based on a movie-ratings dataset that has been linked to Freebase entities.", "target": "Leveraging Usage Data for Linked Data Movie Entity Summarization"}
{"id": "task1540-49d2976e14af48128cba20515004d588", "input": "The purported \u201cblack box\u201d nature of neural networks is a barrier to adoption in applications where interpretability is essential. Here we present DeepLIFT (Learning Important FeaTures), an efficient and effective method for computing importance scores in a neural network. DeepLIFT compares the activation of each neuron to its \u2018reference activation\u2019 and assigns contribution scores according to the difference. We apply DeepLIFT to models trained on natural images and genomic data, and show significant advantages over gradient-based methods.", "target": "Not Just A Black Box:  Interpretable Deep Learning by Propagating Activation Differences"}
{"id": "task1540-1313b1cb1f304c90a32a45e8b2e4482a", "input": "We present a novel natural language generation system for spoken dialogue systems capable of entraining (adapting) to users\u2019 way of speaking, providing contextually appropriate responses. The generator is based on recurrent neural networks and the sequence-to-sequence approach. It is fully trainable from data which include preceding context along with responses to be generated. We show that the context-aware generator yields significant improvements over the baseline in both automatic metrics and a human pairwise preference test.", "target": "A Context-aware Natural Language Generator for Dialogue Systems"}
{"id": "task1540-e7104f7ec7bf40c0b06877264e34f5cf", "input": "Automatic text summarization is widely regarded as the highly difficult problem, partially because of the lack of large text summarization data set. Due to the great challenge of constructing the large scale summaries for full text, in this paper, we introduce a large corpus of Chinese short text summarization dataset constructed from the Chinese microblogging website Sina Weibo, which is released to the public1. This corpus consists of over 2 million real Chinese short texts with short summaries given by the author of each text. We also manually tagged the relevance of 10,666 short summaries with their corresponding short texts. Based on the corpus, we introduce recurrent neural network for the summary generation and achieve promising results, which not only shows the usefulness of the proposed corpus for short text summarization research, but also provides a baseline for further research on this topic.", "target": "LCSTS: A Large Scale Chinese Short Text Summarization Dataset"}
{"id": "task1540-191902fac8294c32bbf30e6fda5d418a", "input": "With a weighting scheme proportional to t, a traditional stochastic gradient descent (SGD) algorithm achieves a high probability convergence rate of O(\u03ba/T ) for strongly convex functions, instead of O(\u03ba ln(T )/T ). We also prove that an accelerated SGD algorithm also achieves a rate of O(\u03ba/T ).", "target": "Stochastic gradient descent algorithms for strongly convex functions at O(1/T ) convergence rates"}
{"id": "task1540-38160ae075524ec39c8050e5a049b968", "input": "In this paper, we discussed CNF-SAT problem (NP-Complete problem) and analysis two solutions that can solve the problem, the PL-Resolution algorithm and the WalkSAT algorithm. PL-Resolution is a sound and complete algorithm that can be used to determine satisfiability and unsatisfiability with certainty. WalkSAT can determine satisfiability if it finds a model, but it cannot guarantee to find a model even there exists one. However, WalkSAT is much faster than PL-Resolution, which makes WalkSAT more practical; and we have analysis the performance between these two algorithms, and the performance of WalkSAT is acceptable if the problem is not so hard.", "target": "A novel approach of solving the CNF-SAT problem"}
{"id": "task1540-3fbd10f8e2594f51a9365836fc4f0621", "input": "More and more of the information on the web is dialogic, from Facebook newsfeeds, to forum conversations, to comment threads on news articles. In contrast to traditional, monologic resources such as news, highly social dialogue is very frequent in social media, as illustrated in the snippets in Fig. 1 from the publicly available Internet Argument Corpus (IAC) (Walker et al., 2012). Utterances are frequently sarcastic, e.g., Really? Well, when I have a kid, I\u2019ll be sure to just leave it in the woods, since it can apparently care for itself (R2 in Fig. 1 as well as Q1 and R1), and are often nasty, (R2 in Fig. 1). Note also the frequent use of dialogue specific discourse cues, e.g. the use of No in R1, Really? Well in R2, and okay, well in Q3 in Fig. 1 (Fox Tree and Schrock, 1999; Bryant and Fox Tree, 2002; Fox Tree, 2010).", "target": "Identifying Subjective and Figurative Language in Online Dialogue"}
{"id": "task1540-d8bea45d25ef4328aed8eec02f1bf82b", "input": "Pre-trained word embeddings learned from unlabeled text have become a standard component of neural network architectures for NLP tasks. However, in most cases, the recurrent network that operates on word-level representations to produce context sensitive representations is trained on relatively little labeled data. In this paper, we demonstrate a general semi-supervised approach for adding pretrained context embeddings from bidirectional language models to NLP systems and apply it to sequence labeling tasks. We evaluate our model on two standard datasets for named entity recognition (NER) and chunking, and in both cases achieve state of the art results, surpassing previous systems that use other forms of transfer or joint learning with additional labeled data and task specific gazetteers.", "target": "Semi-supervised sequence tagging with bidirectional language models"}
{"id": "task1540-7fa4be6a64114ca19428108b664de763", "input": "In mechanical design, there is often unavoidable uncertainty in estimates of design performance. Evaluation of design alternatives requires consideration of the impact of this uncertainty. Expert heuristics embody assumptions regarding the designer's attitude towards risk and uncertainty that might be reasonable in most cases but inaccurate in others. We present a technique to allow designers to incorporate their own unique attitude towards uncertainty as opposed to those assumed by the domain expert's rules. The general approach is to eliminate aspects of heuristic rules which directly or indirectly include assumptions regarding the user's attitude towards risk, and replace them with explicit , user-specified probabilistic multiattribute utility and probability distribution functions. We illustrate the method in a system for material selection for automobile bumpers.", "target": "A Method for Integrating Utility Analysis into an Expert System for Design Evaluation under Uncertainty"}
{"id": "task1540-45b67b8e8aba46119cba06e776a916ac", "input": "We investigate the `\u221e-constrained representation which demonstrates robustness to quantization errors, utilizing the tool of deep learning. Based on the Alternating Direction Method of Multipliers (ADMM), we formulate the original convex minimization problem as a feed-forward neural network, named Deep `\u221e Encoder, by introducing the novel Bounded Linear Unit (BLU) neuron and modeling the Lagrange multipliers as network biases. Such a structural prior acts as an effective network regularization, and facilitates the model initialization. We then investigate the effective use of the proposed model in the application of hashing, by coupling the proposed encoders under a supervised pairwise loss, to develop a Deep Siamese `\u221e Network, which can be optimized from end to end. Extensive experiments demonstrate the impressive performances of the proposed model. We also provide an in-depth analysis of its behaviors against the competitors.", "target": "Learning A Deep `\u221e Encoder for Hashing"}
{"id": "task1540-d6c3c06c561f450586b7ac997292b2d7", "input": "We describe two new related resources that facilitate modelling of general knowledge reasoning in 4th grade science exams. The first is a collection of curated facts in the form of tables, and the second is a large set of crowd-sourced multiple-choice questions covering the facts in the tables. Through the setup of the crowd-sourced annotation task we obtain implicit alignment information between questions and tables. We envisage that the resources will be useful not only to researchers working on question answering, but also to people investigating a diverse range of other applications such as information extraction, question parsing, answer type identification, and lexical semantic modelling.", "target": "TabMCQ: A Dataset of General Knowledge Tables and Multiple-choice Questions"}
{"id": "task1540-625e3832faa7419a949e515647ad8ea3", "input": "We describe a technique to minimize weighted tree automata (WTA), a powerful formalisms that subsumes probabilistic context-free grammars (PCFGs) and latent-variable PCFGs. Our method relies on a singular value decomposition of the underlying Hankel matrix defined by the WTA. Our main theoretical result is an efficient algorithm for computing the SVD of an infinite Hankel matrix implicitly represented as a WTA. We provide an analysis of the approximation error induced by the minimization, and we evaluate our method on real-world data originating in newswire treebank. We show that the model achieves lower perplexity than previous methods for PCFG minimization, and also is much more stable due to the absence of local optima.", "target": "Weighted Tree Automata Approximation by Singular Value Truncation"}
{"id": "task1540-10f848302e0a4ddc95e88f877be2c7fc", "input": "In this paper we extend neural Turing machine (NTM) into a dynamic neural Turing machine (D-NTM) by introducing a trainable memory addressing scheme. This scheme maintains for each memory cell two separate vectors, content and address vectors. This allows the D-NTM to learn a wide variety of location-based addressing strategies including both linear and nonlinear ones. We implement the D-NTM with both soft, differentiable and hard, non-differentiable read/write mechanisms. We investigate the mechanisms and effects for learning to read and write to a memory through experiments on Facebook bAbI tasks using both a feedforward and GRU-controller. The D-NTM is evaluated on a set of the Facebook bAbI tasks and shown to outperform NTM and LSTM baselines.", "target": "Dynamic Neural Turing Machine with Soft and Hard Addressing Schemes"}
{"id": "task1540-44237767979746da87e6cc3c09473089", "input": "An efficient policy search algorithm should estimate the local gradient of the objective function, with respect to the policy parameters, from as few trials as possible. Whereas most policy search methods estimate this gradient by observing the rewards obtained during policy trials, we show, both theoretically and empirically, that taking into account the sensor data as well gives better gradient estimates and hence faster learning. The reason is that rewards obtained during policy execution vary from trial to trial due to noise in the environment; sensor data, which correlates with the noise, can be used to partially correct for this variation, resulting in an estimator with lower variance.", "target": "Improving Gradient Estimation by Incorporating Sensor Data"}
{"id": "task1540-4b7209fb52594d53812fc5830de28a84", "input": "Summit work of the Spanish Golden Age and forefather of the so-called picaresque novel, The Life of Lazarillo de Tormes and of His Fortunes and Adversities still remains an anonymous text. Although distinguished scholars have tried to attribute it to different authors based on a variety of criteria, a consensus has yet to be reached. The list of candidates is long and not all of them enjoy the same support within the scholarly community. Analyzing their works from a data-driven perspective and applying machine learning techniques for style and text fingerprinting, we shed light on the authorship of the Lazarillo. As in a state-of-the-art survey, we discuss the methods used and how they perform in our specific case. According to our methodology, the most likely author seems to be Juan Arce de Ot\u00e1lora, closely followed by Alfonso de Vald\u00e9s. The method states that not certain attribution can be made with the given corpus.", "target": "The Life of Lazarillo de Tormes and of His Machine Learning Adversities"}
{"id": "task1540-25b6d6dffbbc4a758ad3e672b7904f97", "input": "This paper presents a new algorithm for online linear regression whose efficiency guarantees satisfy the requirements of the KWIK (Knows What It Knows) framework. The algorithm improves on the complexity bounds of the current state-of-the-art procedure in this setting. We explore several applications of this algorithm for learning compact reinforcement-learning representations. We show that KWIK linear regression can be used to learn the reward function of a factored MDP and the probabilities of action outcomes in Stochastic STRIPS and Object Oriented MDPs, none of which have been proven to be efficiently learnable in the RL setting before. We also combine KWIK linear regression with other KWIK learners to learn larger portions of these models, including experiments on learning factored MDP transition and reward functions together.", "target": "Exploring compact reinforcement-learning representations with linear regression"}
{"id": "task1540-02314a75792448b4acc2419bf1e052ce", "input": "Relation extraction is a fundamental task in information extraction. Most existing methods have heavy reliance on annotations labeled by human experts, which are costly and time-consuming. To overcome this drawback, we propose a novel framework, REHESSION, to conduct relation extractor learning using annotations from heterogeneous information source, e.g., knowledge base and domain heuristics. These annotations, referred as heterogeneous supervision, often conflict with each other, which brings a new challenge to the original relation extraction task: how to infer the true label from noisy labels for a given instance. Identifying context information as the backbone of both relation extraction and true label discovery, we adopt embedding techniques to learn the distributed representations of context, which bridges all components with mutual enhancement in an iterative fashion. Extensive experimental results demonstrate the superiority of REHESSION over the state-of-the-art.", "target": "Heterogeneous Supervision for Relation Extraction: A Representation Learning Approach"}
{"id": "task1540-22e6e93e902e48a49dd88fc6482309ba", "input": "Clustering is one of the widely used data mining techniques for medical diagnosis. Clustering can be considered as the most important unsupervised learning technique. Most of the clustering methods group data based on distance and few methods cluster data based on similarity. The clustering algorithms classify gene expression data into clusters and the functionally related genes are grouped together in an efficient manner. The groupings are constructed such that the degree of relationship is strong among members of the same cluster and weak among members of different clusters. In this work, we focus on a similarity relationship among genes with similar expression patterns so that a consequential and simple analytical decision can be made from the proposed Fuzzy Soft Rough K-Means algorithm. The algorithm is developed based on Fuzzy Soft sets and Rough sets. Comparative analysis of the proposed work is made with bench mark algorithms like K-Means and Rough K-Means and efficiency of the proposed algorithm is illustrated in this work by using various cluster validity measures such as DB index and Xie-Beni index.", "target": "FUZZY SOFT ROUGH K-MEANS CLUSTERING APPROACH FOR GENE EXPRESSION DATA"}
{"id": "task1540-c06e2b7b8d9a47cc85daf3700971fb67", "input": "We apply multilayer bootstrap network (MBN), a recent pro-<lb>posed unsupervised learning method, to unsupervised speaker<lb>recognition. The proposed method first extracts supervectors<lb>from an unsupervised universal background model, then re-<lb>duces the dimension of the high-dimensional supervectors by<lb>multilayer bootstrap network, and finally conducts unsuper-<lb>vised speaker recognition by clustering the low-dimensional<lb>data. The comparison results with 2 unsupervised and 1 su-<lb>pervised speaker recognition techniques demonstrate the ef-<lb>fectiveness and robustness of the proposed method.", "target": "MULTILAYER BOOTSTRAP NETWORK FOR UNSUPERVISED SPEAKER RECOGNITION"}
{"id": "task1540-d3fcffe8d11841d796a833980d9f28ca", "input": "We consider principal component analysis for contaminated data-set in the high dimensional regime, where the dimensionality of each observation is comparable or even more than the number of observations. We propose a deterministic high-dimensional robust PCA algorithm which inherits all theoretical properties of its randomized counterpart, i.e., it is tractable, robust to contaminated points, easily kernelizable, asymptotic consistent and achieves maximal robustness \u2013 a breakdown point of 50%. More importantly, the proposed method exhibits significantly better computational efficiency, which makes it suitable for large-scale real applications.", "target": "Robust PCA in High-dimension: A Deterministic Approach"}
{"id": "task1540-2f621410160d452ea061f5a9cd2fc972", "input": "Consider the following fundamental estimation problem: there are n entities, each with an unknown parameter pi \u2208 [0, 1], and we observe n independent random variables,X1, . . . , Xn, withXi \u223cBinomial(t, pi). How accurately can one recover the \u201chistogram\u201d (i.e. cumulative density function) of the pis? While the empirical estimates would recover the histogram to earth mover distance \u0398( 1 \u221a t ) (equivalently, `1 distance between the CDFs), we show that, provided n is sufficiently large, we can achieve error O( 1t ) which is information theoretically optimal. We also extend our results to the multi-dimensional parameter case, capturing settings where each member of the population has multiple associated parameters. Beyond the theoretical results, we demonstrate that the recovery algorithm performs well in practice on a variety of datasets, providing illuminating insights into several domains, including politics, and sports analytics.", "target": "Optimally Learning Populations of Parameters"}
{"id": "task1540-7c0d54eba5404366b76200fe9c0a6054", "input": "Modern malware is designed with mutation characteristics, namely polymorphism and metamorphism, which causes an enormous growth in the number of variants of malware samples. Categorization of malware samples on the basis of their behaviors is essential for the computer security community in order to group samples belonging to same family. Microsoft released a malware classification challenge in 2015 with a huge dataset of near 0.5 terabytes of data, containing more than 20K malware samples. The analysis of this dataset inspired the development of a novel paradigm that is effective in categorizing malware variants into their actual family groups. This paradigm is presented and discussed in the present paper, where emphasis has been given to the phases related to the extraction, and selection of a set of novel features for the effective representation of malware samples. Features can be grouped according to different characteristics of malware behavior, and their fusion is performed according to a per-class weighting paradigm. The proposed method achieved a very high accuracy (\u2248 0.998) on the Microsoft Malware Challenge dataset.", "target": "Novel feature extraction, selection and fusion for effective malware family classification"}
{"id": "task1540-d2645add112e4272bb1a05319cdbdd78", "input": "We propose a technique to detect and generate patterns in a network of locally interacting dynamical systems. Central to our approach is a novel spatial superposition logic, whose semantics is defined over the quad-tree of a partitioned image. We show that formulas in this logic can be efficiently learned from positive and negative examples of several types of patterns. We also demonstrate that pattern detection, which is implemented as a model checking algorithm, performs very well for test data sets different from the learning sets. We define a quantitative semantics for the logic and integrate the model checking algorithm with particle swarm optimization in a computational framework for synthesis of parameters leading to desired patterns in reaction-diffusion systems.", "target": "A Formal Methods Approach to Pattern Synthesis in Reaction Diffusion Systems"}
{"id": "task1540-29c5a21f5649499b8c11aa5add78a136", "input": "Selectional preferences have long been claimed to be essential for coreference resolution. However, they are mainly modeled only implicitly by current coreference resolvers. We propose a dependencybased embedding model of selectional preferences which allows fine-grained compatibility judgments with high coverage. We show that the incorporation of our model improves coreference resolution performance on the CoNLL dataset, matching the state-of-the-art results of a more complex system. However, it comes with a cost that makes it debatable how worthwhile such improvements are.", "target": "Revisiting Selectional Preferences for Coreference Resolution"}
{"id": "task1540-14cf22376a554aa2a017c18bb85ca078", "input": "Increasing the capacity of recurrent neural networks (RNN) usually involves augmenting the size of the hidden layer, resulting in a significant increase of computational cost. An alternative is the recurrent neural tensor network (RNTN), which increases capacity by employing distinct hidden layer weights for each vocabulary word. The disadvantage of RNTNs is that memory usage scales linearly with vocabulary size, which can reach millions for word-level language models. In this paper, we introduce restricted recurrent neural tensor networks (r-RNTN) which reserve distinct hidden layer weights for frequent vocabulary words while sharing a single set of weights for infrequent words. Perplexity evaluations using the Penn Treebank corpus show that r-RNTNs improve language model performance over standard RNNs using only a small fraction of the parameters of unrestricted RNTNs.", "target": "Restricted Recurrent Neural Tensor Networks"}
{"id": "task1540-39656f4691874e69a360bbea4dc34280", "input": "To be considered for the 2017 IEEE Jack Keil Wolf ISIT Student Paper Award. Interaction information is one of the multivariate generalizations of mutual information, which expresses the amount information shared among a set of variables, beyond the information, which is shared in any proper subset of those variables. Unlike (conditional) mutual information, which is always non-negative, interaction information can be negative. We utilize this property to find the direction of causal influences among variables in a triangle topology under some mild assumptions.", "target": "Interaction Information for Causal Inference: The Case of Directed Triangle"}
{"id": "task1540-7a3269a954414eb8807aa9136e954a31", "input": "An approach to incorporate deep learning within an iterative image reconstruction framework to reconstruct images from severely incomplete measurement data is presented. Specifically, we utilize a convolutional neural network (CNN) as a quasi-projection operator within a least squares minimization procedure. The CNN is trained to encode high level information about the class of images being imaged; this information is utilized to mitigate artifacts in intermediate images produced by use of an iterative method. The structure of the method was inspired by the proximal gradient descent method, where the proximal operator is replaced by a deep CNN and the gradient descent step is generalized by use of a linear reconstruction operator. It is demonstrated that this approach improves image quality for several cases of limited-view image reconstruction and that using a CNN in an iterative method increases performance compared to conventional image reconstruction approaches. We test our method on several limited-view image reconstruction problems. Qualitative and quantitative results demonstrate state-of-the-art performance.", "target": "Deep Learning-Guided Image Reconstruction from Incomplete Data"}
{"id": "task1540-a2649f61c6a84315a858028a45f50d8c", "input": "We present a practical and statistically consistent scheme for actively learning binary classifiers under general loss functions. Our algorithm uses importance weighting to correct sampling bias, and by controlling the variance, we are able to give rigorous label complexity bounds for the learning process. Experiments on passively labeled data show that this approach reduces the label complexity required to achieve good predictive performance on many learning problems.", "target": "Importance Weighted Active Learning"}
{"id": "task1540-4021eeefb56e48d1b9f35486a42187c5", "input": "We present a practical approach for processing mobile sensor time series data for continual deep learning predictions. The approach comprises data cleaning, normalization, capping, time-based compression, and finally classification with a recurrent neural network. We demonstrate the effectiveness of the approach in a case study with 279 participants. On the basis of sparse sensor events, the network continually predicts whether the participants would attend to a notification within 10 minutes. Compared to a random baseline, the classifier achieves a 40% performance increase (AUC of 0.702) on a withheld test set. This approach allows to forgo resource-intensive, domain-specific, error-prone feature engineering, which may drastically increase the applicability of machine learning to mobile phone sensor data.", "target": "Practical Processing of Mobile Sensor Data for Continual Deep Learning Predictions"}
{"id": "task1540-30988980adef4ec4899fe96dc273b5e2", "input": "We introduce a copula mixture model to perform dependency-seeking clustering when cooccurring samples from different data sources are available. The model takes advantage of the great flexibility offered by the copulas framework to extend mixtures of Canonical Correlation Analysis to multivariate data with arbitrary continuous marginal densities. We formulate our model as a non-parametric Bayesian mixture, while providing efficient MCMC inference. Experiments on synthetic and real data demonstrate that the increased flexibility of the copula mixture significantly improves the clustering and the interpretability of the results.", "target": "Copula Mixture Model for Dependency-seeking Clustering"}
{"id": "task1540-5eed8d144efe4146a3336aa936852f83", "input": "The problem of makespan optimal solving of cooperative path finding (CPF) is addressed in this paper. The task in CPF is to relocate a group of agents in a non-colliding way so that each agent eventually reaches its goal location from the given initial location. The abstraction adopted in this work assumes that agents are discrete items moving in an undirected graph by traversing edges. Makespan optimal solving of CPF means to generate solutions that are as short as possible in terms of the total number of time steps required for the execution of the solution. We show that reducing CPF to propositional satisfiability (SAT) represents a viable option for obtaining makespan optimal solutions. Several encodings of CPF into propositional formulae are suggested and experimentally evaluated. The evaluation indicates that SAT based CPF solving outperforms other makespan optimal methods significantly in highly constrained situations (environments that are densely occupied by agents).", "target": "Makespan Optimal Solving of Cooperative Path-Finding"}
{"id": "task1540-04a74f0735384985a3dc654cede8eb9c", "input": "Understanding a long document requires tracking how entities are introduced and evolve over time. We present a new type of language model, ENTITYNLM, that can explicitly model entities, dynamically update their representations, and contextually generate their mentions. Our model is generative and flexible; it can model an arbitrary number of entities in context while generating each entity mention at an arbitrary length. In addition, it can be used for several different tasks such as language modeling, coreference resolution, and entity prediction. Experimental results with all these tasks demonstrate that our model consistently outperforms strong baselines and prior work.", "target": "Dynamic Entity Representations in Neural Language Models"}
{"id": "task1540-2cf53a29e942455389799d92663712c7", "input": "In this paper we deal with a new approach to probabilistic reasoning in a logical frame\u00ad work. Nearly almost all logics of probabil\u00ad ity that have been proposed in the litera\u00ad ture are based on classical two-valued logic. After making clear the differences between fuzzy logic and probability theory, here we propose a fuzzy logic of probability for which completeness results (in a probabilistic sense) are provided. The main idea behind this approach is that probability values of crisp propositions can be understood as truth\u00ad values of some suitable fuzzy propositions as\u00ad sociated to the crisp ones. Moreover, sug\u00ad gestiotlS and examples of how to extend the formalism to cope with conditional probabil\u00ad ities and with other uncertainty formalisms are also provided.", "target": "Fuzzy logic and probability"}
{"id": "task1540-6c51b494ad164c9c872cba92a43fc445", "input": "We propose a probabilistic video model, the Video Pixel Network (VPN), that estimates the discrete joint distribution of the raw pixel values in a video. The model and the neural architecture reflect the time, space and color structure of video tensors and encode it as a four-dimensional dependency chain. The VPN approaches the best possible performance on the Moving MNIST benchmark, a leap over the previous state of the art, and the generated videos show only minor deviations from the ground truth. The VPN also produces detailed samples on the action-conditional Robotic Pushing benchmark and generalizes to the motion of novel objects.", "target": "Video Pixel Networks"}
{"id": "task1540-a3db446d0f3944009019383c266188c7", "input": "In this paper, we propose a novel neural network model called RNN Encoder\u2013Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder\u2013Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.", "target": "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation"}
{"id": "task1540-070218d9cfca4a098a344d23ade024d8", "input": "In this paper we present an interface between a symbolic planner and a geometric task planner, which is different to a standard trajectory planner in that the former is able to perform geometric reasoning on abstract entities\u2014tasks. We believe that this approach facilitates a more principled interface to symbolic planning, while also leaving more room for the geometric planner to make independent decisions. We show how the two planners could be interfaced, and how their planning and backtracking could be interleaved. We also provide insights for a methodology for using the combined system, and experimental results to use as a benchmark with future extensions to both the combined system, as well as to the geometric task planner.", "target": "Towards Combining HTN Planning and Geometric Task Planning"}
{"id": "task1540-351b5217f745437dbafb540364998681", "input": "We introduce the first, general purpose, slice sampling inference engine for probabilistic programs. This engine is released as part of StocPy, a new Turing-Complete probabilistic programming language, available as a Python library. We present a transdimensional generalisation of slice sampling which is necessary for the inference engine to work on traces with different numbers of random variables. We show that StocPy compares favourably to other PPLs in terms of flexibility and usability, and that slice sampling can outperform previously introduced inference methods. Our experiments include a logistic regression, HMM, and Bayesian Neural Net.", "target": "Slice Sampling for Probabilistic Programming"}
{"id": "task1540-9f67d1de3d5c46b1bf8b1f78fd2d0133", "input": "In this work we develop Curvature Propagation (CP), a general technique for efficiently computing unbiased approximations of the Hessian of any function that is computed using a computational graph. At the cost of roughly two gradient evaluations, CP can give a rank-1 approximation of the whole Hessian, and can be repeatedly applied to give increasingly precise unbiased estimates of any or all of the entries of the Hessian. Of particular interest is the diagonal of the Hessian, for which no general approach is known to exist that is both efficient and accurate. We show in experiments that CP turns out to work well in practice, giving very accurate estimates of the Hessian of neural networks, for example, with a relatively small amount of work. We also apply CP to Score Matching, where a diagonal of a Hessian plays an integral role in the Score Matching objective, and where it is usually computed exactly using inefficient algorithms which do not scale to larger and more complex models.", "target": "Estimating the Hessian by Back-propagating Curvature"}
{"id": "task1540-1a95e56d89324ab5b4c8d26e465407ca", "input": "<lb>Sequentially learning to place items in multi-position displays or lists is a task that can be cast into the<lb>multiple-play semi-bandit setting. However, a major concern in this context is when the system cannot decide<lb>whether the user feedback for each item is actually exploitable. Indeed, much of the content may have been<lb>simply ignored by the user. The present work proposes to exploit available information regarding the display<lb>position bias under the so-called Position-based click model (PBM). We first discuss how this model differs<lb>from the Cascade model and its variants considered in several recent works on multiple-play bandits. We<lb>then provide a novel regret lower bound for this model as well as computationally efficient algorithms that<lb>display good empirical and theoretical performance.", "target": "Multiple-Play Bandits in the Position-Based Model"}
{"id": "task1540-68acc779aa6d4f5c93f667f4a93bfb79", "input": "Prepositions are very common and very ambiguous, and understanding their sense is critical for understanding the meaning of the sentence. Supervised corpora for the preposition-sense disambiguation task are small, suggesting a semi-supervised approach to the task. We show that signals from unannotated multilingual data can be used to improve supervised prepositionsense disambiguation. Our approach pre-trains an LSTM encoder for predicting the translation of a preposition, and then incorporates the pre-trained encoder as a component in a supervised classification system, and fine-tunes it for the task. The multilingual signals consistently improve results on two preposition-sense datasets.", "target": "Semi Supervised Preposition-Sense Disambiguation using Multilingual Data"}
{"id": "task1540-992683af9bb34481b370968f09b0b3d5", "input": "The Lumiere Project centers on harnessing probability and utility to provide assistance to computer software users. We review work on Bayesian user models that can be em\u00ad ployed to infer a user's needs by consider\u00ad ing a user's background, actions, and queries. Several problems were tackled in Lumiere research, including ( 1) the construction of Bayesian models for reasoning about the time-varying goals of computer users from their observed actions and queries, (2) gain\u00ad ing access to a stream of events from soft\u00ad ware applications, (3) developing a language for transforming system events into observa\u00ad tional variables represented in Bayesian user models, ( 4) developing persistent profiles to capture changes in a user's expertise, and (5) the development of an overall architecture for an intelligent user interface. Lumiere proto\u00ad types served as the basis for the Office Assis\u00ad tant in the Microsoft Office '97 suite of pro\u00ad ductivity applications.", "target": "The Lumiere Project: Bayesian User Modeling for Inferring the Goals and Needs of Software Users"}
{"id": "task1540-5cfd1780c4274989a5dc05ca86e45db5", "input": "The online Markov decision process (MDP) is a generalization of the classical Markov decision process that incorporates changing reward functions. In this paper, we propose practical online MDP algorithms with policy iteration and theoretically establish a sublinear regret bound. A notable advantage of the proposed algorithm is that it can be easily combined with function approximation, and thus large and possibly continuous state spaces can be efficiently handled. Through experiments, we demonstrate the usefulness of the proposed algorithm.", "target": "Online Markov decision processes with policy iteration"}
{"id": "task1540-6d596d617d0d4c1f983734091a865d35", "input": "Solving symmetric Bayesian decision prob\u00ad lems is a computationally intensive task to perform regardless of the algorithm used. In this paper we propose a method for improv\u00ad ing the efficiency of algorithms for solving Bayesian decision problems. The method is based on the principle of lazy evalua\u00ad tion a principle recently shown to improve the efficiency of inference in Bayesian net\u00ad works. The basic idea is to maintain de\u00ad compositions of potentials and to postpone computations for as long as possible. The efficiency improvements obtained with the lazy evaluation based method is emphasized through examples. Finally, the lazy evalu\u00ad ation based method is compared with the HUGIN and valuation-based systems architec\u00ad tures for solving symmetric Bayesian decision problems.", "target": "Lazy Evaluation of Symmetric Bayesian Decision Problems"}
{"id": "task1540-efdae6f7317849db9a1341aef15fbeac", "input": "Researches have shown accent classification can be improved by integrating semantic information into pure acoustic approach. In this work, we combine phonetic knowledge, such as vowels, with enhanced acoustic features to build an improved accent classification system. The classifier is based on Gaussian Mixture Model-Universal Background Model (GMM-UBM), with normalized Perceptual Linear Predictive (PLP) features. The features are further optimized by Principle Component Analysis (PCA) and Hetroscedastic Linear Discriminant Analysis (HLDA). Using 7 major types of accented speech from the Foreign Accented English (FAE) corpus, the system achieves classification accuracy 54% with input test data as short as 20 seconds, which is competitive to the state of the art in this field.", "target": "Improved Accent Classification Combining Phonetic Vowels with Acoustic Features"}
{"id": "task1540-df9c291c33d14c48b63ecf3ac2de8585", "input": "We provide tight upper and lower bounds on the complexity of minimizing the average of m convex functions using gradient and prox oracles of the component functions. We show a significant gap between the complexity of deterministic vs randomized optimization. For smooth functions, we show that accelerated gradient descent (AGD) and an accelerated variant of SVRG are optimal in the deterministic and randomized settings respectively, and that a gradient oracle is sufficient for the optimal rate. For non-smooth functions, having access to prox oracles reduces the complexity and we present optimal methods based on smoothing that improve over methods using just gradient accesses.", "target": "Tight Complexity Bounds for Optimizing Composite Objectives"}
{"id": "task1540-cafb84f7982a4a7488ae9e638192e02e", "input": "This paper presents generalized probabilistic models for high-order projective dependency parsing and an algorithmic framework for learning these statistical models involving dependency trees. Partition functions and marginals for high-order dependency trees can be computed efficiently, by adapting our algorithms which extend the inside-outside algorithm to higher-order cases. To show the effectiveness of our algorithms, we perform experiments on three languages\u2014 English, Chinese and Czech, using maximum conditional likelihood estimation for model training and L-BFGS for parameter estimation. Our methods achieve competitive performance for English, and outperform all previously reported dependency parsers for Chinese and Czech.", "target": "Probabilistic Models for High-Order Projective Dependency Parsing"}
{"id": "task1540-0c011bfbb8b64048919d3b2497938cbc", "input": "We introduce a new approach for disfluency detection using a Bidirectional Long-Short Term Memory neural network (BLSTM). In addition to the word sequence, the model takes as input pattern match features that were developed to reduce sensitivity to vocabuary size in training, which lead to improved performance over the word sequence alone. The BLSTM takes advantage of explicit repair states in addition to the standard reparandum states. The final output leverages integer linear programming to incorporate constraints of disluency structure. In experiments on the Switchboard corpus, the model achieves state-of-the-art performance for both the standard disfluency detection task and the correction detection task. Analysis shows that the model has better detection of non-repetition disfluencies, which tend to be much harder to detect.", "target": "Disfluency Detection using a Bidirectional LSTM"}
{"id": "task1540-64f584570a3f4dbea409f0945d7ca03a", "input": "The EM-algorithm is a general procedure to get maximum likelihood estimates if part of the observations on the variables of a network are missing. In this paper a stochastic version of the algorithm is adapted to probabilistic neural networks describing the associative dependency of variables. These networks have a proba\u00ad bility distribution, which is a special case of the distribution generated by proba\u00ad bilistic inference networks. Hence both types of networks can be combined al\u00ad lowing to integrate probabilistic rules as well as unspecified associations in a sound way. The resulting network may have a number of interesting features including cycles of probabilistic rules, hidden 'un\u00ad observable' variables, and uncertain and contradictory evidence.", "target": "Integrating Probabilistic Rules into Neural Networks: A Stochastic EM Learning Algorithm"}
{"id": "task1540-9f55e4460665415a8565588fb4dd4dac", "input": "Abstract Stochastic gradient descent (SGD) on a low-rank factorization [9] is commonly employed to speed up matrix problems including matrix completion, subspace tracking, and SDP relaxation. In this paper, we exhibit a step size scheme for SGD on a low-rank least-squares problem, and we prove that, under broad sampling conditions, our method converges globally from a random starting point within O(\u01ebn logn) steps with constant probability for constant-rank problems. Our modification of SGD relates it to stochastic power iteration. We also show experiments to illustrate the runtime and convergence of the algorithm.", "target": "Global Convergence of Stochastic Gradient Descent for Some Non-convex Matrix Problems"}
{"id": "task1540-a43254aae729496b80eeef970dc19480", "input": "Social Internet content plays an increasingly critical role in many domains, including public health, disaster management, and politics. However, its utility is limited by missing geographic information; for example, fewer than 1.6% of Twitter messages (tweets) contain a geotag. We propose a scalable, content-based approach to estimate the location of tweets using a novel yet simple variant of gaussian mixture models. Further, because real-world applications depend on quantified uncertainty for such estimates, we propose novel metrics of accuracy, precision, and calibration, and we evaluate our approach accordingly. Experiments on 13 million global, comprehensively multi-lingual tweets show that our approach yields reliable, well-calibrated results competitive with previous computationally intensive methods. We also show that a relatively small number of training data are required for good estimates (roughly 30,000 tweets) and models are quite time-invariant (effective on tweets many weeks newer than the training set). Finally, we show that toponyms and languages with small geographic footprint provide the most useful location signals.", "target": "Inferring the Origin Locations of Tweets with Quantitative Confidence"}
{"id": "task1540-3ada5b05e29746cf9a9f085940b2f37d", "input": "This paper introduces the Deep Recurrent Attentive Writer (DRAW) neural network architecture for image generation. DRAW networks combine a novel spatial attention mechanism that mimics the foveation of the human eye, with a sequential variational auto-encoding framework that allows for the iterative construction of complex images. The system substantially improves on the state of the art for generative models on MNIST, and, when trained on the Street View House Numbers dataset, it generates images that cannot be distinguished from real data with the naked eye.", "target": "DRAW: A Recurrent Neural Network For Image Generation"}
{"id": "task1540-74652770153a4a1e9745c5563d9e2bb7", "input": "This paper presents an investigation of the entropy of the Telugu script. Since this script is syllabic, and not alphabetic, the computation of entropy is somewhat complicated.", "target": "Entropy of Telugu"}
{"id": "task1540-3dfaa4d914c74af3a3702377bb7b07ec", "input": "Traditional image clustering methods take a two-step approach, feature learning and clustering, sequentially. However, recent research results demonstrated that combining the separated phases in a unified framework and training them jointly can achieve a better performance. In this paper, we first introduce fully convolutional auto-encoders for image feature learning and then propose a unified clustering framework to learn image representations and cluster centers jointly based on a fully convolutional auto-encoder and soft k-means scores. At initial stages of the learning procedure, the representations extracted from the auto-encoder may not be very discriminative for latter clustering. We address this issue by adopting a boosted discriminative distribution, where high score assignments are highlighted and low score ones are de-emphasized. With the gradually boosted discrimination, clustering assignment scores are discriminated and cluster purities are enlarged. Experiments on several vision benchmark datasets show that our methods can achieve a state-of-the-art performance.", "target": "Discriminatively Boosted Image Clustering with Fully Convolutional Auto-Encoders"}
{"id": "task1540-2d31a182f52e471b8b7d00f61f14b473", "input": "We propose to prune a random forest (RF) for resource-constrained prediction. We first construct a RF and then prune it to optimize expected feature cost & accuracy. We pose pruning RFs as a novel 0-1 integer program with linear constraints that encourages feature re-use. We establish total unimodularity of the constraint set to prove that the corresponding LP relaxation solves the original integer program. We then exploit connections to combinatorial optimization and develop an efficient primal-dual algorithm, scalable to large datasets. In contrast to our bottom-up approach, which benefits from good RF initialization, conventional methods are top-down acquiring features based on their utility value and is generally intractable, requiring heuristics. Empirically, our pruning algorithm outperforms existing state-of-the-art resource-constrained algorithms.", "target": "Pruning Random Forests for Prediction on a Budget"}
{"id": "task1540-affd8d316c014445b516abc6fc483ed8", "input": "Classification involves the learning of the mapping function that associates input samples to corresponding target label. There are two major categories of classification problems: Single-label classification and Multi-label classification. Traditional binary and multi-class classifications are subcategories of single-label classification. Several classifiers are developed for binary, multi-class and multi-label classification problems, but there are no classifiers available in the literature capable of performing all three types of classification. In this paper, a novel online universal classifier capable of performing all the three types of classification is proposed. Being a high speed online classifier, the proposed technique can be applied to streaming data applications. The performance of the developed classifier is evaluated using datasets from binary, multi-class and multi-label problems. The results obtained are compared with state-of-the-art techniques from each of the classification types. Keywords\u2014Universal, Classification, Binary, Multi-class, Multi-label, Online, Extreme learning machines, Data stream.", "target": "An Online Universal Classifier for Binary, Multi- class and Multi-label Classification"}
{"id": "task1540-2980ceba15a546f9aeb1f8e65159d734", "input": "Rough set theory, a mathematical tool to deal with vague concepts, has originally described the indiscernibility of elements by equivalence relations. Covering rough sets are a natural extension of classical rough sets by relaxing the partitions arising from equivalence relations to covers. Recently, some topological concepts such as neighborhood have been applied to covering rough sets. In this paper, we further investigate the covering rough sets based on neighborhoods by approximation operations. We show that the upper approximation based on neighborhoods can be defined equivalently without using neighborhoods. To analyze the covers themselves, we introduce unary and composition operations on covers. A notion of homomorphism is provided to relate two covering approximation spaces. We also examine the properties of approximations preserved by the operations and homomorphisms, respectively.", "target": "Covering rough sets based on neighborhoods"}
{"id": "task1540-03d1cc3b52c34c14a0ac36e40f2c5e90", "input": "In this paper, we present a joint compression and classification approach of EEG and EMG signals using a deep learning approach. Specifically, we build our system based on the deep autoencoder architecture which is designed not only to extract discriminant features in the multimodal data representation but also to reconstruct the data from the latent representation using encoder-decoder layers. Since autoencoder can be seen as a compression approach, we extend it to handle multimodal data at the encoder layer, reconstructed and retrieved at the decoder layer. We show through experimental results, that exploiting both multimodal data intercorellation and intracorellation 1) Significantly reduces signal distortion particularly for high compression levels 2) Achieves better accuracy in classifying EEG and EMG signals recorded and labeled according to the sentiments of the volunteer.", "target": "Multimodal deep learning approach for joint EEG-EMG data compression and classification"}
{"id": "task1540-6f23f51f22134181a004058a3e600ac3", "input": "We show how to efficiently project a vector onto the top principal components of a matrix, without explicitly computing these components. Specifically, we introduce an iterative algorithm that provably computes the projection using few calls to any black-box routine for ridge regression. By avoiding explicit principal component analysis (PCA), our algorithm is the first with no runtime dependence on the number of top principal components. We show that it can be used to give a fast iterative method for the popular principal component regression problem, giving the first major runtime improvement over the naive method of combining PCA with regression. To achieve our results, we first observe that ridge regression can be used to obtain a \u201csmooth projection\u201d onto the top principal components. We then sharpen this approximation to true projection using a low-degree polynomial approximation to the matrix step function. Step function approximation is a topic of long-term interest in scientific computing. We extend prior theory by constructing polynomials with simple iterative structure and rigorously analyzing their behavior under limited precision.", "target": "Principal Component Projection Without Principal Component Analysis"}
{"id": "task1540-452f2cc354e44835ac253f3978e8429b", "input": "Estimation of causal effects of interventions in dynamical systems of interacting agents is under-developed. In this paper, we explore the intricacies of this problem through standard approaches, and demonstrate the need for more appropriate methods. Working under the Neyman-Rubin causal model, we proceed to develop a causal inference method and we explicate the stability assumptions that are necessary for valid causal inference. Our method consists of a temporal component that models the evolution of behaviors that agents adopt over time, and a behavioral component that models the distribution of agent actions conditional on adopted behaviors. This allows the imputation of long-term estimates of quantities of interest, and thus the estimation of long-term causal effects of interventions. We demonstrate our method on a dataset from behavioral game theory, and discuss open problems to stimulate future research.", "target": "Statistical inference of long-term causal effects in multiagent systems under the Neyman-Rubin model"}
{"id": "task1540-b9ade4ba1b924c14824c5fdc657f9fa6", "input": "The ability to mimic human notions of semantic distance has widespread applications. Some measures rely only on raw text (distributional measures) and some rely on knowledge sources such as WordNet. Although extensive studies have been performed to compare WordNet-based measures with human judgment, the use of distributional measures as proxies to estimate semantic distance has received little attention. Even though they have traditionally performed poorly when compared to WordNet-based measures, they lay claim to certain uniquely attractive features, such as their applicability in resource-poor languages and their ability to mimic both semantic similarity and semantic relatedness. Therefore, this paper presents a detailed study of distributional measures. Particular attention is paid to flesh out the strengths and limitations of both WordNet-based and distributional measures, and how distributional measures of distance can be brought more in line with human notions of semantic distance. We conclude with a brief discussion of recent work on hybrid measures.", "target": "Distributional Measures of Semantic Distance: A Survey"}
{"id": "task1540-e217a70d61f44722929d5a225f011f68", "input": "The Dialog State Tracking Challenge 4 (DSTC 4) differentiates itself from the previous three editions as follows: the number of slot-value pairs present in the ontology is much larger, no spoken language understanding output is given, and utterances are labeled at the subdialog level. This paper describes a novel dialog state tracking method designed to work robustly under these conditions, using elaborate string matching, coreference resolution tailored for dialogs and a few other improvements. The method can correctly identify many values that are not explicitly present in the utterance. On the final evaluation, our method came in first among 7 competing teams and 24 entries. The F1-score achieved by our method was 9 and 7 percentage points higher than that of the runner-up for the utterance-level evaluation and for the subdialog-level evaluation, respectively.", "target": "Robust Dialog State Tracking for Large Ontologies"}
{"id": "task1540-7c09f85f578f460fbe1c15115afae877", "input": "Computing the probability of evidence even with known error bounds is NP-hard. In this paper we address this hard problem by settling on an easier problem. We propose an approximation which provides high confidence lower bounds on probability of evidence but does not have any guarantees in terms of relative or absolute error. Our proposed approximation is a randomized importance sampling scheme that uses the Markov inequality. However, a straight-forward application of the Markov inequality may lead to poor lower bounds. We therefore propose several heuristic measures to improve its performance in practice. Empirical evaluation of our scheme with stateof-the-art lower bounding schemes reveals the promise of our approach.", "target": "Studies in Lower Bounding Probability of Evidence using the Markov Inequality"}
{"id": "task1540-c6242a833e4c4db18e7ad72b2891317e", "input": "This work uses the L-system to construct a tree structure for the text sequence and derives its complexity [1]. It serves as a measure of structural complexity of the text. It is applied to anomaly detection in data transmission. Keyword: text complexity, anomaly detection, structural complexity, rewriting rule, context-free grammar, L-system", "target": "Syntactic sensitive complexity for symbol-free sequence"}
{"id": "task1540-c155e834f24e469d979d5ce5a00a6cbd", "input": "Transfer in reinforcement learning refers to the notion that generalization should occur not only within a task but also across tasks. Our focus is on transfer where the reward functions vary across tasks while the environment\u2019s dynamics remain the same. The method we propose rests on two key ideas: \u201csuccessor features,\u201d a value function representation that decouples the dynamics of the environment from the rewards, and \u201cgeneralized policy improvement,\u201d a generalization of dynamic programming\u2019s policy improvement step that considers a set of policies rather than a single one. Put together, the two ideas lead to an approach that integrates seamlessly within the reinforcement learning framework and allows transfer to take place between tasks without any restriction. The proposed method also provides performance guarantees for the transferred policy even before any learning has taken place. We derive two theorems that set our approach in firm theoretical ground and present experiments that show that it successfully promotes transfer in practice.", "target": "Successor Features for Transfer in Reinforcement Learning"}
{"id": "task1540-5a65673632194dee84c3c297cd626a79", "input": "We study the problem of learning the best Bayesian network structure with respect to a decomposable score such as BDe, BIC or AIC. This problem is known to be NP-hard, which means that solving it becomes quickly infeasible as the number of variables increases. Nevertheless, in this paper we show that it is possible to learn the best Bayesian network structure with over 30 variables, which covers many practically interesting cases. Our algorithm is less complicated and more efficient than the techniques presented earlier. It can be easily parallelized, and offers a possibility for efficient exploration of the best networks consistent with different variable orderings. In the experimental part of the paper we compare the performance of the algorithm to the previous state-of-the-art algorithm. Free source-code and an online-demo can be found at http://b-course.hiit.fi/bene.", "target": "A Simple Approach for Finding the Globally Optimal Bayesian Network Structure"}
{"id": "task1540-fe5318972cf7492ea6863de2d7b700bb", "input": "We introduce a simple, general framework for likelihood-free Bayesian reinforcement learning, through Approximate Bayesian Computation (ABC). The advantage is that we only require a prior distribution on a class of simulators. This is useful when a probabilistic model of the underlying process is too complex to formulate, but where detailed simulation models are available. ABC-RL allows the use of any Bayesian reinforcement learning technique in this case. It can be seen as an extension of simulation methods to both planning and inference. We experimentally demonstrate the potential of this approach in a comparison with LSPI. Finally, we introduce a theorem showing that ABC is sound.", "target": "ABC Reinforcement Learning"}
{"id": "task1540-5f36de3940ae44a4be3978e80009d9a4", "input": "Rohit and I go back a long way. We started talking about Dynamic Logic back when I was a graduate student, when we would meet at seminars at MIT (my advisor Albert Meyer was at MIT, although I was at Harvard, and Rohit was then at Boston University). Right from the beginning I appreciated Rohit\u2019s breadth, his quick insights, his wit, and his welcoming and gracious style. Rohit has been interested in the interplay between logic, philosophy, and language ever since I\u2019ve known him. Over the years, both of us have gotten interested in game theory. I would like to dedicate this short note, which discusses issues at the intersection of all these areas, to him.", "target": "Why Bother With Syntax?"}
{"id": "task1540-13d9796e7c03458facd22253cbff1bc1", "input": "First-order probabilistic models combine representational power of first-order logic with graphical models. There is an ongoing effort to design lifted inference algorithms for first-order probabilistic models. We analyze lifted inference from the perspective of constraint processing and, through this viewpoint, we analyze and compare existing approaches and expose their advantages and limitations. Our theoretical results show that the wrong choice of constraint processing method can lead to exponential increase in computational complexity. Our empirical tests confirm the importance of constraint processing in lifted inference. This is the first theoretical and empirical study of constraint processing in lifted inference.", "target": "Constraint Processing in Lifted Probabilistic Inference"}
{"id": "task1540-aa07353f6794459f84204d821b7bd473", "input": "Methods based on representation learning currently hold the state-of-the-art in many natural language processing and knowledge base inference tasks. Yet, a major challenge is how to efficiently incorporate commonsense knowledge into such models. A recent approach regularizes relation and entity representations by propositionalization of first-order logic rules. However, propositionalization does not scale beyond domains with only few entities and rules. In this paper we present a highly efficient method for incorporating implication rules into distributed representations for automated knowledge base construction. We map entity-tuple embeddings into an approximately Boolean space and encourage a partial ordering over relation embeddings based on implication rules mined from WordNet. Surprisingly, we find that the strong restriction of the entity-tuple embedding space does not hurt the expressiveness of the model and even acts as a regularizer that improves generalization. By incorporating few commonsense rules, we achieve an increase of 2 percentage points mean average precision over a matrix factorization baseline, while observing a negligible increase in runtime.", "target": "Lifted Rule Injection for Relation Embeddings"}
{"id": "task1540-393c88577c3c46598f1045bc5d784b5b", "input": "Cross-document coreference, the problem of resolving entity mentions across multi-document collections, is crucial to automated knowledge base construction and data mining tasks. However, the scarcity of large labeled data sets has hindered supervised machine learning research for this task. In this paper we develop and demonstrate an approach based on \u201cdistantly-labeling\u201d a data set from which we can train a discriminative cross-document coreference model. In particular we build a dataset of more than a million people mentions extracted from 3.5 years of New York Times articles, leverage Wikipedia for distant labeling with a generative model (and measure the reliability of such labeling); then we train and evaluate a conditional random field coreference model that has factors on cross-document entities as well as mention-pairs. This coreference model obtains high accuracy in resolving mentions and entities that are not present in the training data, indicating applicability to non-Wikipedia data. Given the large amount of data, our work is also an exercise demonstrating the scalability of our approach.", "target": "Distantly Labeling Data for Large Scale Cross-Document Coreference"}
{"id": "task1540-d9d7798ee8e942adb806779532045046", "input": "Cooperative pathfinding is a problem of finding a set of non-conflicting trajectories for a number of mobile agents. Its applications include planning for teams of mobile robots, such as autonomous aircrafts, cars, or underwater vehicles. The state-of-the-art algorithms for cooperative pathfinding typically rely on some heuristic forward-search pathfinding technique, where A* is often the algorithm of choice. Here, we propose MA-RRT*, a novel algorithm for multi-agent path planning that builds upon a recently proposed asymptotically-optimal sampling-based algorithm for finding single-agent shortest path called RRT*. We experimentally evaluate the performance of the algorithm and show that the sampling-based approach offers better scalability than the classical forward-search approach in relatively large, but sparse environments, which are typical in real-world applications such as multi-aircraft collision avoidance.", "target": "Multi-agent RRT*: Sampling-based Cooperative Pathfinding"}
{"id": "task1540-083bbe36b573457d9efdf76b55051bfe", "input": "One of the core components of modern spoken dialogue systems is the belief tracker, which estimates the user\u2019s goal at every step of the dialogue. However, most current approaches have difficulty scaling to larger, more complex dialogue domains. This is due to their dependency on either: a) Spoken Language Understanding models that require large amounts of annotated training data; or b) hand-crafted lexicons for capturing some of the linguistic variation in users\u2019 language. We propose a novel Neural Belief Tracking (NBT) framework which overcomes these problems by building on recent advances in representation learning. NBT models reason over pre-trained word vectors, learning to compose them into distributed representations of user utterances and dialogue context. Our evaluation on two datasets shows that this approach surpasses past limitations, matching the performance of state-of-the-art models which rely on hand-crafted semantic lexicons and outperforming them when such lexicons are not provided.", "target": "Neural Belief Tracker: Data-Driven Dialogue State Tracking"}
{"id": "task1540-25b1f16f5d094745a8f88edcdee36ecf", "input": "We present a novel neural network model that learns POS tagging and graph-based dependency parsing jointly. Our model uses bidirectional LSTMs to learn feature representations shared for both POS tagging and dependency parsing tasks, thus handling the feature-engineering problem. Our extensive experiments, on 19 languages from the Universal Dependencies project, show that our model outperforms the state-of-the-art neural networkbased Stack-propagation model for joint POS tagging and transition-based dependency parsing, resulting in a new state of the art. Our code is open-source and available at: https://github.com/ datquocnguyen/jPTDP.", "target": "A Novel Neural Network Model for Joint POS Tagging and Graph-based Dependency Parsing"}
{"id": "task1540-540d4fd402ed419f910fec1d4e50ceda", "input": "In this paper, we consider the patient similarity matching problem over a cancer cohort of more than 220,000 patients. Our approach first leverages on Word2Vec framework to embed ICD codes into vector-valued representation. We then propose a sequential algorithm for case-control matching on this representation space of diagnosis codes. The novel practice of applying the sequential matching on the vector representation lifted the matching accuracy measured through multiple clinical outcomes. We reported the results on a large-scale dataset to demonstrate the effectiveness of our method. For such a large dataset where most clinical information has been codified, the new method is particularly relevant.", "target": "Control Matching via Discharge Code Sequences"}
{"id": "task1540-4140bd680c3a42bcb44885e22f4b8abb", "input": "In this work, we develop a simple algorithm for semi-supervised regression. The key idea is to use the top eigenfunctions of integral operator derived from both labeled and unlabeled examples as the basis functions and learn the prediction function by a simple linear regression. We show that under appropriate assumptions about the integral operator, this approach is able to achieve an improved regression error bound better than existing bounds of supervised learning. We also verify the effectiveness of the proposed algorithm by an empirical study.", "target": "A Simple Algorithm for Semi-supervised Learning with Improved Generalization Error Bound"}
{"id": "task1540-04c8f1f43c7746ba9a08386db48b2054", "input": "Statistical Relational Learning (SRL) methods have shown that classification accuracy can be improved by integrating relations between samples. Techniques such as iterative classification or relaxation labeling achieve this by propagating information between related samples during the inference process. When only a few samples are labeled and connections between samples are sparse, collective inference methods have shown large improvements over standard feature-based ML methods. However, in contrast to feature based ML, collective inference methods require complex inference procedures and often depend on the strong assumption of label consistency among related samples. In this paper, we introduce new relational features for standard ML methods by extracting information from direct and indirect relations. We show empirically on three standard benchmark datasets that our relational features yield results comparable to collective inference methods. Finally we show that our proposal outperforms these methods when additional information is available.", "target": "Graph Based Relational Features for Collective Classification"}
{"id": "task1540-a08ce95480244762b7fbb0bc370fec6b", "input": "We theoretically analyze and compare the following five popular multiclass classification methods: One vs. All, All Pairs, Tree-based classifiers, Error Correcting Output Codes (ECOC) with randomly generated code matrices, and Multiclass SVM. In the first four methods, the classification is based on a reduction to binary classification. We consider the case where the binary classifier comes from a class of VC dimension d, and in particular from the class of halfspaces over R. We analyze both the estimation error and the approximation error of these methods. Our analysis reveals interesting conclusions of practical relevance, regarding the success of the different approaches under various conditions. Our proof technique employs tools from VC theory to analyze the approximation error of hypothesis classes. This is in sharp contrast to most, if not all, previous uses of VC theory, which only deal with estimation error.", "target": "Multiclass Learning Approaches: A Theoretical Comparison with Implications"}
{"id": "task1540-1ed67470024f4cb3923c85a75e354da8", "input": "Clustering is an effective technique in data mining to generate groups that are the matter of interest. Among various clustering approaches, the family of k-means algorithms and min-cut algorithms gain most popularity due to their simplicity and efficacy. The classical k-means algorithm partitions a number of data points into several subsets by iteratively updating the clustering centers and the associated data points. By contrast, a weighted undirected graph is constructed in min-cut algorithms which partition the vertices of the graph into two sets. However, existing clustering algorithms tend to cluster minority of data points into a subset, which shall be avoided when the target dataset is balanced. To achieve more accurate clustering for balanced dataset, we propose to leverage exclusive lasso on k-means and min-cut to regulate the balance degree of the clustering results. By optimizing our objective functions that build atop the exclusive lasso, we can make the clustering result as much balanced as possible. Extensive experiments on several large-scale datasets validate the advantage of the proposed algorithms compared to the state-of-the-art clustering algorithms.", "target": "Balanced k-Means and Min-Cut Clustering"}
{"id": "task1540-060be754058148df86792f075b2e02d1", "input": "In last few years there are major changes and evolution has been done on classification of data. As the application area of technology is increases the size of data also increases. Classification of data becomes difficult because of unbounded size and imbalance nature of data. Class imbalance problem become greatest issue in data mining. Imbalance problem occur where one of the two classes having more sample than other classes. The most of algorithm are more focusing on classification of major sample while ignoring or misclassifying minority sample. The minority samples are those that rarely occur but very important. There are different methods available for classification of imbalance data set which is divided into three main categories, the algorithmic approach, datapreprocessing approach and feature selection approach. Each of this technique has their own advantages and disadvantages. In this paper systematic study of each approach is define which gives the right direction for research in class imbalance problem.", "target": "Class Imbalance Problem in Data Mining: Review"}
{"id": "task1540-a615128f8a764c5eb77350be159fff2f", "input": "The AGM model is the most remarkable framework for modeling belief revision. However, it is not perfect in all aspects. Paraconsistent belief revision, multi-agent belief revision and non-prioritized belief revision are three different extensions to AGM to address three important criticisms applied to it. In this article, we propose a framework based on AGM that takes a position in each of these categories. Also, we discuss some features of our framework and study the satisfiability of AGM postulates in this new context.", "target": "Source-Sensitive Belief Change"}
{"id": "task1540-52a0da7e094d4e2d84cb6d1e38c3ac17", "input": "Cooperative games model the allocation of profit from joint actions, following considerations such as stability and fairness. We propose the reliability extension of such games, where agents may fail to participate in the game. In the reliability extension, each agent only \u201csurvives\u201d with a certain probability, and a coalition\u2019s value is the probability that its surviving members would be a winning coalition in the base game. We study prominent solution concepts in such games, showing how to approximate the Shapley value and how to compute the core in games with few agent types. We also show that applying the reliability extension may stabilize the game, making the core non-empty even when the base game has an empty core.", "target": "Solving Cooperative Reliability Games"}
{"id": "task1540-9948c75eef674b3cb89f52b8275700ff", "input": "Harnessing the statistical power of neural networks to perform language understanding and symbolic reasoning is difficult, when it requires executing efficient discrete operations against a large knowledge-base. In this work, we introduce a Neural Symbolic Machine, which contains (a) a neural \u201cprogrammer\u201d, i.e., a sequence-to-sequence model that maps language utterances to programs and utilizes a key-variable memory to handle compositionality (b) a symbolic \u201ccomputer\u201d, i.e., a Lisp interpreter that performs program execution, and helps find good programs by pruning the search space. We apply REINFORCE to directly optimize the task reward of this structured prediction problem. To train with weak supervision and improve the stability of REINFORCE we augment it with an iterative maximum-likelihood process. NSM outperforms state-of-the-art on the WEBQUESTIONSSP dataset when trained from question-answer pairs only, without requiring any feature engineering or domain-specific knowledge.", "target": "Neural Symbolic Machines: Learning Semantic Parsers on Freebase with Weak Supervision"}
{"id": "task1540-71b85e37e87b46c29ee7f6c0d2bbb58e", "input": "We consider learning a sequence classifier without labeled data by using sequential output statistics. The problem is highly valuable since obtaining labels in training data is often costly, while the sequential output statistics (e.g., language models) could be obtained independently of input data and thus with low or no cost. To address the problem, we propose an unsupervised learning cost function and study its properties. We show that, compared to earlier works, it is less inclined to be stuck in trivial solutions and avoids the need for a strong generative model. Although it is harder to optimize in its functional form, a stochastic primal-dual gradient method is developed to effectively solve the problem. Experiment results on real-world datasets demonstrate that the new unsupervised learning method gives drastically lower errors than other baseline methods. Specifically, it reaches test errors about twice of those obtained by fully supervised learning.", "target": "Unsupervised Sequence Classification using Sequential Output Statistics"}
{"id": "task1540-0a3b185a4e8749aeb25cf81219388848", "input": "The lack of diversity in a genetic algorithm\u2019s population may lead to a bad performance of the genetic operators since there is not an equilibrium between exploration and exploitation. In those cases, genetic algorithms present a fast and unsuitable convergence. In this paper we develop a novel hybrid genetic algorithm which attempts to obtain a balance between exploration and exploitation. It confronts the diversity problem using the named greedy diversification operator. Furthermore, the proposed algorithm applies a competition between parent and children so as to exploit the high quality visited solutions. These operators are complemented by a simple selection mechanism designed to preserve and take advantage of the population diversity. Additionally, we extend our proposal to the field of memetic algorithms, obtaining an improved model with outstanding results in practice. The experimental study shows the validity of the approach as well as how important is taking into account the exploration and exploitation concepts when designing an evolution-", "target": "GENETIC AND MEMETIC ALGORITHM WITH DIVERSITY EQUILIBRIUM BASED ON GREEDY DIVERSIFICATION"}
{"id": "task1540-f04f99ec0002421a840d07c026eda144", "input": "The computational cost of many signal processing and machine learning techniques is often dominated by the cost of applying certain linear operators to high-dimensional vectors. This paper introduces an algorithm aimed at reducing the complexity of applying linear operators in high dimension by approximately factorizing the corresponding matrix into few sparse factors. The approach relies on recent advances in non-convex optimization. It is first explained and analyzed in details and then demonstrated experimentally on various problems including dictionary learning for image denoising, and the approximation of large matrices arising in inverse problems.", "target": "Flexible Multi-layer Sparse Approximations of Matrices and Applications"}
{"id": "task1540-7496c276b2bb4fcd9186d04b77e251ee", "input": "We explore beyond existing work on learning from demonstration by asking the question: \u201cCan robots learn to teach?\u201d, that is, can a robot autonomously learn an instructional policy from expert demonstration and use it to instruct or collaborate with humans in executing complex tasks in uncertain environments? In this paper we pursue a solution to this problem by leveraging the idea that humans often implicitly decompose a higher level task into several subgoals whose execution brings the task closer to completion. We propose Dirichlet process based non-parametric Inverse Reinforcement Learning (DPMIRL) approach for reward based unsupervised clustering of task space into subgoals. This approach is shown to capture the latent subgoals that a human teacher would have utilized to train a novice. The notion of \u201caction primitive\u201d is introduced as the means to communicate instruction policy to humans in the least complicated manner, and as a computationally efficient tool to segment demonstration data. We evaluate our approach through experiments on hydraulic actuated scaled model of an excavator and evaluate and compare different teaching strategies utilized by the robot.", "target": "Can Co-robots Learn to Teach?"}
{"id": "task1540-b55a95dfa1a542c4a6d597724ad6dd26", "input": "We provide two fundamental results on the population (infinite-sample) likelihood function of Gaussian mixture models with M \u2265 3 components. Our first main result shows that the population likelihood function has bad local maxima even in the special case of equally-weighted mixtures of well-separated and spherical Gaussians. We prove that the log-likelihood value of these bad local maxima can be arbitrarily worse than that of any global optimum, thereby resolving an open question of Srebro [21]. Our second main result shows that the EM algorithm (or a first-order variant of it) with random initialization will converge to bad critical points with probability at least 1 \u2212 e. We further establish that a first-order variant of EM will not converge to strict saddle points almost surely, indicating that the poor performance of the first-order method can be attributed to the existence of bad local maxima rather than bad saddle points. Overall, our results highlight the necessity of careful initialization when using the EM algorithm in practice, even when applied in highly favorable settings.", "target": "Local Maxima in the Likelihood of Gaussian Mixture Models: Structural Results and Algorithmic Consequences"}
{"id": "task1540-0ef4cfd5a11c4acb859862d736b69dfa", "input": "The Complete Tang Poems (CTP) is the most important source to study Tang poems. We look into CTP with computational tools from specific linguistic perspectives, including distributional semantics and collocational analysis. From such quantitative viewpoints, we compare the usage of \u201cwind\u201d and \u201cmoon\u201d in the poems of Li Bai (\u674e\u767d) and Du Fu (\u675c\u752b). Colors in poems function like sounds in movies, and play a crucial role in the imageries of poems. Thus, words for colors are studied, and \u201c\u767d\u201d (bai2, white) is the main focus because it is the most frequent color in CTP. We also explore some cases of using colored words in antithesis(\u5c0d\u4ed7) pairs that were central for fostering the imageries of the poems. CTP also contains useful historical information, and we extract person names in CTP to study the social networks of the Tang poets. Such information can then be integrated with the China Biographical Database of Harvard University.", "target": "Color Aesthetics and Social Networks in Complete Tang Poems: Explorations and Discoveries"}
{"id": "task1540-2d38c991646c4fd9a98f19c3d5b10d94", "input": "In multilingual question answering, either the question needs to be translated into the document language, or vice versa. In addition to direction, there are multiple methods to perform the translation, four of which we explore in this paper: word-based, 10-best, contextbased, and grammar-based. We build a feature for each combination of translation direction and method, and train a model that learns optimal feature weights. On a large forum dataset consisting of posts in English, Arabic, and Chinese, our novel learn-to-translate approach was more effective than a strong baseline (p < 0.05): translating all text into English, then training a classifier based only on English (original or translated) text.", "target": "Learning to Translate for Multilingual Question Answering"}
{"id": "task1540-31b2f35ceef847b8b2ee9b3eee52c6ee", "input": "Type-level word embeddings use the same set of parameters to represent all instances of a word regardless of its context, ignoring the inherent lexical ambiguity in language. Instead, we embed semantic concepts (or synsets) as defined in WordNet and represent a word token in a particular context by estimating a distribution over relevant semantic concepts. We use the new, context-sensitive embeddings in a model for predicting prepositional phrase (PP) attachments and jointly learn the concept embeddings and model parameters. We show that using context-sensitive embeddings improves the accuracy of the PP attachment model by 5.4% absolute points, which amounts to a 34.4% relative reduction in errors.", "target": "Using Ontology-Grounded Token Embeddings To Predict Prepositional Phrase Attachments"}
{"id": "task1540-4d5948af680e40739e884a95f40c590c", "input": "Eliminating the negative effect of highly non-stationary environmental noise is a long-standing research topic for speech recognition but remains an important challenge nowadays. To address this issue, traditional unsupervised signal processing methods seem to have touched the ceiling. However, data-driven based supervised approaches, particularly the ones designed with deep learning, have recently emerged as potential alternatives. In this light, we are going to comprehensively summarise the recently developed and most representative deep learning approaches to deal with the raised problem in this article, with the aim of providing guidelines for those who are going deeply into the field of environmentally robust speech recognition. To better introduce these approaches, we categorise them into singleand multi-channel techniques, each of which is specifically described at the front-end, the back-end, and the joint framework of speech recognition systems. In the meanwhile, we describe the pros and cons of these approaches as well as the relationships among them, which can probably benefit future research.", "target": "Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments"}
{"id": "task1540-48dcee66c0eb4340acd98f5047fc3140", "input": "Motivated by runtime verification of QoS requirements in self-adaptive and self-organizing systems that are able to reconfigure their structure and behavior in response to runtime data, we propose a QoS-aware variant of Thompson sampling for multi-armed bandits. It is applicable in settings where QoS satisfaction of an arm has to be ensured with high confidence efficiently, rather than finding the optimal arm while minimizing regret. Preliminary experimental results encourage further research in the field of QoS-aware decision making.", "target": "QoS-Aware Multi-Armed Bandits"}
{"id": "task1540-4046ccd880be4adca9173f7cad81fa46", "input": "We study the task of online boosting \u2014 combining online weak learners into an online strong learner. While batch boosting has a sound theoretical foundation, online boosting deserves more study from the theoretical perspective. In this paper, we carefully compare the differences between online and batch boosting, and propose a novel and reasonable assumption for the online weak learner. Based on the assumption, we design an online boosting algorithm with a strong theoretical guarantee by adapting from the offline SmoothBoost algorithm that matches the assumption closely. We further tackle the task of deciding the number of weak learners using established theoretical results for online convex programming and predicting with expert advice. Experiments on real-world data sets demonstrate that the proposed algorithm compares favorably with existing online boosting algorithms.", "target": "An Online Boosting Algorithm with Theoretical Justifications"}
{"id": "task1540-36d79d159c7e48f295de413fd26fce17", "input": "Software estimation is a crucial task in software engineering. Software estimation encompasses cost, effort, schedule, and size. The importance of software estimation becomes critical in the early stages of the software life cycle when the details of software have not been revealed yet. Several commercial and non-commercial tools exist to estimate software in the early stages. Most software effort estimation methods require software size as one of the important metric inputs and consequently, software size estimation in the early stages becomes essential. One of the approaches that has been used for about two decades in the early size and effort estimation is called use case points. Use case points method relies on the use case diagram to estimate the size and effort of software projects. Although the use case points method has been widely used, it has some limitations that might adversely affect the accuracy of estimation. This paper presents some techniques using fuzzy logic and neural networks to improve the accuracy of the use case points method. Results showed that an improvement up to 22% can be obtained using the proposed approach.", "target": "Enhancing Use Case Points Estimation Method Using Soft Computing Techniques"}
{"id": "task1540-0989861a5599494fba18801bf0a71639", "input": "A neighborhood graph, which represents the instances as vertices and their relations as weighted edges, is the basis of many semi-supervised and relational models for node labeling and link prediction. Most methods employ a sequential process to construct the neighborhood graph. This process often consists of generating a candidate graph, pruning the candidate graph to make a neighborhood graph, and then performing inference on the variables (i.e., nodes) in the neighborhood graph. In this paper, we propose a framework that can dynamically adapt the neighborhood graph based on the states of variables from intermediate inference results, as well as structural properties of the relations connecting them. A key strength of our framework is its ability to handle multi-relational data and employ varying amounts of relations for each instance based on the intermediate inference results. We formulate the link prediction task as inference on neighborhood graphs, and include preliminary results illustrating the effects of different strategies in our proposed framework.", "target": "Adaptive Neighborhood Graph Construction for Inference in Multi-Relational Networks"}
{"id": "task1540-9902526cfaf1483abf5bce13f6b0cb53", "input": "Participants in recent discussions of AI-related issues ranging from intelligence explosion to technological unemployment have made diverse claims about the nature, pace, and drivers of progress in AI. However, these theories are rarely specified in enough detail to enable systematic evaluation of their assumptions or to extrapolate progress quantitatively, as is often done with some success in other technological domains. After reviewing relevant literatures and justifying the need for more rigorous modeling of AI progress, this paper contributes to that research program by suggesting ways to account for the relationship between hardware speed increases and algorithmic improvements in AI, the role of human inputs in enabling AI capabilities, and the relationships between different sub-fields of AI. It then outlines ways of tailoring AI progress models to generate insights on the specific issue of technological unemployment, and outlines future directions for research on AI progress.", "target": "Modeling Progress in AI"}
{"id": "task1540-5de7d386c07a4dd784f9c1593b7871ef", "input": "Neural machine translation aims at building a single large neural network that can be trained to maximize translation performance. The encoder-decoder architecture with an attention mechanism achieves a translation performance comparable to the existing state-of-the-art phrase-based systems. However, the use of large vocabulary becomes the bottleneck in both training and improving the performance. In this paper, we propose a novel architecture which learns morphology by using two recurrent networks and a hierarchical decoder which translates at character level. This gives rise to a deep character-level model consisting of six recurrent networks. Such a deep model has two major advantages. It avoids the large vocabulary issue radically; at the same time, it is more efficient in training than word-based models. Our model obtains a higher BLEU score than the bpe-based model after training for one epoch on En-Fr and En-Cs translation tasks. Further analyses show that our model is able to learn morphology.", "target": "DEEP CHARACTER-LEVEL NEURAL MACHINE TRANSLATION BY LEARNING MORPHOLOGY"}
{"id": "task1540-b9303471aa2a4f8f910e220274ac67c5", "input": "Predicting the next activity of a running process is an important aspect of process management. Recently, artificial neural networks, so called deep-learning approaches, have been proposed to address this challenge. This demo paper describes a software application that applies the Tensorflow deep-learning framework to process prediction. The software application reads industry-standard XES files for training and presents the user with an easy-to-use graphical user interface for both training and prediction. The system provides several improvements over earlier work. This demo paper focuses on the software implementation and describes the architecture and user interface.", "target": "XES Tensorflow \u2013 Process Prediction using the Tensorflow Deep-Learning Framework"}
{"id": "task1540-31e8da96587a45ca9b79f48318235226", "input": "Attempts to train a comprehensive artificial intelligence capable of solving multiple tasks have been impeded by a chronic problem called catastrophic forgetting. Although simply replaying all previous data alleviates the problem, it requires large memory and even worse, often infeasible in real world applications where the access to past data is limited. Inspired by the generative nature of hippocampus as a short-term memory system in primate brain, we propose the Deep Generative Replay, a novel framework with a cooperative dual model architecture consisting of a deep generative model (\u201cgenerator\u201d) and a task solving model (\u201csolver\u201d). With only these two models, training data for previous tasks can easily be sampled and interleaved with those for a new task. We test our methods in several sequential learning settings involving image classification tasks.", "target": "Continual Learning with Deep Generative Replay"}
{"id": "task1540-5e95e4c53d09454b8f8e5f085aa94c99", "input": "Online learning aims to perform nearly as well as the best hypothesis in hindsight. For some hypothesis classes, though, even finding the best hypothesis offline is challenging. In such offline cases, local search techniques are often employed and only local optimality guaranteed. For online decision-making with such hypothesis classes, we introduce local regret, a generalization of regret that aims to perform nearly as well as only nearby hypotheses. We then present a general algorithm to minimize local regret with arbitrary locality graphs. We also show how the graph structure can be exploited to drastically speed learning. These algorithms are then demonstrated on a diverse set of online problems: online disjunct learning, online Max-SAT, and online decision tree learning.", "target": "On Local Regret"}
{"id": "task1540-719530ffa7214d318a2236d49d606f0d", "input": "SentiWordNet is an important lexical resource supporting sentiment analysis in opinion mining applications. In this paper, we propose a novel approach to construct a Vietnamese SentiWordNet (VSWN). SentiWordNet is typically generated from WordNet in which each synset has numerical scores to indicate its opinion polarities. Many previous studies obtained these scores by applying a machine learning method to WordNet. However, Vietnamese WordNet is not available unfortunately by the time of this paper. Therefore, we propose a method to construct VSWN from a Vietnamese dictionary, not from WordNet. We show the effectiveness of the proposed method by generating a VSWN with 39,561 synsets automatically. The method is experimentally tested with 266 synsets with aspect of positivity and negativity. It attains a competitive result compared with English SentiWordNet that is 0.066 and 0.052 differences for positivity and negativity sets respectively.", "target": "Construction of Vietnamese SentiWordNet by using Vietnamese Dictionary"}
{"id": "task1540-98155329628f4c9e8f7832b462484a9d", "input": "Neural networks have recently been proposed for multi-label classification because they are able to capture and model label dependencies in the output layer. In this work, we investigate limitations of BP-MLL, a neural network (NN) architecture that aims at minimizing pairwise ranking error. Instead, we propose to use a comparably simple NN approach with recently proposed learning techniques for large-scale multi-label text classification tasks. In particular, we show that BP-MLL\u2019s ranking loss minimization can be efficiently and effectively replaced with the commonly used cross entropy error function, and demonstrate that several advances in neural network training that have been developed in the realm of deep learning can be effectively employed in this setting. Our experimental results show that simple NN models equipped with advanced techniques such as rectified linear units, dropout, and AdaGrad perform as well as or even outperform state-of-the-art approaches on six large-scale textual datasets with diverse characteristics.", "target": "Large-scale Multi-label Text Classification \u2014 Revisiting Neural Networks"}
{"id": "task1540-0a1c71da809e444f9d002b745186dbfc", "input": "In this paper we examine the benefit of performing named entity recognition (NER) and co-reference resolution to an English and a Greek corpus used for text segmentation. The aim here is to examine whether the combination of text segmentation and information extraction can be beneficial for the identification of the various topics that appear in a document. NER was performed manually in the English corpus and was compared with the output produced by publicly available annotation tools while, an already existing tool was used for the Greek corpus. Produced annotations from both corpora were manually corrected and enriched to cover four types of named entities. Co-reference resolution i.e., substitution of every reference of the same instance with the same named entity identifier was subsequently performed. The evaluation, using five text segmentation algorithms for the English corpus and four for the Greek corpus leads to the conclusion that, the benefit highly depends on the segment\u2019s topic, the number of named entity instances appearing in it, as well as the segment\u2019s length.", "target": "Text Segmentation using Named Entity Recognition and Co-reference Resolution in English and Greek Texts"}
{"id": "task1540-4d6bf42ce42d403391f3489034362461", "input": "In this paper we argue for the fundamental importance of the value distribution: the distribution of the random return received by a reinforcement learning agent. This is in contrast to the common approach to reinforcement learning which models the expectation of this return, or value. Although there is an established body of literature studying the value distribution, thus far it has always been used for a specific purpose such as implementing risk-aware behaviour. We begin with theoretical results in both the policy evaluation and control settings, exposing a significant distributional instability in the latter. We then use the distributional perspective to design a new algorithm which applies Bellman\u2019s equation to the learning of approximate value distributions. We evaluate our algorithm using the suite of games from the Arcade Learning Environment. We obtain both state-of-the-art results and anecdotal evidence demonstrating the importance of the value distribution in approximate reinforcement learning. Finally, we combine theoretical and empirical evidence to highlight the ways in which the value distribution impacts learning in the approximate setting.", "target": "A Distributional Perspective on Reinforcement Learning"}
{"id": "task1540-c88fd7f8e454493db695f0194c797496", "input": "LP relaxation-based message passing algorithms provide an effective tool for MAP inference over Probabilistic Graphical Models. However, different LP relaxations often have different objective functions and variables of differing dimensions, which presents a barrier to effective comparison and analysis. In addition, the computational complexity of LP relaxation-based methods grows quickly with the number of constraints. Reducing the number of constraints without sacrificing the quality of the solutions is thus desirable. We propose a unified formulation under which existing MAP LP relaxations may be compared and analysed. Furthermore, we propose a new tool called Marginal Polytope Diagrams. Some properties of Marginal Polytope Diagrams are exploited such as node redundancy and edge equivalence. We show that using Marginal Polytope Diagrams allows the number of constraints to be reduced without loosening the LP relaxations. Then, using Marginal Polytope Diagrams and constraint reduction, we develop three novel message passing algorithms, and demonstrate that two of these show a significant improvement in speed over state-of-art algorithms while delivering a competitive, and sometimes higher, quality of solution.", "target": "Constraint Reduction using Marginal Polytope Diagrams for MAP LP Relaxations"}
{"id": "task1540-213a040036e74167801102f6cb62071f", "input": "Deep CCA is a recently proposed deep neural network extension to the traditional canonical correlation analysis (CCA), and has been successful for multi-view representation learning in several domains. However, stochastic optimization of the deep CCA objective is not straightforward, because it does not decouple over training examples. Previous optimizers for deep CCA are either batch-based algorithms or stochastic optimization using large minibatches, which can have high memory consumption. In this paper, we tackle the problem of stochastic optimization for deep CCA with small minibatches, based on an iterative solution to the CCA objective, and show that we can achieve as good performance as previous optimizers and thus alleviate the memory requirement.", "target": "Stochastic Optimization for Deep CCA via Nonlinear Orthogonal Iterations"}
{"id": "task1540-120a9ce93c764797825318a09b30ea11", "input": "The continuing development of Semantic Web technologies and the increasing user adoption in the recent years have accelerated the progress incorporating explicit semantics with data on the Web. With the rapidly growing RDF (Resource Description Framework) data on the Semantic Web, processing large semantic graph data have become more challenging. Constructing a summary graph structure from the raw RDF can help obtain semantic type relations and reduce the computational complexity for graph processing purposes. In this paper, we addressed the problem of graph summarization in RDF graphs, and we proposed an approach for building summary graph structures automatically from RDF graph data. Moreover, we introduced a measure to help discover optimum class dissimilarity thresholds and an effective method to discover the type classes automatically. In future work, we plan to investigate further improvement options on the scalability of the proposed method.", "target": "Dynamic Discovery of Type Classes and Relations in Semantic Web Data"}
{"id": "task1540-674cda1aa7784e3b85c5ed0b6707f836", "input": "The paper presents a knowledge representation language A log which extends ASP with aggregates. The goal is to have a language based on simple syntax and clear intuitive and mathematical semantics. We give some properties of A log, an algorithm for computing its answer sets, and comparison with other approaches.", "target": "Vicious Circle Principle and Logic Programs with Aggregates"}
{"id": "task1540-d4c19f4aaa6b4f08ac4a30a2cee18f01", "input": "In natural-language discourse, related events tend to appear near each other to describe a larger scenario. Such structures can be formalized by the notion of a frame (a.k.a. template), which comprises a set of related events and prototypical participants and event transitions. Identifying frames is a prerequisite for information extraction and natural language generation, and is usually done manually. Methods for inducing frames have been proposed recently, but they typically use ad hoc procedures and are difficult to diagnose or extend. In this paper, we propose the first probabilistic approach to frame induction, which incorporates frames, events, participants as latent topics and learns those frame and event transitions that best explain the text. The number of frames is inferred by a novel application of a split-merge method from syntactic parsing. In end-to-end evaluations from text to induced frames and extracted facts, our method produced state-of-the-art results while substantially reducing engineering effort.", "target": "Probabilistic Frame Induction\u2217"}
{"id": "task1540-d96135393fba46598f5e34d49a5ac4f6", "input": "This paper introduces SGNMT, our experimental platform for machine translation research. SGNMT provides a generic interface to neural and symbolic scoring modules (predictors) with left-to-right semantic such as translation models like NMT, language models, translation lattices, n-best lists or other kinds of scores and constraints. Predictors can be combined with other predictors to form complex decoding tasks. SGNMT implements a number of search strategies for traversing the space spanned by the predictors which are appropriate for different predictor constellations. Adding new predictors or decoding strategies is particularly easy, making it a very efficient tool for prototyping new research ideas. SGNMT is actively being used by students in the MPhil program in Machine Learning, Speech and Language Technology at the University of Cambridge for course work and theses, as well as for most of the research work in our group.", "target": "SGNMT \u2013 A Flexible NMT Decoding Platform for Quick Prototyping of New Models and Search Strategies"}
{"id": "task1540-dd866619ea6a4c34a4e05086bf4d984e", "input": "We present two novel and contrasting Recurrent Neural Network (RNN) based architectures for extractive summarization of documents. The Classifier based architecture sequentially accepts or rejects each sentence in the original document order for its membership in the final summary. The Selector architecture, on the other hand, is free to pick one sentence at a time in any arbitrary order to piece together the summary. Our models under both architectures jointly capture the notions of salience and redundancy of sentences. In addition, these models have the advantage of being very interpretable, since they allow visualization of their predictions broken up by abstract features such as information content, salience and redundancy. We show that our models reach or outperform state-of-the-art supervised models on two different corpora. We also recommend the conditions under which one architecture is superior to the other based on experimental evidence.", "target": "EXTRACTIVE DOCUMENT SUMMARIZATION"}
{"id": "task1540-f17c31bb37694959bd6e80bb06f43c59", "input": "Modelling problems containing a mixture of Boolean and numerical variables<lb>is a long-standing interest of Artificial Intelligence. However, performing<lb>inference and learning in hybrid domains is a particularly daunting task.<lb>The ability to model this kind of domains is crucial in \u201clearning to design\u201d<lb>tasks, that is, learning applications where the goal is to learn from examples<lb>how to perform automatic de novo design of novel objects. In this paper we<lb>present Structured Learning Modulo Theories, a max-margin approach for<lb>learning in hybrid domains based on Satisfiability Modulo Theories, which<lb>allows to combine Boolean reasoning and optimization over continuous linear<lb>arithmetical constraints. We validate our method on artificial and real world<lb>scenarios.<lb>", "target": "Structured Learning Modulo Theories"}
{"id": "task1540-84d4fe8a23d747c0ab1b1ba49f52cfaa", "input": "The application of Deep Neural Networks for ranking in search engines may obviate the need for the extensive feature engineering common to current learning-to-rank methods. However, we show that combining simple relevance matching features like BM25 with existing Deep Neural Net models often substantially improves the accuracy of these models, indicating that they do not capture essential local relevance matching signals. We describe a novel deep Recurrent Neural Net-based model that we call Match-Tensor. The architecture of the Match-Tensor model simultaneously accounts for both local relevance matching and global topicality signals allowing for a rich interplay between them when computing the relevance of a document to a query. On a large held-out test set consisting of social media documents, we demonstrate not only that Match-Tensor outperforms BM25 and other classes of DNNs but also that it largely subsumes signals present in these models.", "target": "Match-Tensor: a Deep Relevance Model for Search"}
{"id": "task1540-e5130cad877843f9b3802b1e2aca2c36", "input": "Multitask learning can be effective when features useful in one task are also useful for other tasks, and the group lasso is a standard method for selecting a common subset of features. In this paper, we are interested in a less restrictive form of multitask learning, wherein (1) the available features can be organized into subsets according to a notion of similarity and (2) features useful in one task are similar, but not necessarily identical, to the features best suited for other tasks. The main contribution of this paper is a new procedure called Sparse Overlapping Sets (SOS) lasso, a convex optimization that automatically selects similar features for related learning tasks. Error bounds are derived for SOSlasso and its consistency is established for squared error loss. In particular, SOSlasso is motivated by multisubject fMRI studies in which functional activity is classified using brain voxels as features. Experiments with real and synthetic data demonstrate the advantages of SOSlasso compared to the lasso and group lasso.", "target": "Sparse Overlapping Sets Lasso for Multitask Learning and its Application to fMRI Analysis"}
{"id": "task1540-2836c23a2f874439a0ea029b13986a52", "input": "Although a number of auto-encoder models enforce sparsity explicitly in their learned representation while others don\u2019t, there has been little formal analysis on what encourages sparsity in these models in general. Therefore, our objective here is to formally study this general problem for regularized auto-encoders. We show that both regularization and activation function play an important role in encouraging sparsity. We provide sufficient conditions on both these criteria and show that multiple popular models\u2013 like De-noising and Contractive auto-encoder\u2013 and activations\u2013 like Rectified Linear and Sigmoid\u2013 satisfy these conditions; thus explaining sparsity in their learned representation. Our theoretical and empirical analysis together, throws light on the properties of regularization/activation that are conducive to sparsity. As a by-product of the insights gained from our analysis, we also propose a new activation function that overcomes the individual drawbacks of multiple existing activations (in terms of sparsity) and hence produces performance at par (or better) with the best performing activation for all auto-encoder models discussed.", "target": "Why Regularized Auto-Encoders learn Sparse Representation?"}
{"id": "task1540-e0b3dafd0136493fbeaf8d50d208363d", "input": "In this paper we introduce RankPL, a modeling language that can be thought of as a qualitative variant of a probabilistic programming language with a semantics based on Spohn\u2019s ranking theory. Broadly speaking, RankPL can be used to represent and reason about processes that exhibit uncertainty expressible by distinguishing \u201cnormal\u201d from \u201csurprising\u201d events. RankPL allows (iterated) revision of rankings over alternative program states and supports various types of reasoning, including abduction and causal inference. We present the language, its denotational semantics, and a number of practical examples. We also discuss an implementation of RankPL that is available for download.", "target": "RankPL: A Qualitative Probabilistic Programming Language"}
{"id": "task1540-03e9a778b61945e9839d5f4253c50e3f", "input": "In this study, an Artificial Neural Network (ANN) approach is utilized to perform a parametric study on the process of extraction of lubricants from heavy petroleum cuts. To train the model, we used field data collected from an industrial plant. Operational conditions of feed and solvent flow rate, Temperature of streams and mixing rate were considered as the input to the model, whereas the flow rate of the main product was considered as the output of the ANN model. A feed-forward Multi-Layer Perceptron Neural Network was successfully applied to capture the relationship between inputs and output parameters.", "target": "On the Parametric Study of Lubricating Oil Production using an Artificial Neural Network (ANN) Approach"}
{"id": "task1540-e8c867cbad364bc8969d2817be31dd37", "input": "To bridge the gap between humans and machines in image understanding and describing, we need further insight into how people describe a perceived scene. In this paper, we study the agreement between bottom-up saliency-based visual attention and object referrals in scene description constructs. We investigate the properties of human-written descriptions and machine-generated ones. We then propose a saliency-boosted image captioning model in order to investigate benefits from low-level cues in language models. We learn that (1) humans mention more salient objects earlier than less salient ones in their descriptions, (2) the better a captioning model performs, the better attention agreement it has with human descriptions, (3) the proposed saliencyboosted model, compared to its baseline form, does not improve significantly on the MS COCO database, indicating explicit bottom-up boosting does not help when the task is well learnt and tuned on a data, (4) a better generalization ability is, however, observed for the saliency-boosted model on unseen data.", "target": "Can Saliency Information Benefit Image Captioning Models?"}
{"id": "task1540-4f59a1c60f2543478f2d8b0dd2eac0bc", "input": "This paper presents the computational logic foundations of a model of agency called the KGP (Knowledge, Goals and Plan) model. This model allows the specification of heterogeneous agents that can interact with each other, and can exhibit both proactive and reactive behaviour allowing them to function in dynamic environments by adjusting their goals and plans when changes happen in such environments. KGP provides a highly modular agent architecture that integrates a collection of reasoning and physical capabilities, synthesised within transitions that update the agent\u2019s state in response to reasoning, sensing and acting. Transitions are orchestrated by cycle theories that specify the order in which transitions are executed while taking into account the dynamic context and agent preferences, as well as selection operators for providing inputs to transitions.", "target": "Computational Logic Foundations of KGP Agents"}
{"id": "task1540-68a492b2e1474ae298fa380353aefb89", "input": "Neural attention models have achieved great success in different NLP tasks. However, they have not fulfilled their promise on the AMR parsing task due to the data sparsity issue. In this paper, we describe a sequence-to-sequence model for AMR parsing and present different ways to tackle the data sparsity problem. We show that our methods achieve significant improvement over a baseline neural attention model and our results are also competitive against state-of-the-art systems that do not use extra linguistic resources.", "target": "Addressing the Data Sparsity Issue in Neural AMR Parsing"}
{"id": "task1540-86f531c4de1247e0a6b8f2aa5481d28a", "input": "Reservoir computing is a new, powerful and flexible machine learning technique that is easily implemented in hardware. Recently, by using a time-multiplexed architecture, hardware reservoir computers have reached performance comparable to digital implementations. Operating speeds allowing for real time information operation have been reached using optoelectronic systems. At present the main performance bottleneck is the readout layer which uses slow, digital postprocessing. We have designed an analog readout suitable for time-multiplexed optoelectronic reservoir computers, capable of working in real time. The readout has been built and tested experimentally on a standard benchmark task. Its performance is better than non-reservoir methods, with ample room for further improvement. The present work thereby overcomes one of the major limitations for the future development of hardware reservoir computers.", "target": "Analog readout for optical reservoir computers"}
{"id": "task1540-0c737af2fe464dfb9f43baf2d971b906", "input": "Shannon\u2019s information entropy measures of the uncertainty of an event\u2019s outcome. If learning about a system reflects a decrease in uncertainty, then a plausible intuition is that learning should be accompanied by a decrease in the entropy of the organism\u2019s actions and/or perceptual states. To address whether this intuition is valid, I examined an artificial organism \u2013 a simple robot \u2013 that learned to navigate in an arena and analyzed the entropy of the outcome variables action, state, and reward. Entropy did indeed decrease in the initial stages of learning, but two factors complicated the scenario: (1) the introduction of new options discovered during the learning process and (2) the shifting patterns of perceptual and environmental states resulting from changes to the robot\u2019s learned movement strategies. These factors lead to a subsequent increase in entropy as the agent learned. I end with a discussion of the utility of information-based characterizations of learning.", "target": "Does Learning Imply a Decrease in the Entropy of Behavior?"}
{"id": "task1540-307fab2d9131487694f4d5a40f48dd8a", "input": "We present a general-purpose tagger based on convolutional neural networks (CNN), used for both composing word vectors and encoding context information. The CNN tagger is robust across different tagging tasks: without task-specific tuning of hyper-parameters, it achieves state-of-theart results in part-of-speech tagging, morphological tagging and supertagging. The CNN tagger is also robust against the outof-vocabulary problem, it performs well on artificially unnormalized texts.", "target": "A General-Purpose Tagger with Convolutional Neural Networks"}
{"id": "task1540-e3b278301ee94b4d91ff705ad8fd0d61", "input": "In business analytics, measure values, such as sales numbers or volumes of cargo transported, are often summed along values of one or more corresponding categories, such as time or shipping container. However, not every measure should be added by default (e.g., one might more typically want a mean over the heights of a set of people); similarly, some measures should only be summed within certain constraints (e.g., population measures need not be summed over years). In systems such as Watson Analytics, the exact additive behaviour of a measure is often determined by a human expert. In this work, we propose a small set of features for this issue. We use these features in a case-based reasoning approach, where the system suggests an aggregation behaviour, with 86% accuracy in our collected dataset.", "target": "Learning measures of semi-additive behaviour"}
{"id": "task1540-b5e04af88cb648c1a06cfba5259e7ba5", "input": "We study the problem of learning classifiers with a fairness constraint, with three main contributions towards the goal of quantifying the problem\u2019s inherent tradeoffs. First, we relate two existing fairness measures to cost-sensitive risks. Second, we show that for cost-sensitive classification and fairness measures, the optimal classifier is an instance-dependent thresholding of the class-probability function. Third, we show how the tradeoff between accuracy and fairness is determined by the alignment between the class-probabilities for the target and sensitive features. Underpinning our analysis is a general framework that casts the problem of learning with a fairness requirement as one of minimising the difference of two statistical risks.", "target": "The cost of fairness in classification"}
{"id": "task1540-1f71fb84feb144d78de6bf6c071bc3d1", "input": "Social media is a rich source of rumours and corresponding community reactions. Rumours reflect different characteristics, some shared and some individual. We formulate the problem of classifying tweet level judgements of rumours as a supervised learning task. Both supervised and unsupervised domain adaptation are considered, in which tweets from a rumour are classified on the basis of other annotated rumours. We demonstrate how multi-task learning helps achieve good results on rumours from the 2011 England riots.", "target": "Classifying Tweet Level Judgements of Rumours in Social Media"}
{"id": "task1540-32d088c0d40442a19339dfc29599dcfc", "input": "Variational autoencoders (VAE) often use Gaussian or category distribution to model the inference process. This puts a limit on variational learning because this simplified assumption does not match the true posterior distribution, which is usually much more sophisticated. To break this limitation and apply arbitrary parametric distribution during inference, this paper derives a semi-continuous latent representation, which approximates a continuous density up to a prescribed precision, and is much easier to analyze than its continuous counterpart because it is fundamentally discrete. We showcase the proposition by applying polynomial exponential family distributions as the posterior, which are universal probability density function generators. Our experimental results show consistent improvements over commonly used VAE models.", "target": "Coarse Grained Exponential Variational Autoencoders"}
{"id": "task1540-19d9ec37b1bf41b1b9d7a48525d50621", "input": "A standard model for Recommender Systems is the Matrix Completion setting: given partially known matrix of ratings given by users (rows) to items (columns), infer the unknown ratings. In the last decades, few attempts where done to handle that objective with Neural Networks, but recently an architecture based on Autoencoders proved to be a promising approach. In current paper, we enhanced that architecture (i) by using a loss function adapted to input data with missing values, and (ii) by incorporating side information. The experiments demonstrate that while side information only slightly improve the test error averaged on all users/items, it has more impact on cold users/items.", "target": "Hybrid Recommender System based on Autoencoders"}
{"id": "task1540-b87e1ce5b31144ccb426b84352110ca5", "input": "Number of web services available on Internet and its usage are increasing very fast. In many cases, one service is not enough to complete the business requirement; composition of web services is carried out. Autonomous composition of web services to achieve new functionality is generating considerable attention in semantic web domain. Development time and effort for new applications can be reduced with service composition. Various approaches to carry out automated composition of web services are discussed in literature. Web service composition using ontologies is one of the effective approaches. In this paper we demonstrate how the ontology based composition can be made faster for each customer. We propose a framework to provide precomposed web services to fulfil user requirements. We detail how ontology merging can be used for composition which expedites the whole process. We discuss how framework provides customer specific ontology merging and repository. We also elaborate on how merging of ontologies is carried out. Keywords\u2014 Semantic Web, Web service composition, domain ontology, OWL-S, ontology merging", "target": "A Framework for Semi-automated Web Service Composition in Semantic Web"}
{"id": "task1540-fb0c40e3239d4896926db301f581d6d0", "input": "The abstract should summarize the contents of the paper using at least 70 and at most 150 words. It will be set in 9-point font size and be inset 1.0 cm from the right and left margins. There will be two blank lines before and after the Abstract. . . .", "target": "Discriminative Parameter Estimation for Random Walks Segmentation: Technical Report"}
{"id": "task1540-396741e0e9d84954b742b0c7de602133", "input": "Nearest neighbor (k-NN) graphs are widely used in machine learning and data mining applications, and our aim is to better understand what they reveal about the cluster structure of the unknown underlying distribution of points. Moreover, is it possible to identify spurious structures that might arise due to sampling variability? Our first contribution is a statistical analysis that reveals how certain subgraphs of a k-NN graph form a consistent estimator of the cluster tree of the underlying distribution of points. Our second and perhaps most important contribution is the following finite sample guarantee. We carefully work out the tradeoff between aggressive and conservative pruning and are able to guarantee the removal of all spurious cluster structures at all levels of the tree while at the same time guaranteeing the recovery of salient clusters. This is the first such finite sample result in the context of clustering.", "target": "Pruning nearest neighbor cluster trees"}
{"id": "task1540-f9ca459cf4d54b14a4b0a6e4abd2dfdb", "input": "We consider a multidimensional search problem that is motivated by questions in contextual decision-making, such as dynamic pricing and personalized medicine. Nature selects a state from a d-dimensional unit ball and then generates a sequence of d-dimensional directions. We are given access to the directions, but not access to the state. After receiving a direction, we have to guess the value of the dot product between the state and the direction. Our goal is to minimize the number of times when our guess is more than \u01eb away from the true answer. We construct a polynomial time algorithm that we call Projected Volume achieving regret O(d log(d/\u01eb)), which is optimal up to a log d factor. The algorithm combines a volume cutting strategy with a new geometric technique that we call cylindrification.", "target": "Multidimensional Binary Search for Contextual Decision-Making"}
{"id": "task1540-4b0a7b961ade425f90a6142bd2bae971", "input": "In this paper we present a novel iterative multiphase clustering technique for efficiently clustering high dimensional data points. For this purpose we implement clustering feature (CF) tree on a real data set and a Gaussian density distribution constraint on the resultant CF tree. The post processing by the application of Gaussian density distribution function on the micro-clusters leads to refinement of the previously formed clusters thus improving their quality. This algorithm also succeeds in overcoming the inherent drawbacks of conventional hierarchical methods of clustering like inability to undo the change made to the dendogram of the data points. Moreover, the constraint measure applied in the algorithm makes this clustering technique suitable for need driven data analysis. We provide veracity of our claim by evaluating our algorithm with other similar clustering algorithms.", "target": "Using Gaussian Measures for Efficient Constraint Based Clustering"}
{"id": "task1540-84fb9e8b40454e93b783bfe6eb2a0055", "input": "Image captioning has so far been explored mostly in English, as most available datasets are in this language. However, the application of image captioning should not be restricted by language. Only few studies have been conducted for image captioning in a cross-lingual se\u008aing. Di\u0082erent from these works that manually build a dataset for a target language, we aim to learn a cross-lingual captioning model fully from machine-translated sentences. To conquer the lack of \u0083uency in the translated sentences, we propose in this paper a \u0083uency-guided learning framework. \u008ce framework comprises a module to automatically estimate the \u0083uency of the sentences and another module to utilize the estimated \u0083uency scores to e\u0082ectively train an image captioning model for the target language. As experiments on two bilingual (English-Chinese) datasets show, our approach improves both \u0083uency and relevance of the generated captions in Chinese, but without using any manually wri\u008aen sentences from the target language.", "target": "Fluency-Guided Cross-Lingual Image Captioning"}
{"id": "task1540-1611980a3ea543db9d91279cc6c32a46", "input": "In this paper we present architecture of a fuzzy expert system used for therapy of dyslalic children. With fuzzy approach we can create a better model for speech therapist decisions. A software interface was developed for validation of the system. The main objectives of this task are: personalized therapy (the therapy must be in according with child\u2019s problems level, context and possibilities), speech therapist assistant (the expert system offer some suggestion regarding what exercises are better for a specific moment and from a specific child), (self) teaching (when system\u2019s conclusion is different that speech therapist\u2019s conclusion the last one must have the knowledge base change possibility).", "target": "ARCHITECTURE OF A FUZZY EXPERT SYSTEM USED FOR DYSLALIC CHILDREN THERAPY"}
{"id": "task1540-c27d98351c3e4c499729bae9a76d6ff7", "input": "Convolutional neural networks (CNNs) with convolutional and pooling operations along the frequency axis have been proposed to attain invariance to frequency shifts of features. However, this is inappropriate with regard to the fact that acoustic features vary in frequency. In this paper, we contend that convolution along the time axis is more effective. We also propose the addition of an intermap pooling (IMP) layer to deep CNNs. In this layer, filters in each group extract common but spectrally variant features, then the layer pools the feature maps of each group. As a result, the proposed IMP CNN can achieve insensitivity to spectral variations characteristic of different speakers and utterances. The effectiveness of the IMP CNN architecture is demonstrated on several LVCSR tasks. Even without speaker adaptation techniques, the architecture achieved a WER of 12.7% on the SWB part of the Hub5\u20192000 evaluation test set, which is competitive with other state-of-the-art methods.", "target": "Deep CNNs along the Time Axis with Intermap Pooling for Robustness to Spectral Variations"}
{"id": "task1540-d626801c0be8451ca89f5173023ff7e3", "input": "Lexical features are a major source of information in state-of-the-art coreference resolvers. Lexical features implicitly model some of the linguistic phenomena at a fine granularity level. They are especially useful for representing the context of mentions. In this paper we investigate a drawback of using many lexical features in state-of-the-art coreference resolvers. We show that if coreference resolvers mainly rely on lexical features, they can hardly generalize to unseen domains. Furthermore, we show that the current coreference resolution evaluation is clearly flawed by only evaluating on a specific split of a specific dataset in which there is a notable overlap between the training, development and test sets.", "target": "Lexical Features in Coreference Resolution: To be Used With Caution"}
{"id": "task1540-e3cddcbba26f41f4b79b44618ec61c1d", "input": "To what extent is the success of deep visualization due to the training? Could we do deep visualization using untrained, random weight networks? To address this issue, we explore new and powerful generative models for three popular deep visualization tasks using untrained, random weight convolutional neural networks. First we invert representations in feature spaces and reconstruct images from white noise inputs. The reconstruction quality is statistically higher than that of the same method applied on well trained networks with the same architecture. Next we synthesize textures using scaled correlations of representations in multiple layers and our results are almost indistinguishable with the original natural texture and the synthesized textures based on the trained network. Third, by recasting the content of an image in the style of various artworks, we create artistic images with high perceptual quality, highly competitive to the prior work of Gatys et al. on pretrained networks. To our knowledge this is the first demonstration of image representations using untrained deep neural networks. Our work provides a new and fascinating tool to study the representation of deep network architecture and sheds light on new understandings on deep visualization.", "target": "A Powerful Generative Model Using Random Weights for the Deep Image Representation"}
{"id": "task1540-d66b8f1028d646ffbb764d269a164a40", "input": "The collection and analysis of user data drives improvements in the app and web ecosystems, but comes with risks to privacy. This paper examines discrete distribution estimation under local privacy, a setting wherein service providers can learn the distribution of a categorical statistic of interest without collecting the underlying data. We present new mechanisms, including hashed k-ary Randomized Response (k-RR), that empirically meet or exceed the utility of existing mechanisms at all privacy levels. New theoretical results demonstrate the order-optimality of k-RR and the existing RAPPOR mechanism at different privacy regimes.", "target": "Discrete Distribution Estimation under Local Privacy"}
{"id": "task1540-b763c6b7be6e4b9194abba22a543a548", "input": "The aim of this paper is to show the interest in fitting features with an \u03b1-stable distribution to classify imperfect data. The supervised pattern recognition is thus based on the theory of continuous belief functions, which is a way to consider imprecision and uncertainty of data. The distributions of features are supposed to be unimodal and estimated by a single Gaussian and \u03b1-stable model. Experimental results are first obtained from synthetic data by combining two features of one dimension and by considering a vector of two features. Mass functions are calculated from plausibility functions by using the generalized Bayes theorem. The same study is applied to the automatic classification of three types of sea floor (rock, silt and sand) with features acquired by a mono-beam echo-sounder. We evaluate the quality of the \u03b1-stable model and the Gaussian model by analyzing qualitative results, using a Kolmogorov-Smirnov test (K-S test), and quantitative results with classification rates. The performances of the belief classifier are compared with a Bayesian approach.", "target": "Features modeling with an \u03b1-stable distribution: application to pattern recognition based on continuous belief functions"}
{"id": "task1540-3589da6b7f1844faa36aa7b5c0924d40", "input": "The current information analysis capabilities of legal professionals are still lagging behind the explosive growth in legal document availability through digital means, driving the need for higher efficiency Legal Information Retrieval (IR) and Question Answering (QA) methods. The IR task in particular has a set of unique challenges that invite the use of semantic motivated NLP techniques. In this work, a two-stage method for Legal Information Retrieval is proposed, combining lexical statistics and distributional sentence representations in the context of Competition on Legal Information Extraction/Entailment (COLIEE). The combination is done by means of disambiguation rules, applied over the lexical rankings when those deemed unreliable for a given query. Competition and experimental results indicate small gains in overall retrieval performance using the proposed approach. Additionally, a analysis of error and improvement cases is presented for a better understanding of the contributions.", "target": "Improving Legal Information Retrieval by Distributional Composition with Term Order Probabilities"}
{"id": "task1540-90e6aee6738e4950be25b37611ea9320", "input": "We study optimization algorithms based on variance reduction for stochastic gradient descent (SGD). Remarkable recent progress has been made in this direction through development of algorithms like SAG, SVRG, SAGA. These algorithms have been shown to outperform SGD, both theoretically and empirically. However, asynchronous versions of these algorithms\u2014a crucial requirement for modern large-scale applications\u2014have not been studied. We bridge this gap by presenting a unifying framework for many variance reduction techniques. Subsequently, we propose an asynchronous algorithm grounded in our framework, and prove its fast convergence. An important consequence of our general approach is that it yields asynchronous versions of variance reduction algorithms such as SVRG and SAGA as a byproduct. Our method achieves near linear speedup in sparse settings common to machine learning. We demonstrate the empirical performance of our method through a concrete realization of asynchronous SVRG.", "target": "On Variance Reduction in Stochastic Gradient Descent and its Asynchronous Variants"}
{"id": "task1540-1a2c48bdf0e54a1382491d8613a7856b", "input": "Mined Semantic Analysis (MSA) is a novel concept space model which employs unsupervised learning to generate semantic representations of text. MSA represents textual structures (terms, phrases, documents) as a bag-of-concepts where concepts are derived from concept rich encyclopedic corpora. Traditional concept space models exploit only target corpus content to construct the concept space. MSA, alternatively, uncovers implicit relations between concepts by mining for their associations (e.g., mining Wikipedia\u2019s \"See also\" link graph). We evaluate MSA\u2019s performance on benchmark data sets for measuring lexical semantic relatedness. Empirical results show competitive performance of MSA compared to prior stateof-the-art methods. Additionally, we introduce the first analytical study to examine statistical significance of results reported by different semantic relatedness methods. Our study shows that, the nuances of results across top performing methods could be statistically insignificant. The study positions MSA as one of state-of-theart methods for measuring semantic relatedness.", "target": "Measuring Semantic Relatedness using Mined Semantic Analysis"}
{"id": "task1540-ebe584e564764fb193cd3e2c391eabb6", "input": "Fuzzy controllers are known to serve as efficient and interpretable system controllers for continuous state and action spaces. To date these controllers have been constructed by hand, or automatically trained either on expert generated problem specific cost functions or by incorporating detailed knowledge about the optimal control strategy. Both requirements for automatic training processes are not given in the majority of real world reinforcement learning (RL) problems. We introduce a new particle swarm reinforcement learning (PSRL) approach which is capable of constructing fuzzy RL policies solely by training parameters on world models produced from randomly generated samples of the real system. This approach relates self-organizing fuzzy controllers to model-based RL for the first time. PSRL can be used straightforward on any RL problem, which is demonstrated on three standard RL benchmarks, mountain car, cart pole balancing and cart pole swing up. Our experiments yielded high performing and well interpretable fuzzy policies.", "target": "Particle Swarm Optimization for Generating Fuzzy Reinforcement Learning Policies"}
{"id": "task1540-191d8e8b688c45e4897c4993e2a6ef3f", "input": "We adress the problem of dueling bandits defined on partially ordered sets, or posets. In this setting, arms may not be comparable, and there may be several (incomparable) optimal arms. We propose an algorithm, UnchainedBandits, that efficiently finds the set of optimal arms of any poset even when pairs of comparable arms cannot be distinguished from pairs of incomparable arms, with a set of minimal assumptions. This algorithm relies on the concept of decoys, which stems from social psychology. For the easier case where the incomparability information may be accessible, we propose a second algorithm, SlicingBandits, which takes advantage of this information and achieves a very significant gain of performance compared to UnchainedBandits. We provide theoretical guarantees and experimental evaluation for both algorithms.", "target": "Decoy Bandits Dueling on a Poset"}
{"id": "task1540-47270aee5bd94008ba9a52b6f56c1e5c", "input": "We address the problem of determining correspondences between two images in agreement with a geometric model such as an affine or thin-plate-spline transformation, and estimating its parameters. The contributions of this work are three-fold. First, we propose a convolutional neural network architecture for geometric matching. The architecture is based on three main components that mimic the standard steps of feature extraction, matching and simultaneous inlier detection and model parameter estimation, while being trainable end-to-end. Second, we demonstrate that the network parameters can be trained from synthetically generated imagery without the need for manual annotation and that our matching layer significantly increases generalization capabilities to never seen before images. Finally, we show that the same model can perform both instance-level and category-level matching giving state-of-the-art results on the challenging Proposal Flow dataset.", "target": "Convolutional neural network architecture for geometric matching"}
{"id": "task1540-3f2dda70e3e7434abc5de1102c3736f1", "input": "We present a deep hierarchical recurrent neural network for sequence tagging. Given a sequence of words, our model employs deep gated recurrent units on both character and word levels to encode morphology and context information, and applies a conditional random field layer to predict the tags. Our model is task independent, language independent, and feature engineering free. We further extend our model to multi-task and crosslingual joint training by sharing the architecture and parameters. Our model achieves state-of-the-art results in multiple languages on several benchmark tasks including POS tagging, chunking, and NER. We also demonstrate that multi-task and cross-lingual joint training can improve the performance in various cases.", "target": "Multi-Task Cross-Lingual Sequence Tagging from Scratch"}
{"id": "task1540-3c9860fb24b142beb9b71d501bf25604", "input": "Recent studies have shown that deep neural networks (DNN) are vulnerable to adversarial samples: maliciously-perturbed samples crafted to yield incorrect model outputs. Such attacks can severely undermine DNN systems, particularly in security-sensitive settings. It was observed that an adversary could easily generate adversarial samples by making a small perturbation on irrelevant feature dimensions that are unnecessary for the current classification task. To overcome this problem, we introduce a defensive mechanism called DeepMask. By identifying and removing unnecessary features in a DNN model, DeepMask limits the capacity an attacker can use generating adversarial samples and therefore increase the robustness against such inputs. Comparing with other defensive approaches, DeepMask is easy to implement and computationally efficient. Experimental results show that DeepMask can increase the performance of state-of-the-art DNN models against adversarial samples.", "target": "DEEPMASK: MASKING DNN MODELS FOR ROBUST- NESS AGAINST ADVERSARIAL SAMPLES"}
{"id": "task1540-286c758f81604be4a35e7fc1dde62ab3", "input": "The convergence of Stochastic Gradient Descent (SGD) using convex loss functions has been widely studied. However, vanilla SGD methods using convex losses cannot perform well with noisy labels, which adversely affect the update of the primal variable in SGD methods. Unfortunately, noisy labels are ubiquitous in real world applications such as crowdsourcing. To handle noisy labels, in this paper, we present a family of robust losses for SGD methods. By employing our robust losses, SGD methods successfully reduce negative effects caused by noisy labels on each update of the primal variable. We not only reveal that the convergence rate is O(1/T ) for SGD methods using robust losses, but also provide the robustness analysis on two representative robust losses. Comprehensive experimental results on six real-world datasets show that SGD methods using robust losses are obviously more robust than other baseline methods in most situations with fast convergence.", "target": "On the Convergence of A Family of Robust Losses for Stochastic Gradient Descent"}
{"id": "task1540-1a93f3c38aa14ee2b1a1293c7aaeafbd", "input": "The role of semantics in zero-shot learning is considered. The effectiveness of previous approaches is analyzed according to the form of supervision provided. While some learn semantics independently, others only supervise the semantic subspace explained by training classes. Thus, the former is able to constrain the whole space but lacks the ability to model semantic correlations. The latter addresses this issue but leaves part of the semantic space unsupervised. This complementarity is exploited in a new convolutional neural network (CNN) framework, which proposes the use of semantics as constraints for recognition.Although a CNN trained for classification has no transfer ability, this can be encouraged by learning an hidden semantic layer together with a semantic code for classification. Two forms of semantic constraints are then introduced. The first is a loss-based regularizer that introduces a generalization constraint on each semantic predictor. The second is a codeword regularizer that favors semantic-to-class mappings consistent with prior semantic knowledge while allowing these to be learned from data. Significant improvements over the state-of-the-art are achieved on several datasets.", "target": "Semantically Consistent Regularization for Zero-Shot Recognition"}
{"id": "task1540-10c53693d5694de8adff70a944f2e5b7", "input": "N-tuple networks have been successfully used as position evaluation functions for board games such as Othello or Connect Four. The effectiveness of such networks depends on their architecture, which is determined by the placement of constituent n-tuples, sequences of board locations, providing input to the network. The most popular method of placing ntuples consists in randomly generating a small number of long, snake-shaped board location sequences. In comparison, we show that learning n-tuple networks is significantly more effective if they involve a large number of systematically placed, short, straight n-tuples. Moreover, we demonstrate that in order to obtain the best performance and the steepest learning curve for Othello it is enough to use n-tuples of size just 2, yielding a network consisting of only 288 weights. The best such network evolved in this study has been evaluated in the online Othello League, obtaining the performance of nearly 96% \u2014 more than any other player to date.", "target": "Systematic N-tuple Networks for Position Evaluation: Exceeding 90% in the Othello League"}
{"id": "task1540-6843132fe1bc4d8ba815d0b7a07e380d", "input": "This paper proposes to use probabilistic model checking to synthesize optimal robot policies in multi-tasking autonomous systems that are subject to human-robot interaction. Given the convincing empirical evidence that human behavior can be related to reinforcement models, we take as input a well-studied Q-table model of the human behavior for flexible scenarios. We first describe an automated procedure to distill a Markov decision process (MDP) for the human in an arbitrary but fixed scenario. The distinctive issue is that \u2013 in contrast to existing models \u2013 under-specification of the human behavior is included. Probabilistic model checking is used to predict the human\u2019s behavior. Finally, the MDP model is extended with a robot model. Optimal robot policies are synthesized by analyzing the resulting two-player stochastic game. Experimental results with a prototypical implementation using PRISM show promising results.", "target": "Probabilistic Model Checking for Complex Cognitive Tasks"}
{"id": "task1540-aa078e170b914914b04693368d1cead4", "input": "Named Entities (NEs) are often written with no orthographic changes across different languages that share a common alphabet. We show that this can be leveraged so as to improve named entity recognition (NER) by using unsupervised word clusters from secondary languages as features in state-of-the-art discriminative NER systems. We observe significant increases in performance, finding that person and location identification is particularly improved, and that phylogenetically close languages provide more valuable features than more distant languages.", "target": "\u201cTranslation can\u2019t change a name\u201d: Using Multilingual Data for Named Entity Recognition"}
{"id": "task1540-3232e505472d4c3bbac4330f7f9ce344", "input": "In this paper, we present a multimodal Recurrent Neural Network (m-RNN) model for generating novel image captions. It directly models the probability distribution of generating a word given previous words and an image. Image captions are generated according to this distribution. The model consists of two sub-networks: a deep recurrent neural network for sentences and a deep convolutional network for images. These two sub-networks interact with each other in a multimodal layer to form the whole m-RNN model. The effectiveness of our model is validated on four benchmark datasets (IAPR TC-12, Flickr 8K, Flickr 30K and MS COCO). Our model outperforms the state-of-the-art methods. In addition, we apply the m-RNN model to retrieval tasks for retrieving images or sentences, and achieves significant performance improvement over the state-of-the-art methods which directly optimize the ranking objective function for retrieval. The project page of this work is: www.stat.ucla.edu/ \u0303junhua.mao/m-RNN.html. 1", "target": "DEEP CAPTIONING WITH MULTIMODAL RECURRENT NEURAL NETWORKS (M-RNN)"}
{"id": "task1540-b690024170b04e42aca0c37f635b5513", "input": "In this paper, we address the problem of data description using a Bayesian framework. The goal of data description is to draw a boundary around objects of a certain class of interest to discriminate that class from the rest of the feature space. Data description is also known as one-class learning and has a wide range of applications. The proposed approach uses a Bayesian framework to precisely compute the class boundary and therefore can utilize domain information in form of prior knowledge in the framework. It can also operate in the kernel space and therefore recognize arbitrary boundary shapes. Moreover, the proposed method can utilize unlabeled data in order to improve accuracy of discrimination. We evaluate our method using various real-world datasets and compare it with other state of the art approaches of data description. Experiments show promising results and improved performance over other data description and one-class learning algorithms.", "target": "A Bayesian Approach to the Data Description Problem"}
{"id": "task1540-240710ada1514d109403627fb6c5e0af", "input": "Most past work on social network link fraud detection tries to separate genuine users from fraudsters, implicitly assuming that there is only one type of fraudulent behavior. But is this assumption true? And, in either case, what are the characteristics of such fraudulent behaviors? In this work, we set up honeypots, (\u201cdummy\u201d social network accounts), and buy fake followers (after careful IRB approval). We report the signs of such behaviors including oddities in local network connectivity, account attributes, and similarities and differences across fraud providers. Most valuably, we discover and characterize several types of fraud behaviors. We discuss how to leverage our insights in practice by engineering strongly performing entropy-based features and demonstrating high classification accuracy. Our contributions are (a) instrumentation: we detail our experimental setup and carefully engineered data collection process to scrape Twitter data while respecting API rate-limits, (b) observations on fraud multimodality: we analyze our honeypot fraudster ecosystem and give surprising insights into the multifaceted behaviors of these fraudster types, and (c) features: we propose novel features that give strong (>0.95 precision/recall) discriminative power on ground-truth Twitter data.", "target": "The Many Faces of Link Fraud"}
{"id": "task1540-5aa992ed9980425dace5f98cd3308ed7", "input": "Research has shown that convolutional neural networks contain significant redundancy, and high classification accuracy can be obtained even when weights and activations are reduced from floating point to binary values. In this paper, we present Finn, a framework for building fast and flexible FPGA accelerators using a flexible heterogeneous streaming architecture. By utilizing a novel set of optimizations that enable efficient mapping of binarized neural networks to hardware, we implement fully connected, convolutional and pooling layers, with per-layer compute resources being tailored to user-provided throughput requirements. On a ZC706 embedded FPGA platform drawing less than 25 W total system power, we demonstrate up to 12.3 million image classifications per second with 0.31 \u03bcs latency on the MNIST dataset with 95.8% accuracy, and 21906 image classifications per second with 283 \u03bcs latency on the CIFAR-10 and SVHN datasets with respectively 80.1% and 94.9% accuracy. To the best of our knowledge, ours are the fastest classification rates reported to date on these benchmarks.", "target": "FINN: A Framework for Fast, Scalable Binarized Neural Network Inference"}
{"id": "task1540-d6879b74ddaf4c29bf392690b143cc3b", "input": "The aim of this paper is to investigate the interplay between knowledge shared by a group of agents and its coalition ability. We characterize this relation in the standard context of imperfect information concurrent game. We assume that whenever a set of agents form a coalition to achieve a goal, they share their knowledge before acting. Based on this assumption, we propose new semantics for alternating-time temporal logic with imperfect information and perfect recall. It turns out that this semantics is sufficient to preserve all the desirable properties of coalition ability in traditional coalition logics. Meanwhile, we investigate how knowledge sharing within a group of agents contributes to its coalitional ability through the interplay of epistemic and coalition modalities. This work provides a partial answer to the question: which kind of group knowledge is required for a group to achieve their goals in the context of imperfect information.", "target": "Knowledge Sharing in Coalitions"}
{"id": "task1540-ca7210a8c0fc4161b3fd22518652b8c0", "input": "In this work, we propose CLass-Enhanced Attentive Response (CLEAR): an approach to visualize and understand the decisions made by deep neural networks (DNNs) given a specific input. CLEAR facilitates the visualization of attentive regions and levels of interest of DNNs during the decision-making process. It also enables the visualization of the most dominant classes associated with these attentive regions of interest. As such, CLEAR can mitigate some of the shortcomings of heatmap-based methods associated with decision ambiguity, and allows for better insights into the decision-making process of DNNs. Quantitative and qualitative experiments across three different datasets demonstrate the efficacy of CLEAR for gaining a better understanding of the inner workings of DNNs during the decision-making process.", "target": "Explaining the Unexplained: A CLass-Enhanced Attentive Response (CLEAR) Approach to Understanding Deep Neural Networks"}
{"id": "task1540-e548cb4952164568b081a99c6af89872", "input": "Asynchronous parallel implementations for stochastic optimization have received huge successes in theory and practice recently. Asynchronous implementations with lock-free are more efficient than the one with writing or reading lock. In this paper, we focus on a composite objective function consisting of a smooth convex function f and a block separable convex function, which widely exists in machine learning and computer vision. We propose an asynchronous stochastic block coordinate descent algorithm with the accelerated technology of variance reduction (AsySBCDVR), which are with lock-free in the implementation and analysis. AsySBCDVR is particularly important because it can scale well with the sample size and dimension simultaneously. We prove that AsySBCDVR achieves a linear convergence rate when the function f is with the optimal strong convexity property, and a sublinear rate when f is with the general convexity. More importantly, a near-linear speedup on a parallel system with shared memory can be obtained.", "target": "Asynchronous Stochastic Block Coordinate Descent with Variance Reduction"}
{"id": "task1540-40de837961a14cc9b655749b7a51cf0f", "input": "This work focuses on answering singlerelation factoid questions over Freebase. Each question can acquire the answer from a single fact of form (subject, predicate, object) in Freebase. This task, simple question answering (SimpleQA), can be addressed via a twostep pipeline: entity linking and fact selection. In fact selection, we match the subject entity in fact with the entity mention in question by a character-level convolutional neural network (char-CNN), and match the predicate in fact with the question by a word-level CNN (wordCNN). This work makes two main contributions. (i) A simple and effective entity linker over Freebase is proposed. Our entity linker outperforms the state-of-the-art entity linker of SimpleQA task. (ii) A novel attentive maxpooling is stacked over word-CNN, so that the predicate representation can be matched with the predicate-focused question representation more effectively. Experiments show that our system sets new state-of-the-art in this task.", "target": "Simple Question Answering by Attentive Convolutional Neural Network"}
{"id": "task1540-a1a90ccf75ad4e7491616924d288ca1d", "input": "We present Confidence-Based Autonomy (CBA), an interactive algorithm for policy learning from demonstration. The CBA algorithm consists of two components which take advantage of the complimentary abilities of humans and computer agents. The first component, Confident Execution, enables the agent to identify states in which demonstration is required, to request a demonstration from the human teacher and to learn a policy based on the acquired data. The algorithm selects demonstrations based on a measure of action selection confidence, and our results show that using Confident Execution the agent requires fewer demonstrations to learn the policy than when demonstrations are selected by a human teacher. The second algorithmic component, Corrective Demonstration, enables the teacher to correct any mistakes made by the agent through additional demonstrations in order to improve the policy and future task performance. CBA and its individual components are compared and evaluated in a complex simulated driving domain. The complete CBA algorithm results in the best overall learning performance, successfully reproducing the behavior of the teacher while balancing the tradeoff between number of demonstrations and number of incorrect actions during learning.", "target": "Interactive Policy Learning through Confidence-Based Autonomy"}
{"id": "task1540-d15dcce487024eee8a84f9dd8378f577", "input": "In this report, we will be interested at Dynamic Bayesian Network (DBNs) as a model that tries to incorporate temporal dimension with uncertainty. We start with basics of DBN where we especially focus in Inference and Learning concepts and algorithms. Then we will present different levels and methods of creating DBNs as well as approaches of incorporating temporal dimension in static Bayesian network. KeywordsDBN, DAG, Inference, Learning, HMM, EM Algorithm, SEM, MLE, coupled HMMs", "target": "Characterization of Dynamic Bayesian Network The Dynamic Bayesian Network as temporal network"}
{"id": "task1540-32ad8e18ca9a4184a3a4752d0a377f8b", "input": "Directed possibly cyclic graphs have been proposed by Didelez (2000) and Nodelmann et al. (2002) in order to represent the dynamic dependencies among stochastic processes. These dependencies are based on a generalization of Granger\u2013causality to continuous time, first developed by Schweder (1970) for Markov processes, who called them local dependencies. They deserve special attention as they are asymmetric. In this paper we focus on their graphical representation and develop an asymmetric notion of separation. The properties of this graph separation as well as local independence are investigated in detail within a framework of asymmetric (semi)graphoids allowing insight into what information can be read off these graphs.", "target": "Asymmetric Separation for Local Independence Graphs"}
{"id": "task1540-13d2b87347f14ebbadb8a7a44c3cb8ca", "input": "Constraint Programming (CP) solvers typically tackle optimization problems by repeatedly finding solutions to a problem while placing tighter and tighter bounds on the solution cost. This approach is somewhat naive, especially for soft-constraint optimization problems in which the soft constraints are mostly satisfied. Unsatisfiable-core approaches to solving soft constraint problems in SAT (e.g. MAXSAT) force all soft constraints to be hard initially. When solving fails they return an unsatisfiable core, as a set of soft constraints that cannot hold simultaneously. These are reverted to soft and solving continues. Since lazy clause generation solvers can also return unsatisfiable cores we can adapt this approach to constraint programming. We adapt the original MAXSAT unsatisfiable core solving approach to be usable for constraint programming and define a number of extensions. Experimental results show that our methods are beneficial on a broad class of CP-optimization benchmarks involving soft constraints, cardinality or preferences.", "target": "Unsatisfiable Cores and Lower Bounding for Constraint Programming"}
{"id": "task1540-ee2e8fdd5c0b4e56b41f7d8f7757cba7", "input": "We propose the concept of Action-Related Place (ARPlace) as a powerful and flexible representation of task-related place in the context of mobile manipulation. ARPlace represents robot base locations not as a single position, but rather as a collection of positions, each with an associated probability that the manipulation action will succeed when located there. ARPlaces are generated using a predictive model that is acquired through experience-based learning, and take into account the uncertainty the robot has about its own location and the location of the object to be manipulated. When executing the task, rather than choosing one specific goal position based only on the initial knowledge about the task context, the robot instantiates an ARPlace, and bases its decisions on this ARPlace, which is updated as new information about the task becomes available. To show the advantages of this least-commitment approach, we present a transformational planner that reasons about ARPlaces in order to optimize symbolic plans. Our empirical evaluation demonstrates that using ARPlaces leads to more robust and efficient mobile manipulation in the face of state estimation uncertainty on our simulated robot.", "target": "Learning and Reasoning with Action-Related Places for Robust Mobile Manipulation"}
{"id": "task1540-2815a6fca8d04bb3accdb469c4fa6647", "input": "In this paper, we establish some theoretical connections between Sum-Product Networks (SPNs) and Bayesian Networks (BNs). We prove that every SPN can be converted into a BN in linear time and space in terms of the network size. The key insight is to use Algebraic Decision Diagrams (ADDs) to compactly represent the local conditional probability distributions at each node in the resulting BN by exploiting context-specific independence (CSI). The generated BN has a simple directed bipartite graphical structure. We show that by applying the Variable Elimination algorithm (VE) to the generated BN with ADD representations, we can recover the original SPN where the SPN can be viewed as a history record or caching of the VE inference process. To help state the proof clearly, we introduce the notion of normal SPN and present a theoretical analysis of the consistency and decomposability properties. We conclude the paper with some discussion of the implications of the proof and establish a connection between the depth of an SPN and a lower bound of the tree-width of its corresponding BN.", "target": "On the Relationship between Sum-Product Networks and Bayesian Networks"}
{"id": "task1540-7df25aec3c4e43768b808521a67f58d7", "input": "In the Internet of Things (IoT) domain, various heterogeneous ubiquitous devices would be able to connect and communicate with each other seamlessly, irrespective of the domain. Semantic representation of data through detailed standardized annotation has shown to improve the integration of the interconnected heterogeneous devices. However, the semantic representation of these heterogeneous data sources for environmental monitoring systems is not yet well supported. To achieve the maximum benefits of IoT for drought forecasting, a dedicated semantic middleware solution is required. This research proposes a middleware that semantically represents and integrates heterogeneous data sources with indigenous knowledge based on a unified ontology for an accurate IoT-based drought early warning system (DEWS).", "target": "Towards Semantic Integration of Heterogeneous Sensor Data with Indigenous Knowledge for Drought Forecasting"}
{"id": "task1540-a1708346c3174b3a803f466f4a7390f1", "input": "Although a number of related algorithms have been developed to evaluate influence diagrams, exploiting the conditional independence in the diagram, the exact solution has remained intractable for many important problems. In this paper we introduce decision circuits as a means to exploit the local structure usually found in decision problems and to improve the performance of influence diagram analysis. This work builds on the probabilistic inference algorithms using arithmetic circuits to represent Bayesian belief networks [Darwiche, 2003]. Once compiled, these arithmetic circuits efficiently evaluate probabilistic queries on the belief network, and methods have been developed to exploit both the global and local structure of the network. We show that decision circuits can be constructed in a similar fashion and promise similar benefits.", "target": "Evaluating influence diagrams with decision circuits"}
{"id": "task1540-9bb1677fec814a4d852004a287b3fa00", "input": "We analyze and evaluate an online gradient descent algorithm with adaptive per-coordinate adjustment of learning rates. Our algorithm can be thought of as an online version of batch gradient descent with a diagonal preconditioner. This approach leads to regret bounds that are stronger than those of standard online gradient descent for general online convex optimization problems. Experimentally, we show that our algorithm is competitive with state-of-the-art algorithms for large scale machine learning problems.", "target": "Less Regret via Online Conditioning"}
{"id": "task1540-7af12ec9a971438bb9571ca0c696658b", "input": "We present two related methods for creating MasterPrints, synthetic fingerprints that a fingerprint verification system identifies as many different people. Both methods start with training a Generative Adversarial Network (GAN) on a set of real fingerprint images. The generator network is then used to search for images that can be recognized as multiple individuals. The first method uses evolutionary optimization in the space of latent variables, and the second uses gradient-based search. Our method is able to design a MasterPrint that a commercial fingerprint system matches to 22% of all users in a strict security setting, and 75% of all users at a looser security setting.", "target": "DeepMasterPrint: Generating Fingerprints for Presentation Attacks"}
{"id": "task1540-922ee293824146d7b1afe038520dce3c", "input": "The explosive growth of the location-enabled devices coupled with the increasing use of Internet services has led to an increasing awareness of the importance and usage of geospatial information in many applications. The navigation apps (often called \u201cMaps\u201d), use a variety of available data sources to calculate and predict the travel time as well as several options for routing in public transportation, car or pedestrian modes. This paper evaluates the pedestrian mode of Maps apps in three major smartphone operating systems (Android, iOS and Windows Phone). In the paper, we will show that the Maps apps on iOS, Android and Windows Phone in pedestrian mode, predict travel time without learning from the individual\u2019s movement profile. In addition, we will exemplify that those apps suffer from a specific data quality issue which relates to the absence of information about location and type of pedestrian crossings. Finally, we will illustrate learning from movement profile of individuals using various predictive analytics models to improve the accuracy of travel time estimation.", "target": "Predictive Analytics for Enhancing Travel Time Estimation in Navigation Apps of Apple, Google, and Microsoft"}
{"id": "task1540-1b72dc450b164216ac0140ff5d558614", "input": "Past research has challenged us with the task of showing relational patterns between text-based data and then clustering for predictive analysis using Golay Code technique. We focus on a novel approach to extract metaknowledge in multimedia datasets. Our collaboration has been an on-going task of studying the relational patterns between datapoints based on metafeatures extracted from metaknowledge in multimedia datasets. Those selected are significant to suit the mining technique we applied, Golay Code algorithm. In this research paper we summarize findings in optimization of metaknowledge representation for 23-bit representation of structured and unstructured multimedia data in order to be processed in 23-bit Golay Code for cluster recognition. Keywords\u2014 Big Multimedia Data Processing and Analytics; Information Retrieval Challenges; Content Identification, Metafeature Extraction and Selection; Metalearning System; 23-Bit Meta-knowledge template; Knowledge Discovery, Golay Code.", "target": "Novel Metaknowledge-based Processing Technique for Multimedia Big Data clustering challenges"}
{"id": "task1540-cbb54febb9d0425499285d30b279aad5", "input": "Opinion Mining and Sentiment Analysis is a process of identifying opinions in large unstructured/structured data and then analysing polarity of those opinions. Opinion mining and sentiment analysis have found vast application in analysing online ratings, analysing product based reviews, egovernance, and managing hostile content over the internet. This paper proposes an algorithm to implement aspect level sentiment analysis. The algorithm takes input from the remarks submitted by various teachers of a student. An aspect tree is formed which has various levels and weights are assigned to each branch to identify level of aspect. Aspect value is calculated by the algorithm by means of the proposed aspect tree. Dictionary based method is implemented to evaluate the polarity of the remark. The algorithm returns the aspect value clubbed with opinion value and sentiment value which helps in concluding the summarized value of remark. Keywords\u2014aspect tree, aspect value, opinion mining, opinion value, sentiment analysis", "target": "Aspect Based Sentiment Analysis to Extract Meticulous Opinion Value"}
{"id": "task1540-899f300880744a96b7669b9f6dfac9f9", "input": "In this paper, we describe a system for generating threedimensional visual simulations of natural language motion expressions. We use a rich formal model of events and their participants to generate simulations that satisfy the minimal constraints entailed by the associated utterance, relying on semantic knowledge of physical objects and motion events. This paper outlines technical considerations and discusses implementing the aforementioned semantic models into such a system.", "target": "Multimodal Semantic Simulations of Linguistically Underspecified Motion Events"}
{"id": "task1540-b3bbbb0bdaf94fd2bc87d6bb686707df", "input": "In a recent paper, Levy and Goldberg [2] pointed out an interesting connection between prediction-based word embedding models and count models based on pointwise mutual information. Under certain conditions, they showed that both models end up optimizing equivalent objective functions. This paper explores this connection in more detail and lays out the factors leading to differences between these models. We find that the most relevant differences from an optimization perspective are (i) predict models work in a low dimensional space where embedding vectors can interact heavily; (ii) since predict models have fewer parameters, they are less prone to overfitting. Motivated by the insight of our analysis, we show how count models can be regularized in a principled manner and provide closed-form solutions for L1 and L2 regularization. Finally, we propose a new embedding model with a convex objective and the additional benefit of being intelligible.", "target": "Towards a Better Understanding of Predict and Count Models"}
{"id": "task1540-ecb03bd404e746e4af8c6a18b1417e4e", "input": "We showed in this work how the Hassanat distance metric enhances the performance of the nearest neighbour classifiers. The results demonstrate the superiority of this distance metric over the traditional and most-used distances, such as Manhattan distance and Euclidian distance. Moreover, we proved that the Hassanat distance metric is invariant to data scale, noise and outliers. Throughout this work, it is clearly notable that both ENN and IINC performed very well with the distance investigated, as their accuracy increased significantly by 3.3% and 3.1% respectively, with no significant advantage of the ENN over the IINC in terms of accuracy. Correspondingly, it can be noted from our results that there is no optimal algorithm that can solve all reallife problems perfectly; this is supported by the no-free-lunch theorem.", "target": "ON ENHANCING THE PERFORMANCE OF NEAREST NEIGHBOUR CLASSIFIERS USING HASSANAT DISTANCE METRIC"}
{"id": "task1540-fcbc93956fce4d8684f7e8484600d313", "input": "In this paper, the idea of applying Computational Intelligence in the process of creation board games, in particular mazes, is presented. For two different algorithms the proposed idea has been examined. The results of the experiments are shown and discussed to present advantages and disadvantages. Keywords\u2014Computational Intelligence, Heuristic Algorithm", "target": "Is swarm intelligence able to create mazes?"}
{"id": "task1540-55960529e7164a23a3f5df29ccac43aa", "input": "Sentiment analysis predicts the presence of positive or negative emotions in a text document. In this paper, we consider higher dimensional extensions of the sentiment concept, which represent a richer set of human emotions. Our approach goes beyond previous work in that our model contains a continuous manifold rather than a finite set of human emotions. We investigate the resulting model, compare it to psychological observations, and explore its predictive capabilities.", "target": "The Manifold of Human Emotions"}
{"id": "task1540-ad75f3e4f08f4e1a89774d3739c752ac", "input": "claims trigrams trigrams dependencies AMT-trained fixed dependencies 3.699% 4.697% 5.974% 18.18% 3.99% 2.797% 3.696% 2.597% 3.297% FEATURES (TFIDFs of) SVM Classifier's Error Rate DATASET", "target": "Improving Automated Patent Claim Parsing: Dataset, System, and Experiments"}
{"id": "task1540-846a6cc4db7a411f86a34d0b0489694a", "input": "Acoustic models using probabilistic linear discriminant analysis (PLDA) capture the correlations within feature vectors using subspaces which do not vastly expand the model. This allows high dimensional and correlated feature spaces to be used, without requiring the estimation of multiple high dimension covariance matrices. In this letter we extend the recently presented PLDA mixture model for speech recognition through a tied PLDA approach, which is better able to control the model size to avoid overfitting. We carried out experiments uisng the Switchboard corpus, with both mel frequency cepstral coefficient features and bottleneck feature derived from a deep neural network. Reductions in word error rate were obtained by using tied PLDA, compared with the PLDA mixture model, subspace Gaussian mixture models, and deep neural networks.", "target": "Tied Probabilistic Linear Discriminant Analysis for Speech Recognition"}
{"id": "task1540-ad6503b23a59475d88b9aa2a41bbe6b2", "input": "Generative model has been one of the most common approaches for solving the Dialog State Tracking Problem with the capabilities to model the dialog hypotheses in an explicit manner. The most important task in such Bayesian networks models is constructing the most reliable user models by learning and reflecting the training data into the probability distribution of user actions conditional on networks\u2019 states. This paper provides an overall picture of the learning process in a Bayesian framework with an emphasize on the state-of-the-art theoretical analyses of the Expectation Maximization learning algorithm.", "target": "The Dialog State Tracking Challenge with Bayesian Approach"}
{"id": "task1540-63ec1b56dbb04fbc97d9ef4d4176d192", "input": "We study the skip-thought model proposed by Kiros et al. (2015) with neighborhood information as weak supervision. More specifically, we propose a skip-thought neighbor model to consider the adjacent sentences as a neighborhood. We train our skip-thought neighbor model on a large corpus with continuous sentences, and then evaluate the trained model on 7 tasks, which include semantic relatedness, paraphrase detection, and classification benchmarks. Both quantitative comparison and qualitative investigation are conducted. We empirically show that, our skip-thought neighbor model performs as well as the skip-thought model on evaluation tasks. In addition, we found that, incorporating an autoencoder path in our model didn\u2019t aid our model to perform better, while it hurts the performance of the skip-thought model.", "target": "Rethinking Skip-thought: A Neighborhood based Approach"}
{"id": "task1540-9f6b6bed5517440a959ca45f24bde517", "input": "We consider a transfer-learning problem by using the parameter transfer approach, where a suitable parameter of feature mapping is learned through one task and applied to another objective task. Then, we introduce the notion of the local stability of parametric feature mapping and parameter transfer learnability, and thereby derive a learning bound for parameter transfer algorithms. As an application of parameter transfer learning, we discuss the performance of sparse coding in selftaught learning. Although self-taught learning algorithms with plentiful unlabeled data often show excellent empirical performance, their theoretical analysis has not been studied. In this paper, we also provide the first theoretical learning bound for self-taught learning.", "target": "Learning Bound for Parameter Transfer Learning"}
{"id": "task1540-1b772fa288ee4363a92c90277c63fc1e", "input": "Robots will eventually be part of every household. It is thus critical to enable algorithms to learn from and be guided by non-expert users. In this paper, we bring a human in the loop, and enable a human teacher to give feedback to a learning agent in the form of natural language. We argue that a descriptive sentence can provide a much stronger learning signal than a numeric reward in that it can easily point to where the mistakes are and how to correct them. We focus on the problem of image captioning in which the quality of the output can easily be judged by non-experts. We propose a hierarchical phrase-based captioning model trained with policy gradients, and design a feedback network that provides reward to the learner by conditioning on the human-provided feedback. We show that by exploiting descriptive feedback our model learns to perform better than when given independently written human captions.", "target": "Teaching Machines to Describe Images via Natural Language Feedback"}
{"id": "task1540-09ead4392ab7458090b92ae91f4ef120", "input": "We introduce the first global recursive neural parsing model with optimality guarantees during decoding. To support global features, we give up dynamic programs and instead search directly in the space of all possible subtrees. Although this space is exponentially large in the sentence length, we show it is possible to learn an efficient A* parser. We augment existing parsing models, which have informative bounds on the outside score, with a global model that has loose bounds but only needs to model non-local phenomena. The global model is trained with a novel objective that encourages the parser to search both efficiently and accurately. The approach is applied to CCG parsing, improving state-of-the-art accuracy by 0.4 F1. The parser finds the optimal parse for 99.9% of held-out sentences, exploring on average only 190 subtrees.", "target": "Global Neural CCG Parsing with Optimality Guarantees"}
{"id": "task1540-195f436ad7d442ed82c0f6752333fd81", "input": "Deep reinforcement learning methods attain super-human performance in a wide range of environments. Such methods are grossly inefficient, often taking orders of magnitudes more data than humans to achieve reasonable performance. We propose Neural Episodic Control: a deep reinforcement learning agent that is able to rapidly assimilate new experiences and act upon them. Our agent uses a semi-tabular representation of the value function: a buffer of past experience containing slowly changing state representations and rapidly updated estimates of the value function. We show across a wide range of environments that our agent learns significantly faster than other state-of-the-art, general purpose deep reinforcement learning agents.", "target": "Neural Episodic Control"}
{"id": "task1540-02dc182a89254b79936f6dfcd5fb37bf", "input": "This paper details the implementation of an algorithm for automatically generating a high-level knowledge network to perform commonsense reasoning, specifically with the application of robotic task repair. The network is represented using a Bayesian Logic Network (BLN) (Jain, Waldherr, and Beetz 2009), which combines a set of directed relations between abstract concepts, including IsA, AtLocation, HasProperty, and UsedFor, with a corresponding probability distribution that models the uncertainty inherent in these relations. Inference over this network enables reasoning over the abstract concepts in order to perform appropriate objectconcepts in order to perform appropriate object substitution or to locate missing objects in the robot\u2019s environment. The structure of the network is generated by combining information from two existing knowledge sources: ConceptNet (Speer and Havasi 2012), and WordNet (Miller 1995). This is done in a \"situated\" manner by only including information relevant a given context. Results show that the generated network is able to accurately predict object categories, locations, properties, and affordances in three different household", "target": "Situated Structure Learning of a Bayesian Logic Network for Commonsense Reasoning"}
{"id": "task1540-b7f85b9e18ce409ba612669981b31f6e", "input": "We use some of the largest order statistics of the random projections of a reference signal to construct a binary embedding that is adapted to signals correlated with such signal. The embedding is characterized from the analytical standpoint and shown to provide improved performance on tasks such as classification in a reduced-dimensionality space. Keywords\u2014Binary Embeddings, Random projections", "target": "Binary adaptive embeddings from order statistics of random projections"}
{"id": "task1540-9de96c87197a44a0ae544faf6a063737", "input": "The recent application of RNN encoder-decoder models has resulted in substantial progress in fully data-driven dialogue systems, but evaluation remains a challenge. An adversarial loss could be a way to directly evaluate the extent to which generated dialogue responses sound like they came from a human. This could reduce the need for human evaluation, while more directly evaluating on a generative task. In this work, we investigate this idea by training an RNN to discriminate a dialogue model\u2019s samples from human-generated samples. Although we find some evidence this setup could be viable, we also note that many issues remain in its practical application. We discuss both aspects and conclude that future work is warranted.", "target": "Adversarial Evaluation of Dialogue Models"}
{"id": "task1540-e7b7b95bd04845e28a6ed131b6a7bcd4", "input": "This paper presents results of our experiments using the Ubuntu Dialog Corpus \u2013<lb>the largest publicly available multi-turn dialog corpus. First, we use an in-house<lb>implementation of previously reported models to do an independent evaluation<lb>using the same data. Second, we evaluate the performances of various LSTMs,<lb>Bi-LSTMs and CNNs on the dataset. Third, we create an ensemble by averaging<lb>predictions of multiple models. The ensemble further improves the performance<lb>and it achieves a state-of-the-art result for this dataset. Finally, we discuss our<lb>future plans using this corpus.", "target": "Improved Deep Learning Baselines for Ubuntu Corpus Dialogs"}
{"id": "task1540-7d388dd8a7584cf797be6cfa0e1c8555", "input": "In this paper we propose a neural network model with a novel Sequential Attention layer that extends soft attention by assigning weights to words in an input sequence in a way that takes into account not just how well that word matches a query, but how well surrounding words match. We evaluate this approach on the task of reading comprehension (Who did What and CNN) and show that it dramatically improves a strong baseline like the Stanford Reader. The resulting model is competitive with the state of the art.", "target": "Sequential Attention"}
{"id": "task1540-18ec3bd6ab3a497e9bdd8986e96c13fc", "input": "Deep networks rely on massive amounts of labeled data to learn powerful models. For a target task short of labeled data, transfer learning enables model adaptation from a different source domain. This paper addresses deep transfer learning under a more general scenario that the joint distributions of features and labels may change substantially across domains. Based on the theory of Hilbert space embedding of distributions, a novel joint distribution discrepancy is proposed to directly compare joint distributions across domains, eliminating the need of marginal-conditional factorization. Transfer learning is enabled in deep convolutional networks, where the dataset shifts may linger in multiple task-specific feature layers and the classifier layer. A set of joint adaptation networks are crafted to match the joint distributions of these layers across domains by minimizing the joint distribution discrepancy, which can be trained efficiently using back-propagation. Experiments show that the new approach yields state of the art results on standard domain adaptation datasets.", "target": "Deep Transfer Learning with Joint Adaptation Networks"}
{"id": "task1540-d1f2de787f344d129360a76ad2a890b0", "input": "Separable Bayesian Networks, or the Influence Model, are dynamic Bayesian Networks in which the conditional probability distribution can be separated into a function of only the marginal distribution of a node\u2019s parents, instead of the joint distributions. We describe the connection between an arbitrary Conditional Probability Table (CPT) and separable systems using linear algebra. We give an alternate proof to [Pfeffer00] on the equivalence of sufficiency and separability. We present a computational method for testing whether a given CPT is separable.", "target": "Linear Algebra Approach to Separable Bayesian Networks"}
{"id": "task1540-21e95016e96846ab809e12d41e69103d", "input": "We propose a novel approach to automatically produce multiple colorized versions of a grayscale image. Our method results from the observation that the task of automated colorization is relatively easy given a low-resolution version of the color image. We first train a conditional PixelCNN to generate a low resolution color for a given grayscale image. Then, given the generated low-resolution color image and the original grayscale image as inputs, we train a second CNN to generate a high-resolution colorization of an image. We demonstrate that our approach produces more diverse and plausible colorizations than existing methods, as judged by human raters in a \u201dVisual Turing Test\u201d.", "target": "PIXCOLOR: PIXEL RECURSIVE COLORIZATION"}
{"id": "task1540-c87da039b563493292efbe6bd1d9d16c", "input": "Fundamental discrepancy between first order logic and statistical inference (global versus local properties of universe) is shown to be the obstacle for integration of logic and probability in L.p. logic of Bacchus. To overcome the counterintuitiveness of L.p. behaviour, a 3-valued logic is proposed.", "target": "Beliefs and Probability in Bacchus\u2019 l.p. Logic: A 3-Valued Logic Solution to Apparent Counter-intuition"}
{"id": "task1540-d2a7146b7bf247f8a7d636946d2704ed", "input": "Positive unlabeled (PU) learning is useful in various practical situations, where there is a need to learn a classifier for a class of interest from an unlabeled data set, which may contain anomalies as well as samples from unknown classes. The learning task can be formulated as an optimization problem under the framework of statistical learning theory. Recent studies have theoretically analyzed its properties and generalization performance, nevertheless, little effort has been made to consider the problem of scalability, especially when large sets of unlabeled data are available. In this work we propose a novel scalable PU learning algorithm that is theoretically proven to provide the optimal solution, while showing superior computational and memory performance. Experimental evaluation confirms the theoretical evidence and shows that the proposed method can be successfully applied to a large variety of real-world problems involving PU learning.", "target": "Efficient Training for Positive Unlabeled Learning"}
{"id": "task1540-7b855f2e9018447592f76b6df8a12dd3", "input": "The present study introduces a method for improving the classification performance of imbalanced multiclass data streams from wireless body worn sensors. Data imbalance is an inherent problem in activity recognition caused by the irregular time distribution of activities, which are sequential and dependent on previous movements. We use conditional random fields (CRF), a graphical model for structured classification, to take advantage of dependencies between activities in a sequence. However, CRFs do not consider the negative effects of class imbalance during training. We propose a class-wise dynamically weighted CRF (dWCRF) where weights are automatically determined during training by maximizing the expected overall F-score. Our results based on three case studies from a healthcare application using a batteryless body worn sensor, demonstrate that our method, in general, improves overall and minority class F-score when compared to other CRF based classifiers and achieves similar or better overall and class-wise performance when compared to SVM based classifiers under conditions of limited training data. We also confirm the performance of our approach using an additional battery powered body worn sensor dataset, achieving similar results in cases of high class imbalance.", "target": "Learning from Imbalanced Multiclass Sequential Data Streams Using Dynamically Weighted Conditional Random Fields"}
{"id": "task1540-f67492689bf54838824bb58eab8c6b4e", "input": "State-space models are successfully used in many areas of science, engineering and economics to model time series and dynamical systems. We present a fully Bayesian approach to inference and learning (i.e. state estimation and system identification) in nonlinear nonparametric state-space models. We place a Gaussian process prior over the state transition dynamics, resulting in a flexible model able to capture complex dynamical phenomena. To enable efficient inference, we marginalize over the transition dynamics function and infer directly the joint smoothing distribution using specially tailored Particle Markov Chain Monte Carlo samplers. Once a sample from the smoothing distribution is computed, the state transition predictive distribution can be formulated analytically. Our approach preserves the full nonparametric expressivity of the model and can make use of sparse Gaussian processes to greatly reduce computational complexity.", "target": "Bayesian Inference and Learning in Gaussian Process State-Space Models with Particle MCMC"}
{"id": "task1540-1faee0146cba407b8b215df0b0069865", "input": "MXNet is a multi-language machine learning (ML) library to ease the development of ML algorithms, especially for deep neural networks. Embedded in the host language, it blends declarative symbolic expression with imperative tensor computation. It offers auto differentiation to derive gradients. MXNet is computation and memory efficient and runs on various heterogeneous systems, ranging from mobile devices to distributed GPU clusters. This paper describes both the API design and the system implementation of MXNet, and explains how embedding of both symbolic expression and tensor operation is handled in a unified fashion. Our preliminary experiments reveal promising results on large scale deep neural network applications using multiple GPU machines.", "target": "MXNet: A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems"}
{"id": "task1540-8b56d0a230964ae9a29a4913fcfc0723", "input": "Conventional dependency parsers rely on a statistical model and a transition system or graph algorithm to enforce tree-structured outputs during training and inference. In this work we formalize dependency parsing as the problem of selecting the head (a.k.a. parent) of each word in a sentence. Our model which we call DENSE (as shorthand for Dependency Neural Selection) employs bidirectional recurrent neural networks for the head selection task. Without enforcing any structural constraints during training, DENSE generates (at inference time) trees for the overwhelming majority of sentences (95% on an English dataset), while remaining non-tree outputs can be adjusted with a maximum spanning tree algorithm. We evaluate DENSE on four languages (English, Chinese, Czech, and German) with varying degrees of non-projectivity. Despite the simplicity of our approach, experiments show that the resulting parsers are on par with or outperform the state of the art.", "target": "Dependency Parsing as Head Selection"}
{"id": "task1540-1eebaa1f6771459a8509158c5ccf83ae", "input": "Retrieving spoken content with spoken queries, or query-byexample spoken term detection (STD), is attractive because it makes possible the matching of signals directly on the acoustic level without transcribing them into text. Here, we propose an end-to-end query-by-example STD model based on an attention-based multi-hop network, whose input is a spoken query and an audio segment containing several utterances; the output states whether the audio segment includes the query. The model can be trained in either a supervised scenario using labeled data, or in an unsupervised fashion. In the supervised scenario, we find that the attention mechanism and multiple hops improve performance, and that the attention weights indicate the time span of the detected terms. In the unsupervised setting, the model mimics the behavior of the existing query-by-example STD system, yielding performance comparable to the existing system but with a lower search time complexity.", "target": "QUERY-BY-EXAMPLE SPOKEN TERM DETECTION USING ATTENTION-BASED MULTI-HOP NETWORKS"}
{"id": "task1540-34fcd74e1e5a4832b3347d9ba934303e", "input": "We proposed a deep learning method for interpretable diabetic retinopathy (DR) detection. The visualinterpretable feature of the proposed method is achieved by adding the regression activation map (RAM) after the global averaging pooling layer of the convolutional networks (CNN). With RAM, the proposed model can localize the discriminative regions of an retina image to show the specific region of interest in terms of its severity level. We believe this advantage of the proposed deep learning model is highly desired for DR detection because in practice, users are not only interested with high prediction performance, but also keen to understand the insights of DR detection and why the adopted learning model works. In the experiments conducted on a large scale of retina image dataset, we show that the proposed CNN model can achieve high performance on DR detection compared with the state-ofthe-art while achieving the merits of providing the RAM to highlight the salient regions of the input image.", "target": "Diabetic Retinopathy Detection via Deep Convolutional Networks for Disciminative Localization and Visual Explanation"}
{"id": "task1540-9ce1c080dc5a4a3ca21cf59449a16839", "input": "Training Deep Neural Networks is complicated by the fact that the distribution of each layer\u2019s inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batchnormalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9% top-5 validation error (and 4.8% test error), exceeding the accuracy of human raters.", "target": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"}
{"id": "task1540-699fa37c8bdf461db66c11f940a3db70", "input": "Humans are generally good at learning abstract concepts about objects and scenes (e.g. spatial orientation, relative sizes, etc.). Over the last years convolutional neural networks have achieved almost human performance in recognizing concrete classes (i.e. specific object categories). This paper tests the performance of a current CNN (GoogLeNet) on the task of differentiating between abstract classes which are trivially differentiable for humans. We trained and tested the CNN on the two abstract classes of horizontal and vertical orientation and determined how well the network is able to transfer the learned classes to other, previously unseen objects.", "target": "Learning Abstract Classes using Deep Learning"}
{"id": "task1540-ddf75108ee44421aaed528dccbb9dfdb", "input": "Decision-making problems in uncertain or stochastic domains are often formulated as Markov decision processes (MD Ps). Pol\u00ad icy iteration (PI) is a popular algorithm for searching over policy-space, the size of which is exponential in the number of states. We are interested in bounds on the complexity of PI that do not depend on the value of the discount factor. In this paper we prove the first such non-trivial, worst-case, upper bounds on the number of iterations required by PI to converge to the optimal policy. Our analysis also sheds new light on the manner in which PI progresses through the space of policies.", "target": "On the Complexity of Policy Iteration"}
{"id": "task1540-063121feb6eb4f0eaf05fa1ae71fcbc7", "input": "<lb>Recent price-of-anarchy analyses of games of complete information suggest that coarse correlated<lb>equilibria, which characterize outcomes resulting from no-regret learning dynamics, have near-optimal<lb>welfare. This work provides two main technical results that lift this conclusion to games of incomplete<lb>information, a.k.a., Bayesian games. First, near-optimal welfare in Bayesian games follows directly from<lb>the smoothness-based proof of near-optimal welfare in the same game when the private information<lb>is public. Second, no-regret learning dynamics converge to Bayesian coarse correlated equilibrium in<lb>these incomplete information games. These results are enabled by interpretation of a Bayesian game<lb>as a stochastic game of complete information.", "target": "No-Regret Learning in Repeated Bayesian Games"}
{"id": "task1540-b7fe8c9fd7ab45a7b986ac55c32da464", "input": "We propose and compare various sentence selection strategies for active learning for the task of detecting mentions of entities. The best strategy employs the sum of con dences of two statistical classi ers trained on di erent views of the data. Our experimental results show that, compared to the random selection strategy, this strategy reduces the amount of required labeled training data by over 50% while achieving the same performance. The e ect is even more signi cant when only named mentions are considered: the system achieves the same performance by using only 42% of the training data required by the random selection strategy.", "target": "Active Learning for Mention Detection: A Comparison of Sentence Selection Strategies"}
{"id": "task1540-8e8d80205de5477893ae045c5efd913f", "input": "Reinforcement learning is a powerful technique to train an agent to perform a task. However, an agent that is trained using reinforcement learning is only capable of achieving the single task that is specified via its reward function. Such an approach does not scale well to settings in which an agent needs to perform a diverse set of tasks, such as navigating to varying positions in a room or moving objects to varying locations. Instead, we propose a method that allows an agent to automatically discover the range of tasks that it is capable of performing in its environment. We use a generator network to propose tasks for the agent to try to achieve, each task being specified as reaching a certain parametrized sub-set of the state-space. The generator network is optimized using adversarial training to produce tasks that are always at the appropriate level of difficulty for the agent. Our method thus automatically produces a curriculum of tasks for the agent to learn. We show that, by using this framework, an agent can efficiently and automatically learn to perform a wide set of tasks without requiring any prior knowledge of its environment. Our method can also learn to achieve tasks with sparse rewards, which traditionally pose significant challenges.", "target": "Automatic Goal Generation for Reinforcement Learning Agents"}
{"id": "task1540-cfcfafb4fdbe449ab0444d97c8e3e5b7", "input": "In cocktail party listening scenarios, the human brain is able to separate competing speech signals. However, the signal processing implemented by the brain to perform cocktail party listening is not well understood. Here, we trained two separate convolutive autoencoder deep neural networks (DNN) to separate monaural and binaural mixtures of two concurrent speech streams. We then used these DNNs as convolutive deep transform (CDT) devices to perform probabilistic re-synthesis. The CDTs operated directly in the time-domain. Our simulations demonstrate that very simple neural networks are capable of exploiting monaural and binaural information available in a cocktail party listening scenario.", "target": "Deep Transform: Cocktail Party Source Separation via Probabilistic Re-Synthesis"}
{"id": "task1540-b5c256581eb4417e91570ec636b187c4", "input": "To harness modern multicore processors, it is imperative to develop parallel versions of fundamental algorithms. In this paper, we compare different approaches to parallel best-first search in a shared-memory setting. We present a new method, PBNF, that uses abstraction to partition the state space and to detect duplicate states without requiring frequent locking. PBNF allows speculative expansions when necessary to keep threads busy. We identify and fix potential livelock conditions in our approach, proving its correctness using temporal logic. Our approach is general, allowing it to extend easily to suboptimal and anytime heuristic search. In an empirical comparison on STRIPS planning, grid pathfinding, and sliding tile puzzle problems using 8-core machines, we show that A*, weighted A* and Anytime weighted A* implemented using PBNF yield faster search than improved versions of previous parallel search proposals.", "target": "Best-First Heuristic Search for Multicore Machines"}
{"id": "task1540-fe56469b435e472c9fa4b5b05c0cd9ad", "input": "In this paper \u2217\u2013compatible extensions of fuzzy relations are studied, generalizing some results obtained by Duggan in case of crisp relations. From this general result are obtained as particular cases fuzzy versions of some important extension theorems for crisp relations (Szpilrajn, Hansson, Suzumura). Two notions of consistent closure of a fuzzy relation are introduced.", "target": "Compatible extensions and consistent closures: a fuzzy approach"}
{"id": "task1540-bcef4e0abb0043e5b2d0e2e391b39eb5", "input": "Minimum vertex cover problem is an NP-Hard problem with the aim of finding minimum number of vertices to cover graph. In this paper, a learning automaton based algorithm is proposed to find minimum vertex cover in graph. In the proposed algorithm, each vertex of graph is equipped with a learning automaton that has two actions in the candidate or noncandidate of the corresponding vertex cover set. Due to characteristics of learning automata, this algorithm significantly reduces the number of covering vertices of graph. The proposed algorithm based on learning automata iteratively minimize the candidate vertex cover through the update its action probability. As the proposed algorithm proceeds, a candidate solution nears to optimal solution of the minimum vertex cover problem. In order to evaluate the proposed algorithm, several experiments conducted on DIMACS dataset which compared to conventional methods. Experimental results show the major superiority of the proposed algorithm over the other methods. Keywords\u2014 Minimum Vertex Cover; NP-Hard problems; Learning Automata; Distributed learning automata.", "target": "Solving Minimum Vertex Cover Problem Using Learning Automata"}
{"id": "task1540-919a71c074384bf8a4ca9633c6276579", "input": "There is growing interest in representing image data and feature descriptors using compact binary codes for fast near neighbor search. Although binary codes are motivated by their use as direct indices (addresses) into a hash table, codes longer than 32 bits are not being used as such, as it was thought to be ineffective. We introduce a rigorous way to build multiple hash tables on binary code substrings that enables exact k-nearest neighbor search in Hamming space. The approach is straightforward to implement and storage efficient. Theoretical analysis shows that the algorithm exhibits sub-linear run-time behavior for uniformly distributed codes. Empirical results show dramatic speed-ups over a linear scan baseline for datasets of up to one billion codes of 64, 128, or 256 bits.", "target": "Fast Exact Search in Hamming Space with Multi-Index Hashing"}
{"id": "task1540-0610589040524db68fa60562554b04ca", "input": "In this paper we present an Action Language-Answer Set Programming based approach to solving planning and scheduling problems in hybrid domains domains that exhibit both discrete and continuous behavior. We use action language H to represent the domain and then translate the resulting theory into an A-Prolog program. In this way, we reduce the problem of finding solutions to planning and scheduling problems to computing answer sets of A-Prolog programs. We cite a planning and scheduling example from the literature and show how to model it in H. We show how to translate the resulting H theory into an equivalent A-Prolog program. We compute the answer sets of the resulting program using a hybrid solver called EZCSP which loosely integrates a constraint solver with an answer set solver. The solver allows us reason about constraints over reals and compute solutions to complex planning and scheduling problems. Results have shown that our approach can be applied to any planning and scheduling problem in hybrid domains.", "target": "Planning and Scheduling in Hybrid Domains Using Answer Set Programming"}
{"id": "task1540-776ebce04e8e4bd6812eadc1476504f6", "input": "The success of deep learning in numerous application domains created the desire to run and train them on mobile devices. This however, conflicts with their computationally, memory and energy intense nature, leading to a growing interest in compression. Recent work by Han et al. (2015a) propose a pipeline that involves retraining, pruning and quantization of neural network weights, obtaining state-of-the-art compression rates. In this paper, we show that competitive compression rates can be achieved by using a version of \u201dsoft weight-sharing\u201d (Nowlan & Hinton, 1992). Our method achieves both quantization and pruning in one simple (re-)training procedure. This point of view also exposes the relation between compression and the minimum description length (MDL) principle.", "target": "NEURAL NETWORK COMPRESSION"}
{"id": "task1540-fe3cb0d7087e4dada1b627e9dea6e032", "input": "We define a notion of rational closure for the logic SHIQ, which does not enjoys the finite model property, building on the notion of rational closure introduced by Lehmann and Magidor in [23]. We provide a semantic characterization of rational closure in SHIQ in terms of a preferential semantics, based on a finite rank characterization of minimal models. We show that the rational closure of a TBox can be computed in EXPTIME using entailment in SHIQ.", "target": "Rational closure in SHIQ"}
{"id": "task1540-e17a0c9da274461195a3cb5973b01fad", "input": "In this work we consider the stochastic minimization of nonsmooth convex loss functions, a central problem in machine learning. We propose a novel algorithm called Accelerated Nonsmooth Stochastic Gradient Descent (ANSGD), which exploits the structure of common nonsmooth loss functions to achieve optimal convergence rates for a class of problems including SVMs. It is the first stochastic algorithm that can achieve the optimal O(1/t) rate for minimizing nonsmooth loss functions (with strong convexity). The fast rates are confirmed by empirical comparisons, in which ANSGD significantly outperforms previous subgradient descent algorithms including SGD.", "target": "Stochastic Smoothing for Nonsmooth Minimizations: Accelerating SGD by Exploiting Structure"}
{"id": "task1540-a9efca9a29224a669624ac4b6fd8357b", "input": "This paper presents Centre for Development of Advanced Computing Mumbai\u2019s (CDACM) submission to the NLP Tools Contest on Part-Of-Speech (POS) Tagging For Code-mixed Indian Social Media Text (POSCMISMT) 2015 (collocated with ICON 2015). We submitted results for Hindi (hi), Bengali (bn), and Telugu (te) languages mixed with English (en). In this paper, we have described our approaches to the POS tagging techniques, we exploited for this task. Machine learning has been used to POS tag the mixed language text. For POS tagging, distributed representations of words in vector space (word2vec) for feature extraction and Log-linear models have been tried. We report our work on all three languages hi, bn, and te mixed with en.", "target": "Experiments with POS Tagging Code-mixed Indian Social Media Text"}
{"id": "task1540-fba279a0122442c095feb254eec8d004", "input": "In this paper we discuss a novel framework for multiclass learning, defined by a suitable coding/decoding strategy, namely the simplex coding, that allows to generalize to multiple classes a relaxation approach commonly used in binary classification. In this framework, a relaxation error analysis can be developed avoiding constraints on the considered hypotheses class. Moreover, we show that in this setting it is possible to derive the first provably consistent regularized method with training/tuning complexity which is independent to the number of classes. Tools from convex analysis are introduced that can be used beyond the scope of this paper.", "target": "Multiclass Learning with Simplex Coding"}
{"id": "task1540-138ecd7cd85d41e19fa8d785126dd8ab", "input": "This paper describes our solution to the multi-modal learning challenge of ICML. This solution comprises constructing threelevel representations in three consecutive stages and choosing correct tag words with a data-specific strategy. Firstly, we use typical methods to obtain level-1 representations. Each image is represented using MPEG-7 and gist descriptors with additional features released by the contest organizers. And the corresponding word tags are represented by bag-of-words model with a dictionary of 4000 words. Secondly, we learn the level-2 representations using two stacked RBMs for each modality. Thirdly, we propose a bimodal auto-encoder to learn the similarities/dissimilarities between the pairwise image-tags as level-3 representations. Finally, during the test phase, based on one observation of the dataset, we come up with a data-specific strategy to choose the correct tag words leading to a leap of an improved overall performance. Our final average accuracy on the private test set is 100%, which ranks the first place in this challenge.", "target": "Constructing Hierarchical Image-tags Bimodal Representations  for Word Tags Alternative Choice"}
{"id": "task1540-6c6754d912a14331ac8b6c4459ab7a97", "input": "The Teacher Forcing algorithm trains recurrent networks by supplying observed sequence values as inputs during training and using the network\u2019s own one-stepahead predictions to do multi-step sampling. We introduce the Professor Forcing algorithm, which uses adversarial domain adaptation to encourage the dynamics of the recurrent network to be the same when training the network and when sampling from the network over multiple time steps. We apply Professor Forcing to language modeling, vocal synthesis on raw waveforms, handwriting generation, and image generation. Empirically we find that Professor Forcing acts as a regularizer, improving test likelihood on character level Penn Treebank and sequential MNIST. We also find that the model qualitatively improves samples, especially when sampling for a large number of time steps. This is supported by human evaluation of sample quality. Trade-offs between Professor Forcing and Scheduled Sampling are discussed. We produce T-SNEs showing that Professor Forcing successfully makes the dynamics of the network during training and sampling more similar.", "target": "Professor Forcing: A New Algorithm for Training Recurrent Networks"}
{"id": "task1540-9c95662d9e9f460296f7b3e4508c2c67", "input": "Multilingual spoken dialogue systems have gained prominence in the recent past necessitating the requirement for a front-end Language Identification (LID) system. Most of the existing LID systems rely on modeling the language discriminative information from low-level acoustic features. Due to the variabilities of speech (speaker and emotional variabilities, etc.), large-scale LID systems developed using low-level acoustic features suffer from a degradation in the performance. In this approach, we have attempted to model the higher level language discriminative phonotactic information for developing an LID system. In this paper, the input speech signal is tokenized to phone sequences by using a language independent phone recognizer. The language discriminative phonotactic information in the obtained phone sequences are modeled using statistical and recurrent neural network based language modeling approaches. As this approach, relies on higher level phonotactical information it is more robust to variabilities of speech. Proposed approach is computationally light weight, highly scalable and it can be used in complement with the existing LID systems.", "target": "A LANGUAGE MODEL BASED APPROACH TOWARDS LARGE SCALE AND LIGHTWEIGHT LANGUAGE IDENTIFICATION SYSTEMS"}
{"id": "task1540-2f2243639b62431e929c2ac043b34579", "input": "Learning effective configurations in computer systems without hand-crafting models for every parameter is a long-standing problem. This paper investigates the use of deep reinforcement learning for runtime parameters of cloud databases under latency constraints. Cloud services serve up to thousands of concurrent requests per second and can adjust critical parameters by leveraging performance metrics. In this work, we use continuous deep reinforcement learning to learn optimal cache expirations for HTTP caching in content delivery networks. To this end, we introduce a technique for asynchronous experience management called delayed experience injection, which facilitates delayed reward and next-state computation in concurrent environments where measurements are not immediately available. Evaluation results show that our approach based on normalized advantage functions and asynchronous CPU-only training outperforms a statistical estimator.", "target": "Learning Runtime Parameters in Computer Systems with Delayed Experience Injection"}
{"id": "task1540-74763bf8518b40b08db2a80542f29d05", "input": "Security surveillance is one of the most important issues in smart cities, especially in an era of terrorism. Deploying a number of (video) cameras is a common surveillance approach. Given the never-ending power offered by vehicles to metropolises, exploiting vehicle traffic to design camera placement strategies could potentially facilitate security surveillance. This article constitutes the first effort toward building the linkage between vehicle traffic and security surveillance, which is a critical problem for smart cities. We expect our study could influence the decision making of surveillance camera placement, and foster more research of principled ways of security surveillance beneficial to our physical-world life.", "target": "Vehicle Traffic Driven Camera Placement for Better Metropolis Security Surveillance"}
{"id": "task1540-388ec181bbed4262982cee313ed311bb", "input": "The role of kernels is central to machine learning. Motivated by the importance of power law distributions in modeling, simulation and learning, in this paper, we propose a powerlaw generalization of the Gaussian kernel. This generalization is based on q-Gaussian distribution, which is a power-law distribution studied in context of nonextensive statistical mechanics. We prove that the proposed kernel is positive definite, and provide some insights regarding the corresponding Reproducing Kernel Hilbert Space (RKHS). We also study practical significance of qGaussian kernels in classification, regression and clustering, and present some simulation results.", "target": "On q-Gaussian kernel and its Reproducing Kernel Hilbert Space"}
{"id": "task1540-38b52bd3318242159db68b841413a07d", "input": "Many intelligent user interfaces employ applica\u00ad tion and user models to determine the user's pref\u00ad erences, goals and likely future actions. Such models require application analysis, adaptation and expansion. Building and maintaining such models adds a substantial amount of time and labour to the application development cycle. We present a system that observes the interface of an unmodified application and records users' inter\u00ad actions with the application. From a history of such observations we build a coarse state space of observed interface states and actions between them. To refine the space, we hypothesize sub\u00ad states based upon the histories that led users to a given state. We evaluate the information gain of possible state splits, varying the length of the histories considered in such splits. In this way, we automatically produce a stochastic dynamic model of the application and of how it is used. To evaluate our approach, we present models de\u00ad rived from real-world application usage data.", "target": "Building a Stochastic Dynamic Model of Application Use"}
{"id": "task1540-576f82dc08cb4d4281618a76860fcc50", "input": "This is a preliminary report on the work aimed at making CR-Prolog \u2013 a version of ASP with consistency restoring rules \u2013 more suitable for use in teaching and large applications. First we describe a sorted version of CR-Prolog called SPARC. Second, we translate a basic version of the CR-Prolog into the language of DLV and compare the performance with the state of the art CR-Prolog solver. The results form the foundation for future more efficient and user friendly implementation of SPARC and shed some light on the relationship between two useful knowledge representation constructs: consistency restoring rules and weak constraints of DLV.", "target": "SPARC \u2013 Sorted ASP with Consistency Restoring Rules"}
{"id": "task1540-3bf53c52f2df48dd9ee1fe09c340f78f", "input": "Despite being so vital to success of Support Vector Machines, the principle of separating margin maximisation is not used in deep learning. We show that minimisation of margin variance and not maximisation of the margin is more suitable for improving generalisation in deep architectures. We propose the Halfway loss function that minimises the Normalised Margin Variance (NMV) at the output of a deep learning models and evaluate its performance against the Softmax Cross-Entropy loss on the MNIST, smallNORB and CIFAR-10 datasets.", "target": "Effects of the optimisation of the margin distribution on generalisation in deep architectures"}
{"id": "task1540-d3d37656ec8f487fbf81680b0c25a6db", "input": "In this paper, we improve the attention or alignment accuracy of neural machine translation by utilizing the alignments of training sentence pairs. We simply compute the distance between the machine attentions and the \u201ctrue\u201d alignments, and minimize this cost in the training procedure. Our experiments on large-scale Chinese-to-English task show that our model improves both translation and alignment qualities significantly over the large-vocabulary neural machine translation system, and even beats a state-of-the-art traditional syntax-based system.", "target": "Supervised Attentions for Neural Machine Translation"}
{"id": "task1540-16fd58e410be4388a74f586c606954c4", "input": "Large-scale knowledge bases have currently reached impressive sizes; however, these knowledge bases are still far from complete. In addition, most of the existing methods for knowledge base completion only consider the direct links between entities, ignoring the vital impact of the consistent semantics of relation paths. In this paper, we study the problem of how to better embed entities and relations of knowledge bases into different low-dimensional spaces by taking full advantage of the additional semantics of relation paths, and we propose a compositional learning model of relation path embedding (RPE). Specifically, with the corresponding relation and path projections, RPE can simultaneously embed each entity into two types of latent spaces. It is also proposed that type constraints could be extended from traditional relation-specific constraints to the new proposed path-specific constraints. The results of experiments show that the proposed model achieves significant and consistent improvements compared with the state-of-the-art algorithms.", "target": "Compositional Learning of Relation Path Embedding for Knowledge Base Completion"}
{"id": "task1540-7387a5cd905244019aff1e101ee22894", "input": "Words in natural language follow a Zipfian distribution whereby some words are frequent but most are rare. Learning representations for words in the \u201clong tail\u201d of this distribution requires enormous amounts of data. Representations of rare words trained directly on end-tasks are usually poor, requiring us to pre-train embeddings on external data, or treat all rare words as out-of-vocabulary words with a unique representation. We provide a method for predicting embeddings of rare words on the fly from small amounts of auxiliary data with a network trained against the end task. We show that this improves results against baselines where embeddings are trained on the end task in a reading comprehension task, a recognizing textual entailment task, and in language modelling.", "target": "Learning to Compute Word Embeddings On the Fly"}
{"id": "task1540-288a12145431418eab0e5320df9e167e", "input": "Hierarchical Reinforcement Learning (HRL) exploits temporal abstraction to solve large Markov Decision Processes (MDP) and provide transferable subtask policies. In this paper, we introduce an off-policy HRL algorithm: Hierarchical Q-value Iteration (HQI). We show that it is possible to effectively learn recursive optimal policies for any valid hierarchical decomposition of the original MDP, given a fixed dataset collected from a flat stochastic behavioral policy. We first formally prove the convergence of the algorithm for tabular MDP. Then our experiments on the Taxi domain show that HQI converges faster than a flat Q-value Iteration and enjoys easy state abstraction. Also, we demonstrate that our algorithm is able to learn optimal policies for different hierarchical structures from the same fixed dataset, which enables model comparison without recollecting data.", "target": "Algorithms for Batch Hierarchical Reinforcement Learning"}
{"id": "task1540-c5077e7ff76840b1a784fc40a1eaa541", "input": "Existing studies on semantic parsing mainly focus on the in-domain setting. We formulate cross-domain semantic parsing as a domain adaptation problem: train a semantic parser on some source domains and then adapt it to the target domain. Due to the diversity of logical forms in different domains, this problem presents unique and intriguing challenges. By converting logical forms into canonical utterances in natural language, we reduce semantic parsing to paraphrasing, and develop an attentive sequence-to-sequence paraphrase model that is general and flexible to adapt to different domains. We discover two problems, small micro variance and large macro variance, of pretrained word embeddings that hinder their direct use in neural networks, and propose standardization techniques as a remedy. On the popular OVERNIGHT dataset, which contains eight domains, we show that both cross-domain training and standardized pre-trained word embedding can bring significant improvement.", "target": "Cross-domain Semantic Parsing via Paraphrasing"}
{"id": "task1540-d3918c9b321342d2ab83078b1c0f4da4", "input": "Translating in real-time, a.k.a. simultaneous translation, outputs translation words before the input sentence ends, which is a challenging problem for conventional machine translation methods. We propose a neural machine translation (NMT) framework for simultaneous translation in which an agent learns to make decisions on when to translate from the interaction with a pre-trained NMT environment. To trade off quality and delay, we extensively explore various targets for delay and design a method for beam-search applicable in the simultaneous MT setting. Experiments against state-of-the-art baselines on two language pairs demonstrate the efficacy of the proposed framework both quantitatively and qualitatively. 1", "target": "Learning to Translate in Real-time with Neural Machine Translation"}
{"id": "task1540-71765e186dc4483288ae02246eb7c5ee", "input": "Service level agreement (SLA) is an essential part of cloud systems to ensure maximum availability of services for customers. With a violation of SLA, the provider has to pay penalties. Thus, being able to predict SLA violations favors both the customers and the providers. In this paper, we explore two machine learning models: Naive Bayes and Random Forest Classifiers to predict SLA violations. Since SLA violations are a rare event in the real world (\u223c 0.2%), the classification task becomes more challenging. In order to overcome these challenges, we use several re-sampling methods such as Random Over and Under Sampling, SMOTH, NearMiss (1,2,3), One-sided Selection, Neighborhood Cleaning Rule, etc. to re-balance the dataset. We use the Google Cloud Cluster trace as the dataset to examine these different methods. We find that random forests with SMOTE-ENN re-sampling have the best performance among other methods with the accuracy of 0.9988% and F1 score of 0.9980.", "target": "SLA Violation Prediction In Cloud Computing: A Machine Learning Perspective"}
{"id": "task1540-2dbf55ba1b1943e08804906a392b7a6a", "input": "The interest in the combination of probability with logics for modeling the world has rapidly increased in the last few years. One of the most effective approaches is the Distribution Semantics which was adopted by many logic programming languages and in Descripion Logics. In this paper, we illustrate the work we have done in this research field by presenting a probabilistic semantics for description logics and reasoning and learning algorithms. In particular, we present in detail the system TRILL , which computes the probability of queries w.r.t. probabilistic knowledge bases, which has been implemented in Prolog. Note: An extended abstract / full version of a paper accepted to be presented at the Doctoral Consortium of the 30th International Conference on Logic Programming (ICLP 2014), July 19-22, Vienna, Austria", "target": "Reasoning with Probabilistic Logics"}
{"id": "task1540-1a0aa56b2d614af2821581c478e9e89b", "input": "This paper presents the experiments carried out by us at Jadavpur University as part of the participation in FIRE 2015 task: Entity Extraction from Social Media Text Indian Languages (ESM-IL). The tool that we have developed for the task is based on Trigram Hidden Markov Model that utilizes information like gazetteer list, POS tag and some other word level features to enhance the observation probabilities of the known tokens as well as unknown tokens. We submitted runs for English only. A statistical HMM (Hidden Markov Models) based model has been used to implement our system. The system has been trained and tested on the datasets released for FIRE 2015 task: Entity Extraction from Social Media Text Indian Languages (ESM-IL). Our system is the best performer for English language and it obtains precision, recall and F-measures of 61.96, 39.46 and 48.21 respectively.", "target": "A Hidden Markov Model Based System for Entity Extraction from Social Media English Text at FIRE 2015"}
{"id": "task1540-f42eb4df6a1b42a1880e66ee9e31b87c", "input": "In this paper we present an approach to polyphonic sound event detection in real life recordings based on bi-directional long short term memory (BLSTM) recurrent neural networks (RNNs). A single multilabel BLSTM RNN is trained to map acoustic features of a mixture signal consisting of sounds from multiple classes, to binary activity indicators of each event class. Our method is tested on a large database of real-life recordings, with 61 classes (e.g. music, car, speech) from 10 different everyday contexts. The proposed method outperforms previous approaches by a large margin, and the results are further improved using data augmentation techniques. Overall, our system reports an average F1-score of 65.5% on 1 second blocks and 64.7% on single frames, a relative improvement over previous state-of-the-art approach of 6.8% and 15.1% respectively.", "target": "RECURRENT NEURAL NETWORKS FOR POLYPHONIC SOUND EVENT DETECTION IN REAL LIFE RECORDINGS"}
{"id": "task1540-5a3867e1930149e1a3073e73d104936d", "input": "In state-of-the-art Neural Machine Translation, an attention mechanism is used during decoding to enhance the translation. At every step, the decoder uses this mechanism to focus on different parts of the source sentence to gather the most useful information before outputting its target word. Recently, the effectiveness of the attention mechanism has also been explored for multimodal tasks, where it becomes possible to focus both on sentence parts and image regions. Approaches to pool two modalities usually include element-wise product, sum or concatenation. In this paper, we evaluate the more advanced Multimodal Compact Bilinear pooling method, which takes the outer product of two vectors to combine the attention features for the two modalities. This has been previously investigated for visual question answering. We try out this approach for multimodal image caption translation and show improvements compared to basic combination methods.", "target": "MULTIMODAL NEURAL MACHINE TRANSLATION"}
{"id": "task1540-a18c37fa24424be5acf4c5e9a8f1bced", "input": "In machine learning contests such as the ImageNet Large Scale Visual Recognition Challenge [RDS15] and the KDD Cup, contestants can submit candidate solutions and receive from an oracle (typically the organizers of the competition) the accuracy of their guesses compared to the ground-truth labels. One of the most commonly used accuracy metrics for binary classification tasks is the Area Under the Receiver Operating Characteristics Curve (AUC). In this paper we provide proofs-of-concept of how knowledge of the AUC of a set of guesses can be used, in two different kinds of attacks, to improve the accuracy of those guesses. On the other hand, we also demonstrate the intractability of one kind of AUC exploit by proving that the number of possible binary labelings of n examples for which a candidate solution obtains a AUC score of c grows exponentially in n, for every c \u2208 (0, 1).", "target": "Exploiting an Oracle that Reports AUC Scores in Machine Learning Contests"}
{"id": "task1540-b59287e247d84eb8b185f797c5c3579f", "input": "Multi-instance multi-label (MIML) learning is a challenging problem in many aspects. Such learning approaches might be useful for many medical diagnosis applications including breast cancer detection and classification. In this study subset of digiPATH dataset (whole slide digital breast cancer histopathology images) are used for training and evaluation of six state-ofthe-art MIML methods. At the end, performance comparison of these approaches are given by means of effective evaluation metrics. It is shown that MIML-kNN achieve the best performance that is %65.3 average precision, where most of other methods attain acceptable results as well.", "target": "Evaluation of Joint Multi-Instance Multi-Label Learning For Breast Cancer Diagnosis"}
{"id": "task1540-af9f4b4f650346dba7624009e44c0977", "input": "In this paper, first we present a new explanation for the relation between logical circuits and artificial neural networks, logical circuits and fuzzy logic, and artificial neural networks and fuzzy inference systems. Then, based on these results, we propose a new neuro-fuzzy computing system which can effectively be implemented on the memristor-crossbar structure. One important feature of the proposed system is that its hardware can directly be trained using the Hebbian learning rule and without the need to any optimization. The system also has a very good capability to deal with huge number of input-out training data without facing problems like overtraining.", "target": "Neuro-Fuzzy Computing System with the Capacity of Implementation on Memristor-Crossbar and Optimization-Free Hardware Training"}
{"id": "task1540-bdc75fc77cfe40a8ab6a3baf79e412af", "input": "Vector space models have become popular in distributional semantics, despite the challenges they face in capturing various semantic phenomena. We propose a novel probabilistic framework which draws on both formal semantics and recent advances in machine learning. In particular, we separate predicates from the entities they refer to, allowing us to perform Bayesian inference based on logical forms. We describe an implementation of this framework using a combination of Restricted Boltzmann Machines and feedforward neural networks. Finally, we demonstrate the feasibility of this approach by training it on a parsed corpus and evaluating it on established similarity datasets.", "target": "Functional Distributional Semantics"}
{"id": "task1540-1f90c19562074e539409bc08bb3304ad", "input": "In this work, we are interested in structure learning for a set of spatially distributed dynamical systems, where individual subsystems are coupled via latent variables and observed through a filter. We represent this model as a directed acyclic graph (DAG) that characterises the unidirectional coupling between subsystems. Standard approaches to structure learning are not applicable in this framework due to the hidden variables, however we can exploit the properties of certain dynamical systems to formulate exact methods based on state space reconstruction. We approach the problem by using reconstruction theorems to analytically derive a tractable expression for the KL-divergence of a candidate DAG from the observed dataset. We show this measure can be decomposed as a function of two informationtheoretic measures, transfer entropy and stochastic interaction. We then present two mathematically robust scoring functions based on transfer entropy and statistical independence tests. These results support the previously held conjecture that transfer entropy can be used to infer effective connectivity in complex networks.", "target": "Inferring Coupling of Distributed Dynamical Systems via Transfer Entropy"}
{"id": "task1540-3f030bde72954ee9b7ee222bf8d10bf2", "input": "The Kaldi 1 toolkit is becoming popular for constructing automated speech recognition (ASR) systems. Meanwhile, in recent years, deep neural networks (DNNs) have shown state-of-the-art performance on various ASR tasks. This document describes our recipes to implement fully-fledged DNN acoustic modeling using Kaldi and PDNN. PDNN is a lightweight deep learning toolkit developed under the Theano environment. Using these recipes, we can build up multiple systems including DNN hybrid systems, convolutional neural network (CNN) systems and bottleneck feature systems. These recipes are directly based on the Kaldi Switchboard 110-hour setup. However, adapting them to new datasets is easy to achieve.", "target": "Kaldi+PDNN: Building DNN-based ASR Systems with Kaldi and PDNN"}
{"id": "task1540-72e8147d9d6e403280db82b644b38e6c", "input": "A Verbal Autopsy is the record of an interview about the circumstances of an uncertified death. In developing countries, if a death occurs away from health facilities, a field-worker interviews a relative of the deceased about the circumstances of the death; this Verbal Autopsy can be reviewed offsite. We report on a comparative study of the processes involved in Text Classification applied to classifying Cause of Death: feature value representation; machine learning classification algorithms; and feature reduction strategies in order to identify the suitable approaches applicable to the classification of Verbal Autopsy text. We demonstrate that normalised term frequency and the standard TFiDF achieve comparable performance across a number of classifiers. The results also show Support Vector Machine is superior to other classification algorithms employed in this research. Finally, we demonstrate the effectiveness of employing a \u2019locally-semisupervised\u2019 feature reduction strategy in order to increase performance accuracy.", "target": "A Comparative Study of Machine Learning Methods for Verbal Autopsy Text Classification"}
{"id": "task1540-5055f81da8d840c19349c6bce4bc72f4", "input": "The paper proposes a fresh look at the concept of goal and advances that motivational attitudes like desire, goal and intention are just facets of the broader notion of (acceptable) outcome. We propose to encode the preferences of an agent as sequences of \u201calternative acceptable outcomes\u201d. We then study how the agent\u2019s beliefs and norms can be used to filter the mental attitudes out of the sequences of alternative acceptable outcomes. Finally, we formalise such intuitions in a novel Modal Defeasible Logic and we prove that the resulting formalisation is computationally feasible.", "target": "The Rationale behind the Concept of Goal"}
{"id": "task1540-413242eb66d04776bee75b6f70b4a0e1", "input": "Temporal information conveyed by language describes how the world around us changes through time. Events, durations and times are all temporal elements that can be viewed as intervals. These intervals are sometimes temporally related in text. Automatically determining the nature of such relations is a complex and unsolved problem. Some words can act as \u201csignals\u201d which suggest a temporal ordering between intervals. In this paper, we use these signal words to improve the accuracy of a recent approach to classification of temporal links.", "target": "Using Signals to Improve Automatic Classification of Temporal Relations"}
{"id": "task1540-503a2cbf26c442d1ac89d5805c206630", "input": "Neural machine translation has shown very promising results lately. Most NMT models follow the encoder-decoder framework. To make encoder-decoder models more flexible, attention mechanism was introduced to machine translation and also other tasks like speech recognition and image captioning. We observe that the quality of translation by attention-based encoder-decoder can be significantly damaged when the alignment is incorrect. We attribute these problems to the lack of distortion and fertility models. Aiming to resolve these problems, we propose new variations of attention-based encoderdecoder and compare them with other models on machine translation. Our proposed method achieved an improvement of 2 BLEU points over the original attentionbased encoder-decoder.", "target": "Implicit Distortion and Fertility Models for Attention-based Encoder-Decoder NMT Model"}
{"id": "task1540-6121820c643746428b8ae9cbd03009fe", "input": "Deep Learning\u2019s recent successes have mostly relied on Convolutional Networks, which exploit fundamental statistical properties of images, sounds and video data: the local stationarity and multi-scale compositional structure, that allows expressing long range interactions in terms of shorter, localized interactions. However, there exist other important examples, such as text documents or bioinformatic data, that may lack some or all of these strong statistical regularities. In this paper we consider the general question of how to construct deep architectures with small learning complexity on general non-Euclidean domains, which are typically unknown and need to be estimated from the data. In particular, we develop an extension of Spectral Networks which incorporates a Graph Estimation procedure, that we test on large-scale classification problems, matching or improving over Dropout Networks with far less parameters to estimate.", "target": "Deep Convolutional Networks on Graph-Structured Data"}
{"id": "task1540-eed93479b11942fcab3a7d5105e505c7", "input": "This paper considers the problem of knowledge\u00ad based model construction in the presence of uncertainty about the association of domain entities to random variables. Multi-entity Bayesian networks (MEBNs) are defined as a representation for knowledge in domains characterized by uncertainty in the number of relevant entities, their interrelationships, and their association with observables. An MEBN implicitly specifies a probability distribution in terms of a hierarchically structured collection of Bayesian network fragments that together encode a joint probability distribution over arbitrarily many interrelated hypotheses. Although a finite query-complete model can always be constructed, association uncertainty typically makes exact model construction and evaluation intractable. The objective of hypothesis management is to balance tractability against accuracy. We describe an approach to hypothesis management, present an application to the problem of military situation awareness, and compare our approach to related work in the tracking and fusion literature.", "target": "Hypothesis Management in Situation-Specific Network Construction"}
{"id": "task1540-4fcda61fa20e4f16913f00d8c0a49e8f", "input": "We train a generative convolutional neural network<lb>which is able to generate images of objects given object<lb>type, viewpoint, and color. We train the network in a su-<lb>pervised manner on a dataset of rendered 3D chair mod-<lb>els. Our experiments show that the network does not merely<lb>learn all images by heart, but rather finds a meaningful<lb>representation of a 3D chair model allowing it to assess<lb>the similarity of different chairs, interpolate between given<lb>viewpoints to generate the missing ones, or invent new chair<lb>styles by interpolating between chairs from the training set.<lb>We show that the network can be used to find correspon-<lb>dences between different chairs from the dataset, outper-<lb>forming existing approaches on this task.", "target": "Learning to Generate Chairs with Convolutional Neural Networks"}
{"id": "task1540-24e4a1642a90444ba7e2bc7efdc19457", "input": "Feature reduction is an important concept which is used for reducing dimensions to decrease the computation complexity and time of classification. Since now many approaches have been proposed for solving this problem, but almost all of them just presented a fix output for each input dataset that some of them aren\u2019t satisfied cases for classification. In this we proposed an approach as processing input dataset to increase accuracy rate of each feature extraction methods. First of all, a new concept called dispelling classes gradually (DCG) is proposed to increase separability of classes based on their labels. Next, this method is used to process input dataset of the feature reduction approaches to decrease the misclassification error rate of their outputs more than when output is achieved without any processing. In addition our method has a good quality to collate with noise based on adapting dataset with feature reduction approaches. In the result part, two conditions (With process and without that) are compared to support our idea by using some of UCI datasets.", "target": "Dispelling Classes Gradually to Improve Quality of Feature Reduction Approaches"}
{"id": "task1540-1be1be609d7247478cdbf7c0e0363dda", "input": "Connectionist Temporal Classification has recently attracted a lot of interest as it offers an elegant approach to building acoustic models (AMs) for speech recognition. The CTC loss function maps an input sequence of observable feature vectors to an output sequence of symbols. Output symbols are conditionally independent of each other under CTC loss, so a language model (LM) can be incorporated conveniently during decoding, retaining the traditional separation of acoustic and linguistic components in ASR. For fixed vocabularies, Weighted Finite State Transducers provide a strong baseline for efficient integration of CTC AMs with n-gram LMs. Character-based neural LMs provide a straight forward solution for open vocabulary speech recognition and all-neural models, and can be decoded with beam search. Finally, sequence-to-sequence models can be used to translate a sequence of individual sounds into a word string. We compare the performance of these three approaches, and analyze their error patterns, which provides insightful guidance for future research and development in this important area.", "target": "Comparison of Decoding Strategies for CTC Acoustic Models"}
{"id": "task1540-0c3d698d2b8340429f6c66bc3652d819", "input": "Recurrent Neural Network (RNN) are a popular choice for modeling temporal and sequential tasks and achieve many state-of-the-art performance on various complex problems. However, most of the state-of-the-art RNNs have millions of parameters and require many computational resources for training and predicting new data. This paper proposes an alternative RNN model to reduce the number of parameters significantly by representing the weight parameters based on Tensor Train (TT) format. In this paper, we implement the TT-format representation for several RNN architectures such as simple RNN and Gated Recurrent Unit (GRU). We compare and evaluate our proposed RNN model with uncompressed RNN model on sequence classification and sequence prediction tasks. Our proposed RNNs with TT-format are able to preserve the performance while reducing the number of RNN parameters significantly up to 40 times smaller.", "target": "Compressing Recurrent Neural Network with Tensor Train"}
{"id": "task1540-3ec14676ccbe45488bd6905ee85df52d", "input": "Time-series classification is an important problem for the data mining community due to the wide range of application domains involving time-series data. A recent paradigm, called shapelets, represents patterns that are highly predictive for the target variable. Shapelets are discovered by measuring the prediction accuracy of a set of potential (shapelet) candidates. The candidates typically consist of all the segments of a dataset, therefore, the discovery of shapelets is computationally expensive. This paper proposes a novel method that avoids measuring the prediction accuracy of similar candidates in Euclidean distance space, through an online clustering pruning technique. In addition, our algorithm incorporates a supervised shapelet selection that filters out only those candidates that improve classification accuracy. Empirical evidence on 45 datasets from the UCR collection demonstrate that our method is 3-4 orders of magnitudes faster than the fastest existing shapelet-discovery method, while providing better prediction accuracy.", "target": "Scalable Discovery of Time-Series Shapelets"}
{"id": "task1540-144cfaaf08a64d45a530fdfadb2cf3de", "input": "This work aims to address the problem of imagebased question-answering (QA) with new models and datasets. In our work, we propose to use recurrent neural networks and visual semantic embeddings without intermediate stages such as object detection and image segmentation. Our model performs 1.8 times better than the recently published results on the same dataset. Another main contribution is an automatic question generation algorithm that converts the currently available image description dataset into QA form, resulting in a 10 times bigger dataset with more evenly distributed answers.", "target": "Image Question Answering: A Visual Semantic Embedding Model and a New Dataset"}
{"id": "task1540-5a6c483b9ee545ca96e3d3cf55acd468", "input": "The paper presents an application of Conformal Predictors to a chemoinformatics problem of identifying activities of chemical compounds. The paper addresses some specific challenges of this domain: a large number of compounds (training examples), high-dimensionality of feature space, sparseness and a strong class imbalance. A variant of conformal predictors called Inductive Mondrian Conformal Predictor is applied to deal with these challenges. Results are presented for several non-conformity measures (NCM) extracted from underlying algorithms and different kernels. A number of performance measures are used in order to demonstrate the flexibility of Inductive Mondrian Conformal Predictors in dealing with such a complex set of data.", "target": "Conformal Predictors for Compound Activity Prediction"}
{"id": "task1540-86d45b3cb2954e7b94a4851ad1a3bb41", "input": "We propose a soft attention based model for the task of action recognition in videos. We use multi-layered Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) units which are deep both spatially and temporally. Our model learns to focus selectively on parts of the video frames and classifies videos after taking a few glimpses. The model essentially learns which parts in the frames are relevant for the task at hand and attaches higher importance to them. We evaluate the model on UCF-11 (YouTube Action), HMDB-51 and Hollywood2 datasets and analyze how the model focuses its attention depending on the scene and the action being performed.", "target": "ACTION RECOGNITION USING VISUAL ATTENTION"}
{"id": "task1540-69a18977854e4cbbac5f28622517cd52", "input": "We develop a novel bi-directional attention model for dependency parsing, which learns to agree on headword predictions from the forward and backward parsing directions. The parsing procedure for each direction is formulated as sequentially querying the memory component that stores continuous headword embeddings. The proposed parser makes use of soft headword embeddings, allowing the model to implicitly capture high-order parsing history without dramatically increasing the computational complexity. We conduct experiments on English, Chinese, and 12 other languages from the CoNLL 2006 shared task, showing that the proposed model achieves state-of-the-art unlabeled attachment scores on 6 languages.1", "target": "Bi-directional Attention with Agreement for Dependency Parsing"}
{"id": "task1540-ca53648aeac34e4aa836c85adc6fc783", "input": "Job interview simulation with a virtual agents aims at improving people\u2019s social skills and supporting professional inclusion. In such simulators, the virtual agent must be capable of representing and reasoning about the user\u2019s mental state based on social cues that inform the system about his/her affects and social attitude. In this paper, we propose a formal model of Theory of Mind (ToM) for virtual agent in the context of human-agent interaction that focuses on the affective dimension. It relies on a hybrid ToM that combines the two major paradigms of the domain. Our framework is based on modal logic and inference rules about the mental states, emotions and social relations of both actors. Finally, we present preliminary results regarding the impact of such a model on natural interaction in the context of job interviews simulation.", "target": "A logical model of Theory of Mind for virtual agents in the context of job interview simulation"}
{"id": "task1540-4949828906e343389bad7a0cce44f926", "input": "We present a new fast online clustering algorithm that reliably recovers arbitrary-shaped data clusters in high throughout data streams. Unlike the existing state-of-the-art online clustering methods based on k-means or k-medoid, it does not make any restrictive generative assumptions. In addition, in contrast to existing nonparametric clustering techniques such as DBScan or DenStream, it gives provable theoretical guarantees. To achieve fast clustering, we propose to represent each cluster by a skeleton set which is updated continuously as new data is seen. A skeleton set consists of weighted samples from the data where weights encode local densities. The size of each skeleton set is adapted according to the cluster geometry. The proposed technique automatically detects the number of clusters and is robust to outliers. The algorithm works for the infinite data stream where more than one pass over the data is not feasible. We provide theoretical guarantees on the quality of the clustering and also demonstrate its advantage over the existing state-of-the-art on several datasets.", "target": "Fast Online Clustering with Randomized Skeleton Sets"}
{"id": "task1540-bf6f870b06de4b4d956107ae84e32348", "input": "The Resource Description Framework (RDF) is a Semantic Web standard that provides a data language, simply called RDF, as well as a lightweight ontology language, called RDF Schema. We investigate embeddings of RDF in logic and show how standard logic programming and description logic technology can be used for reasoning with RDF. We subsequently consider extensions of RDF with datatype support, considering D entailment, defined in the RDF semantics specification, and D* entailment, a semantic weakening of D entailment, introduced by ter Horst. We use the embeddings and properties of the logics to establish novel upper bounds for the complexity of deciding entailment. We subsequently establish two novel lower bounds, establishing that RDFS entailment is PTime-complete and that simple-D entailment is coNP-hard, when considering arbitrary datatypes, both in the size of the entailing graph. The results indicate that RDFS may not be as lightweight as one may expect.", "target": "Logical Foundations of RDF(S) with Datatypes"}
{"id": "task1540-40fb5964c1804bc4b8a95f02902c4904", "input": "Syntax-Guided Synthesis (SyGuS) is the computational problem of finding an implementation f that meets both a semantic constraint given by a logical formula \u03c6 in a background theory T , and a syntactic constraint given by a grammar G, which specifies the allowed set of candidate implementations. Such a synthesis problem can be formally defined in SyGuS-IF, a language that is built on top of SMT-LIB. The Syntax-Guided Synthesis Competition (SyGuS-Comp) is an effort to facilitate, bring together and accelerate research and development of efficient solvers for SyGuS by providing a platform for evaluating different synthesis techniques on a comprehensive set of benchmarks. In this year\u2019s competition we added a new track devoted to programming by examples. This track consisted of two categories, one using the theory of bit-vectors and one using the theory of strings. This paper presents and analyses the results of SyGuS-Comp\u201916.", "target": "SyGuS-Comp 2016: Results and Analysis"}
{"id": "task1540-12c1573e1f1048db92073077f7aa4f12", "input": "We consider a general framework of online learning with expert advice where the regret is defined with respect to a competitor class defined by a weighted automaton over sequences of experts. Our framework covers several problems previously studied, in particular that of competing against k-shifting experts. We give a series of algorithms for this problem, including an automata-based algorithm extending weightedmajority and more efficient algorithms based on the notion of failure transitions. We further present efficient algorithms based on a compact approximation of the competitor automaton, in particular efficient n-gram models obtained by minimizing the R\u00e9nyi divergence, and present an extensive study of the approximation properties of such models. We also extend our algorithms and results to the framework of sleeping experts. Finally, we describe the extension of our approximation methods to online convex optimization and a general mirror descent setting.", "target": "Online Learning against Expert Automata"}
{"id": "task1540-af3c30997c124889a31e913e92d1136c", "input": "We design a new nonparametric method that allows one to estimate the matrix of integrated kernels of a multivariate Hawkes process. This matrix not only encodes the mutual influences of each node of the process, but also disentangles the causality relationships between them. Our approach is the first that leads to an estimation of this matrix without any parametric modeling and estimation of the kernels themselves. As a consequence, it can give an estimation of causality relationships between nodes (or users), based on their activity timestamps (on a social network for instance), without knowing or estimating the shape of the activities lifetime. For that purpose, we introduce a moment matching method that fits the second-order and the third-order integrated cumulants of the process. A theoretical analysis allows us to prove that this new estimation technique is consistent. Moreover, we show, on numerical experiments, that our approach is indeed very robust with respect to the shape of the kernels and gives appealing results on the MemeTracker database and on financial order book data.", "target": "Uncovering Causality from Multivariate Hawkes Integrated Cumulants"}
{"id": "task1540-7d46e723f16d44beb5c79f548c33a994", "input": "This paper presents an information theoretic approach to the concept of intelligence in the computational sense. We introduce a probabilistic framework from which computational intelligence is shown to be an entropy minimizing process at the local level. Using this new scheme, we develop a simple data driven clustering example and discuss its applications.", "target": "The Computational Theory of Intelligence: Information Entropy"}
{"id": "task1540-0ca0eaf40e2440ecbc2e5375e2c6430b", "input": "This paper is a survey work for a bigger project for designing a Visual SLAM robot to generate 3D dense map of an unknown unstructured environment. A lot of factors have to be considered while designing a SLAM robot. Sensing method of the SLAM robot should be determined by considering the kind of environment to be modelled. Similarly the type of environment determines the suitable feature extraction method. This paper goes through the sensing methods used in some recently published papers. The main objective of this survey is to conduct a comparative study among the current sensing methodsandfeature extraction algorithms and to extract out the best for our work.", "target": "A SURVEY ON SENSING METHODS"}
{"id": "task1540-988ac8f85a374986a79266741e52daff", "input": "Characterizing relationships between people is fundamental for the understanding of narratives. In this work, we address the problem of inferring the polarity of relationships between people in narrative summaries. We formulate the problem as a joint structured prediction for each narrative, and present a model that combines evidence from linguistic and semantic features, as well as features based on the structure of the social community in the text. We also provide a clustering-based approach that can exploit regularities in narrative types. e.g., learn an affinity for love-triangles in romantic stories. On a dataset of movie summaries from Wikipedia, our structured models provide more than a 30% errorreduction over a competitive baseline that considers pairs of characters in isolation.", "target": "Inferring Interpersonal Relations in Narrative Summarie"}
{"id": "task1540-554f302ea2ab40e9ad0d36c35a969870", "input": "We present paired learning and inference algorithms for significantly reducing computation and increasing speed of the vector dot products in the classifiers that are at the heart of many NLP components. This is accomplished by partitioning the features into a sequence of templates which are ordered such that high confidence can often be reached using only a small fraction of all features. Parameter estimation is arranged to maximize accuracy and early confidence in this sequence. Our approach is simpler and better suited to NLP than other related cascade methods. We present experiments in left-to-right part-of-speech tagging, named entity recognition, and transition-based dependency parsing. On the typical benchmarking datasets we can preserve POS tagging accuracy above 97% and parsing LAS above 88.5% both with over a five-fold reduction in run-time, and NER F1 above 88 with more than 2x increase in speed.", "target": "Learning Dynamic Feature Selection for Fast Sequential Prediction"}
{"id": "task1540-7df46f30095b44809dcffa7d42e23a61", "input": "We present a concrete design for Solomonoff\u2019s incremental machine learning system suitable for desktop computers. We use R5RS Scheme and its standard library with a few omissions as the reference machine. We introduce a Levin Search variant based on a stochastic Context Free Grammar together with new update algorithms that use the same grammar as a guiding probability distribution for incremental machine learning. The updates include adjusting production probabilities, re-using previous solutions, learning programming idioms and discovery of frequent subprograms. The issues of extending the a priori probability distribution and bootstrapping are discussed. We have implemented a good portion of the proposed algorithms. Experiments with toy problems show that the update algorithms work as expected.", "target": "Gigamachine: incremental machine learning on desktop computers"}
{"id": "task1540-c7f5b4ef462e42de849f0b9abf6d804f", "input": "It is hypothesized that creativity arises from the self-mending capacity of an internal model of the world, or worldview. The uniquely honed worldview of a creative individual results in a distinctive style that is recognizable within and across domains. It is further hypothesized that creativity is domaingeneral in the sense that there exist multiple avenues by which the distinctiveness of one\u2019s worldview can be expressed. These hypotheses were tested using art students and creative writing students. Art students guessed significantly above chance both which painting was done by which of five famous artists, and which artwork was done by which of their peers. Similarly, creative writing students guessed significantly above chance both which passage was written by which of five famous writers, and which passage was written by which of their peers. These findings support the hypothesis that creative style is recognizable. Moreover, creative writing students guessed significantly above chance which of their peers produced particular works of art, supporting the hypothesis that creative style is recognizable not just within but across domains.", "target": "Recognizability of Individual Creative Style Within and Across Domains: Preliminary Studies"}
{"id": "task1540-65be896035754d61b0860c4228c98ea4", "input": "We explore deep reinforcement learning methods for multi-agent domains. We begin by analyzing the difficulty of traditional algorithms in the multi-agent case: Q-learning is challenged by an inherent non-stationarity of the environment, while policy gradient suffers from a variance that increases as the number of agents grows. We then present an adaptation of actor-critic methods that considers action policies of other agents and is able to successfully learn policies that require complex multiagent coordination. Additionally, we introduce a training regimen utilizing an ensemble of policies for each agent that leads to more robust multi-agent policies. We show the strength of our approach compared to existing methods in cooperative as well as competitive scenarios, where agent populations are able to discover various physical and informational coordination strategies.", "target": "Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments"}
{"id": "task1540-7757e6c7e47740899623d7ce2f1d92fd", "input": "This paper describes the best first search strategy used by U-Plan (Mansell 1993a), a planning system that constructs quantitatively ranked plans given an incomplete description of an uncertain environment. U-Plan uses uncertain and incomplete evidence describing the environment, characterises it using a Dempster\u00ad Shafer interval, and generates a set of possible world states. Plan construction takes place in an abstraction hierarchy where strategic decisions are made before tactical decisions. Search through this abstraction hierarchy is guided by a quantitative measure (expected fulfilment) based on decision theory. The search strategy is best first with the provision to update expected fulfilments and review previous decisions in the light of planning developments. U-Pian generates multiple plans for multiple possible worlds, and attempts to use existing plans for new world situations. A super-plan is then constructed, based on merging the set of plans and appropriately timed knowledge acquisition operators, which are used to decide between plan alternatives during plan execution.", "target": "Operator Selection While Planning Under Uncertainty"}
{"id": "task1540-401e0d47b370490493237ad17126cfa9", "input": "This paper examines methods of decision making that are able to accommodate limitations on both the form in which uncertainty pertaining to a deci\u00ad sion problem can be realistically represented and the amount of computing time available before a deci\u00ad sion must be made. The methods are anytime algo\u00ad \"_th\ufffd in the sense of Boddy and Dean [1989] . Tech\u00ad Diques are presented for use with Frisch and Haddawy's [ 1992] anytime deduction system, with an anytime adaptation of Nilsson's [1986j probabilis\u00ad tic logic, and with a probabilistic database model. 1 ANYTIME ALGORITHMS FOR", "target": "Anytime Decision Making with Imprecise Probabilities"}
{"id": "task1540-7bf6087474784698b3f35d8152585bfb", "input": "The paper summarizes the development of the LVCSR system built as a part of the Pashto speech-translation system at the SCALE (Summer Camp for Applied Language Exploration) 2015 workshop on \u201cSpeech-to-text-translation for low-resource languages\u201d. The Pashto language was chosen as a good \u201cproxy\u201d low-resource language, exhibiting multiple phenomena which make the speech-recognition and and speech-to-text-translation systems development hard. Even when the amount of data is seemingly sufficient, given the fact that the data originates from multiple sources, the preliminary experiments reveal that there is little to no benefit in merging (concatenating) the corpora and more elaborate ways of making use of all of the data must be worked out. This paper concentrates only on the LVCSR part and presents a range of different techniques that were found to be useful in order to benefit from multiple different corpora", "target": "USING OF HETEROGENEOUS CORPORA FOR TRAINING OF AN ASR SYSTEM"}
{"id": "task1540-a4a3ea81f2a24984b1e32753a08b258b", "input": "Understanding physical phenomena is a key competence that enables humans and animals to act and interact under uncertain perception in previously unseen environments containing novel object and their configurations. Developmental psychology has shown that such skills are acquired by infants from observations at a very early stage. In this paper, we contrast a more traditional approach of taking a modelbased route with explicit 3D representations and physical simulation by an end-to-end approach that directly predicts stability and related quantities from appearance. We ask the question if and to what extent and quality such a skill can directly be acquired in a data-driven way\u2014 bypassing the need for an explicit simulation. We present a learning-based approach based on simulated data that predicts stability of towers comprised of wooden blocks under different conditions and quantities related to the potential fall of the towers. The evaluation is carried out on synthetic data and compared to human judgments on the same stimuli.", "target": "To Fall Or Not To Fall: A Visual Approach to Physical Stability Prediction"}
{"id": "task1540-407a11151dd6486786456d431e4ce913", "input": "The computational mechanisms by which nonlinear recurrent neural networks (RNNs) achieve their goals remains an open question. There exist many problem domains where intelligibility of the network model is crucial for deployment. Here we introduce a recurrent architecture composed of input-switched affine transformations, in other words an RNN without any nonlinearity and with one set of weights per input. We show that this architecture achieves near identical performance to traditional architectures on language modeling of Wikipedia text, for the same number of model parameters. It can obtain this performance with the potential for computational speedup compared to existing methods, by precomputing the composed affine transformations corresponding to longer input sequences. As our architecture is affine, we are able to understand the mechanisms by which it functions using linear methods. For example, we show how the network linearly combines contributions from the past to make predictions at the current time step. We show how representations for words can be combined in order to understand how context is transferred across word boundaries. Finally, we demonstrate how the system can be executed and analyzed in arbitrary bases to aid understanding.", "target": "INTELLIGIBLE LANGUAGE MODELING WITH INPUT SWITCHED AFFINE NETWORKS"}
{"id": "task1540-87752b8c0c274c91adb2aac9a2a2c2d3", "input": "We combine Riemannian geometry with the mean field theory of high dimensional chaos to study the nature of signal propagation in generic, deep neural networks with random weights. Our results reveal an order-to-chaos expressivity phase transition, with networks in the chaotic phase computing nonlinear functions whose global curvature grows exponentially with depth but not width. We prove this generic class of deep random functions cannot be efficiently computed by any shallow network, going beyond prior work restricted to the analysis of single functions. Moreover, we formalize and quantitatively demonstrate the long conjectured idea that deep networks can disentangle highly curved manifolds in input space into flat manifolds in hidden space. Our theoretical analysis of the expressive power of deep networks broadly applies to arbitrary nonlinearities, and provides a quantitative underpinning for previously abstract notions about the geometry of deep functions.", "target": "Exponential expressivity in deep neural networks through transient chaos"}
{"id": "task1540-3a81a9d51fe24fb1929e803bf7d38fe6", "input": "Neural network based approaches for sentence relation modeling automatically generate hidden matching features from raw sentence pairs. However, the quality of matching feature representation may not be satisfied due to complex semantic relations such as entailment or contradiction. To address this challenge, we propose a new deep neural network architecture that jointly leverage pre-trained word embedding and auxiliary character embedding to learn sentence meanings. The two kinds of word sequence representations as inputs into multi-layer bidirectional LSTM to learn enhanced sentence representation. After that, we construct matching features followed by another temporal CNN to learn high-level hidden matching feature representations. Experimental results demonstrate that our approach consistently outperforms the existing methods on standard evaluation datasets.", "target": "Enhancing Sentence Relation Modeling with Auxiliary Character-level Embedding"}
{"id": "task1540-1a449315d2cb4064aed43a6ddffd7efd", "input": "We propose an algorithm to uncover the intrinsic low-rank component of a high-dimensional, graph-smooth and grossly-corrupted dataset, under the situations that the underlying graph is unknown. Based on a model with a low-rank component plus a sparse perturbation, and an initial graph estimation, our proposed algorithm simultaneously learns the low-rank component and refines the graph. The refined graph improves the effectiveness of the graph smoothness constraint and increases the accuracy of the low-rank estimation. We derive the learning steps using ADMM. Our evaluations using synthetic and real brain imaging data in a supervised classification task demonstrate encouraging performance.", "target": "SIMULTANEOUS LOW-RANK COMPONENT AND GRAPH ESTIMATION FOR HIGH-DIMENSIONAL GRAPH SIGNALS: APPLICATION TO BRAIN IMAGING"}
{"id": "task1540-a6f9ea0e8d254353acb1d359eca75811", "input": "Functional neuroimaging can measure the brain\u2019s response to an external stimulus. It is used to perform brain mapping: identifying from these observations the brain regions involved. This problem can be cast into a linear supervised learning task where the neuroimaging data are used as predictors for the stimulus. Brain mapping is then seen as a support recovery problem. On functional MRI (fMRI) data, this problem is particularly challenging as i) the number of samples is small due to limited acquisition time and ii) the variables are strongly correlated. We propose to overcome these difficulties using sparse regression models over new variables obtained by clustering of the original variables. The use of randomization techniques, e.g. bootstrap samples, and clustering of the variables improves the recovery properties of sparse methods. We demonstrate the benefit of our approach on an extensive simulation study as well as two fMRI datasets.", "target": "Small-sample brain mapping: sparse recovery on spatially correlated designs with randomization and clustering"}
{"id": "task1540-58dc4e7de9f947eaafe6d69d1f798050", "input": "Recent applications of neural language models have led to an increased interest in the automatic generation of natural language. However impressive, the evaluation of neurally generated text has so far remained rather informal and anecdotal. Here, we present an attempt at the systematic assessment of one aspect of the quality of neurally generated text. We focus on a specific aspect of neural language generation: its ability to reproduce authorial writing styles. Using established models for authorship attribution, we empirically assess the stylistic qualities of neurally generated text. In comparison to conventional language models, neural models generate fuzzier text that is relatively harder to attribute correctly. Nevertheless, our results also suggest that neurally generated text offers more valuable perspectives for the augmentation of training data.", "target": "Assessing the Stylistic Properties of Neurally Generated Text in Authorship Attribution"}
{"id": "task1540-c34e1eddc8e4445f90ca0418995cb3fb", "input": "Nanson\u2019s and Baldwin\u2019s voting rules select a winner by successively eliminating candidates with low Borda scores. We show that these rules have a number of desirable computational properties. In particular, with unweighted votes, it is NP-hard to manipulate either rule with one manipulator, whilst with weighted votes, it is NP-hard to manipulate either rule with a small number of candidates and a coalition of manipulators. As only a couple of other voting rules are known to be NP-hard to manipulate with a single manipulator, Nanson\u2019s and Baldwin\u2019s rules appear to be particularly resistant to manipulation from a theoretical perspective. We also propose a number of approximation methods for manipulating these two rules. Experiments demonstrate that both rules are often difficult to manipulate in practice. These results suggest that elimination style voting rules deserve further study.", "target": "Manipulation of Nanson\u2019s and Baldwin\u2019s Rules"}
{"id": "task1540-2f7b06d64b5e4f679b3fd68de0bbac09", "input": "Dawid, Kjrerulff & Lauritzen (1994) provided a preliminary description of a hybrid between Monte-Carlo sampling methods and exact lo\u00ad cal computations in junction trees. Utiliz\u00ad ing the strengths of both methods, such hy\u00ad brid inference methods has the potential of expanding the class of problems which can be solved under bounded resources as well as solving problems which otherwise resist ex\u00ad act solutions. The paper provides a detailed description of a particular instance of such a hybrid scheme; namely, combination of ex\u00ad act inference and Gibbs sampling in discrete Bayesian networks. We argue that this com\u00ad bination calls for an extension of the usual message passing scheme of ordinary junction trees.", "target": "HUGS: Combining Exact Inference and Gibbs Sampling in Junction Trees"}
{"id": "task1540-ed57436408dd4833b5a06b376756530f", "input": "We propose a learning setting in which unlabeled data is free, and the cost of a label depends on its value, which is not known in advance. We study binary classification in an extreme case, where the algorithm only pays for negative labels. Our motivation are applications such as fraud detection, in which investigating an honest transaction should be avoided if possible. We term the setting auditing, and consider the auditing complexity of an algorithm: the number of negative labels the algorithm requires in order to learn a hypothesis with low relative error. We design auditing algorithms for simple hypothesis classes (thresholds and rectangles), and show that with these algorithms, the auditing complexity can be significantly lower than the active label complexity. We also discuss a general competitive approach for auditing and possible modifications to the framework.", "target": "Auditing: Active Learning with Outcome-Dependent Query Costs"}
{"id": "task1540-a02be19c44754ff3be64afbb76f43342", "input": "We present a novel neural model HyperVec to learn hierarchical embeddings for hypernymy detection and directionality. While previous embeddings have shown limitations on prototypical hypernyms, HyperVec represents an unsupervised measure where embeddings are learned in a specific order and capture the hypernym\u2013hyponym distributional hierarchy. Moreover, our model is able to generalize over unseen hypernymy pairs, when using only small sets of training data, and by mapping to other languages. Results on benchmark datasets show that HyperVec outperforms both state-of-theart unsupervised measures and embedding models on hypernymy detection and directionality, and on predicting graded lexical entailment.", "target": "Hierarchical Embeddings for Hypernymy Detection and Directionality"}
{"id": "task1540-197633d2289b492486ca6ee2a26f3d00", "input": "The increase in the use of microblogging came along with the rapid growth on short linguistic data. On the other hand deep learning is considered to be the new frontier to extract meaningful information out of large amount of raw data in an automated manner. In this study, we engaged these two emerging fields to come up with a robust language identifier on demand, namely Language Identification Engine (LIDE). As a result, we achieved 95.12% accuracy in Discriminating between Similar Languages (DSL) Shared Task 2015 dataset, which is comparable to the maximum reported accuracy of 95.54% achieved so far.", "target": "LIDE: Language Identification from Text Documents"}
{"id": "task1540-580d36faf97d4d0a95c8ddcfc094fa3d", "input": "Prediction markets provide an efficient means to assess uncertain quantities from forecasters. Traditional and competitive strictly proper scoring rules have been shown to incentivize players to provide truthful probabilistic forecasts. However, we show that when those players can cooperate, these mechanisms can instead discourage them from reporting what they really believe. When players with different beliefs are able to cooperate and form a coalition, these mechanisms admit arbitrage and there is a report that will always pay coalition members more than their truthful forecasts. If the coalition were created by an intermediary, such as a web portal, the intermediary would be guaranteed a profit.", "target": "Strictly Proper Mechanisms with Cooperating Players"}
{"id": "task1540-031ed59f7a7844f4bc4498808acff441", "input": "We present DataGrad, a general back-propagation style training procedure for deep neural architectures that uses regularization of a deep Jacobian-based penalty. It can be viewed as a deep extension of the layerwise contractive auto-encoder penalty. More importantly, it unifies previous proposals for adversarial training of deep neural nets \u2013 this list includes directly modifying the gradient, training on a mix of original and adversarial examples, using contractive penalties, and approximately optimizing constrained adversarial objective functions. In an experiment using a Deep Sparse Rectifier Network, we find that the deep Jacobian regularization of DataGrad (which also has L1 and L2 flavors of regularization) outperforms traditional L1 and L2 regularization both on the original dataset as well as on adversarial examples.", "target": "Unifying Adversarial Training Algorithms with Flexible Deep Data Gradient Regularization"}
{"id": "task1540-3e814ea1f04d48ef8bd9ff534fe89a63", "input": "Delay discounting, a behavioral measure of impulsivity, is often used to quantify the human tendency to choose a smaller, sooner reward (e.g., $1 today) over a larger, later reward ($2 tomorrow). Delay discounting and its relation to human decision making is a hot topic in economics and behavior science since pitting the demands of long-term goals against short term desires is among the most difficult tasks in human decision making [Hirsh et al., 2008]. Previously, small-scale studies based on questionnaires were used to analyze an individual\u2019s delay discounting rate (DDR) and his/her realworld behavior (e.g., substance abuse) [Kirby et al., 1999]. In this research, we employ large-scale social media analytics to study DDR and its relation to people\u2019s social media behavior (e.g., Facebook Likes). We also build computational models to automatically infer DDR from Social Media Likes. Our investigation has revealed interesting results.", "target": "$1 Today or $2 Tomorrow? The Answer is in Your Facebook Likes"}
{"id": "task1540-cdac41f58a8f4267ac715a088163a95f", "input": "In neural machine translation (NMT), generation of a target word depends on both source and target contexts. We find that source contexts have a direct impact on the adequacy of a translation while target contexts on the fluency. Intuitively, generation of a content word should rely more on the source context and generation of a functional word should rely more on the target context. Due to lack of effective control on the influence from source and target contexts, conventional NMT tends to yield fluent but inadequate translations. To address this problem, we propose to use context gates to dynamically control the ratios at which source and target contexts contribute to the generation of target words. In this way, we can enhance the adequacy of NMT while keeping the fluency unchanged. Experiments show that our approach significantly improves upon a standard attention-based NMT system by +2.3 BLEU points.", "target": "Context Gates for Neural Machine Translation"}
{"id": "task1540-974a19b7ae50484a9ee4af69a50d0fb0", "input": "Finite-sum optimization problems are ubiquitous in machine learning, and are commonly solved using first-order methods which rely on gradient computations. Recently, there has been growing interest in second-order methods, which rely on both gradients and Hessians. In principle, second-order methods can require much fewer iterations than first-order methods, and hold the promise for more efficient algorithms. Although computing and manipulating Hessians is prohibitive for high-dimensional problems in general, the Hessians of individual functions in finite-sum problems can often be efficiently computed, e.g. because they possess a low-rank structure. Can second-order information indeed be used to solve such problems more efficiently? In this paper, we provide evidence that the answer \u2013 perhaps surprisingly \u2013 is negative, at least in terms of worst-case guarantees. However, we also discuss what additional assumptions and algorithmic approaches might potentially circumvent this negative result.", "target": "Oracle Complexity of Second-Order Methods for Finite-Sum Problems"}
{"id": "task1540-42463462c5044da59a6074ff95d2e284", "input": "The early classifications of the computational complexity of planning under various restrictions in STRIPS (Bylander) and SAS (B\u00e4ckstr\u00f6m and Nebel) have influenced following research in planning in many ways. We go back and reanalyse their subclasses, but this time using the more modern tool of parameterized complexity analysis. This provides new results that together with the old results give a more detailed picture of the complexity landscape. We demonstrate separation results not possible with standard complexity theory, which contributes to explaining why certain cases of planning have seemed simpler in practice than theory has predicted. In particular, we show that certain restrictions of practical interest are tractable in the parameterized sense of the term, and that a simple heuristic is sufficient to make a well-known partial-order planner exploit this fact.", "target": "The Complexity of Planning Revisited \u2013 A Parameterized Analysis"}
{"id": "task1540-471edcfa44134795843771109fe3425f", "input": "Robust search procedures are a central component in the design of black-box constraint-programming solvers. This paper proposes activity-based search, the idea of using the activity of variables during propagation to guide the search. Activity-based search was compared experimentally to impact-based search and the wdeg heuristics. Experimental results on a variety of benchmarks show that activity-based search is more robust than other heuristics and may produce significant improvements in performance.", "target": "Activity-Based Search for Black-Box Contraint-Programming Solvers"}
{"id": "task1540-310ac04505e6421da927eaadb5fb5121", "input": "Davoud Mougouei School of Computer Science, Engineering, and Mathematics Flinders University Adelaide, Australia davoud.mougouei@\u0083inders.edu.au David M. W. Powers School of Computer Science, Engineering, and Mathematics Flinders University Adelaide, Australia david.powers@\u0083inders.edu.au Asghar Moeini School of Computer Science, Engineering, and Mathematics Flinders University Adelaide, Australia asghar.moeini@\u0083inders.edu.au", "target": "An Integer Programming Model for Binary Knapsack Problem with Value-Related Dependencies among Elements"}
{"id": "task1540-330085762dfc47a783b31f92d6aa7481", "input": "We discuss several modifications and extensions over the previous proposed Cnvlutin (CNV) accelerator for convolutional and fully-connected layers of Deep Learning Network. We first describe different encodings of the activations that are deemed ineffectual. The encodings have different memory overhead and energy characteristics. We propose using a level of indirection when accessing activations from memory to reduce their memory footprint by storing only the effectual activations. We also present a modified organization that detects the activations that are deemed as ineffectual while fetching them from memory. This is different than the original design that instead detected them at the output of the preceding layer. Finally, we present an extended CNV that can also skip ineffectual weights.", "target": "Cnvlutin2: Ineffectual-Activation-and-Weight-Free Deep Neural Network Computing"}
{"id": "task1540-2c88e6bd245248c88130e3c55447e75a", "input": "We propose the first multistage intervention framework that tackles fake news in social networks by combining reinforcement learning with a point process network activity model. The spread of fake news and mitigation events within the network is modeled by a multivariate Hawkes process with additional exogenous control terms. By choosing a feature representation of states, defining mitigation actions and constructing reward functions to measure the effectiveness of mitigation activities, we map the problem of fake news mitigation into the reinforcement learning framework. We develop a policy iteration method unique to the multivariate networked point process, with the goal of optimizing the actions for maximal total reward under budget constraints. Our method shows promising performance in real-time intervention experiments on a Twitter network to mitigate a surrogate fake news campaign, and outperforms alternatives on synthetic datasets.", "target": "Fake News Mitigation via Point Process Based Intervention "}
{"id": "task1540-24a8973e3d0949af8693af22ebd26046", "input": "Many sequential processing tasks require complex nonlinear transition functions from one step to the next. However, recurrent neural networks with such \u201cdeep\" transition functions remain difficult to train, even when using Long Short-Term Memory networks. We introduce a novel theoretical analysis of recurrent networks based on Ger\u0161gorin\u2019s circle theorem that illuminates several modeling and optimization issues and improves our understanding of the LSTM cell. Based on this analysis we propose Recurrent Highway Networks (RHN), which are long not only in time but also in space, generalizing LSTMs to larger step-to-step depths. Experiments indicate that the proposed architecture results in complex but efficient models, beating previous models for character prediction on the Hutter Prize dataset with less than half of the parameters.", "target": "Recurrent Highway Networks"}
{"id": "task1540-46cc087bd1df483e8fc3b71ebacb6cf4", "input": "Generative adversarial networks have been proposed as a way of efficiently training deep generative neural networks. We propose a generative adversarial model that works on continuous sequential data, and apply it by training it on a collection of classical music. We conclude that it generates music that sounds better and better as the model is trained, report statistics on generated music, and let the reader judge the quality by downloading the generated songs.", "target": "C-RNN-GAN: Continuous recurrent neural networks with adversarial training"}
{"id": "task1540-acbfeb06af794ae788257917aa1564bf", "input": "We report investigations into speaker classification of larger quantities of unlabelled speech data using small sets of manually phonemically annotated speech. The Kohonen speech typewriter [1] is a semi-supervised method comprised of selforganising maps (SOMs) that achieves low phoneme error rates. A SOM is a 2D array of cells that learn vector representations of the data based on neighbourhoods. In this paper, we report a method to evaluate pronunciation using multilevel SOMs with /hVd/ single syllable utterances for the study of vowels, following [2] (for Australian pronunciation).", "target": "Characterisation of speech diversity using self-organising maps"}
{"id": "task1540-d3f7ac6bd3434a1f9e030e60de010529", "input": "Generic text embeddings are successfully used in a variety of tasks. However, they are often learnt by capturing the co-occurrence structure from pure text corpora, resulting in limitations of their ability to generalize. In this paper, we explore models that incorporate visual information into the text representation. Based on comprehensive ablation studies, we propose a conceptually simple, yet well performing architecture. It outperforms previous multimodal approaches on a set of well established benchmarks. We also improve the state-of-the-art results for image-related text datasets, using orders of magnitude less data.", "target": "Better Text Understanding Through Image-To-Text Transfer"}
{"id": "task1540-fde34b1273b14536b9f82186975130c7", "input": "Most contemporary multi-task learning methods assume linear models. This setting is considered shallow in the era of deep learning. In this paper, we present a new deep multi-task representation learning framework that learns cross-task sharing structure at every layer in a deep network. Our approach is based on generalising the matrix factorisation techniques explicitly or implicitly used by many conventional MTL algorithms to tensor factorisation, to realise automatic learning of end-to-end knowledge sharing in deep networks. This is in contrast to existing deep learning approaches that need a user-defined multi-task sharing strategy. Our approach applies to both homogeneous and heterogeneous MTL. Experiments demonstrate the efficacy of our deep multi-task representation learning in terms of both higher accuracy and fewer design choices.", "target": "DEEP MULTI-TASK REPRESENTATION LEARNING: A TENSOR FACTORISATION APPROACH"}
{"id": "task1540-4efa79c3adc84a27987d7912963b42e0", "input": "As the number of applications that use machine learning algorithms increases, the need for labeled data useful for training such algorithms intensifies. Getting labels typically involves employing humans to do the annotation, which directly translates to training and working costs. Crowdsourcing platforms have made labeling cheaper and faster, but they still involve significant costs, especially for the cases where the potential set of candidate data to be labeled is large. In this paper we describe a methodology and a prototype system aiming at addressing this challenge for Web-scale problems in an industrial setting. We discuss ideas on how to efficiently select the data to use for training of machine learning algorithms in an attempt to reduce cost. We show results achieving good performance with reduced cost by carefully selecting which instances to label. Our proposed algorithm is presented as part of a framework for managing and generating training datasets, which includes, among other components, a human computation element.", "target": "A Data Management Approach for Dataset Selection Using Human Computation"}
{"id": "task1540-6350b7fc88394bb29317611e0c846bea", "input": "We consider regret minimization in repeated games with non-convex loss functions. Minimizing the standard notion of regret is computationally intractable. Thus, we define a natural notion of regret which permits efficient optimization and generalizes offline guarantees for convergence to an approximate local optimum. We give gradient-based methods that achieve optimal regret, which in turn guarantee convergence to equilibrium in this framework.", "target": "Efficient Regret Minimization in Non-Convex Games"}
{"id": "task1540-4d951c5de9b243f0bd1cdfb2c5f0b552", "input": "Several recently developed Multi-Agent Path Finding (MAPF) solvers scale to large MAPF instances by searching for MAPF plans on 2 levels: The high-level search resolves collisions between agents, and the low-level search plans paths for single agents under the constraints imposed by the high-level search. We make the following contributions to solve the MAPF problem with imperfect plan execution with small average makespans: First, we formalize the MAPF Problem with Delay Probabilities (MAPF-DP), define valid MAPF-DP plans and propose the use of robust plan-execution policies for valid MAPF-DP plans to control how each agent proceeds along its path. Second, we discuss 2 classes of decentralized robust plan-execution policies (called Fully Synchronized Policies and Minimal Communication Policies) that prevent collisions during plan execution for valid MAPF-DP plans. Third, we present a 2-level MAPF-DP solver (called Approximate Minimization in Expectation) that generates valid MAPF-DP plans.", "target": "Multi-Agent Path Finding with Delay Probabilities"}
{"id": "task1540-06b26215e7a54b378f09cc42fa58780d", "input": "As Wireless Sensor Networks are penetrating into the industrial domain, many research opportunities are emerging. One such essential and challenging application is that of node localization. A feed-forward neural network based methodology is adopted in this paper. The Received Signal Strength Indicator (RSSI) values of the anchor node beacons are used. The number of anchor nodes and their configurations has an impact on the accuracy of the localization system, which is also addressed in this paper. Five different training algorithms are evaluated to find the training algorithm that gives the best result. The multi-layer Perceptron (MLP) neural network model was trained using Matlab. In order to evaluate the performance of the proposed method in real time, the model obtained was then implemented on the Arduino microcontroller. With four anchor nodes, an average 2D localization error of 0.2953 m has been achieved with a 12-12-2 neural network structure. The proposed method can also be implemented on any other embedded microcontroller system.", "target": "LOCALIZATION FOR WIRELESS SENSOR NETWORKS: A NEURAL NETWORK APPROACH"}
{"id": "task1540-3b20429d6bb745bd9b7df9ba5dbaf3c3", "input": "We demonstrate how quantum computation can provide non-trivial improvements in the computational and statistical complexity of the perceptron model. We develop two quantum algorithms for perceptron learning. The first algorithm exploits quantum information processing to determine a separating hyperplane using a number of steps sublinear in the number of data points N , namely O( \u221a N). The second algorithm illustrates how the classical mistake bound of O( 1 \u03b32 ) can be further improved to O( 1 \u221a \u03b3 ) through quantum means, where \u03b3 denotes the margin. Such improvements are achieved through the application of quantum amplitude amplification to the version space interpretation of the perceptron model.", "target": "Quantum Perceptron Models"}
{"id": "task1540-34302f3dd4244fcb88fa8189f1718ee0", "input": "A major investment made by a telecom operator goes into the infrastructure and its maintenance, while business revenues are proportional to how big and good the customer base is. We present a data-driven analytic strategy based on combinatorial optimization and analysis of historical data. The data cover historical mobility of the users in one region of Sweden during a week. Applying the proposed method to the case study, we have identified the optimal proportion of geo-demographic segments in the customer base, developed a functionality to assess the potential of a planned marketing campaign, and explored the problem of an optimal number and types of the geo-demographic segments to target through marketing campaigns. With the help of fuzzy logic, the conclusions of data analysis are automatically translated into comprehensible recommendations in a natural language.", "target": "Recommendations for Marketing Campaigns in Telecommunication Business based on the footprint analysis"}
{"id": "task1540-c6ade02f06364d30a6d9979382618e7e", "input": "This paper proposes a new model for extracting an interpretable sentence embedding by introducing self-attention. Instead of using a vector, we use a 2-D matrix to represent the embedding, with each row of the matrix attending on a different part of the sentence. We also propose a self-attention mechanism and a special regularization term for the model. As a side effect, the embedding comes with an easy way of visualizing what specific parts of the sentence are encoded into the embedding. We evaluate our model on 3 different tasks: author profiling, sentiment classification and textual entailment. Results show that our model yields a significant performance gain compared to other sentence embedding methods in all of the 3 tasks.", "target": "A STRUCTURED SELF-ATTENTIVE SENTENCE EMBEDDING"}
{"id": "task1540-6cc0378a23c94a488182542f760db58e", "input": "Restricted non-monotonicity has been shown beneficial for the projective arceager dependency parser in previous research, as posterior decisions can repair mistakes made in previous states due to the lack of information. In this paper, we propose a novel, fully non-monotonic transition system based on the non-projective Covington algorithm. As a non-monotonic system requires exploration of erroneous actions during the training process, we develop several non-monotonic variants of the recently defined dynamic oracle for the Covington parser, based on tight approximations of the loss. Experiments on datasets from the CoNLL-X and CoNLL-XI shared tasks show that a non-monotonic dynamic oracle outperforms the monotonic version in the majority of languages.", "target": "A Full Non-Monotonic Transition System for Unrestricted Non-Projective Parsing"}
{"id": "task1540-a30c625850894a7c95fed9fe3253da05", "input": "Wide-angle sonar mapping of the environ\u00ad ment by mobile robot is nontrivial due to sev\u00ad eral sources of uncertainty: dropouts due to \"specular\" reflections, obstacle location un\u00ad certainty due to the wide beam, and distance measurement error. Earlier papers address the latter problems, but dropouts remain a problem in many environments. We present an approach that lifts the overoptimistic in\u00ad dependence assumption used in earlier work, and use Bayes nets to represent the depen\u00ad dencies between objects of the model. Ob\u00ad jects of the model consist of readings, and of regions in which \"quasi location invari\u00ad ance\" of the (possible) obstacles exists, with respect to the readings. Simulation supports the method's feasibility. The model is readily extensible to allow for prior distributions, as well as other types of sensing operations.", "target": "Bayes Networks for Sonar Sensor Fusion"}
{"id": "task1540-28ba2c1cee404b4e9ea719a918c823ce", "input": "This paper discusses models for dialogue state tracking using recurrent neural networks (RNN). We present experiments on the standard dialogue state tracking (DST) dataset, DSTC2 [7]. On the one hand, RNN models became the state of the art models in DST, on the other hand, most state-of-the-art DST models are only turn-based and require dataset-specific preprocessing (e.g. DSTC2-specific) in order to achieve such results. We implemented two architectures which can be used in an incremental setting and require almost no preprocessing. We compare their performance to the benchmarks on DSTC2 and discuss their properties. With only trivial preprocessing, the performance of our models is close to the state-ofthe-art results.1", "target": "Recurrent Neural Networks for Dialogue State Tracking"}
{"id": "task1540-a7f44d7b7a5445d08e7b7696c4720fc4", "input": "Passengers\u2019 experience is becoming a key metric to evaluate the air transportation system\u2019s performance. Efficient and robust tools to handle airport operations are needed along with a better understanding of passengers\u2019 interests and concerns. Among various airport operations, this paper studies airport gate scheduling for improved passengers\u2019 experience. Three objectives accounting for passengers, aircraft, and operation are presented. Trade-offs between these objectives are analyzed, and a balancing objective function is proposed. The results show that the balanced objective can improve the efficiency of traffic flow in passenger terminals and on ramps, as well as the robustness of gate operations.", "target": "Airport Gate Scheduling for Passengers, Aircraft, and Operation"}
{"id": "task1540-a1c633ff27eb437b9b99ae9c33f6e827", "input": "The paper introduces AND/OR importance sampling for probabilistic graphical models. In contrast to importance sampling, AND/OR importance sampling caches samples in the AND/OR space and then extracts a new sample mean from the stored samples. We prove that AND/OR importance sampling may have lower variance than importance sampling; thereby providing a theoretical justification for preferring it over importance sampling. Our empirical evaluation demonstrates that AND/OR importance sampling is far more accurate than importance sampling in many cases.", "target": "AND/OR Importance Sampling"}
{"id": "task1540-0afa4e6d5e1f47238e5cee97bd9c13a9", "input": "To achieve state-of-the-art results on challenges in vision, Convolutional Neural Networks learn stationary filters that take advantage of the underlying image structure. Our purpose is to propose an efficient layer formulation that extends this property to any domain described by a graph. Namely, we use the support of its adjacency matrix to design learnable weight sharing filters able to exploit the underlying structure of signals. The proposed formulation makes it possible to learn the weights of the filter as well as a scheme that controls how they are shared across the graph. We perform validation experiments with image datasets and show that these filters offer performances comparable with convolutional ones.", "target": "Learning Local Receptive Fields and their Weight Sharing Scheme on Graphs"}
{"id": "task1540-b345d1fd381d4d778273ae80861cf0b2", "input": "We introduce novel mathematical models and algorithms to generate (shortest or k different) explanations for biomedical queries, using answer set programming. We implement these algorithms and integrate them in BIOQUERY-ASP. We illustrate the usefulness of these methods with some complex biomedical queries related to drug discovery, over the biomedical knowledge resources PHARMGKB, DRUGBANK, BIOGRID, CTD, SIDER, DISEASE ONTOLOGY and ORPHADATA.", "target": "Generating Explanations for Biomedical Queries"}
{"id": "task1540-e531bd47205347e1aac9dbdd68108502", "input": "In a real-world data set there is always the possibility, rather high in our opinion, that different features may have different degrees of relevance. Most machine learning algorithms deal with this fact by either selecting or deselecting features in the data preprocessing phase. However, we maintain that even among relevant features there may be different degrees of relevance, and this should be taken into account during the clustering process. With over 50 years of history, K-Means is arguably the most popular partitional clustering algorithm there is. The first K-Means based clustering algorithm to compute feature weights was designed just over 30 years ago. Various such algorithms have been designed since but there has not been, to our knowledge, a survey integrating empirical evidence of cluster recovery ability, common flaws, and possible directions for future research. This paper elaborates on the concept of feature weighting and addresses these issues by critically analysing some of the most popular, or innovative, feature weighting mechanisms based in K-Means.", "target": "A survey on feature weighting based K-Means algorithms"}
{"id": "task1540-01f0886bf96f4c4daf003a743f4b9b32", "input": "The GLEU metric was proposed for evaluating grammatical error corrections using n-gram overlap with a set of reference sentences, as opposed to precision/recall of specific annotated errors (Napoles et al., 2015). This paper describes improvements made to the GLEU metric that address problems that arise when using an increasing number of reference sets. Unlike the originally presented metric, the modified metric does not require tuning. We recommend that this version be used instead of the original version.1", "target": "GLEU Without Tuning"}
{"id": "task1540-e4e09494efc140659cf2a524f88575e9", "input": "In this paper, we briefly introduce MiniNLP, a natural language processing library for clinical narratives. MiniNLP is an experiment of our ideas on efficient and effective medical language processing. We introduce the overall design of MiniNLP and its major components, and show the performance of it in real projects.", "target": "A Short Introduction to MiniNLP"}
{"id": "task1540-6a12f816f2284fdebd7e40ba960e5086", "input": "We revisit the leaderboard problem introduced by Blum and Hardt (2015) in an effort to reduce overfitting in machine learning benchmarks. We show that a randomized version of their Ladder algorithm achieves leaderboard error O(1/n0.4) compared with the previous best rate of O(1/n1/3). Short of proving that our algorithm is optimal, we point out a major obstacle toward further progress. Specifically, any improvement to our upper bound would lead to asymptotic improvements in the general adaptive estimation setting as have remained elusive in recent years. This connection also directly leads to lower bounds for specific classes of algorithms. In particular, we exhibit a new attack on the leaderboard algorithm that both theoretically and empirically distinguishes between our algorithm and previous leaderboard algorithms.", "target": "Climbing a shaky ladder: Better adaptive risk estimation"}
{"id": "task1540-ada298aad14f4929a1ac98ce37a39a63", "input": "Many prediction domains, such as ad placement, recommendation, trajectory prediction, and document summarization, require predicting a set or list of options. Such lists are often evaluated using submodular reward functions that measure both quality and diversity. We propose a simple, efficient, and provably near-optimal approach to optimizing such prediction problems based on noregret learning. Our method leverages a surprising result from online submodular optimization: a single no-regret online learner can compete with an optimal sequence of predictions. Compared to previous work, which either learn a sequence of classifiers or rely on stronger assumptions such as realizability, we ensure both data-efficiency as well as performance guarantees in the fully agnostic setting. Experiments validate the efficiency and applicability of the approach on a wide range of problems including manipulator trajectory optimization, news recommendation and document summarization.", "target": "Learning Policies for Contextual Submodular Prediction"}
{"id": "task1540-6f5e91c63d124a5aa3dc522655c24b65", "input": "Many natural language generation tasks, such as abstractive summarization and text simplification, are paraphrase-orientated. In these tasks, copying and rewriting are two main writing modes. Most previous sequence-to-sequence (Seq2Seq) models use a single decoder and neglect this fact. In this paper, we develop a novel Seq2Seq model to fuse a copying decoder and a restricted generative decoder. The copying decoder finds the position to be copied based on a typical attention model. The generative decoder produces words limited in the source-specific vocabulary. To combine the two decoders and determine the final output, we develop a predictor to predict the mode of copying or rewriting. This predictor can be guided by the actual writing mode in the training data. We conduct extensive experiments on two different paraphrase datasets. The result shows that our model outperforms the stateof-the-art approaches in terms of both informativeness and language quality.", "target": "Joint Copying and Restricted Generation for Paraphrase"}
{"id": "task1540-9940fe62ddbb462dab9f35a680c5135a", "input": "Probabilistic independence can dramatically sim\u00ad plify the task of eliciting, representing, and com\u00ad puting with probabilities in large domains. A key technique in achieving these benefits is the idea of graphical modeling. We survey existing no\u00ad tions of independence for utility functions in a multi-attribute space, and suggest that these can be used to achieve similar advantages. Our new results concern conditional additive in\u00ad dependence, which we show always has a per\u00ad fect representation as separation in an undirected graph (a Markov network). Conditional addi\u00ad tive independencies entail a particular functional form for the utility function that is analogous to a product decomposition of a probability function, and confers analogous benefits. This functional form has been utilized in the Bayesian network and influence diagram literature, but generally without an explanation in terms of independence. The functional form yields a decomposition of the utility function that can greatly speed up expected utility calculations, particularly when the utility graph has a similar topology to the probabilistic network being used.", "target": "Graphical models for preference and utility"}
{"id": "task1540-bceb536444e2412082bbea4f27b2fed2", "input": "We present a position paper advocating the notion that Stoic philosophy and ethics can inform the development of ethical A.I. systems. This is in sharp contrast to most work on building ethical A.I., which has focused on Utilitarian or Deontological ethical theories. We relate ethical A.I. to several core Stoic notions, including the dichotomy of control, the four cardinal virtues, the ideal Sage, Stoic practices, and Stoic perspectives on emotion or affect. More generally, we put forward an ethical view of A.I. that focuses more on internal states of the artificial agent rather than on external actions of the agent. We provide examples relating to near-term A.I. systems as well as hypothetical superintelligent", "target": "Stoic Ethics for Artificial Agents"}
{"id": "task1540-8493e3d00a36470ba29ce486fec38849", "input": "Online fashion sales present a challenging use case for personalized recommendation: Stores offer a huge variety of items in multiple sizes. Small stocks, high return rates, seasonality, and changing trends cause continuous turnover of articles for sale on all time scales. Customers tend to shop rarely, but often buy multiple items at once. We report on backtest experiments with sales data of 100k frequent shoppers at Zalando, Europe\u2019s leading online fashion platform. To model changing customer and store environments, our recommendation method employs a pair of neural networks: To overcome the cold start problem, a feedforward network generates article embeddings in \u201cfashion space,\u201d which serve as input to a recurrent neural network that predicts a style vector in this space for each client, based on their past purchase sequence. We compare our results with a static collaborative filtering approach, and a popularity ranking baseline.", "target": "An LSTM-Based Dynamic Customer Model for Fashion Recommendation"}
{"id": "task1540-1e7c0143fb1d4a7f9ec213986fcb12b6", "input": "Citation texts are sometimes not very informative or in some cases inaccurate by themselves; they need the appropriate context from the referenced paper to re ect its exact contributions. To address this problem, we propose an unsupervised model that uses distributed representation of words as well as domain knowledge to extract the appropriate context from the reference paper. Evaluation results show the e ectiveness of our model by signi cantly outperforming the state-of-the-art. We furthermore demonstrate how an e ective contextualization method results in improving citation-based summarization of the scienti c articles.", "target": "Contextualizing Citations for Scientific Summarization using Word Embeddings and Domain Knowledge"}
{"id": "task1540-c326160908a941af84402c58a0d9367b", "input": "We study the representation and encoding of phonemes in a recurrent neural network model of grounded speech. We use a model which processes images and their spoken descriptions, and projects the visual and auditory representations into the same semantic space. We perform a number of analyses on how information about individual phonemes is encoded in the MFCC features extracted from the speech signal, and the activations of the layers of the model. Via experiments with phoneme decoding and phoneme discrimination we show that phoneme representations are most salient in the lower layers of the model, where low-level signals are processed at a fine-grained level, although a large amount of phonological information is retain at the top recurrent layer. We further find out that the attention mechanism following the top recurrent layer significantly attenuates encoding of phonology and makes the utterance embeddings much more invariant to synonymy. Moreover, a hierarchical clustering of phoneme representations learned by the network shows an organizational structure of phonemes similar to those proposed in linguistics.", "target": "Encoding of phonology in a recurrent neural model of grounded speech"}
{"id": "task1540-adde2cc6c63945a79101600380050ac1", "input": "We seek decision rules for prediction-time cost reduction, where complete data is available for training, but during prediction-time, each feature can only be acquired for an additional cost. We propose a novel random forest algorithm to minimize prediction error for a user-specified average feature acquisition budget. While random forests yield strong generalization performance, they do not explicitly account for feature costs and furthermore require low correlation among trees, which amplifies costs. Our random forest grows trees with low acquisition cost and high strength based on greedy minimax cost-weighted-impurity splits. Theoretically, we establish near-optimal acquisition cost guarantees for our algorithm. Empirically, on a number of benchmark datasets we demonstrate superior accuracy-cost curves against state-of-the-art prediction-time algorithms.", "target": "Feature-Budgeted Random Forest"}
{"id": "task1540-001094e72c21467c9854e5d83b7cb8a9", "input": "We propose zoneout, a novel method for regularizing RNNs. At each timestep, zoneout stochastically forces some hidden units to maintain their previous values. Like dropout, zoneout uses random noise to train a pseudo-ensemble, improving generalization. But by preserving instead of dropping hidden units, gradient information and state information are more readily propagated through time, as in feedforward stochastic depth networks. We perform an empirical investigation of various RNN regularizers, and find encouraging results: zoneout gives significant performance improvements across tasks, yielding state-ofthe-art results in character-level language modeling on the Penn Treebank dataset and competitive results on word-level Penn Treebank and permuted sequential MNIST classification tasks.", "target": "Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations"}
{"id": "task1540-6efd47cad32d46ef81d7b725a430d6d6", "input": "The paradigm shift from shallow classifiers with hand-crafted features to endto-end trainable deep learning models has shown significant improvements on supervised learning tasks. Despite the promising power of deep neural networks (DNN), how to alleviate overfitting during training has been a research topic of interest. In this paper, we present a Generative-Discriminative Variational Model (GDVM) for visual classification, in which we introduce a latent variable inferred from inputs for exhibiting generative abilities towards prediction. In other words, our GDVM casts the supervised learning task as a generative learning process, with data discrimination to be jointly exploited for improved classification. In our experiments, we consider the tasks of multi-class classification, multi-label classification, and zero-shot learning. We show that our GDVM performs favorably against the baselines or recent generative DNN models.", "target": "Generative-Discriminative Variational Model for Visual Recognition"}
{"id": "task1540-8af5d585dfbb4e8a81d4f73ce15e105a", "input": "Minimum error rate training (MERT) is a widely used training procedure for statistical machine translation. A general problem of this approach is that the search space is easy to converge to a local optimum and the acquired weight set is not in accord with the real distribution of feature functions. This paper introduces coordinate system selection (RSS) into the search algorithm for MERT. Contrary to previous approaches in which every dimension only corresponds to one independent feature function, we create several coordinate systems by moving one of the dimensions to a new direction. The basic idea is quite simple but critical that the training procedure of MERT should be based on a coordinate system formed by search directions but not directly on feature functions. Experiments show that by selecting coordinate systems with tuning set results, better results can be obtained without any other language knowledge.", "target": "Coordinate System Selection for Minimum Error Rate Training in Statistical Machine Translation"}
{"id": "task1540-c362c9df88474f3a85b1e279627c1128", "input": "This paper contains analysis and extension of exploiters-based knowledge extraction methods, which allow generation of new knowledge, based on the basic ones. The main achievement of the paper is useful features of some universal exploiters proof, which allow extending set of basic classes and set of basic relations by finite set of new classes of objects and relations among them, which allow creating of complete lattice. Proposed approach gives an opportunity to compute quantity of new classes, which can be generated using it, and quantity of different types, which each of obtained classes describes; constructing of defined hierarchy of classes with determined subsumption relation; avoidance of some problems of inheritance and more efficient restoring of basic knowledge within the database.", "target": "Object-Oriented Knowledge Extraction using Universal Exploiters"}
{"id": "task1540-8526ca693e51427e8ebe15e888568394", "input": "Visual question answering (VQA) has witnessed great progress since May, 2015 as a classic problem unifying visual and textual data into a system. Many enlightening VQA works explore deep into the image and question encodings and fusing methods, of which attention is the most effective and infusive mechanism. Current attention based methods focus on adequate fusion of visual and textual features, but lack the attention to where people focus to ask questions about the image. Traditional attention based methods attach a single value to the feature at each spatial location, which losses many useful information. To remedy these problems, we propose a general method to perform saliency-like pre-selection on overlapped region features by the interrelation of bidirectional LSTM (BiLSTM), and use a novel element-wise multiplication based attention method to capture more competent correlation information between visual and textual features. We conduct experiments on the large-scale COCO-VQA dataset and analyze the effectiveness of our model demonstrated by strong empirical results.", "target": "Task-driven Visual Saliency and Attention-based Visual Question Answering"}
{"id": "task1540-266b9eb0f82e4e00a9538f26c1c4c814", "input": "Plagiarism is one of the growing issues in academia and is always a concern in Universities and other academic institutions. The situation is becoming even worse with the availability of ample resources on the web. This paper focuses on creating an effective and fast tool for plagiarism detection for text based electronic assignments. Our plagiarism detection tool named AntiPlag is developed using the tri-gram sequence matching technique. Three sets of text based assignments were tested by AntiPlag and the results were compared against an existing commercial plagiarism detection tool. AntiPlag showed better results in terms of false positives compared to the commercial tool due to the pre-processing steps performed in AntiPlag. In addition, to improve the detection latency, AntiPlag applies a data clustering technique making it four times faster than the commercial tool considered. AntiPlag could be used to isolate plagiarized text based assignments from non-plagiarised assignments easily. Therefore, we present AntiPlag, a fast and effective tool for plagiarism detection on text based electronic assignments.", "target": "AntiPlag: Plagiarism Detection on Electronic Submissions of Text Based Assignments"}
{"id": "task1540-1869881353fd4ccbbdcaf8f1724994fa", "input": "Kinect skeleton tracker is able to achieve considerable human body tracking performance in convenient and a low-cost manner. However, The tracker often captures unnatural human poses such as discontinuous and vibrated motions when self-occlusions occur. A majority of approaches tackle this problem by using multiple Kinect sensors in a workspace. Combination of the measurements from different sensors is then conducted in Kalman filter framework or optimization problem is formulated for sensor fusion. However, these methods usually require heuristics to measure reliability of measurements observed from each Kinect sensor. In this paper, we developed a method to improve Kinect skeleton using single Kinect sensor, in which supervised learning technique was employed to correct unnatural tracking motions. Specifically, deep recurrent neural networks were used for improving joint positions and velocities of Kinect skeleton, and three methods were proposed to integrate the refined positions and velocities for further enhancement. Moreover, we suggested a novel measure to evaluate naturalness of captured motions. We evaluated the proposed approach by comparison with the ground truth obtained using a commercial optical maker-based motion capture system.", "target": "Tracking Human-like Natural Motion Using Deep Recurrent Neural Networks"}
{"id": "task1540-87959de5c589482a9f937fdc711e27a1", "input": "We introduce a method for transliteration generation that can produce transliterations in every language. Where previous results are only as multilingual as Wikipedia, we show how to use training data from Wikipedia as surrogate training for any language. Thus, the problem becomes one of ranking Wikipedia languages in order of suitability with respect to a target language. We introduce several task-specific methods for ranking languages, and show that our approach is comparable to the oracle ceiling, and even outperforms it in some cases.", "target": "Transliteration in Any Language with Surrogate Languages"}
{"id": "task1540-b3037a520b164227ac2dc358246e8a72", "input": "In this paper we compare different types of recurrent units in recurrent neural networks (RNNs). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found GRU to be comparable to LSTM.", "target": "Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling"}
{"id": "task1540-2a4aed0ebce14ea2ba33731dba1f02bf", "input": "Hybrid Probabilistic Programs (HPPs) are logic programs that allow the programmer to explicitly encode his knowledge of the de\u00ad pendencies between events being described in the program. In this paper, we classify HPPs into three classes called H P P1, H P P2 and H P Pr, r 2: 3. For these classes, we pro\u00ad vide three types of results for HPPs. First, we develop algorithms to compute the set of all ground consequences of an HPP. Then we provide algorithms and complexity results for the problems of entailment (\"Given an HPP P and a query Q as input, is Q a logical con\u00ad sequence of P?\") and consistency (\"Given an HPP Pas input, is P consistent?\"). Our re\u00ad sults provide a fine characterization of when polynomial algorithms exist for the above problems, and when these problems become intractable.", "target": "Hybrid Probabilistic Programs: Algorithms and Complexity"}
{"id": "task1540-5b2f87f17acf4de4824c50dfa1cd5a54", "input": "This paper emphasizes the significance to jointly exploit the problem structure and the parameter structure, in the context of deep modeling. As a specific and interesting example, we describe the deep double sparsity encoder (DDSE), which is inspired by the double sparsity model for dictionary learning. DDSE simultaneously sparsities the output features and the learned model parameters, under one unified framework. In addition to its intuitive model interpretation, DDSE also possesses compact model size and low complexity. Extensive simulations compare DDSE with several carefully-designed baselines, and verify the consistently superior performance of DDSE. We further apply DDSE to the novel application domain of brain encoding, with promising preliminary results achieved.", "target": "Deep Double Sparsity Encoder: Learning to Sparsify Not Only Features But Also Parameters"}
{"id": "task1540-2b7454a2b24c4952b611db829052192a", "input": "In this paper, we propose a novel learning based method for automated segmentation of brain tumor in multimodal MRI images. The machine learned features from fully convolutional neural network (FCN) and hand-designed texton features are used to classify the MRI image voxels. The score map with pixelwise predictions is used as a feature map which is learned from multimodal MRI training dataset using the FCN. The learned features are then applied to random forests to classify each MRI image voxel into normal brain tissues and different parts of tumor. The method was evaluated on BRATS 2013 challenge dataset. The results show that the application of the random forest classifier to multimodal MRI images using machine-learned features based on FCN and hand-designed features based on textons provides promising segmentations. The Dice overlap measure for automatic brain tumor segmentation against ground truth is 0.88, 080 and 0.73 for complete tumor, core and enhancing tumor, respectively.", "target": "Multimodal MRI brain tumor segmentation using random forests with features learned from fully convolutional neural network"}
{"id": "task1540-802d5179248c449db43470c53e3c2475", "input": "A college student\u2019s life can be primarily categorized into domains such as education, health, social and other activities which may include daily chores and travelling time. Time management is crucial for every student. A self realisation of one\u2019s daily time expenditure in various domains is therefore essential to maximize one\u2019s effective output. This paper presents how a mobile application using Fuzzy Logic and Global Positioning System (GPS) analyzes a student\u2019s lifestyle and provides recommendations and suggestions based on the results. Keywords\u2014Fuzzy Logic, GPS, Android Application", "target": "A Fuzzy Logic System to Analyze a Student\u2019s Lifestyle"}
{"id": "task1540-d94af19ba75f4ae7b8ef6765d6c84fc3", "input": "A major challenge in paraphrase research is the lack of parallel corpora. In this paper, we present a new method to collect large-scale sentential paraphrases from Twitter by linking tweets through shared URLs. The main advantage of our method is its simplicity, as it gets rid of the classifier or human in the loop needed to select data before annotation and subsequent application of paraphrase identification algorithms in the previous work. We present the largest human-labeled paraphrase corpus to date of 51,524 sentence pairs and the first cross-domain benchmarking for automatic paraphrase identification. In addition, we show that more than 30,000 new sentential paraphrases can be easily and continuously captured every month at \u223c70% precision, and demonstrate their utility for downstream NLP tasks through phrasal paraphrase extraction. We make our code and data freely available.1", "target": "A Continuously Growing Dataset of Sentential Paraphrases"}
{"id": "task1540-c4c56c539c684471b18029dfcfdb1b54", "input": "The recently introduced Intelligent Trial and Error algorithm (IT&E) enables robots to creatively adapt to damage in a matter of minutes by combining an off-line evolutionary algorithm and an on-line learning algorithm based on Bayesian Optimization. We extend the IT&E algorithm to allow for robots to learn to compensate for damages while executing their task(s). This leads to a semi-episodic learning scheme that increases the robot\u2019s life-time autonomy and adaptivity. Preliminary experiments on a toy simulation and a 6-legged robot locomotion task show promising results.", "target": "Towards semi-episodic learning for robot damage recovery"}
{"id": "task1540-b0f6c8a6a515433db7ca1ced3015f157", "input": "One of the benefits of belief networks and influence diagrams is that so much knowl\u00ad edge is captured in the graphical structure. In particular, statements of conditional irrel\u00ad evance (or independence) can be verified in time linear in the size of the graph. To re\u00ad solve a particular inference query or decision problem, only some of the possible states and probability distributions must be specified, the \"requisite information.\" This paper presents a new, simple, and effi\u00ad cient \"Bayes-ball\" algorithm which is well\u00ad suited to both new students of belief net\u00ad works and state of the art implementations. The Bayes-ball algorithm determines irrele\u00ad vant sets and requisite information more ef\u00ad ficiently than existing methods, and is linear in the size of the graph for belief networks and influence diagrams.", "target": "Bayes-Ball: The Rational Pastime (for Determining Irrelevance and Requisite Information in Belief Networks and Influence Diagrams)"}
{"id": "task1540-4cb603db8aa742848ee08e62eadcc983", "input": "A fundamental advantage of neural models for NLP is their ability to learn representations from scratch. However, in practice this often means ignoring existing external linguistic resources, e.g., WordNet or domain specific ontologies such as the Unified Medical Language System (UMLS). We propose a general, novel method for exploiting such resources via weight sharing. Prior work on weight sharing in neural networks has considered it largely as a means of model compression. In contrast, we treat weight sharing as a flexible mechanism for incorporating prior knowledge into neural models. We show that this approach consistently yields improved performance on classification tasks compared to baseline strategies that do not exploit weight sharing.", "target": "Exploiting Domain Knowledge via Grouped Weight Sharing with Application to Text Categorization"}
{"id": "task1540-bda3792257fa48d0ad3b58f9b41f5d4e", "input": "Microarrays are made it possible to simultaneously monitor the expression profiles of thousands of genes under various experimental conditions. It is used to identify the co-expressed genes in specific cells or tissues that are actively used to make proteins. This method is used to analysis the gene expression, an important task in bioinformatics research. Cluster analysis of gene expression data has proved to be a useful tool for identifying co-expressed genes, biologically relevant groupings of genes and samples. In this paper we applied K-Means with Automatic Generations of Merge Factor for ISODATAAGMFI. Though AGMFI has been applied for clustering of Gene Expression Data, this proposed Enhanced Automatic Generations of Merge Factor for ISODATAEAGMFI Algorithms overcome the drawbacks of AGMFI in terms of specifying the optimal number of clusters and initialization of good cluster centroids. Experimental results on Gene Expression Data show that the proposed EAGMFI algorithms could identify compact clusters with perform well in terms of the Silhouette Coefficients cluster measure.", "target": "Performance Analysis of Enhanced Clustering Algorithm for Gene Expression Data"}
{"id": "task1540-02ea6f742ff740cbb9386f88c386e96b", "input": "Argument Component Boundary Detection (ACBD) is an important sub-task in argumentation mining; it aims at identifying the word sequences that constitute argument components, and is usually considered as the first sub-task in the argumentation mining pipeline. Existing ACBD methods heavily depend on task-specific knowledge, and require considerable human efforts on feature-engineering. To tackle these problems, in this work, we formulate ACBD as a sequence labeling problem and propose a variety of Recurrent Neural Network (RNN) based methods, which do not use domain specific or handcrafted features beyond the relative position of the sentence in the document. In particular, we propose a novel joint RNN model that can predict whether sentences are argumentative or not, and use the predicted results to more precisely detect the argument component boundaries. We evaluate our techniques on two corpora from two different genres; results suggest that our joint RNN model obtain the state-of-the-art performance on both datasets.", "target": "Joint RNN Model for Argument Component Boundary Detection"}
{"id": "task1540-60980ced86924832997173d2639bf5b8", "input": "Bayesian optimization has proven to be a highly effective methodology for the global optimization of unknown, expensive and multimodal functions. The ability to accurately model distributions over functions is critical to the effectiveness of Bayesian optimization. Although Gaussian processes provide a flexible prior over functions, there are various classes of functions that remain difficult to model. One of the most frequently occurring of these is the class of non-stationary functions. The optimization of the hyperparameters of machine learning algorithms is a problem domain in which parameters are often manually transformed a priori, for example by optimizing in \u201clog-space,\u201d to mitigate the effects of spatially-varying length scale. We develop a methodology for automatically learning a wide family of bijective transformations or warpings of the input space using the Beta cumulative distribution function. We further extend the warping framework to multi-task Bayesian optimization so that multiple tasks can be warped into a jointly stationary space. On a set of challenging benchmark optimization tasks, we observe that the inclusion of warping greatly improves on the state-of-the-art, producing better results faster and more reliably.", "target": "INPUT WARPING FOR BAYESIAN OPTIMIZATION OF NON-STATIONARY FUNCTIONS BY JASPER SNOEK"}
{"id": "task1540-17d0635ecbf941a09b826c3aa2a70028", "input": "In recent years, Deep Neural Networks (DNN) based methods have achieved remarkable performance in a wide range of tasks and have been among the most powerful and widely used techniques in computer vision, speech recognition and Natural Language Processing. However, DNN-based methods are both computational-intensive and resource-consuming, which hinders the application of these methods on embedded systems like smart phones. To alleviate this problem, we introduce a novel Fixed-point Factorized Networks (FFN) on pre-trained models to reduce the computational complexity as well as the storage requirement of networks. Extensive experiments on large-scale ImageNet classification task show the effectiveness of our proposed method.", "target": "Fixed-point Factorized Networks"}
{"id": "task1540-d1445ebe52cf4840a6bbc66dc6181981", "input": "A key issue in statistics and machine learning is to automatically select the \u201cright\u201d model complexity, e.g., the number of neighbors to be averaged over in k nearest neighbor (kNN) regression or the polynomial degree in regression with polynomials. We suggest a novel principle the Loss Rank Principle (LoRP) for model selection in regression and classification. It is based on the loss rank, which counts how many other (fictitious) data would be fitted better. LoRP selects the model that has minimal loss rank. Unlike most penalized maximum likelihood variants (AIC, BIC, MDL), LoRP depends only on the regression functions and the loss function. It works without a stochastic noise model, and is directly applicable to any non-parametric regressor, like kNN.", "target": "Model Selection with the Loss Rank Principle"}
{"id": "task1540-f421cc922e2a41d697ea063e9f52e678", "input": "When training deep neural networks, it is typically assumed that the training examples are uniformly difficult to learn. Or, to restate, it is assumed that the training error will be uniformly distributed across the training examples. Based on these assumptions, each training example is used an equal number of times. However, this assumption may not be valid in many cases. \u201cOddball SGD\u201d (novelty-driven stochastic gradient descent) was recently introduced to drive training probabilistically according to the error distribution \u2013 training frequency is proportional to training error magnitude. In this article, using a deep neural network to encode a video, we show that oddball SGD can be used to enforce uniform error across the training set.", "target": "Uniform Learning in a Deep Neural Network via \"Oddball\" Stochastic Gradient Descent"}
{"id": "task1540-dd2bad1a1f78480ea7c6e061d6c8f330", "input": "This paper describes our participation in Task 5 track 2 of SemEval 2017 to predict the sentiment of financial news headlines for a specific company on a continuous scale between -1 and 1. We tackled the problem using a number of approaches, utilising a Support Vector Regression (SVR) and a Bidirectional Long Short-Term Memory (BLSTM). We found an improvement of 4-6% using the LSTM model over the SVR and came fourth in the track. We report a number of different evaluations using a finance specific word embedding model and reflect on the effects of using different evaluation metrics.", "target": "Lancaster A at SemEval-2017 Task 5: Evaluation metrics matter: predicting sentiment from financial news headlines"}
{"id": "task1540-3de654e58fc14405a4ff75386b901ce2", "input": "This paper describes our deep learning-based approach to sentiment analysis in Twitter as part of SemEval-2016 Task 4. We use a convolutional neural network to determine sentiment and participate in all subtasks, i.e. two-point, three-point, and five-point scale sentiment classification and two-point and five-point scale sentiment quantification. We achieve competitive results for two-point scale sentiment classification and quantification, ranking fifth and a close fourth (third and second by alternative metrics) respectively despite using only pre-trained embeddings that contain no sentiment information. We achieve good performance on three-point scale sentiment classification, ranking eighth out of 35, while performing poorly on fivepoint scale sentiment classification and quantification. An error analysis reveals that this is due to low expressiveness of the model to capture negative sentiment as well as an inability to take into account ordinal information. We propose improvements in order to address these and other issues.", "target": "INSIGHT-1 at SemEval-2016 Task 4: Convolutional Neural Networks for Sentiment Classification and Quantification"}
{"id": "task1540-8b6ca43179bb482d94ccf6e03c19c849", "input": "We consider the task of identifying attitudes towards a given set of entities from text. Conventionally, this task is decomposed into two separate subtasks: target detection that identifies whether each entity is mentioned in the text, either explicitly or implicitly, and polarity classification that classifies the exact sentiment towards an identified entity (the target) into positive, negative, or neutral. Instead, we show that attitude identification can be solved with an end-to-end machine learning architecture, in which the two subtasks are interleaved by a deep memory network. In this way, signals produced in target detection provide clues for polarity classification, and reversely, the predicted polarity provides feedback to the identification of targets. Moreover, the treatments for the set of targets also influence each other \u2013 the learned representations may share the same semantics for some targets but vary for others. The proposed deep memory network, the AttNet, outperforms methods that do not consider the interactions between the subtasks or those among the targets, including conventional machine learning methods and the state-of-the-art deep learning models.", "target": "Deep Memory Networks for Attitude Identification"}
{"id": "task1540-ccf9b4f56c5d4df184fdac96fc56296c", "input": "Speech Recognition searches to predict the spoken words automatically. These systems are known to be very expensive because of using several pre-recorded hours of speech. Hence, building a model that minimizes the cost of the recognizer will be very interesting. In this paper, we present a new approach for recognizing speech based on belief HMMs instead of probabilistic HMMs. Experiments shows that our belief recognizer is insensitive to the lack of the data and it can be trained using only one exemplary of each acoustic unit and it gives a good recognition rates. Consequently, using the belief HMM recognizer can greatly minimize the cost of these systems.", "target": "Belief Hidden Markov Model for Speech Recognition"}
{"id": "task1540-fdbea99ac8e943088229e2b210e967b4", "input": "Cold start problem in Collaborative Filtering can be solved by asking new users to rate a small seed set of representative items or by asking representative users to rate a new item. The question is how to build a seed set that can give enough preference information for making good recommendations. One of the most successful approaches, called Representative Based Matrix Factorization, is based on Maxvol algorithm. Unfortunately, this approach has one important limitation \u2014 a seed set of a particular size requires a rating matrix factorization of fixed rank that should coincide with that size. This is not necessarily optimal in the general case. In the current paper, we introduce a fast algorithm for an analytical generalization of this approach that we call Rectangular Maxvol. It allows the rank of factorization to be lower than the required size of the seed set. Moreover, the paper includes the theoretical analysis of the method\u2019s error, the complexity analysis of the existing methods and the comparison to the state-of-the-art approaches.", "target": "Efficient Rectangular Maximal-Volume Algorithm for Rating Elicitation in Collaborative Filtering"}
{"id": "task1540-d81b06c16e4b4cb3a44b4651fba5cf20", "input": "A rich coreset is a subset of the data which contains nearly all the essential information. We give deterministic, low order polynomial-time algorithms to construct rich coresets for simple and multiple response linear regression, together with lower bounds indicating that there is not much room for improvement upon our results.", "target": "Rich Coresets For Constrained Linear Regression"}
{"id": "task1540-57c4190addb04f45b5b0241d4fe5aea4", "input": "Knowledge representation and reasoning capacities are vital to cognitive robotics because they provide higher level cognitive functions for reasoning about actions, environments, goals, perception, etc. Although Answer Set Programming (ASP) is well suited for modelling such functions, there was so far no seamless way to use ASP in a robotic environment. We address this shortcoming and show how a recently developed reactive ASP system can be harnessed to provide appropriate reasoning capacities within a robotic system. To be more precise, we furnish a package integrating the reactive ASP solver oClingo with the popular open-source robotic middleware ROS. The resulting system, ROSoClingo, provides a generic way by which an ASP program can be used to control the behaviour of a robot and to respond to the results of the robot\u2019s actions.", "target": "ROSoClingo: A ROS package for ASP-based robot control"}
{"id": "task1540-028dbdaf3517454e8cd492611357b1ea", "input": "Nonlinear independent component analysis (ICA) provides an appealing framework<lb>for unsupervised feature learning, but the models proposed so far are not identifiable.<lb>Here, we first propose a new intuitive principle of unsupervised deep learning<lb>from time series which uses the nonstationary structure of the data. Our learning<lb>principle, time-contrastive learning (TCL), finds a representation which allows<lb>optimal discrimination of time segments (windows). Surprisingly, we show how<lb>TCL can be related to a nonlinear ICA model, when ICA is redefined to include<lb>temporal nonstationarities. In particular, we show that TCL combined with linear<lb>ICA estimates the nonlinear ICA model up to point-wise transformations of the<lb>sources, and this solution is unique \u2014 thus providing the first identifiability result<lb>for nonlinear ICA which is rigorous, constructive, as well as very general.", "target": "Unsupervised Feature Extraction by Time-Contrastive Learning and Nonlinear ICA"}
{"id": "task1540-137d93d36c3641b0bb6c79de41ec7cf7", "input": "We present an efficient method for training slackrescaled structural SVM. Although finding the most violating label in a margin-rescaled formulation is often easy since the target function decomposes with respect to the structure, this is not the case for a slack-rescaled formulation, and finding the most violated label might be very difficult. Our core contribution is an efficient method for finding the most-violatinglabel in a slack-rescaled formulation, given an oracle that returns the most-violating-label in a (slightly modified) margin-rescaled formulation. We show that our method enables accurate and scalable training for slack-rescaled SVMs, reducing runtime by an order of magnitude compared to previous approaches to slack-rescaled SVMs.", "target": "Fast and Scalable Structural SVM with Slack Rescaling"}
{"id": "task1540-a4ac7838a679432e9ce57d804c2b9aaf", "input": "The essence of distantly supervised relation extraction is that it is an incomplete multi-label classification problem with sparse and noisy features. To tackle the sparsity and noise challenges, we propose solving the classification problem using matrix completion on factorized matrix of minimized rank. We formulate relation classification as completing the unknown labels of testing items (entity pairs) in a sparse matrix that concatenates training and testing textual features with training labels. Our algorithmic framework is based on the assumption that the rank of item-by-feature and item-by-label joint matrix is low. We apply two optimization models to recover the underlying low-rank matrix leveraging the sparsity of featurelabel matrix. The matrix completion problem is then solved by the fixed point continuation (FPC) algorithm, which can find the global optimum. Experiments on two widely used datasets with different dimensions of textual features demonstrate that our low-rank matrix completion approach significantly outperforms the baseline and the state-of-the-art methods.", "target": "Errata: Distant Supervision for Relation Extraction with Matrix Completion"}
{"id": "task1540-0816275800c343558d01073b586b2eae", "input": "A long-standing challenge in coreference resolution has been the incorporation of entity-level information \u2013 features defined over clusters of mentions instead of mention pairs. We present a neural network based coreference system that produces high-dimensional vector representations for pairs of coreference clusters. Using these representations, our system learns when combining clusters is desirable. We train the system with a learning-to-search algorithm that teaches it which local decisions (cluster merges) will lead to a high-scoring final coreference partition. The system substantially outperforms the current state-of-the-art on the English and Chinese portions of the CoNLL 2012 Shared Task dataset despite using few hand-engineered features.", "target": "Improving Coreference Resolution by Learning Entity-Level Distributed Representations"}
{"id": "task1540-aa70fbe45a1f43739913f609ff73b972", "input": "In this paper, we propose a new autonomous braking system based on deep reinforcement learning. The proposed autonomous braking system automatically decides whether to apply the brake at each time step when confronting the risk of collision using the information on the obstacle obtained by the sensors. The problem of designing brake control is formulated as searching for the optimal policy in Markov decision process (MDP) model where the state is given by the relative position of the obstacle and the vehicle\u2019s speed, and the action space is defined as whether brake is stepped or not. The policy used for brake control is learned through computer simulations using the deep reinforcement learning method called deep Q-network (DQN). In order to derive desirable braking policy, we propose the reward function which balances the damage imposed to the obstacle in case of accident and the reward achieved when the vehicle runs out of risk as soon as possible. DQN is trained for the scenario where a vehicle is encountered with a pedestrian crossing the urban road. Experiments show that the control agent exhibits desirable control behavior and avoids collision without any mistake in various uncertain environments.", "target": "Autonomous Braking System via Deep Reinforcement Learning"}
{"id": "task1540-41eac14527f34cac9777658314789383", "input": "As mobile devices have become indispensable in modern life, mobile security is becoming much more important. Traditional password or PIN-like point-of-entry security measures score low on usability and are vulnerable to brute force and other types of attacks. In order to improve mobile security, an adaptive neuro-fuzzy inference system(ANFIS)-based implicit authentication system is proposed in this paper to provide authentication in a continuous and transparent manner. To illustrate the applicability and capability of ANFIS in our implicit authentication system, experiments were conducted on behavioural data collected for up to 12 weeks from different Android users. The ability of the ANFIS-based system to detect an adversary is also tested with scenarios involving an attacker with varying levels of knowledge. The results demonstrate that ANFIS is a feasible and efficient approach for implicit authentication with an average of 95% user recognition rate. Moreover, the use of ANFIS-based system for implicit authentication significantly reduces manual tuning and configuration tasks due to its selflearning capability.", "target": "Continuous Implicit Authentication for Mobile Devices based on Adaptive Neuro-Fuzzy Inference System"}
{"id": "task1540-754bab22060942d189d99efeb58bff20", "input": "Language understanding is a key component in a spoken dialogue system. In this paper, we investigate how the language understanding module influences the dialogue system performance by conducting a series of systematic experiments on a task-oriented neural dialogue system in a reinforcement learning based setting. The empirical study shows that among different types of language understanding errors, slot-level errors can have more impact on the overall performance of a dialogue system compared to intent-level errors. In addition, our experiments demonstrate that the reinforcement learning based dialogue system is able to learn when and what to confirm in order to achieve better performance and greater robustness.", "target": "Investigation of Language Understanding Impact for Reinforcement Learning Based Dialogue Systems"}
{"id": "task1540-b07359fed3a540358fb10196a67bcf81", "input": "Words can be represented by composing the representations of subword units such as word segments, characters, and/or character n-grams. While such representations are effective and may capture the morphological regularities of words, they have not been systematically compared, and it is not understood how they interact with different morphological typologies. On a language modeling task, we present experiments that systematically vary (1) the basic unit of representation, (2) the composition of these representations, and (3) the morphological typology of the language modeled. Our results largely confirm previous findings that character representations are effective across many languages, though we find that a previously unstudied combination of character trigram representations composed with bi-LSTMs outperforms most other settings. However, we also find room for improvement: character models do not match the predictive accuracy of a model with access to explicit morphological analyses.", "target": "From Characters to Words to in Between: Do We Capture Morphology?"}
{"id": "task1540-ff9fee13dcfc46358adc8c823a1bdb81", "input": "Task-oriented dialogue focuses on conversational agents that participate in userinitiated dialogues on domain-specific topics. In contrast to chatbots, which simply seek to sustain open-ended meaningful discourse, existing task-oriented agents usually explicitly model user intent and belief states. This paper examines bypassing such an explicit representation by depending on a latent neural embedding of state and learning selective attention to dialogue history together with copying to incorporate relevant prior context. We complement recent work by showing the effectiveness of simple sequence-to-sequence neural architectures with a copy mechanism. Our model outperforms more complex memory-augmented models by 7% in per-response generation and is on par with the current state-of-the-art on DSTC2.", "target": "A Copy-Augmented Sequence-to-Sequence Architecture Gives Good Performance on Task-Oriented Dialogue"}
{"id": "task1540-283f3867f8cb4fad99ca207bedfc60b8", "input": "Knowing the largest rate at which data can be sent on an end-to-end path such that the egress rate is equal to the ingress rate with high probability can be very practical when choosing transmission rates in video streaming or selecting peers in peer-to-peer applications. We introduce probabilistic available bandwidth, which is defined in terms of ingress rates and egress rates of traffic on a path, rather than in terms of capacity and utilization of the constituent links of the path like the standard available bandwidth metric. In this paper, we describe a distributed algorithm, based on a probabilistic graphical model and Bayesian active learning, for simultaneously estimating the probabilistic available bandwidth of multiple paths through a network. Our procedure exploits the fact that each packet train provides information not only about the path it traverses, but also about any path that shares a link with the monitored path. Simulations and PlanetLab experiments indicate that this process can dramatically reduce the number of probes required to generate accurate estimates.", "target": "Multi-path Probabilistic Available Bandwidth Estimation through Bayesian Active Learning"}
{"id": "task1540-4a2d5a1087f34049a5ae6cb18aef2ab9", "input": "We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning. Our algorithm introduces a recognition model to represent approximate posterior distributions, and that acts as a stochastic encoder of the data. We develop stochastic backpropagation \u2013 rules for back-propagation through stochastic variables \u2013 and use this to develop an algorithm that allows for joint optimisation of the parameters of both the generative and recognition model. We demonstrate on several real-world data sets that the model generates realistic samples, provides accurate imputations of missing data and is a useful tool for high-dimensional data visualisation.", "target": "Stochastic Back-propagation and Variational Inference in  Deep Latent Gaussian Models"}
{"id": "task1540-41c2ff86dab74629859eaf4a6750e0ff", "input": "Choquet expected utility (CEU) is one of the most sophisticated decision criteria used in decision theory under uncertainty. It provides a generalisation of expected utility enhancing both descriptive and prescriptive possibilities. In this paper, we investigate the use of CEU for path-planning under uncertainty with a special focus on robust solutions. We first recall the main features of the CEU model and introduce some examples showing its descriptive potential. Then we focus on the search for Choquet-optimal paths in multivalued implicit graphs where costs depend on different scenarios. After discussing complexity issues, we propose two different heuristic search algorithms to solve the problem. Finally, numerical experiments are reported, showing the practical efficiency of the proposed algorithms.", "target": "Search for Choquet-optimal paths under uncertainty"}
{"id": "task1540-55077fe95e3742ff8db5c9f92c32549c", "input": "We propose a new algorithm for topic modeling, Vec2Topic, that identifies the main topics in a corpus using semantic information captured via high-dimensional distributed word embeddings. Our technique is unsupervised and generates a list of topics ranked with respect to importance. We find that it works better than existing topic modeling techniques such as Latent Dirichlet Allocation for identifying key topics in user-generated content, such as emails, chats, etc., where topics are diffused across the corpus. We also find that Vec2Topic works equally well for non-user generated content, such as papers, reports, etc., and for small corpora such as a single-document.", "target": "Topic Modeling Using Distributed Word Embeddings"}
{"id": "task1540-65999922c46a4dafa76569ec97c5273f", "input": "This paper presents a dataset collected from natural dialogs which enables to test the ability of dialog systems to learn new facts from user utterances throughout the dialog. This interactive learning will help with one of the most prevailing problems of open domain dialog system, which is the sparsity of facts a dialog system can reason about. The proposed dataset, consisting of 1900 collected dialogs, allows simulation of an interactive gaining of denotations and questions explanations from users which can be used for the interactive learning.", "target": "Data Collection for Interactive Learning through the Dialog"}
{"id": "task1540-588509e9e9f344f5a2d080e76d9f58d6", "input": "Financial news contains useful information on public companies and the market. In this paper we apply the popular word embedding methods and deep neural networks to leverage financial news to predict stock price movements in the market. Experimental results have shown that our proposed methods are simple but very effective, which can significantly improve the stock prediction accuracy on a standard financial database over the baseline system using only the historical price information.", "target": "Leverage Financial News to Predict Stock Price Movements Using Word Embeddings and Deep Neural Networks"}
{"id": "task1540-755400e4d5884d959a2739e70be92410", "input": "We present a skill analysis with time series image data using data mining methods, focused on table tennis. We do not use body model, but use only hi-speed movies, from which time series data are obtained and analyzed using data mining methods such as C4.5 and so on. We identify internal models for technical skills as evaluation skillfulness for the forehand stroke of table tennis, and discuss mono and meta-functional skills for improving skills. Keywords\u2014component; Time Series Data, Sport Skill, Data Mining, Image Processing, Knowledge Acquisition", "target": "Skill Analysis with Time Series Image Data"}
{"id": "task1540-b624aa33000148439c482fa4b3967c6e", "input": "Online sequence prediction is the problem of predicting the next element of a sequence given previous elements. This problem has been extensively studied in the context of individual sequence prediction, where no prior assumptions are made on the origin of the sequence. Individual sequence prediction algorithms work quite well for long sequences, where the algorithm has enough time to learn the temporal structure of the sequence. However, they might give poor predictions for short sequences. A possible remedy is to rely on the general model of prediction with expert advice, where the learner has access to a set of r experts, each of which makes its own predictions on the sequence. It is well known that it is possible to predict almost as well as the best expert if the sequence length is order of log(r). But, without firm prior knowledge on the problem, it is not clear how to choose a small set of good experts. In this paper we describe and analyze a new algorithm that learns a good set of experts using a training set of previously observed sequences. We demonstrate the merits of our approach by applying it on the task of click prediction on the web.", "target": "Learning the Experts for Online Sequence Prediction"}
{"id": "task1540-689114d1eb4a4796b9b4cf0db6ea6c28", "input": "Time series is attracting more attention across statistics, machine learning and pattern recognition as it appears widely in both industry and academia, but few advances has been achieved in effective time series visualization due to its temporal dimensionality and complex dynamics. Inspired by recent effort on using network metrics to characterize time series for classification, we present an approach to visualize time series as complex networks based on first order Markov process and temporal ordering. Different to classical bar charts, line plots and other statistics based graph, our approach delivers more intuitive visualization that better preserves both the temporal dependency and frequency structures. It provides a natural inverse operation to map the graph back to time series, making it possible to use graph statistics to characterize time series for better visual exploration and statistical analysis. Our experimental results suggest the effectiveness on various tasks such as system identification, classification and anomaly detection on both synthetic and the real time series data.", "target": "Encoding Temporal Markov Dynamics in Graph for Time Series Visualization"}
{"id": "task1540-027474bea82e4b48bcfec9f5e74c186f", "input": "Multimodal sentiment analysis is an increasingly popular research area, which extends the conventional language-based definition of sentiment analysis to a multimodal setup where other relevant modalities accompany language. In this paper, we pose the problem of multimodal sentiment analysis as modeling intra-modality and inter-modality dynamics. We introduce a novel model, termed Tensor Fusion Network, which learns both such dynamics end-to-end. The proposed approach is tailored for the volatile nature of spoken language in online videos as well as accompanying gestures and voice. In the experiments, our model outperforms state-ofthe-art approaches for both multimodal and unimodal sentiment analysis.", "target": "Tensor Fusion Network for Multimodal Sentiment Analysis"}
{"id": "task1540-43569bbaebd24cc6803af657694d536f", "input": "Automated writing evaluation (AWE) has been shown to be an effective mechanism for quickly providing feedback to students. It has already seen wide adoption in enterprise-scale applications and is starting to be adopted in large-scale contexts. Training an AWE model has historically required a single batch of several hundred writing examples and human scores for each of them. This requirement limits large-scale adoption of AWE since human-scoring essays is costly. Here we evaluate algorithms for ensuring that AWE models are consistently trained using the most informative essays. Our results show how to minimize training set sizes while maximizing predictive performance, thereby reducing cost without unduly sacrificing accuracy. We conclude with a discussion of how to integrate this approach into large-scale AWE systems.", "target": "Effective sampling for large-scale automated writing evaluation systems"}
{"id": "task1540-9a3498a7ed98480db45fa005c68b3e57", "input": "We study the power of different types of adaptive (nonoblivious) adversaries in the setting of prediction with expert advice, under both full-information and bandit feedback. We measure the player\u2019s performance using a new notion of regret, also known as policy regret, which better captures the adversary\u2019s adaptiveness to the player\u2019s behavior. In a setting where losses are allowed to drift, we characterize \u2014in a nearly complete manner\u2014 the power of adaptive adversaries with bounded memories and switching costs. In particular, we show that with switching costs, the attainable rate with bandit feedback is \u0398\u0303(T ). Interestingly, this rate is significantly worse than the \u0398( \u221a T ) rate attainable with switching costs in the full-information case. Via a novel reduction from experts to bandits, we also show that a bounded memory adversary can force \u0398\u0303(T ) regret even in the full information case, proving that switching costs are easier to control than bounded memory adversaries. Our lower bounds rely on a new stochastic adversary strategy that generates loss processes with strong dependencies.", "target": "Online Learning with Switching Costs and Other Adaptive Adversaries"}
{"id": "task1540-c2798123e94f4c04bcea41594582930e", "input": "We undertook a study of the use of a memristor network for music generation, making use of the memristor\u2019s memory to go beyond the Markov hypothesis. Seed transition matrices are created and populated using memristor equations, and which are shown to generate musical melodies and change in style over time as a result of feedback into the transition matrix. The spiking properties of simple memristor networks are demonstrated and discussed with reference to applications of music making. The limitations of simulating composing memristor networks in von Neumann hardware is discussed and a hardware solution based on physical memristor properties is presented.", "target": "Beyond Markov Chains, Towards Adaptive Memristor Network-based Music Generation"}
{"id": "task1540-7526ccc63b684d6bba542f800bc95b70", "input": "We formulate and study a fundamental search and detection problem, Schedule Optimization, motivated by a variety of real-world applications, ranging from monitoring content changes on the web, social networks, and user activities to detecting failure on large systems with many individual machines. We consider a large system consists of many nodes, where each node has its own rate of generating new events, or items. A monitoring application can probe a small number of nodes at each step, and our goal is to compute a probing schedule that minimizes the expected number of undiscovered items at the system, or equivalently, minimizes the expected time to discover a new item in the system. We study the Schedule Optimization problem both for deterministic and randomized memoryless algorithms. We provide lower bounds on the cost of an optimal schedule and construct close to optimal schedules with rigorous mathematical guarantees. Finally, we present an adaptive algorithm that starts with no prior information on the system and converges to the optimal memoryless algorithms by adapting to observed data.", "target": "Optimizing Static and Adaptive Probing Schedules for Rapid Event Detection"}
{"id": "task1540-2d7ffea1f1a347e28f6bebb200500934", "input": "Unsupervised classification algorithm based on clonal selection principle named Unsupervised Clonal Selection Classification (UCSC) is proposed in this paper. The new proposed algorithm is data driven and self-adaptive, it adjusts its parameters to the data to make the classification operation as fast as possible. The performance of UCSC is evaluated by comparing it with the well known K-means algorithm using several artificial and real-life data sets. The experiments show that the proposed UCSC algorithm is more reliable and has high classification precision comparing to traditional classification methods such as K-means. General Terms Pattern Recognition, Algorithms.", "target": "Unsupervised Classification Using Immune Algorithm"}
{"id": "task1540-5e77563c235343f7a5c1e0fcb3dff5a6", "input": "The paper introduces a generalization for known probabilistic models such as log-linear and graphical models, called here multiplicative models. These models, that express probabilities via product of parameters are shown to capture multiple forms of contextual independence between variables, including decision graphs and noisy-OR functions. An inference algorithm for multiplicative models is provided and its correctness is proved. The complexity analysis of the inference algorithm uses a more refined parameter than the tree-width of the underlying graph, and shows the computational cost does not exceed that of the variable elimination algorithm in graphical models. The paper ends with examples where using the new models and algorithm is computationally beneficial.", "target": "Inference for Multiplicative Models"}
{"id": "task1540-f49e94297b2e480697ea6b2a22be6f83", "input": "Multi Expression Programming (MEP) is an evolutionary technique that may be used for solving computationally difficult problems. MEP uses a linear solution representation. Each MEP individual is a string encoding complex expressions (computer programs). A MEP individual may encode multiple solutions of the current problem. In this paper MEP is used for evolving a Traveling Salesman Problem (TSP) heuristic for graphs satisfying triangle inequality. Evolved MEP heuristic is compared with Nearest Neighbor Heuristic (NN) and Minimum Spanning Tree Heuristic (MST) on some difficult problems in TSPLIB. For most of the considered problems the evolved MEP heuristic outperforms NN and MST. The obtained algorithm was tested against some problems in TSPLIB. The results emphasizes that evolved MEP heuristic is a powerful tool for solving difficult TSP instances.", "target": "Evolving TSP heuristics using Multi Expression Programming"}
{"id": "task1540-feb14178636c4a138dc8e3df349d806b", "input": "An evaluation of distributed word representation is generally conducted using a word similarity task and/or a word analogy task. There are many datasets readily available for these tasks in English. However, evaluating distributed representation in languages that do not have such resources (e.g., Japanese) is difficult. Therefore, as a first step toward evaluating distributed representations in Japanese, we constructed a Japanese word similarity dataset. To the best of our knowledge, our dataset is the first resource that can be used to evaluate distributed representations in Japanese. Moreover, our dataset contains various parts of speech and includes rare words in addition to common words.", "target": "Construction of a Japanese Word Similarity Dataset"}
{"id": "task1540-3c860ab81ba74ec994cd316fb29e6406", "input": "We present a language complexity analysis of World of Warcraft (WoW) community texts, which we compare to texts from a general corpus of web English. Results from several complexity types are presented, including lexical diversity, density, readability and syntactic complexity. The language of WoW texts is found to be comparable to the general corpus on some complexity measures, yet more specialized on other measures. Our findings can be used by educators willing to include game-related activities into school curricula.", "target": "An investigation into language complexity of World-of-Warcraft game-external texts"}
{"id": "task1540-3bca5f440cc34379b6afe2f8b8178b36", "input": "In this short note, we present an extension of long short-term memory (LSTM) neural networks to using a depth gate to connect memory cells of adjacent layers. Doing so introduces a linear dependence between lower and upper layer recurrent units. Importantly, the linear dependence is gated through a gating function, which we call depth gate. This gate is a function of the lower layer memory cell, the input to and the past memory cell of this layer. We conducted experiments and verified that this new architecture of LSTMs was able to improve machine translation and language modeling performances.", "target": "Depth-Gated LSTM"}
{"id": "task1540-1324365ad59740febe38788c84517582", "input": "In this work we extend to the interval-valued setting the notion of an overlap functions and we discuss a method which makes use of interval-valued overlap functions for constructing OWA operators with interval-valued weights. . Some properties of intervalvalued overlap functions and the derived interval-valued OWA operators are analysed. We specially focus on the homogeneity and migrativity properties.", "target": "Generalized Interval-valued OWA Operators with Interval Weights Derived from Interval-valued Overlap Functions"}
{"id": "task1540-154f1c834e0e409cb68832b40f98ec25", "input": "We present a novel approach to constraintbased causal discovery, that takes the form of straightforward logical inference, applied to a list of simple, logical statements about causal relations that are derived directly from observed (in)dependencies. It is both sound and complete, in the sense that all invariant features of the corresponding partial ancestral graph (PAG) are identified, even in the presence of latent variables and selection bias. The approach shows that every identifiable causal relation corresponds to one of just two fundamental forms. More importantly, as the basic building blocks of the method do not rely on the detailed (graphical) structure of the corresponding PAG, it opens up a range of new opportunities, including more robust inference, detailed accountability, and application to large models.", "target": "A Logical Characterization of Constraint-Based Causal Discovery"}
{"id": "task1540-5bac14cac5694e828b500690e188e6d8", "input": "Recommendation systems are emerging as an important business application with significant economic impact. Currently popular systems include Amazon\u2019s book recommendations, Netflix\u2019s movie recommendations, and Pandora\u2019s music recommendations. In this paper we address the problem of estimating probabilities associated with recommendation system data using non-parametric kernel smoothing. In our estimation we interpret missing items as randomly censored observations and obtain efficient computation schemes using combinatorial properties of generating functions. We demonstrate our approach with several case studies involving real world movie recommendation data. The results are comparable with state-of-the-art techniques while also providing probabilistic preference estimates outside the scope of traditional recommender systems.", "target": "Estimating Probabilities in Recommendation Systems"}
{"id": "task1540-944d7b8ba5df4d5b9e75ac0d31e972cb", "input": "This paper deals with the problem of esti\u00ad mating the probability that one event was a cause of another in a given scenario. Us\u00ad ing structural-semantical definitions of the probabilities of necessary or sufficient cau\u00ad sation (or both), we show how to optimally bound these quantities from data obtained in experimental and observational studies, making minimal assumptions concerning the data-generating process. In particular, we strengthen the results of Pearl (1999) by weakening the data-generation assumptions and deriving theoretically sharp bounds on the probabilities of causation. These results delineate precisely how empirical data can be used both in settling questions of attribution and in solving attribution-related problems of decision making.", "target": "Probabilities of Causation: Bounds and Identification"}
{"id": "task1540-44c061fa20e04455b6bfdf6dc3e67015", "input": "Network models play an important role in studying complex systems in many scientific disciplines. Graph signal processing is receiving growing interest as to design novel tools to combine the analysis of topology and signals. The graph Fourier transform, defined as the eigendecomposition of the graph Laplacian, allows extending conventional signal-processing operations to graphs. One main feature is to let emerge global organization from local interactions; i.e., the Fiedler vector has the smallest non-zero eigenvalue and is key for Laplacian embedding and graph clustering. Here, we introduce the design of Slepian graph signals, by maximizing energy concentration in a predefined subgraph for a given spectral bandlimit. We also establish a link with classical Laplacian embedding and graph clustering, for which the graph Slepian design can serve as a generalization.", "target": "When Slepian Meets Fiedler: Putting a Focus on the Graph Spectrum"}
{"id": "task1540-234979f81bda430faa7d5d05f315cdb2", "input": "We explore techniques to maximize the effectiveness of discourse information in the task of authorship attribution. We present a novel method to embed discourse features in a Convolutional Neural Network text classifier, which achieves a state-ofthe-art result by a substantial margin. We empirically investigate several featurization methods to understand the conditions under which discourse features contribute non-trivial performance gains, and analyze discourse embeddings.", "target": "Leveraging Discourse Information Effectively for Authorship Attribution\u2217"}
{"id": "task1540-fbe8e9ff297c4d3c81d1ba1befe03aeb", "input": "Reactive (memoryless) policies are sufficient in completely observable Markov decision pro\u00ad cesses (MDPs), but some kind of memory is usually necessary for optimal control of a par\u00ad tially observable MDP. Policies with finite mem\u00ad ory can be represented as finite-state automata. In this paper, we extend Baird and Moore's YAPS algorithm to the problem of learning gen\u00ad eral finite-state automata. Because it performs stochastic gradient descent, this algorithm can be shown to converge to a locally optimal finite\u00ad state controller. We provide the details of the algorithm and then consider the question of un\u00ad der what conditions stochastic gradient descent will outperform exact gradient descent. We con\u00ad clude with empirical results comparing the per\u00ad formance of stochastic and exact gradient de\u00ad scent, and showing the ability of our algorithm to extract the useful information contained in the sequence of past observations to compensate for the lack of observability at each time-step.", "target": "Learning Finite-State Controllers for Partially Observable Environments"}
{"id": "task1540-8506f887f27f435b816703c7697f1745", "input": "Lossy image compression algorithms are pervasively used to reduce the size of images transmitted over the web and recorded on data storage media. However, we pay for their high compression rate with visual artifacts degrading the user experience. Deep convolutional neural networks have become a widespread tool to address high-level computer vision tasks very successfully. Recently, they have found their way into the areas of low-level computer vision and image processing to solve regression problems mostly with relatively shallow networks. We present a novel 12-layer deep convolutional network for image compression artifact suppression with hierarchical skip connections and a multi-scale loss function. We achieve a boost of up to 1.79 dB in PSNR over ordinary JPEG and an improvement of up to 0.36 dB over the best previous ConvNet result. We show that a network trained for a specific quality factor (QF) is resilient to the QF used to compress the input image\u2014a single network trained for QF 60 provides a PSNR gain of more than 1.5 dB over the wide QF range from 40 to 76.", "target": "CAS-CNN: A Deep Convolutional Neural Network for Image Compression Artifact Suppression"}
{"id": "task1540-4f6d6a7fd01a4e2780a50ecda4acc256", "input": "Bound Founded Answer Set Programming (BFASP) is an extension of Answer Set Programming (ASP) that extends stable model semantics to numeric variables. While the theory of BFASP is defined on ground rules, in practice BFASP programs are written as complex non-ground expressions. Flattening of BFASP is a technique used to simplify arbitrary expressions of the language to a small and well defined set of primitive expressions. In this paper, we first show how we can flatten arbitrary BFASP rule expressions, to give equivalent BFASP programs. Next, we extend the bottom-up grounding technique and magic set transformation used by ASP to BFASP programs. Our implementation shows that for BFASP problems, these techniques can significantly reduce the ground program size, and improve subsequent solving.", "target": "Grounding Bound Founded Answer Set Programs"}
{"id": "task1540-b752ee3f279e41c8948b0d7f22228317", "input": "A method is presented for the rhythmic pars\u00ad<lb>ing problem: Given a sequence of observed<lb>musical note onset times, we simultaneously<lb>estimate the corresponding notated rhythm<lb>and tempo process. A graphical model is<lb>developed that<lb>represents the evolution of<lb>tempo and rhythm and relates these hid\u00ad<lb>den quantities to an observable performance.<lb>The rhythm variables are discrete and the<lb>tempo and observation variables are contin\u00ad<lb>uous. We show how to compute the<lb>glob\u00ad<lb>ally most likely configuration of the tempo<lb>and rhythm variables given an observation of<lb>note onset times. Preliminary experiments<lb>are presented on a small data set. A gen\u00ad<lb>eralization to computing MAP estimates for<lb>arbitrary conditional Gaussian distributions<lb>is outlined.", "target": "A Mixed Graphical Model for Rhythmic Parsing"}
{"id": "task1540-11f868a2b42b482e94649be93020478b", "input": "Collaborative data consist of ratings relating two distinct sets of objects: users and items. Much of the work with such data focuses on filtering: predicting unknown ratings for pairs of users and items. In this paper we focus on the problem of visualizing the information. Given all of the ratings, our task is to embed all of the users and items as points in the same Euclidean space. We would like to place users near items that they have rated (or would rate) high, and far away from those they would give low ratings. We pose this problem as a real-valued non-linear Bayesian network and employ Markov chain Monte Carlo and expectation maximization to find an embedding. We present a metric by which to judge the quality of a visualization and compare our results to Eigentaste, locally linear embedding and cooccurrence data embedding on three real-world datasets.", "target": "Visualization of Collaborative Data"}
{"id": "task1540-2c6bf4b5d6e14c29ac8ad8482ae4bf82", "input": "Bio-inspired optimization algorithms have been gaining more popularity recently. One of the most important of these algorithms is particle swarm optimization (PSO). PSO is based on the collective intelligence of a swam of particles. Each particle explores a part of the search space looking for the optimal position and adjusts its position according to two factors; the first is its own experience and the second is the collective experience of the whole swarm. PSO has been successfully used to solve many optimization problems. In this work we use PSO to improve the performance of a well-known representation method of time series data which is the symbolic aggregate approximation (SAX). As with other time series representation methods, SAX results in loss of information when applied to represent time series. In this paper we use PSO to propose a new minimum distance WMD for SAX to remedy this problem. Unlike the original minimum distance, the new distance sets different weights to different segments of the time series according to their information content. This weighted minimum distance enhances the performance of SAX as we show through experiments using different time series datasets.", "target": "Particle Swarm Optimization of Information-Content Weighting of Symbolic Aggregate Approximation"}
{"id": "task1540-f8e662b9582b4be3baf9a20b3b869b12", "input": "The approach described here allows to use the fuzzy Object Based Representation of imprecise and uncertain knowledge. This representation has a great practical interest due to the possibility to realize reasoning on classification with a fuzzy semantic network based system. For instance, the distinction between necessary, possible and user classes allows to take into account exceptions that may appear on fuzzy knowledge-base and facilitates integration of user's Objects in the base. This approach describes the theoretical aspects of the architecture of the whole experimental A.I. system we built in order to provide effective on-line assistance to users of new technological systems: the understanding of \"how it works\" and \"how to complete tasks\" from queries in quite natural languages. In our model, procedural semantic networks are used to describe the knowledge of an \"ideal\" expert while fuzzy sets are used both to describe the approximative and uncertain knowledge of novice users in fuzzy semantic networks which intervene to match fuzzy labels of a query with categories from our \"ideal\" expert.", "target": "Uncertain and Approximative Knowledge Representation to Reasoning on Classification with a Fuzzy Networks Based System"}
{"id": "task1540-bcb4f937d8e3413fa09d58944ff30f5b", "input": "In this paper we introduce a class of Markov decision processes that arise as a natural model for many renewable resource allocation problems. Upon extending results from the inventory control literature, we prove that they admit a closed form solution and we show how to exploit this structure to speed up its computation. We consider the application of the proposed framework to several problems arising in very different domains, and as part of the ongoing effort in the emerging field of Computational Sustainability we discuss in detail its application to the Northern Pacific Halibut marine fishery. Our approach is applied to a model based on real world data, obtaining a policy with a guaranteed lower bound on the utility function that is structurally very different from the one currently employed.", "target": "Playing games against nature: optimal policies for renewable resource allocation"}
{"id": "task1540-0077571cd27348fea97ed5ea26743058", "input": "Recurrent Neural Networks (RNNs), and specifically a variant with Long ShortTerm Memory (LSTM), are enjoying renewed interest as a result of successful applications in a wide range of machine learning problems that involve sequential data. However, while LSTMs provide exceptional results in practice, the source of their performance and their limitations remain rather poorly understood. Using character-level language models as an interpretable testbed, we aim to bridge this gap by providing a comprehensive analysis of their representations, predictions and error types. In particular, our experiments reveal the existence of interpretable cells that keep track of long-range dependencies such as line lengths, quotes and brackets. Moreover, an extensive analysis with finite horizon n-gram models suggest that these dependencies are actively discovered and utilized by the networks. Finally, we provide detailed error analysis that suggests areas for further study.", "target": "Visualizing and Understanding Recurrent Networks"}
{"id": "task1540-c795ff55476e44329d45befef535c5f3", "input": "We describe a neural network model that jointly learns distributed representations of texts and knowledge base (KB) entities. Given a text in the KB, we train our proposed model to predict entities that are relevant to the text. Our model is designed to be generic with the ability to address various NLP tasks with ease. We train the model using a large corpus of texts and their entity annotations extracted from Wikipedia. We evaluated the model on three important NLP tasks (i.e., sentence textual similarity, entity linking, and factoid question answering) involving both unsupervised and supervised settings. As a result, we achieved state-of-the-art results on all three of these tasks.", "target": "Learning Distributed Representations of Texts and Entities from Knowledge Base"}
{"id": "task1540-693e482e4120406ba17df2b50114926b", "input": "We present a system that produces sentential descriptions of video: who did what to whom, and where and how they did it. Action class is rendered as a verb, participant objects as noun phrases, properties of those objects as adjectival modifiers in those noun phrases, spatial relations between those participants as prepositional phrases, and characteristics of the event as prepositional-phrase adjuncts and adverbial modifiers. Extracting the information needed to render these linguistic entities requires an approach to event recognition that recovers object tracks, the track-to-role assignments, and changing body posture.", "target": "Video In Sentences Out"}
{"id": "task1540-36ac0c7858b44bd7888304a151701df4", "input": "Music emotion recognition (MER) is usually regarded as a multi-label tagging task, and each segment of music can inspire specific emotion tags. Most researchers extract acoustic features from music and explore the relations between these features and their corresponding emotion tags. Considering the inconsistency of emotions inspired by the same music segment for human beings, seeking for the key acoustic features that really affect on emotions is really a challenging task. In this paper, we propose a novel MER method by using deep convolutional neural network (CNN) on the music spectrograms that contains both the original time and frequency domain information. By the proposed method, no additional effort on extracting specific features required, which is left to the training procedure of the CNN model. Experiments are conducted on the standard CAL500 and CAL500exp dataset. Results show that, for both datasets, the proposed method outperforms state-of-the-art methods.", "target": "CNN BASED MUSIC EMOTION CLASSIFICATION"}
{"id": "task1540-4dba947acef94c33961e9c31886aaca5", "input": "LTL synthesis \u2013 the construction of a function to satisfy a logical specification formulated in Linear Temporal Logic \u2013 is a 2EXPTIME-complete problem with relevant applications in controller synthesis and a myriad of artificial intelligence applications. In this research note we consider De Giacomo and Vardi\u2019s variant of the synthesis problem for LTL formulas interpreted over finite rather than infinite traces. Rather surprisingly, given the existing claims on complexity, we establish that LTL synthesis is EXPTIME-complete for the finite interpretation, and not 2EXPTIME-complete as previously reported. Our result coincides nicely with the planning perspective where non-deterministic planning with full observability is EXPTIME-complete and partial observability increases the complexity to 2EXPTIME-complete; a recent related result for LTL synthesis shows that in the finite case with partial observability, the problem is 2EXPTIME-complete.", "target": "Finite LTL Synthesis is EXPTIME-complete"}
{"id": "task1540-2893a8b685dc402b97cea2fb761108ca", "input": "Data representation is an important pre-processing step in many machine learning algorithms. There are a number of methods used for this task such as Deep Belief Networks (DBNs) and Discrete Fourier Transforms (DFTs). Since some of the features extracted using automated feature extraction methods may not always be related to a specific machine learning task, in this paper we propose two methods in order to make a distinction between extracted features based on their relevancy to the task. We applied these two methods to a Deep Belief Network trained for a face recognition task.", "target": "Distinction between features extracted using Deep Belief Networks"}
{"id": "task1540-55fc203025d04a21ade3db9cdf7f05c6", "input": "A methodology for the development of a fuzzy expert system (FES) with application to earthquake prediction is presented. The idea is to reproduce the performance of a human expert in earthquake prediction. To do this, at the first step, rules provided by the human expert are used to generate a fuzzy rule base. These rules are then fed into an inference engine to produce a fuzzy inference system (FIS) and to infer the results. In this paper, we have used a Sugeno type fuzzy inference system to build the FES. At the next step, the adaptive network-based fuzzy inference system (ANFIS) is used to refine the FES parameters and improve its performance. The proposed framework is then employed to attain the performance of a human expert used to predict earthquakes in the Zagros area based on the idea of coupled earthquakes. While the prediction results are promising in parts of the testing set, the general performance indicates that prediction methodology based on coupled earthquakes needs more investigation and more complicated reasoning procedure to yield satisfactory predictions.", "target": "A Fuzzy Expert System for Earthquake Prediction, Case Study: The Zagros Range"}
{"id": "task1540-94ec42701a804e0385714ed126e89234", "input": "Quality assurance remains a key topic in human computation research. Prior work indicates that majority voting is effective for low difficulty tasks, but has limitations for harder tasks. This paper explores two methods of addressing this problem: tournament selection and elimination selection, which exploit 2-, 3and 4-way comparisons between different answers to human computation tasks. Our experimental results and statistical analyses show that both methods produce the correct answer in noisy human computation environment more often than majority voting. Furthermore, we find that the use of 4-way comparisons can significantly reduce the cost of quality assurance relative to the use of 2-way comparisons.", "target": "WHEN MAJORITY VOTING FAILS: COMPARINGQUALITY ASSURANCE METHODS FOR NOISY HUMAN COMPUTATION ENVIRONMENT"}
{"id": "task1540-db13e18993ee430db6a1aee36998c1ce", "input": "We present a probabilistic language model for time-stamped text data which tracks the semantic evolution of individual words over time. The model represents words and contexts by latent trajectories in an embedding space. At each moment in time, the embedding vectors are inferred from a probabilistic version of word2vec (Mikolov et al., 2013b). These embedding vectors are connected in time through a latent diffusion process. We describe two scalable variational inference algorithms\u2014skipgram smoothing and skip-gram filtering\u2014that allow us to train the model jointly over all times; thus learning on all data while simultaneously allowing word and context vectors to drift. Experimental results on three different corpora demonstrate that our dynamic model infers word embedding trajectories that are more interpretable and lead to higher predictive likelihoods than competing methods that are based on static models trained separately on time slices.", "target": "Dynamic Word Embeddings via Skip-Gram Filtering"}
{"id": "task1540-a1e25f13e5694cf9aee847f285d8a3a5", "input": "Despite the remarkable progress recently made in distant speech recognition, state-of-the-art technology still suffers from a lack of robustness, especially when adverse acoustic conditions characterized by non-stationary noises and reverberation are met. A prominent limitation of current systems lies in the lack of matching and communication between the various technologies involved in the distant speech recognition process. The speech enhancement and speech recognition modules are, for instance, often trained independently. Moreover, the speech enhancement normally helps the speech recognizer, but the output of the latter is not commonly used, in turn, to improve the speech enhancement. To address both concerns, we propose a novel architecture based on a network of deep neural networks, where all the components are jointly trained and better cooperate with each other thanks to a full communication scheme between them. Experiments, conducted using different datasets, tasks and acoustic conditions, revealed that the proposed framework can overtake other competitive solutions, including recent joint training approaches.", "target": "A NETWORK OF DEEP NEURAL NETWORKS FOR DISTANT SPEECH RECOGNITION"}
{"id": "task1540-b80f46a5e1d8488da7c2d5fd60a11466", "input": "The problem of scheduling under resource constraints is widely applicable. One prominent example is power management, in which we have a limited continuous supply of power but must schedule a number of power-consuming tasks. Such problems feature tightly coupled continuous resource constraints and continuous temporal constraints. We address such problems by introducing the Time Resource Network (TRN), an encoding for resource-constrained scheduling problems. The definition allows temporal specifications using a general family of representations derived from the Simple Temporal network, including the Simple Temporal Network with Uncertainty, and the probabilistic Simple Temporal Network (Fang et al. (2014)). We propose two algorithms for determining the consistency of a TRN: one based on Mixed Integer Programing and the other one based on Constraint Programming, which we evaluate on scheduling problems with Simple Temporal Constraints and Probabilistic Temporal Constraints.", "target": "Time Resource Networks"}
{"id": "task1540-b896ca22f8814cf892af3fed55f61f1b", "input": "An efficient speech to text converter for mobile application is presented in this work. The prime motive is to formulate a system which would give optimum performance in terms of complexity, accuracy, delay and memory requirements for mobile environment. The speech to text converter consists of two stages namely front-end analysis and pattern recognition. The front end analysis involves preprocessing and feature extraction. The traditional voice activity detection algorithms which track only energy cannot successfully identify potential speech from input because the unwanted part of the speech also has some energy and appears to be speech. In the proposed system , VAD that calculates energy of high frequency part separately as zero crossing rate to differentiate noise from speech is used. Mel Frequency Cepstral Coefficient (MFCC) is used as feature extraction method and Generalized Regression Neural Network is used as recognizer. MFCC provides low word error rate and better feature extraction. Neural Network improves the accuracy. Thus a small database containing all possible syllable pronunciation of the user is sufficient to give recognition accuracy closer to 100%. Thus the proposed technique entertains realization of real time speaker independent applications like mobile phones, PDAs etc.", "target": "Speaker Independent Continuous Speech to Text Converter for Mobile Application"}
{"id": "task1540-5fec2d676c1d477a828e445affe886c3", "input": "In this paper, we analyze the spectrum occupancy using different machine learning techniques. Both supervised techniques (naive Bayesian classifier (NBC), decision trees (DT), support vector machine (SVM), linear regression (LR)) and unsupervised algorithm (hidden markov model (HMM)) are studied to find the best technique with the highest classification accuracy (CA). A detailed comparison of the supervised and unsupervised algorithms in terms of the computational time and classification accuracy is performed. The classified occupancy status is further utilized to evaluate the probability of secondary user outage for the future time slots, which can be used by system designers to define spectrum allocation and spectrum sharing policies. Numerical results show that SVM is the best algorithm among all the supervised and unsupervised classifiers. Based on this, we proposed a new SVM algorithm by combining it with fire fly algorithm (FFA), which is shown to outperform all other algorithms. Index Terms Fire fly algorithm, hidden markov model, spectrum occupancy and support vector machine. March 25, 2015 DRAFT", "target": "Analysis of Spectrum Occupancy Using Machine Learning Algorithms"}
{"id": "task1540-bbed1f3b64e64d6bbb4c0288dfa86a4f", "input": "The manuscript presents an experiment at implementation of a Machine Translation system in a MapReduce model. The empirical evaluation was done using fully implemented translation systems embedded into the MapReduce programming model. Two machine translation paradigms were studied: shallow transfer Rule Based Machine Translation and Statistical Machine Translation. The results show that the MapReduce model can be successfully used to increase the throughput of a machine translation system. Furthermore this method enhances the throughput of a machine translation system without decreasing the quality of the translation output. Thus, the present manuscript also represents a contribution to the seminal work in natural language processing, specifically Machine Translation. It first points toward the importance of the definition of the metric of throughput of translation system and, second, the applicability of the machine translation task to the MapReduce paradigm.", "target": "Increasing the throughput of machine translation systems using clouds"}
{"id": "task1540-5527d0b69498451c937a0049c9cf6b25", "input": "We study the parsing complexity of Combinatory Categorial Grammar (CCG) in the formalism of Vijay-Shanker and Weir (1994). As our main result, we prove that any parsing algorithm for this formalism will necessarily take exponential time when the size of the grammar, and not only the length of the input sentence, is included in the analysis. This result sets the formalism of Vijay-Shanker and Weir (1994) apart from weakly equivalent formalisms such as Tree-Adjoining Grammar (TAG), for which parsing can be performed in time polynomial in the combined size of grammar and input sentence. Our proof highlights important differences between the formalism of Vijay-Shanker and Weir (1994) and contemporary incarnations of CCG.", "target": "On the Complexity of CCG Parsing"}
{"id": "task1540-7a22a412c458420e9782e219936dc271", "input": "Gene and protein networks are very important to model complex large-scale systems in molecular biology. Inferring or reverseengineering such networks can be defined as the process of identifying gene/protein interactions from experimental data through computational analysis. However, this task is typically complicated by the enormously large scale of the unknowns in a rather small sample size. Furthermore, when the goal is to study causal relationships within the network, tools capable of overcoming the limitations of correlation networks are required. In this work, we make use of Bayesian Graphical Models to attach this problem and, specifically, we perform a comparative study of different state-of-the-art heuristics, analyzing their performance in inferring the structure of the Bayesian Network from breast cancer data.", "target": "Combining Bayesian Approaches and Evolutionary Techniques for the Inference of Breast Cancer Networks"}
{"id": "task1540-d094269fdfa94e459cc41488636c17ff", "input": "Keyphrase boundary classification (KBC) is the task of detecting keyphrases in scientific articles and labelling them with respect to predefined types. Although important in practice, this task is so far underexplored, partly due to the lack of labelled data. To overcome this, we explore several auxiliary tasks, including semantic super-sense tagging and identification of multi-word expressions, and cast the task as a multi-task learning problem with deep recurrent neural networks. Our multi-task models perform significantly better than previous state of the art approaches on two scientific KBC datasets, particularly for long keyphrases.", "target": "Multi-Task Learning of Keyphrase Boundary Classification"}
{"id": "task1540-5d4e11dcc9614a77ba793d7d4f970312", "input": "Selecting the right reference class and the right interval when faced with conflicting candidates and no possibility of establish\u00ad ing subset style dominance has been a prob\u00ad lem for Kyburg's Evidential Probability sys\u00ad tem. Various methods have been proposed by Loui and Kyburg to solve this problem in a way that is both intuitively appealing and justifiable within Kyburg's framework. The scheme proposed in this paper leads to stronger statistical assertions without sacri\u00ad ficing too much of the intuitive appeal of Ky\u00ad burg's latest proposal. 1 Overview of the Problem", "target": "A Modification to Evidential Probability"}
{"id": "task1540-35444280be144c45b17c65ca542333fe", "input": "We show that the average stability notion introduced by [12, 4] is invariant to data preconditioning, for a wide class of generalized linear models that includes most of the known exp-concave losses. In other words, when analyzing the stability rate of a given algorithm, we may assume the optimal preconditioning of the data. This implies that, at least from a statistical perspective, explicit regularization is not required in order to compensate for ill-conditioned data, which stands in contrast to a widely common approach that includes a regularization for analyzing the sample complexity of generalized linear models. Several important implications of our findings include: a) We demonstrate that the excess risk of empirical risk minimization (ERM) is controlled by the preconditioned stability rate. This immediately yields a relatively short and elegant proof for the fast rates attained by ERM in our context. b) We strengthen the recent bounds of [9] on the stability rate of the Stochastic Gradient Descent algorithm.", "target": "Average Stability is Invariant to Data Preconditioning. Implications to Exp-concave Empirical Risk Minimization"}
{"id": "task1540-d65188c9af0c49be8666b1d65d4c8ba3", "input": "This paper tackles temporal resolution of documents, such as determining when a document is about or when it was written, based only on its text. We apply techniques from information retrieval that predict dates via language models over a discretized timeline. Unlike most previous works, we rely solely on temporal cues implicit in the text. We consider both document-likelihood and divergence based techniques and several smoothing methods for both of them. Our best model predicts the mid-point of individuals\u2019 lives with a median of 22 and mean error of 36 years for Wikipedia biographies from 3800 B.C. to the present day. We also show that this approach works well when training on such biographies and predicting dates both for nonbiographical Wikipedia pages about specific years (500 B.C. to 2010 A.D.) and for publication dates of short stories (1798 to 2008). Together, our work shows that, even in absence of temporal extraction resources, it is possible to achieve remarkable temporal locality across a diverse set of texts.", "target": "Dating Texts without Explicit Temporal Cues"}
{"id": "task1540-1a0060805da84f88b9bf470a25909c10", "input": "We define a logic of propositional formula schemata adding to the syntax of propositional logic indexed propositions (e.g., pi) and iterated connectives \u2228 or \u2227 ranging over intervals parameterized by arithmetic variables (e.g., \u2227n i=1 pi, where n is a parameter). The satisfiability problem is shown to be undecidable for this new logic, but we introduce a very general class of schemata, called bound-linear, for which this problem becomes decidable. This result is obtained by reduction to a particular class of schemata called regular, for which we provide a sound and complete terminating proof procedure. This schemata calculus (called stab) allows one to capture proof patterns corresponding to a large class of problems specified in propositional logic. We also show that the satisfiability problem becomes again undecidable for slight extensions of this class, thus demonstrating that bound-linear schemata represent a good compromise between expressivity and decidability.", "target": "Decidability and Undecidability Results for Propositional Schemata"}
{"id": "task1540-06f7b0b89e454da1b32f4f5afbf91451", "input": "We tackle the problem of inferring node labels in a partially labeled graph where each node in the graph has multiple label types and each label type has a large number of possible labels. Our primary example, and the focus of this paper, is the joint inference of label types such as hometown, current city, and employers, for users connected by a social network. Standard label propagation fails to consider the properties of the label types and the interactions between them. Our proposed method, called EDGEEXPLAIN, explicitly models these, while still enabling scalable inference under a distributed message-passing architecture. On a billion-node subset of the Facebook social network, EDGEEXPLAIN significantly outperforms label propagation for several label types, with lifts of up to 120% for recall@1 and 60% for recall@3.", "target": "Joint Inference of Multiple Label Types in Large Networks"}
{"id": "task1540-0a0f5a6181a44491bb4779417cacbb86", "input": "Nowadays, supervised learning is commonly used in many domains. Indeed, many works propose to learn new knowledge from examples that translate the expected behaviour of the considered system. A key issue of supervised learning concerns the description language used to represent the examples. In this paper, we propose a method to evaluate the feature set used to describe them. Our method is based on the computation of the consistency of the example base. We carried out a case study in the domain of geomatic in order to evaluate the sets of measures used to characterise geographic objects. The case study shows that our method allows to give relevant evaluations of measure sets. Supervised feature evaluation; consistency computation; geomatic", "target": "Supervised feature evaluation by consistency analysis: application to measure sets used to characterise geographic objects"}
{"id": "task1540-cd2971e64c074fa89a1a34c05e782b25", "input": "Measuring inconsistency is viewed as an important issue related to handling inconsistencies. Good measures are supposed to satisfy a set of rational properties. However, defining sound properties is sometimes problematic. In this paper, we emphasize one such property, named Decomposability, rarely discussed in the literature due to its modeling difficulties. To this end, we propose an independent decomposition which is more intuitive than existing proposals. To analyze inconsistency in a more fine-grained way, we introduce a graph representation of a knowledge base and various MUSdecompositions. One particular MUS-decomposition, named distributable MUS-decomposition leads to an interesting partition of inconsistencies in a knowledge base such that multiple experts can check inconsistencies in parallel, which is impossible under existing measures. Such particular MUSdecomposition results in an inconsistency measure that satisfies a number of desired properties. Moreover, we give an upper bound complexity of the measure that can be computed using 0/1 linear programming or Min Cost Satisfiability problems, and conduct preliminary experiments to show its feasibility.", "target": "On the Measure of the Conflicts: A MUS-Decomposition Based Framework"}
{"id": "task1540-9a90e599089e4d23b6786d01a6d5435d", "input": "This paper introduces a self-organizing traffic signal system for an urban road network. The key elements of this system are agents that control traffic signals at intersections. Each agent uses an interval microscopic traffic model to predict effects of its possible control actions in a short time horizon. The executed control action is selected on the basis of predicted delay intervals. Since the prediction results are represented by intervals, the agents can recognize and suspend those control actions, whose positive effect on the performance of traffic control is uncertain. Evaluation of the proposed traffic control system was performed in a simulation environment. The simulation experiments have shown that the proposed approach results in an improved performance, particularly for non-uniform traffic streams.", "target": "A self-organizing system for urban traffic control based on predictive interval microscopic model"}
{"id": "task1540-24e8ab5d611c47e2812af339d3e11eb8", "input": "Approximations of Laplace-Beltrami operators on manifolds through graph Laplacians have become popular tools in data analysis and machine learning. These discretized operators usually depend on bandwidth parameters whose tuning remains a theoretical and practical problem. In this paper, we address this problem for the unnormalized graph Laplacian by establishing an oracle inequality that opens the door to a well-founded data-driven procedure for the bandwidth selection. Our approach relies on recent results by Lacour and Massart [LM15] on the so-called Lepski\u2019s method.", "target": "Data driven estimation of Laplace-Beltrami operator"}
{"id": "task1540-149e13c4a81e495995e6f09c40f485c0", "input": "Recommendation and collaborative filtering systems are important in modern information and e-commerce applications. As these systems are becoming increasingly popular in the industry, their outputs could affect business decision making, introducing incentives for an adversarial party to compromise the availability or integrity of such systems. We introduce a data poisoning attack on collaborative filtering systems. We demonstrate how a powerful attacker with full knowledge of the learner can generate malicious data so as to maximize his/her malicious objectives, while at the same time mimicking normal user behavior to avoid being detected. While the complete knowledge assumption seems extreme, it enables a robust assessment of the vulnerability of collaborative filtering schemes to highly motivated attacks. We present efficient solutions for two popular factorizationbased collaborative filtering algorithms: the alternative minimization formulation and the nuclear norm minimization method. Finally, we test the effectiveness of our proposed algorithms on real-world data and discuss potential defensive strategies.", "target": "Data Poisoning Attacks on Factorization-Based Collaborative Filtering"}
{"id": "task1540-4208c807d9aa45da953ac6e530d6541a", "input": "Submodular maximization problems belong to the family of combinatorial optimization problems and enjoy wide applications. In this paper, we focus on the problem of maximizing a monotone submodular function subject to a d-knapsack constraint, for which we propose a streaming algorithm that achieves a ( 1 1+2d \u2212 ) -approximation of the optimal value, while it only needs one single pass through the dataset without storing all the data in the memory. In our experiments, we extensively evaluate the effectiveness of our proposed algorithm via two applications: news recommendation and scientific literature recommendation. It is observed that the proposed streaming algorithm achieves both execution speedup and memory saving by several orders of magnitude, compared with existing approaches.", "target": "Streaming Algorithms for News and Scientific Literature Recommendation: Submodular Maximization with a d-Knapsack Constraint"}
{"id": "task1540-4170174e784049568b34e3b5b0852c53", "input": "The similarity between trajectory patterns in clustering has played an important role in discovering movement behaviour of different groups of mobile objects. Several approaches have been proposed to measure the similarity between sequences in trajectory data. Most of these measures are based on Euclidean space or on spatial network and some of them have been concerned with temporal aspect or ordering types. However, they are not appropriate to characteristics of spatiotemporal mobility patterns in wireless networks. In this paper, we propose a new similarity measure for mobility patterns in cellular space of wireless network. The framework for constructing our measure is composed of two phases as follows. First, we present formal definitions to capture mathematically two spatial and temporal similarity measures for mobility patterns. And then, we define the total similarity measure by means of a weighted combination of these similarities. The truth of the partial and total similarity measures are proved in mathematics. Furthermore, instead of the time interval or ordering, our work makes use of the timestamp at which two mobility patterns share the same cell. A case study is also described to give a comparison of the combination measure with other ones.", "target": "A WEIGHTED COMBINATION SIMILARITY MEASURE FOR MOBILITY PATTERNS IN WIRELESS NETWORKS"}
{"id": "task1540-bb298ebd6a6648ce842f403a78db2b8d", "input": "This paper is motivated by the automation of neuropsychological tests involving discourse analysis in the retellings of narratives by patients with potential cognitive impairment. In this scenario the task of sentence boundary detection in speech transcripts is important as discourse analysis involves the application of Natural Language Processing tools, such as taggers and parsers, which depend on the sentence as a processing unit. Our aim in this paper is to verify which embedding induction method works best for the sentence boundary detection task, specifically whether it be those which were proposed to capture semantic, syntactic or morphological similarities.", "target": "Evaluating Word Embeddings for Sentence Boundary Detection in Speech Transcripts"}
{"id": "task1540-d97fb8cb888f4e9fb0e00eb2fce8dfad", "input": "For supervised and unsupervised learning, positive definite kernels allow to use large and potentially infinite dimensional feature spaces with a computational cost that only depends on the number of observations. This is usually done through the penalization of predictor functions by Euclidean or Hilbertian norms. In this paper, we explore penalizing by sparsity-inducing norms such as the l-norm or the block l-norm. We assume that the kernel decomposes into a large sum of individual basis kernels which can be embedded in a directed acyclic graph; we show that it is then possible to perform kernel selection through a hierarchical multiple kernel learning framework, in polynomial time in the number of selected kernels. This framework is naturally applied to non linear variable selection; our extensive simulations on synthetic datasets and datasets from the UCI repository show that efficiently exploring the large feature space through sparsity-inducing norms leads to state-of-the-art predictive performance.", "target": "Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning"}
{"id": "task1540-dc07ef32d16d487a89946918c7cbd474", "input": "Learning to predict multi-label outputs is challenging, but in many problems there is a natural metric on the outputs that can be used to improve predictions. In this paper we develop a loss function for multi-label learning, based on the Wasserstein distance. The Wasserstein distance provides a natural notion of dissimilarity for probability measures. Although optimizing with respect to the exact Wasserstein distance is costly, recent work has described a regularized approximation that is efficiently computed. We describe efficient learning algorithms based on this regularization, extending the Wasserstein loss from probability measures to unnormalized measures. We also describe a statistical learning bound for the loss and show connections with the total variation norm and the Jaccard index. The Wasserstein loss can encourage smoothness of the predictions with respect to a chosen metric on the output space. We demonstrate this property on a real-data tag prediction problem, using the Yahoo Flickr Creative Commons dataset, achieving superior performance over a baseline that doesn\u2019t use the metric.", "target": "Learning with a Wasserstein Loss"}
{"id": "task1540-71432e58cec84f88bb2099cfac654609", "input": "Although the parallel corpus has an irreplaceable role in machine translation, its scale and coverage is still beyond the actual needs. Non-parallel corpus resources on the web have an inestimable potential value in machine translation and other natural language processing tasks. This article proposes a semi-supervised transductive learning method for expanding the training corpus in statistical machine translation system by extracting parallel sentences from the non-parallel corpus. This method only requires a small amount of labeled corpus and a large unlabeled corpus to build a high-performance classifier, especially for when there is short of labeled corpus. The experimental results show that by combining the non-parallel corpus alignment and the semi-supervised transductive learning method, we can more effectively use their respective strengths to improve the performance of machine translation system.", "target": "Machine Translation Model based on "}
{"id": "task1540-0247c3e7efd04fabb4f6b30409da60ed", "input": "It is now a common practice to compare models of human language processing by predicting participant reactions (such as reading times) to corpora consisting of rich naturalistic linguistic materials. However, many of the corpora used in these studies are based on naturalistic text and thus do not contain many of the low-frequency syntactic constructions that are often required to distinguish processing theories. Here we describe a new corpus consisting of English texts edited to contain many low-frequency syntactic constructions while still sounding fluent to native speakers. The corpus is annotated with hand-corrected parse trees and includes self-paced reading time data. Here we give an overview of the content of the corpus and release the data.1", "target": "The Natural Stories Corpus"}
{"id": "task1540-a46db45073f64427b8384d67bfb280bc", "input": "Semantic parsing methods are used for capturing and representing semantic meaning of text. Meaning representation capturing all the concepts in the text may not always be available or may not be sufficiently complete. Ontologies provide a structured and reasoning-capable way to model the content of a collection of texts. In this work, we present a novel approach to joint learning of ontology and semantic parser from text. The method is based on semi-automatic induction of a context-free grammar from semantically annotated text. The grammar parses the text into semantic trees. Both, the grammar and the semantic trees are used to learn the ontology on several levels \u2013 classes, instances, taxonomic and non-taxonomic relations. The approach was evaluated on the first sentences of Wikipedia pages describing people.", "target": "Joint learning of ontology and semantic parser from text"}
{"id": "task1540-6eaf3074ff0746899879c01e100c7613", "input": "Motivated by an application in computational biology, we consider low-rank matrix factorization with {0, 1}-constraints on one of the factors and optionally convex constraints on the second one. In addition to the non-convexity shared with other matrix factorization schemes, our problem is further complicated by a combinatorial constraint set of size 2m\u00b7r, where m is the dimension of the data points and r the rank of the factorization. Despite apparent intractability, we provide \u2212 in the line of recent work on non-negative matrix factorization by Arora et al. (2012)\u2212 an algorithm that provably recovers the underlying factorization in the exact case with O(mr2r +mnr + rn) operations for n datapoints. To obtain this result, we use theory around the Littlewood-Offord lemma from combinatorics.", "target": "Matrix factorization with Binary Components"}
{"id": "task1540-16052384d2134b95acc419bde780758d", "input": "Weighted finite automata and transducers (including hidden Markov models and conditional random fields) are widely used in natural language processing (NLP) to perform tasks such as morphological analysis, part-of-speech tagging, chunking, named entity recognition, speech recognition, and others. Parallelizing finite state algorithms on graphics processing units (GPUs) would benefit many areas of NLP. Although researchers have implemented GPU versions of basic graph algorithms, limited previous work, to our knowledge, has been done on GPU algorithms for weighted finite automata. We introduce a GPU implementation of the Viterbi and forward-backward algorithm, achieving decoding speedups of up to 5.2x over our serial implementation running on different computer architectures and 6093x over OpenFST.", "target": "Decoding with Finite-State Transducers on GPUs"}
{"id": "task1540-275ae3f186594ae18ea81b012822ab77", "input": "Imitation learning has traditionally been applied to learn a single task from demonstrations thereof. The requirement of structured and isolated demonstrations limits the scalability of imitation learning approaches as they are difficult to apply to real-world scenarios, where robots have to be able to execute a multitude of tasks. In this paper, we propose a multi-modal imitation learning framework that is able to segment and imitate skills from unlabelled and unstructured demonstrations by learning skill segmentation and imitation learning jointly. The extensive simulation results indicate that our method can efficiently separate the demonstrations into individual skills and learn to imitate them using a single multi-modal policy. The video of our experiments is available at http://sites.google.com/view/nips17intentiongan.", "target": "Multi-Modal Imitation Learning from Unstructured Demonstrations using Generative Adversarial Nets"}
{"id": "task1540-dfce128ab0964d53a2afe7fb984a611b", "input": "This paper explores the contributions of Answer Set Programming (ASP) to the study of an established theory from the field of Second Language Acquisition: Input Processing. The theory describes default strategies that learners of a second language use in extracting meaning out of a text, based on their knowledge of the second language and their background knowledge about the world. We formalized this theory in ASP, and as a result we were able to determine opportunities for refining its natural language description, as well as directions for future theory development. We applied our model to automating the prediction of how learners of English would interpret sentences containing the passive voice. We present a system, PIas, that uses these predictions to assist language instructors in designing teaching materials. To appear in Theory and Practice of Logic Programming (TPLP).", "target": "An Application of Answer Set Programming to the Field of Second Language Acquisition"}
{"id": "task1540-47869e7845e444679288d7cf6e2df77c", "input": "Americans spend about a third of their time online, with many participating in online conversations on social and political issues. We hypothesize that social media arguments on such issues may be more engaging and persuasive than traditional media summaries, and that particular types of people may be more or less convinced by particular styles of argument, e.g. emotional arguments may resonate with some personalities while factual arguments resonate with others. We report a set of experiments testing at large scale how audience variables interact with argument style to affect the persuasiveness of an argument, an under-researched topic within natural language processing. We show that belief change is affected by personality factors, with conscientious, open and agreeable people being more convinced by emotional arguments.", "target": "Argument Strength is in the Eye of the Beholder: Audience Effects in Persuasion"}
{"id": "task1540-38242a4e67174946b816fac2c9469692", "input": "We show that matrix completion with tracenorm regularization can be significantly hurt when entries of the matrix are sampled nonuniformly. We introduce a weighted version of the trace-norm regularizer that works well also with non-uniform sampling. Our experimental results demonstrate that the weighted trace-norm regularization indeed yields significant gains on the (highly non-uniformly sampled) Netflix dataset.", "target": "Collaborative Filtering in a Non-Uniform World:  Learning with the Weighted Trace Norm"}
{"id": "task1540-5058399492a04853a948fe6260996350", "input": "Traditional information retrieval systems rely on keywords to index documents and queries. In such systems, documents are retrieved based on the number of shared keywords with the query. This lexicalfocused retrieval leads to inaccurate and incomplete results when different keywords are used to describe the documents and queries. Semantic-focused retrieval approaches attempt to overcome this problem by relying on concepts rather than on keywords to indexing and retrieval. The goal is to retrieve documents that are semantically relevant to a given user query. This paper addresses this issue by proposing a solution at the indexing level. More precisely, we propose a novel approach for semantic indexing based on concepts identified from a linguistic resource. In particular, our approach relies on the joint use of WordNet and WordNetDomains lexical databases for concept identification. Furthermore, we propose a semantic-based concept weighting scheme that relies on a novel definition of concept centrality. The resulting system is evaluated on the TIME test collection. Experimental results show the effectiveness of our proposition over traditional IR approaches.", "target": "CONCEPT-BASED INDEXING IN TEXT INFORMATION RETRIEVAL"}
{"id": "task1540-f3168427945d417cb5a828623193bbf3", "input": "We introduce a simple and accurate neural model for dependency-based semantic role labeling. Our model predicts predicate-argument dependencies relying on states of a bidirectional LSTM encoder. The semantic role labeler achieves respectable performance on English even without any kind of syntactic information and only using local inference. However, when automatically predicted part-of-speech tags are provided as input, it substantially outperforms all previous local models and approaches the best reported results on the CoNLL-2009 dataset. Syntactic parsers are unreliable on out-of-domain data, so standard (i.e. syntactically-informed) SRL models are hindered when tested in this setting. Our syntax-agnostic model appears more robust, resulting in the best reported results on the standard out-of-domain test set.1", "target": "A Simple and Accurate Syntax-Agnostic Neural Model for Dependency-based Semantic Role Labeling"}
{"id": "task1540-f405f2c09dbb4c6c9477c432bc7769e5", "input": "We present persona-based models for handling the issue of speaker consistency in neural response generation. A speaker model encodes personas in distributed embeddings that capture individual characteristics such as background information and speaking style. A dyadic speaker-addressee model captures properties of interactions between two interlocutors. Our models yield qualitative performance improvements in both perplexity and BLEU scores over baseline sequence-to-sequence models, with similar gain in speaker consistency as measured by human judges.", "target": "A Persona-Based Neural Conversation Model"}
{"id": "task1540-a7dd754cb4434e98bf51806d8c687174", "input": "The semigraphoid closure of every couple of CI-statements (CI=conditional indepen\u00ad dence) is a stochastic CI-model. As a con\u00ad sequence of this result it is shown that ev\u00ad ery probabilistically sound inference rule for CI-models, having at most two antecedents, is derivable from the semigraphoid inference rules. This justifies the use of semigraphoids as approximations of stochastic CI-models in probabilistic reasoning. The list of all 19 po\u00ad tential dominant elements of the mentioned semigraphoid closure is given as a byproduct.", "target": "Semigraphoids are Two-Antecedental Approximations of Stochastic Conditional Independence Models"}
{"id": "task1540-81f611b37e4d4822878b43aefe73cde0", "input": "Answer Set Programming (ASP) is a powerful modeling formalism for combinatorial problems. However, writing ASP models is not trivial. We propose a novel method, called Sketched Answer Set Programming (SkASP), aiming at supporting the user in resolving this issue. The user writes an ASP program while marking uncertain parts open with question marks. In addition, the user provides a number of positive and negative examples of the desired program behaviour. The sketched model is rewritten into another ASP program, which is solved by traditional methods. As a result, the user obtains a functional and reusable ASP program modelling her problem. We evaluate our approach on 21 well known puzzles and combinatorial problems inspired by Karps 21 NP-complete problems and demonstrate a use-case for a database application based on ASP.", "target": "Sketched Answer Set Programming"}
{"id": "task1540-b0cacdd388584db88b18c003e5a1e28a", "input": "The method of random projections has become very popular for large-scale applications in statistical learning, information retrieval, bio-informatics and other applications. Using a well-designed coding scheme for the projected data, which determines the number of bits needed for each projected value and how to allocate these bits, can significantly improve the effectiveness of the algorithm, in storage cost as well as computational speed. In this paper, we study a number of simple coding schemes, focusing on the task of similarity estimation and on an application to training linear classifiers. We demonstrate that uniform quantization outperforms the standard existing influential method [8]. Indeed, we argue that in many cases coding with just a small number of bits suffices. Furthermore, we also develop a nonuniform 2-bit coding scheme that generally performs well in practice, as confirmed by our experiments on training linear support vector machines (SVM).", "target": "Coding for Random Projections"}
{"id": "task1540-32c3c330f49046b0a3518eb416898cae", "input": "In a composite-domain task-completion dialogue system, a conversation agent often switches among multiple sub-domains before it successfully completes the task. Given such a scenario, a standard deep reinforcement learning based dialogue agent may suffer to find a good policy due to the issues such as: increased state and action spaces, high sample complexity demands, sparse reward and long horizon, etc. In this paper, we propose to use hierarchical deep reinforcement learning approach which can operate at different temporal scales and is intrinsically motivated to attack these problems. Our hierarchical network consists of two levels: the top-level meta-controller for subgoal selection and the low-level controller for dialogue policy learning. Subgoals selected by metacontroller and intrinsic rewards can guide the controller to effectively explore in the state-action space and mitigate the spare reward and long horizon problems. Experiments on both simulations and human evaluation show that our model significantly outperforms flat deep reinforcement learning agents in terms of success rate, rewards and user rating.", "target": "Composite Task-Completion Dialogue System via Hierarchical Deep Reinforcement Learning"}
{"id": "task1540-0bfae2012094417fab086a5602e8a8a4", "input": "We propose a localized approach to multiple kernel learning that can be formulated as a convex optimization problem over a given cluster structure. For which we obtain generalization error guarantees and derive an optimization algorithm based on the Fenchel dual representation. Experiments on real-world datasets from the application domains of computational biology and computer vision show that convex localized multiple kernel learning can achieve higher prediction accuracies than its global and non-convex local counterparts.", "target": "Localized Multiple Kernel Learning\u2014A Convex Approach"}
{"id": "task1540-67a46a437946421b824a79b236d3d043", "input": "Spectral methods have greatly advanced the estimation of latent variable models, generating a sequence of novel and efficient algorithms with strong theoretical guarantees. However, current spectral algorithms are largely restricted to mixtures of discrete or Gaussian distributions. In this paper, we propose a kernel method for learning multi-view latent variable models, allowing each mixture component to be nonparametric. The key idea of the method is to embed the joint distribution of a multi-view latent variable into a reproducing kernel Hilbert space, and then the latent parameters are recovered using a robust tensor power method. We establish that the sample complexity for the proposed method is quadratic in the number of latent components and is a low order polynomial in the other relevant parameters. Thus, our non-parametric tensor approach to learning latent variable models enjoys good sample and computational efficiencies. Moreover, the non-parametric tensor power method compares favorably to EM algorithm and other existing spectral algorithms in our experiments.", "target": "Nonparametric Estimation of Multi-View Latent Variable Models"}
{"id": "task1540-e2f2f6688cd34051b8e0e84c6a25f0a1", "input": "Distributed optimization algorithms for largescale machine learning suffer from a communication bottleneck. Reducing communication makes the efficient aggregation of partial work from different machines more challenging. In this paper we present a novel generalization of the recent communication efficient primal-dual coordinate ascent framework (COCOA). Our framework, COCOA+, allows for additive combination of local updates to the global parameters at each iteration, whereas previous schemes only allowed conservative averaging. We give stronger (primal-dual) convergence rate guarantees for both COCOA as well as our new variants, and generalize the theory for both methods to also cover non-smooth convex loss functions. We provide an extensive experimental comparison on several real-world distributed datasets, showing markedly improved performance, especially when scaling up the number of machines.", "target": "Adding vs. Averaging in Distributed Primal-Dual Optimization "}
{"id": "task1540-1eb071206c0b4a8f8a0f62be907a4a0a", "input": "In many real tasks the features are evolving, with some features being vanished and some other features augmented. For example, in environment monitoring some sensors expired whereas some new ones deployed; in mobile game recommendation some games dropped whereas some new ones added. Learning with such incremental and decremental features is crucial but rarely studied, particularly when the data coming like a stream and thus it is infeasible to keep the whole data for optimization. In this paper, we study this challenging problem and present the OPID approach. Our approach attempts to compress important information of vanished features into functions of survived features, and then expand to include the augmented features. It is the one-pass learning approach, which only needs to scan each instance once and does not need to store the whole data, and thus satisfy the evolving streaming data nature. The effectiveness of our approach is validated theoretically and empirically.", "target": "One-Pass Learning with Incremental and Decremental Features"}
{"id": "task1540-008fd29a66954f549b0e306d0ce0894a", "input": "Path planning is typically considered in Artificial Intelligence as a graph searching problem and R* is state-of-the-art algorithm tailored to solve it. The algorithm decomposes given path finding task into the series of subtasks each of which can be easily (in computational sense) solved by well-known methods (such as A*). Parameterized random choice is used to perform the decomposition and as a result R* performance largely depends on the choice of its input parameters. In our work we formulate a range of assumptions concerning possible upper and lower bounds of R* parameters, their interdependency and their influence on R* performance. Then we evaluate these assumptions by running a large number of experiments. As a result we formulate a set of heuristic rules which can be used to initialize the values of R* parameters in a way that leads to algorithm\u2019s best performance.", "target": "Finetuning Randomized Heuristic Search For 2D Path Planning: Finding The Best Input Parameters For R* Algorithm Through Series Of Experiments"}
{"id": "task1540-73a6ba4bad00459a9389e4026b55d0c5", "input": "Here, I review current state-of-the-arts in many areas of AI to estimate when it\u2019s reasonable to expect human level AI development. Predictions of prominent AI researchers vary broadly from very pessimistic predictions of Andrew Ng to much more moderate predictions of Geoffrey Hinton and optimistic predictions of Shane Legg, DeepMind cofounder. Given huge rate of progress in recent years and this broad range of predictions of AI experts, AI safety questions are also discussed.", "target": "Review of state-of-the-arts in artificial intelligence with application to AI safety problem"}
{"id": "task1540-cb8bec27c17a403db27e0827c4ad8daa", "input": "The Support Vector Machine using Privileged Information (SVM+) has been proposed to train a classifier to utilize the additional privileged information that is only available in the training phase but not available in the test phase. In this work, we propose an efficient solution for SVM+ by simply utilizing the squared hinge loss instead of the hinge loss as in the existing SVM+ formulation, which interestingly leads to a dual form with less variables and in the same form with the dual of the standard SVM. The proposed algorithm is utilized to leverage the additional web knowledge that is only available during training for the image categorization tasks. The extensive experimental results on both Caltech101 and WebQueries datasets show that our proposed method can achieve a factor of up to hundred times speedup with the comparable accuracy when compared with the existing SVM+ method.", "target": "Simple and Efficient Learning using Privileged Information"}
{"id": "task1540-bf85d4645c494a7493b10866ba93ffbd", "input": "We introduce a parametric nonlinear transformation that is well-suited for Gaussianizing data from natural images. After a linear transformation of the data, each component is normalized by a pooled activity measure, computed by exponentiating a weighted sum of rectified and exponentiated components and an additive constant. We optimize the parameters of this transformation (linear transform, exponents, weights, constant) over a database of natural images, directly minimizing the negentropy of the responses. We find that the optimized transformation successfully Gaussianizes the data, achieving a significantly smaller mutual information between transformed components than previous methods including ICA and radial Gaussianization. The transformation is differentiable and can be efficiently inverted, and thus induces a density model on images. We show that samples of this model are visually similar to samples of natural image patches. We also demonstrate the use of the model as a prior density in removing additive noise. Finally, we show that the transformation can be cascaded, with each layer optimized (unsupervised) using the same Gaussianization objective, to capture additional probabilistic structure.", "target": "GENERALIZED NORMALIZATION TRANSFORMATION"}
{"id": "task1540-ce271dbf312d469ba9635466e87a1e30", "input": "This work explores the effects of relevant and irrelevant boolean variables on the accuracy of classifiers. The analysis uses the assumption that the variables are conditionally independent given the class, and focuses on a natural family of learning algorithms for such sources when the relevant variables have a small advantage over random guessing. The main result is that algorithms relying predominately on irrelevant variables have error probabilities that quickly go to 0 in situations where algorithms that limit the use of irrelevant variables have errors bounded below by a positive constant. We also show that accurate learning is possible even when there are so few examples that one cannot determine with high confidence whether or not any individual variable is relevant.", "target": "On the Necessity of Irrelevant Variables"}
{"id": "task1540-a7b61800f9d142629f5ae26531a3ffd8", "input": "In this paper, we claim that Vector Cosine \u2013 which is generally considered one of the most efficient unsupervised measures for identifying word similarity in Vector Space Models \u2013 can be outperformed by a completely unsupervised measure that evaluates the extent of the intersection among the most associated contexts of two target words, weighting such intersection according to the rank of the shared contexts in the dependency ranked lists. This claim comes from the hypothesis that similar words do not simply occur in similar contexts, but they share a larger portion of their most relevant contexts compared to other related words. To prove it, we describe and evaluate APSyn, a variant of Average Precision that \u2013 independently of the adopted parameters \u2013 outperforms the Vector Cosine and the co-occurrence on the ESL and TOEFL test sets. In the best setting, APSyn reaches 0.73 accuracy on the ESL dataset and 0.70 accuracy in the TOEFL dataset, beating therefore the non-English US college applicants (whose average, as reported in the literature, is 64.50%) and several state-of-the-art approaches.", "target": "What a Nerd! Beating Students and Vector Cosine in the ESL and TOEFL Datasets"}
{"id": "task1540-6febae80305648b1ba564e50cf6b00bb", "input": "In the present paper we use principles of fuzzy logic to develop a general model representing several processes in a system\u2019s operation characterized by a degree of vagueness and/or uncertainty. For this, the main stages of the corresponding process are represented as fuzzy subsets of a set of linguistic labels characterizing the system\u2019s performance at each stage. We also introduce three alternative measures of a fuzzy system\u2019s effectiveness connected to our general model. These measures include the system\u2019s total possibilistic uncertainty, the Shannon\u2019s entropy properly modified for use in a fuzzy environment and the \u201ccentroid\u201d method in which the coordinates of the center of mass of the graph of the membership function involved provide an alternative measure of the system\u2019s performance. The advantages and disadvantages of the above measures are discussed and a combined use of them is suggested for achieving a worthy of credit mathematical analysis of the corresponding situation. An application is also developed for the Mathematical Modelling process illustrating the use of our results in practice.", "target": "A Study on Fuzzy Systems"}
{"id": "task1540-7b833986c93a4e248bfb29c73fa40c10", "input": "Text alignment and text quality are critical to the accuracy of Machine Translation (MT) systems, some NLP tools, and any other text processing tasks requiring bilingual data. This research proposes a language independent bi-sentence filtering approach based on Polish (not a positionsensitive language) to English experiments. This cleaning approach was developed on the TED Talks corpus and also initially tested on the Wikipedia comparable corpus, but it can be used for any text domain or language pair. The proposed approach implements various heuristics for sentence comparison. Some of them leverage synonyms and semantic and structural analysis of text as additional information. Minimization of data loss was ensured. An improvement in MT system score with text processed using the tool is discussed.", "target": "Noisy-parallel and comparable corpora filtering methodology for the extraction of bi-lingual equivalent data at sentence level"}
{"id": "task1540-6097be33193146e0a51a00480dd6f6c5", "input": "Verifiability is one of the core editing principles in Wikipedia, editors being encouraged to provide citations for the added content. For a Wikipedia article, determining the citation span of a citation, i.e. what content is covered by a citation, is important as it helps decide for which content citations are still missing. We are the first to address the problem of determining the citation span in Wikipedia articles. We approach this problem by classifying which textual fragments in an article are covered by a citation. We propose a sequence classification approach where for a paragraph and a citation, we determine the citation span at a finegrained level. We provide a thorough experimental evaluation and compare our approach against baselines adopted from the scientific domain, where we show improvement for all evaluation metrics.", "target": "Fine-Grained Citation Span Detection for References in Wikipedia"}
{"id": "task1540-8a991d05a69147f1aae476b74fb21c50", "input": "We study the complexity of functions computable by deep feedforward neural networks with piecewise linear activations in terms of the number of regions of linearity that they have. Deep networks are able to sequentially map portions of each layer\u2019s input space to the same output. In this way, deep models compute functions with a compositional structure that is able to re-use pieces of computation exponentially often in terms of their depth. This note investigates the complexity of such compositional maps and contributes new theoretical results regarding the advantage of depth for neural networks with piece-wise linear activation functions.", "target": "On the Number of Linear Regions of Deep Neural Networks"}
{"id": "task1540-363caffbcb224bbd801891e55bbd58f3", "input": "We propose a novel discriminative model that learns embeddings from multilingual and multi-modal data, meaning that our model can take advantage of images and descriptions in multiple languages to improve embedding quality. To that end, we introduce a modification of a pairwise contrastive estimation optimisation function as our training objective. We evaluate our embeddings on an image\u2013sentence ranking (ISR), a semantic textual similarity (STS), and a neural machine translation (NMT) task. We find that the additional multilingual signals lead to improvements on both the ISR and STS tasks, and the discriminative cost can also be used in re-ranking n-best lists produced by NMT models, yielding strong improvements.", "target": "Multilingual Multi-modal Embeddings for Natural Language Processing"}
{"id": "task1540-cd2b01720d5c4f6da035a155c62501a4", "input": "For many low-resource or endangered languages, spoken language resources are more likely to be annotated with translations than with transcriptions. Recent work exploits such annotations to produce speech-to-translation alignments, without access to any text transcriptions. We investigate whether providing such information can aid in producing better (mismatched) crowdsourced transcriptions, which in turn could be valuable for training speech recognition systems, and show that they can indeed be beneficial through a smallscale case study as a proof-of-concept. We also present a simple phonetically aware string averaging technique that produces transcriptions of higher quality.", "target": "A case study on using speech-to-translation alignments for language documentation"}
{"id": "task1540-31eb0d1439954ed084c14a87b6c2690a", "input": "In this research we address the problem of capturing recurring concepts in a data stream environment. Recurrence capture enables the re-use of previously learned classifiers without the need for re-learning while providing for better accuracy during the concept recurrence interval. We capture concepts by applying the Discrete Fourier Transform (DFT) to Decision Tree classifiers to obtain highly compressed versions of the trees at concept drift points in the stream and store such trees in a repository for future use. Our empirical results on real world and synthetic data exhibiting varying degrees of recurrence show that the Fourier compressed trees are more robust to noise and are able to capture recurring concepts with higher precision than a meta learning approach that chooses to re-use classifiers in their originally occurring form.", "target": "Mining Recurrent Concepts in Data Streams using the Discrete Fourier Transform"}
{"id": "task1540-03def38e85ba45bb8435182ea2061418", "input": "In many embedded systems, such as imaging systems, the system has a single designated purpose, and same threads are executed repeatedly. Profiling thread behavior, allows the system to allocate each thread its resources in a way that improves overall system performance. We study an online resource allocation problem, where a resource manager simultaneously allocates resources (exploration), learns the impact on the different consumers (learning) and improves allocation towards optimal performance (exploitation). We build on the rich framework of multiarmed bandits and present online and offline algorithms. Through extensive experiments with both synthetic data and real-world cache allocation to threads we show the merits and properties of our algorithms.", "target": "Bandits meet Computer Architecture: Designing a Smartly-allocated Cache"}
{"id": "task1540-82a4941be27b4909bfff96ec8c83f9e7", "input": "For computer vision applications, prior works have shown the efficacy of reducing numeric precision of model parameters (network weights) in deep neural networks but also that reducing the precision of activations hurts model accuracy much more than reducing the precision of model parameters. We study schemes to train networks from scratch using reduced-precision activations without hurting the model accuracy. We reduce the precision of activation maps (along with model parameters) using a novel quantization scheme and increase the number of filter maps in a layer, and find that this scheme compensates or surpasses the accuracy of the baseline full-precision network. As a result, one can significantly reduce the dynamic memory footprint, memory bandwidth, computational energy and speed up the training and inference process with appropriate hardware support. We call our scheme WRPN wide reduced-precision networks. We report results using our proposed schemes and show that our results are better than previously reported accuracies on ILSVRC-12 dataset while being computationally less expensive compared to previously reported reduced-precision networks.", "target": "WRPN: Training and Inference using Wide Reduced-Precision Networks"}
{"id": "task1540-b94db145c86c4943a691aea7cf3c8a1c", "input": "We propose an efficient technique for multilabel classification based on calibration, a term we use to mean learning a link function that maps independent predictions to joint predictions. Though a naive implementation of our proposal would require training individual classifiers for each label, we show that for natural datasets and linear classifiers we can sidestep this by leveraging techniques from randomized linear algebra. Moreover, our algorithm applies equally well to multiclass classification. The end result is an algorithm that scales to very large multilabel and multiclass problems, and offers state of the art accuracy on many datasets.", "target": "Multilabel Prediction via Calibration"}
{"id": "task1540-b61ac95b4d044e5c89df6e585018235b", "input": "In many applications of black-box optimization, one can evaluate multiple points simultaneously, e.g. when evaluating the performances of several different neural networks in a parallel computing environment. In this paper, we develop a novel batch Bayesian optimization algorithm \u2014 the parallel knowledge gradient method. By construction, this method provides the one-step Bayes optimal batch of points to sample. We provide an efficient strategy for computing this Bayes-optimal batch of points, and we demonstrate that the parallel knowledge gradient method finds global optima significantly faster than previous batch Bayesian optimization algorithms on both synthetic test functions and when tuning hyperparameters of practical machine learning algorithms, especially when function evaluations are noisy.", "target": "The Parallel Knowledge Gradient Method for Batch Bayesian Optimization"}
{"id": "task1540-fb351cd0d06644ddaefd8d0c6e1be150", "input": "Speaker intent detection and semantic slot filling are two critical tasks in spoken language understanding (SLU) for dialogue systems. In this paper, we describe a recurrent neural network (RNN) model that jointly performs intent detection, slot filling, and language modeling. The neural network model keeps updating the intent prediction as word in the transcribed utterance arrives and uses it as contextual features in the joint model. Evaluation of the language model and online SLU model is made on the ATIS benchmarking data set. On language modeling task, our joint model achieves 11.8% relative reduction on perplexity comparing to the independent training language model. On SLU tasks, our joint model outperforms the independent task training model by 22.3% on intent detection error rate, with slight degradation on slot filling F1 score. The joint model also shows advantageous performance in the realistic ASR settings with noisy speech input.", "target": "Joint Online Spoken Language Understanding and Language Modeling with Recurrent Neural Networks"}
{"id": "task1540-42688863860c4860b93b6e39adaccdc4", "input": "Building Information Modeling (BIM) is a recent construction process based on a 3D model, containing every component related to the building achievement. Architects, structure engineers, method engineers, and others participant to the building process work on this model through the design-to-construction cycle. The high complexity and the large amount of information included in these models raise several issues, delaying its wide adoption in the industrial world. One of the most important is the visualization: professionals have difficulties to find out the relevant information for their job. Actual solutions suffer from two limitations: the BIM models information are processed manually and insignificant information are simply hidden, leading to inconsistencies in the building model. This paper describes a system relying on an ontological representation of the building information to label automatically the building elements. Depending on the user\u2019s department, the visualization is modified according to these labels by automatically adjusting the colors and image properties based on a saliency model. The proposed saliency model incorporates several adaptations to fit the specificities of architectural images.", "target": "Adaptive Visualisation System for Construction Building Information Models Using Saliency"}
{"id": "task1540-c5ba7a132fcf4ff3abc3a3a217e42d07", "input": "A hierarchical clustering method is stable if small perturbations on the data set produce small perturbations in the result. This perturbations<lb>are measured using the Gromov-Hausdorff metric. We study the problem of stability on linkage-based hierarchical clustering methods. We obtain that, under some basic conditions, standard linkage-based methods are semi-stable.<lb>This means that they are stable if the input data is close enough to an ultrametric space. We prove that, apart from exotic examples, introducing any unchaining condition in the algorithm always produces unstable methods.", "target": "GROMOV-HAUSDORFF STABILITY OF LINKAGE-BASED HIERARCHICAL CLUSTERING METHODS"}
{"id": "task1540-dc2fbcbf2b2e4ecbbc80b05e0ecec850", "input": "Network embedding (NE) is playing a critical role in network analysis, due to its ability to represent vertices with efficient low-dimensional embedding vectors. However, existing NE models aim to learn a fixed context-free embedding for each vertex, and neglect the diverse roles when interacting with other vertices. In this paper, we assume that one vertex usually shows different aspects when interacting with different neighbor vertices, and should own different embeddings respectively. Therefore, we present ContextAware Network Embedding (CANE), a novel NE model to address this issue. CANE learns context-aware embeddings for vertices with mutual attention mechanism and is expected to model the semantic relationships between vertices more precisely. In experiments, we compare our model with existing NE models on three real-world datasets. Experimental results shows that CANE achieves significant improvement than state-of-the-art methods on link prediction, and comparable performance on vertex classification.", "target": "CANE: Context-Aware Network Embedding for Relation Modeling"}
{"id": "task1540-55bff21135d840ffa0db7b35de0a5204", "input": "Multiplayer Online Battle Arena (MOBA) is one of the most played game genres nowadays. With the increasing growth of this genre, it becomes necessary to develop effective intelligent agents to play alongside or against human players. In this paper we address the problem of agent development for MOBA games. We implement a two-layered architecture agent that handles both navigation and game mechanics. This architecture relies on the use of Influence Maps, a widely used approach for tactical analysis. Several experiments were performed using League of Legends as a testbed, and show promising results in this highly dynamic real-time context.", "target": "On the Development of Intelligent Agents for MOBA Games"}
{"id": "task1540-a0d5f5f355174d04b28e58a8965060fb", "input": "Logical inference algorithms for conditional independence (CI) statements have important applications from testing consistency during knowledge elicitation to constraintbased structure learning of graphical models. We prove that the implication problem for CI statements is decidable, given that the size of the domains of the random variables is known and fixed. We will present an approximate logical inference algorithm which combines a falsification and a novel validation algorithm. The validation algorithm represents each set of CI statements as a sparse 0-1 matrixA and validates instances of the implication problem by solving specific linear programs with constraint matrix A. We will show experimentally that the algorithm is both effective and efficient in validating and falsifying instances of the probabilistic CI implication problem.", "target": "Logical Inference Algorithms and Matrix Representations for Probabilistic Conditional Independence"}
{"id": "task1540-a4492fb4eb824c2498cafec18d704248", "input": "In many sequential decision-making problems we may want to manage risk by minimizing some measure of variability in costs in addition to minimizing a standard criterion. Conditional value-at-risk (CVaR) is a relatively new risk measure that addresses some of the shortcomings of the well-known variance-related risk measures, and because of its computational efficiencies has gained popularity in finance and operations research. In this paper, we consider the mean-CVaR optimization problem in MDPs. We first derive a formula for computing the gradient of this risk-sensitive objective function. We then devise policy gradient and actor-critic algorithms that each uses a specific method to estimate this gradient and updates the policy parameters in the descent direction. We establish the convergence of our algorithms to locally risk-sensitive optimal policies. Finally, we demonstrate the usefulness of our algorithms in an optimal stopping problem.", "target": "Algorithms for CVaR Optimization in MDPs"}
{"id": "task1540-08e8f6f78a95491893168123394022da", "input": "Model-based Bayesian reinforcement learning has generated significant interest in the AI community as it provides an elegant solution to the optimal exploration-exploitation tradeoff in classical reinforcement learning. Unfortunately, the applicability of this type of approach has been limited to small domains due to the high complexity of reasoning about the joint posterior over model parameters. In this paper, we consider the use of factored representations combined with online planning techniques, to improve scalability of these methods. The main contribution of this paper is a Bayesian framework for learning the structure and parameters of a dynamical system, while also simultaneously planning a (near-)optimal sequence of actions.", "target": "Model-Based Bayesian Reinforcement Learning in Large Structured Domains"}
{"id": "task1540-fcff62c80b2f49e494bb4dcd06283236", "input": "This paper concerns the probabilistic evalu\u00ad ation of the effects of actions in the presence of unmeasured variables. We show that the identification of causal effect between a sin\u00ad gleton variable X and a set of variables Y can be accomplished systematically, in time polynomial in the number of variables in the graph. When the causal effect is identifiable, a closed-form expression can be obtained for the probability that the action will achieve a specified goal, or a set of goals.", "target": "Testing Identifiability of Causal Effects"}
{"id": "task1540-ce1cd846ebea4e8b836fcc18af1b683f", "input": "We evaluate Machine Learning techniques for Green energy (wind, solar and biomass) prediction based on weather forecasts. Weather is constituted by multiple attributes: temperature, cloud cover, wind speed / direction which are discrete random variables. One of our objectives is to predict the weather based on the previous weather data. Additionally we are interested in finding correlation (dependencies in order to reduce the dimensionality of the data set) between these variables, predicting missing data predict deviations in weather forecasts (for job scheduling within the green control center), finding clusters within the data (constituted by closely related variables e.g. PCA that can be used to remove redundant variables), classification, finding (non-linear using SVMs) regression models, training artificial neural networks based on the historical data so that they can be used for prediction in the future.", "target": "Evaluation of Machine Learning Techniques for Green Energy Prediction"}
{"id": "task1540-2192e5d24a3a4afd94f0228193e1b967", "input": "In security-sensitive applications, the success of machine learning depends on a thorough vetting of their resistance to adversarial data. In one pertinent, well-motivated attack scenario, an adversary may attempt to evade a deployed system at test time by carefully manipulating attack samples. In this work, we present a simple but effective gradientbased approach that can be exploited to systematically assess the security of several, widely-used classification algorithms against evasion attacks. Following a recently proposed framework for security evaluation, we simulate attack scenarios that exhibit different risk levels for the classifier by increasing the attacker\u2019s knowledge of the system and her ability to manipulate attack samples. This gives the classifier designer a better picture of the classifier performance under evasion attacks, and allows him to perform a more informed model selection (or parameter setting). We evaluate our approach on the relevant security task of malware detection in PDF files, and show that such systems can be easily evaded. We also sketch some countermeasures suggested by our analysis.", "target": "Evasion attacks against machine learning at test time"}
{"id": "task1540-5bfcecf5f0ee49f2a6f278d9120d1a2b", "input": "Knowledge graph embedding aims to embed entities and relations of knowledge graphs into low-dimensional vector spaces. Translating embedding methods regard relations as the translation from head entities to tail entities, which achieve the state-of-the-art results among knowledge graph embedding methods. However, a major limitation of these methods is the time consuming training process, which may take several days or even weeks for large knowledge graphs, and result in great di\u0081culty in practical applications. In this paper, we propose an e\u0081cient parallel framework for translating embedding methods, called ParTrans-X, which enables the methods to be paralleled without locks by utilizing the distinguished structures of knowledge graphs. Experiments on two datasets with three typical translating embedding methods, i.e., TransE [3], TransH [19], and a more e\u0081cient variant TransEAdaGrad [11] validate that ParTrans-X can speed up the training process by more than an order of magnitude.", "target": "E\u0080icient Parallel Translating Embedding For Knowledge Graphs"}
{"id": "task1540-467cfa3910c64dd8b73f276806360ac7", "input": "While the general analysis of named entities has received substantial research attention, the analysis of relations over named entities has not. In fact, a review of the literature on unstructured as well as structured data revealed a deficiency in research on the abstract conceptualization required to organize relations. We believe that such an abstract conceptualization can benefit various communities and applications such as natural language processing, information extraction, machine learning and ontology engineering. In this paper, we present CEVO (i.e., a Comprehensive EVent Ontology) built on Levin\u2019s conceptual hierarchy of English verbs that categorizes verbs with the shared meaning and syntactic behavior. We present the fundamental concepts and requirements for this ontology. Furthermore, we present three use cases for demonstrating the benefits of this ontology on annotation tasks: 1) annotating relations in plain text, 2) annotating ontological properties and 3) linking textual relations to ontological properties.", "target": "CEVO: Comprehensive EVent Ontology Enhancing Cognitive Annotation"}
{"id": "task1540-d23d6d584776468392920576c0942dca", "input": "With the increasing empirical success of distributional models of compositional semantics, it is timely to consider the types of textual logic that such models are capable of capturing. In this paper, we address shortcomings in the ability of current models to capture logical operations such as negation. As a solution we propose a tripartite formulation for a continuous vector space representation of semantics and subsequently use this representation to develop a formal compositional notion of negation within such models.", "target": "\u201cNot not bad\u201d is not \u201cbad\u201d: A distributional account of negation"}
{"id": "task1540-5b8df3c6670149028e3f1b202fdc8b92", "input": "Using deep neural nets as function approximator for reinforcement learning tasks have recently been shown to be very powerful for solving problems approaching real-world complexity such as [1]. Using these results as a benchmark, we discuss the role that the discount factor may play in the quality of the learning process of a deep Q-network (DQN). When the discount factor progressively increases up to its final value, we empirically show that it is possible to significantly reduce the number of learning steps. When used in conjunction with a varying learning rate, we empirically show that it outperforms original DQN on several experiments. We relate this phenomenon with the instabilities of neural networks when they are used in an approximate Dynamic Programming setting. We also describe the possibility to fall within a local optimum during the learning process, thus connecting our discussion with the exploration/exploitation dilemma.", "target": "How to Discount Deep Reinforcement Learning: Towards New Dynamic Strategies"}
{"id": "task1540-4f4b2eb7477740deb2830dc93a0f95ad", "input": "Automatic image annotation is one of the most challenging problems in machine vision areas. The goal of this task is to predict number of keywords automatically for images captured in real data. Many methods are based on visual features in order to calculate similarities between image samples. But the computation cost of these approaches is very high. These methods require many training samples to be stored in memory. To lessen this burden, a number of techniques have been developed to reduce the number of features in a dataset. Manifold learning is a popular approach to nonlinear dimensionality reduction. In this paper, we investigate Diffusion maps manifold learning method for web image auto-annotation task. Diffusion maps manifold learning method is used to reduce the dimension of some visual features. Extensive experiments and analysis on NUS-WIDE-LITE web image dataset with different visual features show how this manifold learning dimensionality reduction method can be applied effectively to image annotation.", "target": "WEB IMAGE ANNOTATION BY DIFFUSION MAPS MANIFOLD LEARNING ALGORITHM"}
{"id": "task1540-8c74c95d7f2a42c8b1d98aa95c27e163", "input": "In this paper, we develop a framework for information theoretic learning based on infinitely divisible matrices. We formulate an entropy-like functional on positive definite matrices based on Renyi\u2019s axiomatic definition of entropy and examine some key properties of this functional that lead to the concept of infinite divisibility. The proposed formulation avoids the plug in estimation of density and brings along the representation power of reproducing kernel Hilbert spaces. As an application example, we derive a supervised metric learning algorithm using a matrix based analogue to conditional entropy achieving results comparable with the state of the art.", "target": "Information Theoretic Learning with Infinitely Divisible Kernels"}
{"id": "task1540-03af2870434544089a82a1d0c1129a30", "input": "We report on our system for the shared task on discrimination of similar languages (DSL 2016). The system uses only byte representations in a deep residual network (ResNet). The system, named ResIdent, is trained only on the data released with the task (closed training). We obtain 84.88% accuracy on subtask A, 68.80% accuracy on subtask B1, and 69.80% accuracy on subtask B2. A large difference in accuracy on development data can be observed with relatively minor changes in our network\u2019s architecture and hyperparameters. We therefore expect fine-tuning of these parameters to yield higher accuracies.", "target": "Byte-based Language Identification with Deep Convolutional Networks"}
{"id": "task1540-f071b05586d340f085587fe89b7e1246", "input": "We define the concept of an internal symmetry. This is a symmety within a solution of a constraint satisfaction problem. We compare this to solution symmetry, which is a mapping between different solutions of the same problem. We argue that we may be able to exploit both types of symmetry when finding solutions. We illustrate the potential of exploiting internal symmetries on two benchmark domains: Van der Waerden numbers and graceful graphs. By identifying internal symmetries we are able to extend the state of the art in both cases.", "target": "Symmetry within Solutions"}
{"id": "task1540-d8128bbce0aa4c08a08f6b0e8defd00b", "input": "In many real applications that use and analyze networked data, the links in the network graph may be erroneous, or derived from probabilistic techniques. In such cases, the node classification problem can be challenging, since the unreliability of the links may affect the final results of the classification process. If the information about link reliability is not used explicitly, the classification accuracy in the underlying network may be affected adversely. In this paper, we focus on situations that require the analysis of the uncertainty that is present in the graph structure. We study the novel problem of node classification in uncertain graphs, by treating uncertainty as a first-class citizen. We propose two techniques based on a Bayes model and automatic parameter selection, and show that the incorporation of uncertainty in the classification process as a first-class citizen is beneficial. We experimentally evaluate the proposed approach using different real data sets, and study the behavior of the algorithms under different conditions. The results demonstrate the effectiveness and efficiency of our approach.", "target": "Node Classification in Uncertain Graphs"}
{"id": "task1540-352d34d913f54dbaba2ce78a4e8d321f", "input": "To automatically test web applications, crawling-based techniques are usually adopted to mine the behavior models, explore the state spaces or detect the violated invariants of the applications. However, in existing crawlers, rules for identifying the topics of input text fields, such as login ids, passwords, emails, dates and phone numbers, have to be manually configured. Moreover, the rules for one application are very often not suitable for another. In addition, when several rules conflict and match an input text field to more than one topics, it can be difficult to determine which rule suggests a better match. This paper presents a natural-language approach to automatically identify the topics of encountered input fields during crawling by semantically comparing their similarities with the input fields in labeled corpus. In our evaluation with 100 real-world forms, the proposed approach demonstrated comparable performance to the rule-based one. Our experiments also show that the accuracy of the rule-based approach can be improved by up to 19% when integrated with our approach.", "target": "Using Semantic Similarity for Input Topic Identification in Crawling-based Web Application Testing"}
{"id": "task1540-7b15d6208dfd46c79762adcd1217e143", "input": "In this paper a new heuristic optimization algorithm has been introduced based on the performance of the major football leagues within each season in EU countries. The algorithm starts with an initial population including three different groups of teams: the wealthiest (strongest), the regular, the poorest (weakest). Each individual of population constitute a football team while each player is an indication of a player in a post. The optimization can hopefully occurs when the competition among the teams in all the leagues is imitated as the strongest teams usually purchase the best players of the regular teams and in turn, regular teams purchase the best players of the weakest who should always discover young players instead of buying professionals. It has been shown that the algorithm can hopefully converge to an acceptable solution solving various benchmarks.", "target": "Soccer League Optimization: A heuristic Algorithm Inspired by the Football System in European Countries"}
{"id": "task1540-bd5de377b5ef404faf3704e08cb98acc", "input": "The field of Distributed Constraint Optimization has gained momentum in recent years, thanks to its ability to address various applications related to multi-agent cooperation. Nevertheless, solving Distributed Constraint Optimization Problems (DCOPs) optimally is NP-hard. Therefore, in large-scale, complex applications, incomplete DCOP algorithms are necessary. Current incomplete DCOP algorithms suffer of one or more of the following limitations: they (a) find local minima without providing quality guarantees; (b) provide loose quality assessment; or (c) are unable to benefit from the structure of the problem, such as domain-dependent knowledge and hard constraints. Therefore, capitalizing on strategies from the centralized constraint solving community, we propose a Distributed Large Neighborhood Search (D-LNS) framework to solve DCOPs. The proposed framework (with its novel repair phase) provides guarantees on solution quality, refining upper and lower bounds during the iterative process, and can exploit domain-dependent structures. Our experimental results show that D-LNS outperforms other incomplete DCOP algorithms on both structured and unstructured problem instances.", "target": "Solving DCOPs with Distributed Large Neighborhood Search"}
{"id": "task1540-5a3e1a7bce6247dfb2549839e806ddc0", "input": "In this project, a rather complete proof-theoretical formalization of Lambek Calculus (non-associative with arbitrary extensions) has been ported from Coq proof assistent to HOL4 theorem prover, with some improvements and new theorems. Three deduction systems (Syntactic Calculus, Natural Deduction and Sequent Calculus) of Lambek Calculus are defined with many related theorems proved. The equivalance between these systems are formally proved. Finally, a formalization of Sequent Calculus proofs (where Coq has built-in supports) has been designed and implemented in HOL4. Some basic results including the subformula properties of the so-called \u201ccut-free\u201d proofs are formally proved. This work can be considered as the preliminary work towards a language parser based on category grammars which is not multimodal but still has ability to support context-sensitive languages through customized extensions.", "target": "Formalized Lambek Calculus in Higher Order Logic (HOL4)"}
{"id": "task1540-b88fe70e24614df2b775bd4cd7d41e1e", "input": "Events and entities are closely related; entities are often actors or participants in events and events without entities are uncommon. The interpretation of events and entities is highly contextually dependent. Existing work in information extraction typically models events separately from entities, and performs inference at the sentence level, ignoring the rest of the document. In this paper, we propose a novel approach that models the dependencies among variables of events, entities, and their relations, and performs joint inference of these variables across a document. The goal is to enable access to document-level contextual information and facilitate contextaware predictions. We demonstrate that our approach substantially outperforms the stateof-the-art methods for event extraction as well as a strong baseline for entity extraction.", "target": "Joint Extraction of Events and Entities within a Document Context"}
{"id": "task1540-6b8064dc903c41cf95a530d067f20b06", "input": "We propose a method to construct finite-state reactive controllers for systems whose interactions with their adversarial environment are modeled by infinite-duration twoplayer games over (possibly) infinite graphs. The proposed method targets safety games with infinitely many states or with such a large number of states that it would be impractical\u2014 if not impossible\u2014for conventional synthesis techniques that work on the entire state space. We resort to constructing finitestate controllers for such systems through an automata learning approach, utilizing a symbolic representation of the underlying game that is based on finite automata. Throughout the learning process, the learner maintains an approximation of the winning region (represented as a finite automaton) and refines it using different types of counterexamples provided by the teacher until a satisfactory controller can be derived (if one exists). We present a symbolic representation of safety games (inspired by regular model checking), propose implementations of the learner and teacher, and evaluate their performance on examples motivated by robotic motion planning in dynamic environments.", "target": "An Automaton Learning Approach to Solving Safety Games over Infinite Graphs"}
{"id": "task1540-61c58adfd20f46188bf102d410acc148", "input": "The parameters of temporal models, such as dynamic Bayesian networks, may be modelled in a Bayesian context as static or atemporal variables that influence transition probabilities at every time step. Particle filters fail for models that include such variables, while methods that use Gibbs sampling of parameter variables may incur a per-sample cost that grows linearly with the length of the observation sequence. Storvik (2002) devised a method for incremental computation of exact sufficient statistics that, for some cases, reduces the per-sample cost to a constant. In this paper, we demonstrate a connection between Storvik\u2019s filter and a Kalman filter in parameter space and establish more general conditions under which Storvik\u2019s filter works. Drawing on an analogy to the extended Kalman filter, we develop and analyze, both theoretically and experimentally, a Taylor approximation to the parameter posterior that allows Storvik\u2019s method to be applied to a broader class of models. Our experiments on both synthetic examples and real applications show improvement over existing methods.", "target": "The Extended Parameter Filter"}
{"id": "task1540-5f6c898f7d0f4ab3a15970979539b881", "input": "Traditional algorithms for stochastic optimization require projecting the solution at each iteration into a given domain to ensure its feasibility. When facing complex domains, such as positive semi-definite cones, the projection operation can be expensive, leading to a high computational cost per iteration. In this paper, we present a novel algorithm that aims to reduce the number of projections for stochastic optimization. The proposed algorithm combines the strength of several recent developments in stochastic optimization, including mini-batch, extra-gradient, and epoch gradient descent, in order to effectively explore the smoothness and strong convexity. We show, both in expectation and with a high probability, that when the objective function is both smooth and strongly convex, the proposed algorithm achieves the optimal O(1/T ) rate of convergence with only O(log T ) projections. Our empirical study verifies the theoretical result.", "target": "O(logT ) Projections for Stochastic Optimization of Smooth and Strongly Convex Functions"}
{"id": "task1540-b67935c1df5b483da68cf226bac9d656", "input": "This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles.", "target": "Generating Sequences With Recurrent Neural Networks"}
{"id": "task1540-5de714d833344391a074b50ff5bb7b9f", "input": "Structural correspondence learning (SCL) is an effective method for cross-lingual sentiment classification. This approach uses unlabeled documents along with a word translation oracle to automatically induce task specific, cross-lingual correspondences. It transfers knowledge through identifying important features, i.e., pivot features. For simplicity, however, it assumes that the word translation oracle maps each pivot feature in source language to exactly only one word in target language. This one-to-one mapping between words in different languages is too strict. Also the context is not considered at all. In this paper, we propose a cross-lingual SCL based on distributed representation of words; it can learn meaningful one-to-many mappings for pivot words using large amounts of monolingual data and a small dictionary. We conduct experiments on NLP&CC 2013 cross-lingual sentiment analysis dataset, employing English as source language, and Chinese as target language. Our method does not rely on the parallel corpora and the experimental results show that our approach is more competitive than the state-of-the-art methods in cross-lingual sentiment classification.", "target": "Structural Correspondence Learning for Cross-lingual Sentiment Classification with One-to-many Mappings"}
{"id": "task1540-781a1bac122f4bf288a718bf505f2a57", "input": "An extended, revised form of Tim Buckwalter\u2019s Arabic lexical and morphological resource AraMorph, eXtended Revised AraMorph (henceforth XRAM), is presented which addresses a number of weaknesses and inconsistencies of the original model by allowing a wider coverage of real-world Classical and contemporary (both formal and informal) Arabic texts. Building upon previous research, XRAM enhancements include (i) flag-selectable usage markers, (ii) probabilistic mildly context-sensitive POS tagging, filtering, disambiguation and ranking of alternative morphological analyses, (iii) semi-automatic increment of lexical coverage through extraction of lexical and morphological information from existing lexical resources. Testing of XRAM through a front-end Python module showed a remarkable success level.", "target": "Semi-Automatic Data Annotation, POS Tagging and Mildly Context- Sensitive Disambiguation: the eXtended Revised AraMorph (XRAM)"}
{"id": "task1540-3bcc8177aee843688034dcfaeb66c082", "input": "In this thesis we present a new algorithm for the Vehicle Routing Problem called the Enhanced Bees Algorithm. It is adapted from a fairly recent algorithm, the Bees Algorithm, which was developed for continuous optimisation problems. We show that the results obtained by the Enhanced Bees Algorithm are competitive with the best meta-heuristics available for the Vehicle Routing Problem\u2014it is able to achieve results that are within 0.5% of the optimal solution on a commonly used set of test instances. We show that the algorithm has good runtime performance, producing results within 2% of the optimal solution within 60 seconds, making it suitable for use within real world dispatch scenarios. Additionally, we provide a short history of well known results from the literature along with a detailed description of the foundational methods developed to solve the Vehicle Routing Problem.", "target": "The Bees Algorithm for the Vehicle Routing Problem"}
{"id": "task1540-272a36d6f1034245a5c41666fd7592b5", "input": "We propose a novel fully-automated approach towards inducing multilingual taxonomies from Wikipedia. Given an English taxonomy, our approach first leverages the interlanguage links of Wikipedia to automatically construct training datasets for the is-a relation in the target language. Character-level classifiers are trained on the constructed datasets, and used in an optimal path discovery framework to induce high-precision, high-coverage taxonomies in other languages. Through experiments, we demonstrate that our approach significantly outperforms the state-of-the-art, heuristics-heavy approaches for six languages. As a consequence of our work, we release presumably the largest and the most accurate multilingual taxonomic resource spanning over 280 languages.", "target": "280 Birds with One Stone: Inducing Multilingual Taxonomies from Wikipedia Using Character-level Classification"}
{"id": "task1540-44147bf862b74c0c93cc42aecf3c123d", "input": "One of the better studied properties for operators in judgment aggregation is independence, which essentially dictates that the collective judgment on one issue should not depend on the individual judgments given on some other issue(s) in the same agenda. Independence, although considered a desirable property, is too strong, because together with mild additional conditions it implies dictatorship. We propose here a weakening of independence, named agenda separability: a judgment aggregation rule satisfies it if, whenever the agenda is composed of several independent sub-agendas, the resulting collective judgment sets can be computed separately for each sub-agenda and then put together. We show that this property is discriminant, in the sense that among judgment aggregation rules so far studied in the literature, some satisfy it and some do not. We briefly discuss the implications of agenda separability on the computation of judgment aggregation rules.", "target": "Agenda Separability in Judgment Aggregation"}
{"id": "task1540-c384ccf5f49a47e7b9e8b9b09b91025c", "input": "This paper explores the real-time summarization of scheduled events such as soccer games from torrential flows of Twitter streams. We propose and evaluate an approach that substantially shrinks the stream of tweets in real-time, and consists of two steps: (i) sub-event detection, which determines if something new has occurred, and (ii) tweet selection, which picks a representative tweet to describe each sub-event. We compare the summaries generated in three languages for all the soccer games in Copa America 2011 to reference live reports offered by Yahoo! Sports journalists. We show that simple text analysis methods which do not involve external knowledge lead to summaries that cover 84% of the sub-events on average, and 100% of key types of sub-events (such as goals in soccer). Our approach should be straightforwardly applicable to other kinds of scheduled events such as other sports, award ceremonies, keynote talks, TV shows, etc.", "target": "Towards Real-Time Summarization of Scheduled Events from Twitter Streams"}
{"id": "task1540-8e640ed3991f469c8e8b6ad234f2e286", "input": "We develop a framework for post model selection inference, via marginal screening, in linear regression. At the core of this framework is a result that characterizes the exact distribution of linear functions of the response y, conditional on the model being selected (\u201ccondition on selection\u201d framework). This allows us to construct valid confidence intervals and hypothesis tests for regression coefficients that account for the selection procedure. In contrast to recent work in highdimensional statistics, our results are exact (non-asymptotic) and require no eigenvalue-like assumptions on the design matrix X. Furthermore, the computational cost of marginal regression, constructing confidence intervals and hypothesis testing is negligible compared to the cost of linear regression, thus making our methods particularly suitable for extremely large datasets. Although we focus on marginal screening to illustrate the applicability of the condition on selection framework, this framework is much more broadly applicable. We show how to apply the proposed framework to several other selection procedures including orthogonal matching pursuit, non-negative least squares, and marginal screening+Lasso.", "target": "Exact Post Model Selection Inference for Marginal Screening"}
{"id": "task1540-3ac962f640df49418ffe0d1c75ec7af9", "input": "In many domains it is desirable to assess the pref\u00ad erences of users in a qualitative rather than quan\u00ad titative way. Such representations of qualitative preference orderings form an important compo\u00ad nent of automated decision tools. We propose a graphical representation of preferences that re\u00ad flects conditional dependence and independence of preference statements under a ceteris paribus (all else being equal) interpretation. Such a rep\u00ad resentation is often compact and arguably natural. We describe several search algorithms for domi\u00ad nance testing based on this representation; these algorithms are quite effective, especially in spe\u00ad cific network topologies, such as chainand tree\u00ad structured networks, as well as polytrees.", "target": "Reasoning With Conditional Ceteris Paribus Preference Statements"}
{"id": "task1540-df601d90397445b19d0865ec7753fd4f", "input": "CAPTCHAs or reverse Turing tests are real-time assessments used by programs (or computers) to tell humans and machines apart. This is achieved by assigning and assessing hard AI problems that could only be solved easily by human but not by machines. Applications of such assessments range from stopping spammers from automatically filling online forms to preventing hackers from performing dictionary attack. Today, the race between makers and breakers of CAPTCHAs is at a juncture, where the CAPTCHAs proposed are not even answerable by humans. We consider such CAPTCHAs as non user friendly. In this paper, we propose a novel technique for reverse Turing test we call it the Line CAPTCHAs that mainly focuses on user friendliness while not compromising the security aspect that is expected to be provided by such a system.", "target": "User Friendly Line CAPTCHAs"}
{"id": "task1540-14195e2242054558a3a7e008e17cd5c3", "input": "Pervasive systems refers to context-aware systems that can sense their context, and adapt their behavior accordingly to provide adaptable services. Proactive adaptation of such systems allows changing the service and the context based on prediction. However, the definition of the context is still vague and not suitable to prediction. In this paper we discuss and classify previous definitions of context. Then, we present a new definition which allows pervasive systems to understand and predict their contexts. We analyze the essential lines that fall within the context definition, and propose some scenarios to make it clear our approach.", "target": "International Journal of advanced studies in Computer Science and Engineering"}
{"id": "task1540-9569434b45f04920af90a346a072e114", "input": "In this study, we introduce an ensemble-based approach for online machine learning. The ensemble of base classifiers in our approach is obtained by learning Na\u00efve Bayes classifiers on different training sets which are generated by projecting the original training set to lower dimensional space. We propose a mechanism to learn sequences of data using data chunks paradigm. The experiments conducted on a number of UCI datasets and one synthetic dataset demonstrate that the proposed approach performs significantly better than some well-known online learning algorithms.", "target": "An ensemble-based online learning algorithm for streaming data"}
{"id": "task1540-4cfcbaf9643e4c97b7e617c990a35580", "input": "In the data mining field many clustering methods have been proposed, yet standard versions do not take into account uncertain databases. This paper deals with a new approach to cluster uncertain data by using a hierarchical clustering defined within the belief function framework. The main objective of the belief hierarchical clustering is to allow an object to belong to one or several clusters. To each belonging, a degree of belief is associated, and clusters are combined based on the pignistic properties. Experiments with real uncertain data show that our proposed method can be considered as a propitious tool.", "target": "Belief Hierarchical Clustering"}
{"id": "task1540-dc9e2ba03eff4812901ee55d528b7f92", "input": "We introduce new diversification methods for zero-one optimization that significantly extend strategies previously introduced in the setting of metaheuristic search. Our methods incorporate easily implemented strategies for partitioning assignments of values to variables, accompanied by processes called augmentation and shifting which create greater flexibility and generality. We then show how the resulting collection of diversified solutions can be further diversified by means of permutation mappings, which equally can be used to generate diversified collections of permutations for applications such as scheduling and routing. These methods can be applied to non-binary vectors by the use of binarization procedures and by Diversification-Based Learning (DBL) procedures which also provide connections to applications in clustering and machine learning. Detailed pseudocode and numerical illustrations are provided to show the operation of our methods and the collections of solutions they create.", "target": "Diversification Methods for Zero-One Optimization"}
{"id": "task1540-73444ce2013a40108d7709e3e222e474", "input": "Bilattice-based triangle provides an elegant algebraic structure for reasoning with vague and uncertain information. But the truth and knowledge ordering of intervals in bilattice-based triangle can not handle repetitive belief revisions which is an essential characteristic of nonmonotonic reasoning. Moreover the ordering induced over the intervals by the bilattice-based triangle is not sometimes intuitive. In this work, we construct an alternative algebraic structure, namely preorder-based triangle and we formulate proper logical connectives for this. It is an enhancement of the bilattice-based triangle to handle belief revision in nonmonotonic reasoning.", "target": "Preorder-Based Triangle: A Modified Version of Bilattice-Based Triangle for Belief Revision in Nonmonotonic Reasoning"}
{"id": "task1540-b574b1697fbb4af7aa033260b4a9ae94", "input": "We introduce a model that learns active learning algorithms via metalearning. For a distribution of related tasks, our model jointly learns: a data representation, an item selection heuristic, and a method for constructing prediction functions from labeled training sets. Our model uses the item selection heuristic to gather labeled training sets from which to construct prediction functions. Using the Omniglot and MovieLens datasets, we test our model in synthetic and practical settings.", "target": "Learning Algorithms for Active Learning"}
{"id": "task1540-16e5a563d0314872b06ba4aacaa72463", "input": "Voice browser applications in Text-toSpeech (TTS) and Automatic Speech Recognition (ASR) systems crucially depend on a pronunciation lexicon. The present paper describes the model of pronunciation lexicon of Hindi developed to automatically generate the output forms of Hindi at two levels, the <phoneme> and the <PS> (PS, in short for Prosodic Structure). The latter level involves both syllable-division and stress placement. The paper describes the tool developed for generating the two-level outputs of lexica in Hindi.", "target": "A Generative Model of a Pronunciation Lexicon for Hindi"}
{"id": "task1540-436184804fc747439f097010ae1f524b", "input": "Given an existing trained neural network, it is often desirable to be able to add new capabilities without hindering performance of already learned tasks. Existing approaches either learn sub-optimal solutions, require joint training, or incur a substantial increment in the number of parameters for each added task, typically as many as the original network. We propose a method which fully preserves performance on the original task, with only a small increase (around 20%) in the number of required parameters while performing on par with more costly finetuning procedures, which typically double the number of parameters. The learned architecture can be controlled to switch between various learned representations, enabling a single network to solve a task from multiple different domains. We conduct extensive experiments showing the effectiveness of our method and explore different aspects of its behavior.", "target": "Incremental Learning Through Deep Adaptation"}
{"id": "task1540-439cba0897c04303a79250ddc92f4f93", "input": "Recurrent sequence generators conditioned on input data through an attention mechanism have recently shown very good performance on a range of tasks including machine translation, handwriting synthesis [1, 2] and image caption generation [3]. We extend the attention-mechanism with features needed for speech recognition. We show that while an adaptation of the model used for machine translation in [2] reaches a competitive 18.7% phoneme error rate (PER) on the TIMIT phoneme recognition task, it can only be applied to utterances which are roughly as long as the ones it was trained on. We offer a qualitative explanation of this failure and propose a novel and generic method of adding location-awareness to the attention mechanism to alleviate this issue. The new method yields a model that is robust to long inputs and achieves 18% PER in single utterances and 20% in 10-times longer (repeated) utterances. Finally, we propose a change to the attention mechanism that prevents it from concentrating too much on single frames, which further reduces PER to 17.6% level.", "target": "Attention-Based Models for Speech Recognition"}
{"id": "task1540-3b47735c3c0641e8985ba57103c4b9d6", "input": "Numerous machine learning algorithms contain pairwise statistical problems at their core\u2014 that is, tasks that require computations over all pairs of input points if implemented naively. Often, tree structures are used to solve these problems efficiently. Dual-tree algorithms can efficiently solve or approximate many of these problems. Using cover trees, rigorous worstcase runtime guarantees have been proven for some of these algorithms. In this paper, we present a problem-independent runtime guarantee for any dual-tree algorithm using the cover tree, separating out the problem-dependent and the problem-independent elements. This allows us to just plug in bounds for the problem-dependent elements to get runtime guarantees for dual-tree algorithms for any pairwise statistical problem without re-deriving the entire proof. We demonstrate this plug-and-play procedure for nearest-neighbor search and approximate kernel density estimation to get improved runtime guarantees. Under mild assumptions, we also present the first linear runtime guarantee for dual-tree based range search.", "target": "Plug-and-play dual-tree algorithm runtime analysis"}
{"id": "task1540-63eec7df3a7643b6bbbdf21e048f9ae9", "input": "Automatic description generation from natural images is a challenging problem that has recently received a large amount of interest from the computer vision and natural language processing communities. In this survey, we classify the existing approaches based on how they conceptualize this problem, viz., models that cast description as either generation problem or as a retrieval problem over a visual or multimodal representational space. We provide a detailed review of existing models, highlighting their advantages and disadvantages. Moreover, we give an overview of the benchmark image datasets and the evaluation measures that have been developed to assess the quality of machine-generated image descriptions. Finally we extrapolate future directions in the area of automatic image description generation.", "target": "Automatic Description Generation from Images: A Survey of Models, Datasets, and Evaluation Measures"}
{"id": "task1540-031277206adf475b82d6ea64892ea426", "input": "Annotating semantic data with metadata is becoming more and more important to provide information about the statements being asserted. While initial solutions proposed a data model to represent a specific dimension of metainformation (such as time or provenance), the need for a general annotation framework which allows representing different context dimensions is needed. In this paper, we extend the 4dFluents ontology by Welty and Fikes\u2014on associating temporal validity to statements\u2014to any dimension of context, and discuss possible issues that multidimensional context representations have to face and how we address them.", "target": "NdFluents: A Multi-dimensional Contexts Ontology"}
{"id": "task1540-3d2fe5bfbe8343e0ba38c9a6c5725a97", "input": "Recent years have witnessed the increase of competition in science. While promoting the quality of research in many cases, an intense competition among scientists can also trigger unethical scientific behaviors. To increase the total number of published papers, some authors even resort to software tools that are able to produce grammatical, but meaningless scientific manuscripts. Because automatically generated papers can be misunderstood as real papers, it becomes of paramount importance to develop means to identify these scientific frauds. In this paper, I devise a methodology to distinguish real manuscripts from those generated with SCIGen, an automatic paper generator. Upon modeling texts as complex networks (CN), it was possible to discriminate real from fake papers with at least 89% of accuracy. A systematic analysis of features relevance revealed that the accessibility and betweenness were useful in particular cases, even though the relevance depended upon the dataset. The successful application of the methods described here show, as a proof of principle, that network features can be used to identify scientific gibberish papers. In addition, the CN-based approach can be combined in a straightforward fashion with traditional statistical language processing methods to improve the performance in identifying artificially generated papers.", "target": "Comparing the topological properties of real and artificially generated scientific manuscripts"}
{"id": "task1540-d856f0c5548f4323b0e1eeb0dcb94b53", "input": "This paper formulates a novel problem on graphs: find the minimal subset of edges in a fully connected graph, such that the resulting graph contains all spanning trees for a set of specified subgraphs. This formulation is motivated by an unsupervised grammar induction problem from computational linguistics. We present a reduction to some known problems and algorithms from graph theory, provide computational complexity results, and describe an approximation algorithm.", "target": "Matroids Hitting Sets and Unsupervised Dependency Grammar Induction"}
{"id": "task1540-6bcbdaec923643d69e02eb5c9583f3f0", "input": "A graphical multiagent model (GMM) represents a joint distribution over the behavior of a set of agents. One source of knowledge about agents' behavior may come from gametheoretic analysis, as captured by several graphical game representations developed in recent years. GMMs generalize this approach to express arbitrary distributions, based on game descriptions or other sources of knowledge bearing on beliefs about agent behavior. To illustrate the exibility of GMMs, we exhibit game-derived models that allow probabilistic deviation from equilibrium, as well as models based on heuristic action choice. We investigate three di erent methods of integrating these models into a single model representing the combined knowledge sources. To evaluate the predictive performance of the combined model, we treat as actual outcome the behavior produced by a reinforcement learning process. We nd that combining the two knowledge sources, using any of the methods, provides better predictions than either source alone. Among the combination methods, mixing data outperforms the opinion pool and direct update methods investigated in this empirical trial.", "target": "Knowledge Combination in Graphical Multiagent Models"}
{"id": "task1540-60a56b8c457d4e158b4bdf52dcbc30f5", "input": "<lb>We endow prioritised default logic (PDL) with argumentation semantics<lb>using the ASPIC framework for structured argumentation, and prove<lb>that the conclusions of the justified arguments are exactly the prioritised<lb>default extensions. Argumentation semantics for PDL will allow for the<lb>application of argument game proof theories to the process of inference<lb>in PDL, making the reasons for accepting a conclusion transparent and<lb>the inference process more intuitive. This also opens up the possibility for<lb>argumentation-based distributed reasoning and communication amongst<lb>agents with PDL representations of mental attitudes.", "target": "Argumentation Semantics for Prioritised Default Logic"}
{"id": "task1540-e6694dc44e9245e98fa276b5293a4994", "input": "Probabilistic linear discriminant analysis (PLDA) is a popular normalization approach for the i-vector model, and has delivered state-of-the-art performance in speaker recognition. A potential problem of the PLDA model, however, is that it essentially assumes Gaussian distributions over speaker vectors, which is not always true in practice. Additionally, the objective function is not directly related to the goal of the task, e.g., discriminating true speakers and imposters. In this paper, we propose a max-margin metric learning approach to solve the problems. It learns a linear transform with a criterion that the margin between target and imposter trials are maximized. Experiments conducted on the SRE08 core test show that compared to PLDA, the new approach can obtain comparable or even better performance, though the scoring is simply a cosine computation.", "target": "Max-Margin Metric Learning for Speaker Recognition"}
{"id": "task1540-f62c798362144a35b83552c351b1294c", "input": "The first step of processing a question in Question Answering(QA) Systems is to carry out a detailed analysis of the question for the purpose of determining what it is asking for and how to perfectly approach answering it. Our Question analysis uses several techniques to analyze any question given in natural language: a Stanford POS Tagger & parser for Arabic language, a named entity recognizer, tokenizer, Stop-word removal, Question expansion, Question classification and Question focus extraction components. We employ numerous detection rules and trained classifier using features from this analysis to detect important elements of the question, including: 1) the portion of the question that is a referring to the answer (the focus); 2) different terms in the question that identify what type of entity is being asked for (the lexical answer types); 3) Question expansion ; 4) a process of classifying the question into one or more of several and different types; and We describe how these elements are identified and evaluate the effect of accurate detection on our question-answering system using the Mean Reciprocal Rank(MRR) accuracy measure.", "target": "QUESTION ANALYSIS FOR ARABIC QUESTION ANSWERING SYSTEMS"}
{"id": "task1540-e386685286a34c53b6caecd1e34f0dc9", "input": "This thesis studies methods to solve Visual Question-Answering (VQA) tasks with a Deep Learning framework. As a preliminary step, we explore Long Short-Term Memory (LSTM) networks used in Natural Language Processing (NLP) to tackle Question-Answering (text based). We then modify the previous model to accept an image as an input in addition to the question. For this purpose, we explore the VGG-16 and K-CNN convolutional neural networks to extract visual features from the image. These are merged with the word embedding or with a sentence embedding of the question to predict the answer. This work was successfully submitted to the Visual Question Answering Challenge 2016, where it achieved a 53,62% of accuracy in the test dataset. The developed software has followed the best programming practices and Python code style, providing a consistent baseline in Keras for different configurations. The source code and models are publicly available at https://github.com/imatge-upc/vqa-2016-cvprw.", "target": "Open-Ended Visual Question-Answering"}
{"id": "task1540-92dfa56c3d6f46c8b1af6ef8354e421b", "input": "In this paper, we study the use of recurrent neural networks (RNNs) for modeling and forecasting time series. We first illustrate the fact that standard sequence-to-sequence RNNs neither capture well periods in time series nor handle well missing values, even though many real life times series are periodic and contain missing values. We then propose an extended attention mechanism that can be deployed on top of any RNN and that is designed to capture periods and make the RNN more robust to missing values. We show the effectiveness of this novel model through extensive experiments with multiple univariate and multivariate datasets.", "target": "Time Series Forecasting using RNNs: an Extended Attention Mechanism to Model Periods and Handle Missing Values"}
{"id": "task1540-c7dd5028ac9b4c0fb890763c49149e8a", "input": "We propose a pool-based non-parametric active learning algorithm for general metric spaces, called MArgin Regularized Metric Active Nearest Neighbor (MARMANN), which outputs a nearest-neighbor classifier. We give prediction error guarantees that depend on the noisy-margin properties of the input sample, and are competitive with those obtained by previously proposed passive learners. We prove that the label complexity of MARMANN is significantly lower than that of any passive learner with similar error guarantees. MARMANN is based on a generalized sample compression scheme, and a new label-efficient active model-selection procedure.", "target": "Active Nearest-Neighbor Learning in Metric Spaces"}
{"id": "task1540-8c4d655e3c154582881f0d137b853398", "input": "Artificial object perception usually relies on a priori defined models and feature extraction algorithms. We study how the concept of object can be grounded in the sensorimotor experience of a naive agent. Without any knowledge about itself or the world it is immersed in, the agent explores its sensorimotor space and identifies objects as consistent networks of sensorimotor transitions, independent from their context. A fundamental drive for prediction is assumed to explain the emergence of such networks from a developmental standpoint. An algorithm is proposed and tested to illustrate the approach.", "target": "Grounding object perception in a naive agent\u2019s sensorimotor experience"}
{"id": "task1540-2ee66070b9744190b3f82d59bc54098c", "input": "This document describes the Odin framework, which is a domain-independent platform for developing rule-based event extraction models. Odin aims to be powerful (the rule language allows the modeling of complex syntactic structures) and robust (to recover from syntactic parsing errors, syntactic patterns can be freely mixed with surface, token-based patterns), while remaining simple (some domain grammars can be up and running in minutes), and fast (Odin processes over 100 sentences/second in a real-world domain with over 200 rules). Here we include a thorough definition of the Odin rule language, together with a description of the Odin API in the Scala language, which allows one to apply these rules to arbitrary texts. 1 ar X iv :1 50 9. 07 51 3v 1 [ cs .C L ] 2 4 Se p 20 15", "target": "Description of the Odin Event Extraction Framework and Rule Language"}
{"id": "task1540-3be1289ca2ac410c867b0eee08b5886e", "input": "In this work we introduce a comprehensive algorithmic pipeline for multiple parametric model estimation. The proposed approach analyzes the information produced by a random sampling algorithm (e.g., RANSAC) from a machine learning/optimization perspective, using a parameterless biclustering algorithm based on L1 nonnegative matrix factorization (L1-NMF). The proposed framework exploits consistent patterns that naturally arise during the RANSAC execution, while explicitly avoiding spurious inconsistencies. Contrarily to the main trends in the literature, the proposed technique does not impose non-intersecting parametric models. A new accelerated algorithm to compute L1-NMFs allows to handle medium-sized problems faster while also extending the usability of the algorithm to much larger datasets. This accelerated algorithm has applications in any other context where an L1-NMF is needed, beyond the biclustering approach to parameter estimation here addressed. We accompany the algorithmic presentation with theoretical foundations and numerous and diverse examples.", "target": "Fast L1-NMF for Multiple Parametric Model Estimation\u2217"}
{"id": "task1540-814c95bad9524c0abafbb2a9c03bda44", "input": "We propose a method for learning from streaming visual data using a compact, constant size representation of all the data that was seen until a given moment. Specifically, we construct a \u201ccoreset\u201d representation of streaming data using a parallelized algorithm, which is an approximation of a set with relation to the squared distances between this set and all other points in its ambient space. We learn an adaptive object appearance model from the coreset tree in constant time and logarithmic space and use it for object tracking by detection. Our method obtains excellent results for object tracking on three standard datasets over more than 100 videos. The ability to summarize data efficiently makes our method ideally suited for tracking in long videos in presence of space and time constraints. We demonstrate this ability by outperforming a variety of algorithms on the TLD dataset with 2685 frames on average. This coreset based learning approach can be applied for both real-time learning of small, varied data and fast learning of big data.", "target": "Coreset-Based Adaptive Tracking"}
{"id": "task1540-b03e8539a8034267830f6952ced46076", "input": "Here, we propose a brain-inspired winner-take-all emotional neural network (WTAENN) and prove the universal approximation property for the novel architecture. WTAENN is a single layered feedforward neural network that benefits from the excitatory, inhibitory, and expandatory neural connections as well as the winner-take-all (WTA) competitions in the human brain\u2019s nervous system. The WTA competition increases the information capacity of the model without adding hidden neurons. The universal approximation capability of the proposed architecture is illustrated on two example functions, trained by a genetic algorithm, and then applied to several competing recent and benchmark problems such as in curve fitting, pattern recognition, classification and prediction. In particular, it is tested on twelve UCI classification datasets, a facial recognition problem, three real world prediction problems (2 chaotic time series of geomagnetic activity indices and wind farm power generation data), two synthetic case studies with constant and nonconstant noise variance as well as k-selector and linear programming problems. Results indicate the general applicability and often superiority of the approach in terms of higher accuracy and lower model complexity, especially where low computational complexity is", "target": "A Winner-Take-All Approach to Emotional Neural Networks with Universal Approximation Property"}
{"id": "task1540-461263f193b946b3bbfb584f263db998", "input": "Abstract\u2014An important problem in the field of bioinformatics is to identify interactive effects among profiled variables for outcome prediction. In this paper, a logistic regression model with pairwise interactions among a set of binary covariates is considered. Modeling the structure of the interactions by a graph, our goal is to recover the interaction graph from independently identically distributed (i.i.d.) samples of the covariates and the outcome. When viewed as a feature selection problem, a simple quantity called influence is proposed as a measure of the marginal effects of the interaction terms on the outcome. For the case when the underlying interaction graph is known to be acyclic, it is shown that a simple algorithm that is based on a maximum-weight spanning tree with respect to the plug-in estimates of the influences not only has strong theoretical performance guarantees, but can also outperform generic feature selection algorithms for recovering the interaction graph from i.i.d. samples of the covariates and the outcome. Our results can also be extended to the model that includes both individual effects and pairwise interactions via the help of an auxiliary covariate.", "target": "Detection of Cooperative Interactions in Logistic Regression Models"}
{"id": "task1540-6ca6d68491904e84bc66dfd2b9bc4eba", "input": "Recently, resources and tasks were proposed to go beyond state tracking in dialogue systems. An example is the frame tracking task, which requires recording multiple frames, one for each user goal set during the dialogue. This allows a user, for instance, to compare items corresponding to different goals. This paper proposes a model which takes as input the list of frames created so far during the dialogue, the current user utterance as well as the dialogue acts, slot types, and slot values associated with this utterance. The model then outputs the frame being referenced by each triple of dialogue act, slot type, and slot value. We show that on the recently published Frames dataset, this model significantly outperforms a previously proposed rule-based baseline. In addition, we propose an extensive analysis of the frame tracking task by dividing it into sub-tasks and assessing their difficulty with respect to our model.", "target": "A Frame Tracking Model for Memory-Enhanced Dialogue Systems"}
{"id": "task1540-0a6850bc26464d5fb43bbc132ee8ae8e", "input": "Automatic question generation aims to generate questions from a text passage where the generated questions can be answered by certain sub-spans of the given passage. Traditional methods mainly use rigid heuristic rules to transform a sentence into related questions. In this work, we propose to apply the neural encoderdecoder model to generate meaningful and diverse questions from natural language sentences. The encoder reads the input text and the answer position, to produce an answer-aware input representation, which is fed to the decoder to generate an answer focused question. We conduct a preliminary study on neural question generation from text with the SQuAD dataset, and the experiment results show that our method can produce fluent and diverse questions.", "target": "Neural Question Generation from Text: A Preliminary Study"}
{"id": "task1540-13a21f3431b64ab6b3c72470e99295eb", "input": "A new algorithm named EXPected Similarity Estimation (EXPoSE) was recently proposed to solve the problem of large-scale anomaly detection. It is a non-parametric and distribution free kernel method based on the Hilbert space embedding of probability measures. Given a dataset of n samples, EXPoSE needs only O(n) (linear time) to build a model and O(1) (constant time) to make a prediction. In this work we improve the linear computational complexity and show that an -accurate model can be estimated in constant time, which has significant implications for large-scale learning problems. To achieve this goal, we cast the original EXPoSE formulation into a stochastic optimization problem. It is crucial that this approach allows us to determine the number of iteration based on a desired accuracy , independent of the dataset size n. We will show that the proposed stochastic gradient descent algorithm works in general (possible infinite-dimensional) Hilbert spaces, is easy to implement and requires no additional step-size parameters.", "target": "CONSTANT TIME EXPECTED SIMILARITY ESTIMATION USING STOCHASTIC OPTIMIZATION"}
{"id": "task1540-1abeab57a0f543c396bf21fbd982df63", "input": "Advances in sensing technologies and the growth of the internet have resulted in an explosion in the size of modern datasets, while storage and processing power continue to lag behind. This motivates the need for algorithms that are efficient, both in terms of the number of measurements needed and running time. To combat the challenges associated with large datasets, we propose a general framework for active hierarchical clustering that repeatedly runs an off-the-shelf clustering algorithm on small subsets of the data and comes with guarantees on performance, measurement complexity and runtime complexity. We instantiate this framework with a simple spectral clustering algorithm and provide concrete results on its performance, showing that, under some assumptions, this algorithm recovers all clusters of size \u03a9(log n) using O(n log n) similarities and runs in O(n log n) time for a dataset of n objects. Through extensive experimentation we also demonstrate that this framework is practically alluring.", "target": "Efficient Active Algorithms for Hierarchical Clustering"}
{"id": "task1540-68ed2d7534b3466793a8662826beb3bd", "input": "We examine the meaning and the complexity of probabilistic logic programs that consist of a set of rules and a set of independent probabilistic facts (that is, programs based on Sato\u2019s distribution semantics). We focus on two semantics, respectively based on stable and on well-founded models. We show that the semantics based on stable models (referred to as the \u201ccredal semantics\u201d) produces sets of probability models that dominate infinitely monotone Choquet capacities; we describe several useful consequences of this result. We then examine the complexity of inference with probabilistic logic programs. We distinguish between the complexity of inference when a probabilistic program and a query are given (the inferential complexity), and the complexity of inference when the probabilistic program is fixed and the query is given (the query complexity, akin to data complexity as used in database theory). We obtain results on the inferential and query complexity for acyclic, stratified, and cyclic propositional and relational programs; complexity reaches various levels of the counting hierarchy and even exponential levels.", "target": "On the Semantics and Complexity of Probabilistic Logic Programs"}
{"id": "task1540-8d14e0d06a904da689ef62f1dbef3bd0", "input": "<lb>We show that any model trained by a stochastic gradient method with few iterations has<lb>vanishing generalization error. We prove this by showing the method is algorithmically stable<lb>in the sense of Bousquet and Elisseeff. Our analysis only employs elementary tools from convex<lb>and continuous optimization. Our results apply to both convex and non-convex optimization<lb>under standard Lipschitz and smoothness assumptions.<lb>Applying our results to the convex case, we provide new explanations for why multiple<lb>epochs of stochastic gradient descent generalize well in practice. In the nonconvex case, we<lb>provide a new interpretation of common practices in neural networks, and provide a formal<lb>rationale for stability-promoting mechanisms in training large, deep models. Conceptually, our<lb>findings underscore the importance of reducing training time beyond its obvious benefit.", "target": "Stability of stochastic gradient descent"}
{"id": "task1540-40c0f0214d704c8c9a362e99a529810e", "input": "We provide several applications of Optimistic Mirror Descent, an online learning algorithm based on the idea<lb>of predictable sequences. First, we recover theMirror Prox algorithm for offline optimization, prove an extension<lb>to H\u00f6lder-smooth functions, and apply the results to saddle-point type problems. Next, we prove that a version<lb>of Optimistic Mirror Descent (which has a close relation to the Exponential Weights algorithm) can be used by<lb>two strongly-uncoupled players in a finite zero-summatrix game to converge to the minimax equilibrium at the<lb>rate ofO((logT )/T ). This addresses a question of Daskalakis et al [6]. Further, we consider a partial information<lb>version of the problem. We then apply the results to convex programming and exhibit a simple algorithm for the<lb>approximateMax Flow problem.", "target": "Optimization, Learning, and Games with Predictable Sequences"}
{"id": "task1540-785647416af34ca9a9479fafa295452d", "input": "Dropout and other feature noising schemes control overfitting by artificially cor-<lb>rupting the training data. For generalized linear models, dropout performs a form<lb>of adaptive regularization. Using this viewpoint, we show that the dropout regular-<lb>izer is first-order equivalent to an L2 regularizer applied after scaling the features<lb>by an estimate of the inverse diagonal Fisher information matrix. We also establish<lb>a connection to AdaGrad, an online learning algorithm, and find that a close rel-<lb>ative of AdaGrad operates by repeatedly solving linear dropout-regularized prob-<lb>lems. By casting dropout as regularization, we develop a natural semi-supervised<lb>algorithm that uses unlabeled data to create a better adaptive regularizer. We ap-<lb>ply this idea to document classification tasks, and show that it consistently boosts<lb>the performance of dropout training, improving on state-of-the-art results on the<lb>IMDB reviews dataset.", "target": "Dropout Training as Adaptive Regularization"}
{"id": "task1540-b076cccaad484d1b902fb7c1a7bc6978", "input": "The logistic loss function is often advocated in machine learning and statistics as a smooth and strictly convex surrogate for the 0-1 loss. In this paper we investigate the question of whether these smoothness and convexity properties make the logistic loss preferable to other widely considered options such as the hinge loss. We show that in contrast to known asymptotic bounds, as long as the number of prediction/optimization iterations is sub exponential, the logistic loss provides no improvement over a generic non-smooth loss function such as the hinge loss. In particular we show that the convergence rate of stochastic logistic optimization is bounded from below by a polynomial in the diameter of the decision set and the number of prediction iterations, and provide a matching tight upper bound. This resolves the COLT open problem of McMahan and Streeter (2012).", "target": "Logistic Regression: Tight Bounds for Stochastic and Online Optimization\u2217"}
{"id": "task1540-6b24b25669a54ee5a550bcc3044c09ed", "input": "Existing algorithms for subgroup discovery with numerical targets do not optimize the error or target variable dispersion of the groups they find. This often leads to unreliable or inconsistent statements about the data, rendering practical applications, especially in scientific domains, futile. Therefore, we here extend the optimistic estimator framework for optimal subgroup discovery to a new class of objective functions: we show how tight estimators can be computed efficiently for all functions that are determined by subgroup size (non-decreasing dependence), the subgroup median value, and a dispersion measure around the median (nonincreasing dependence). In the important special case when dispersion is measured using the mean absolute deviation from the median, this novel approach yields a linear time algorithm. Empirical evaluation on a wide range of datasets shows that, when used within branch-and-bound search, this approach is highly efficient and indeed discovers subgroups with much smaller errors.", "target": "Identifying Consistent Statements about Numerical Data with Dispersion-Corrected Subgroup Discovery"}
{"id": "task1540-487582a95b7e41d7868fe31f9488a401", "input": "Retention of residual skills for persons who partially lose their cognitive or physical ability is of utmost importance. Research is focused on developing systems that provide need-based assistance for retention of such residual skills. This paper describes a novel cognitive collaborative control architecture CA, designed to address the challenges of developing needbased assistance for wheelchair navigation. Organization of CA is detailed and results from simulation of the proposed architecture is presented. For simulation of our proposed architecture, we have used ROS (Robot Operating System) as a control framework and a 3D robotic simulator called USARSim (Unified System for Automation and Robot Simulation).", "target": "C3A: A Cognitive Collaborative Control Architecture For an Intelligent Wheelchair"}
{"id": "task1540-d331d3d2964e479c99a2dfb6e54cef7c", "input": "Natural Immune system plays a vital role in the survival of the all living being. It provides a mechanism to defend itself from external predates making it consistent systems, capable of adapting itself for survival incase of changes. The human immune system has motivated scientists and engineers for finding powerful information processing algorithms that has solved complex engineering tasks. This paper explores one of the various possibilities for solving problem in a Multiagent scenario wherein multiple robots are deployed to achieve a goal collectively. The final goal is dependent on the performance of individual robot and its survival without having to lose its energy beyond a predetermined threshold value by deploying an evolutionary computational technique otherwise called the artificial immune system that imitates the biological immune system.", "target": "An Artificial Immune System Model for Multi Agents Resource Sharing in Distributed Environments"}
{"id": "task1540-555cc2072fb646058411596f10018210", "input": "We propose Neural Reasoner , a framework for neural network-based reasoning over natural language sentences. Given a question, Neural Reasoner can infer over multiple supporting facts and find an answer to the question in specific forms. Neural Reasoner has 1) a specific interaction-pooling mechanism, allowing it to examine multiple facts, and 2) a deep architecture, allowing it to model the complicated logical relations in reasoning tasks. Assuming no particular structure exists in the question and facts, Neural Reasoner is able to accommodate different types of reasoning and different forms of language expressions. Despite the model complexity, Neural Reasoner can still be trained effectively in an end-to-end manner. Our empirical studies show that Neural Reasoner can outperform existing neural reasoning systems with remarkable margins on two difficult artificial tasks (Positional Reasoning and Path Finding) proposed in [8]. For example, it improves the accuracy on Path Finding(10K) from 33.4% [6] to over 98%.", "target": "Towards Neural Network-based Reasoning"}
{"id": "task1540-765232697f5e40d394e546b65eda3523", "input": "This is a companion note to our recent study of the weak convergence properties of constrained emphatic temporal-difference learning (ETD) algorithms from a theoretic perspective. It supplements the latter analysis with simulation results and illustrates the behavior of some of the ETD algorithms using three example problems.", "target": "Some Simulation Results for Emphatic Temporal-Difference Learning Algorithms\u2217"}
{"id": "task1540-6912cb201d3b4a9c9139102a08631e82", "input": "Learning representations of data, and in particular learning features for a subsequent prediction task, has been a fruitful area of research delivering impressive empirical results in recent years. However, relatively little is understood about what makes a representation \u2018good\u2019. We propose the idea of a risk gap induced by representation learning for a given prediction context, which measures the difference in the risk of some learner using the learned features as compared to the original inputs. We describe a set of sufficient conditions for unsupervised representation learning to provide a benefit, as measured by this risk gap. These conditions decompose the problem of when representation learning works into its constituent parts, which can be separately evaluated using an unlabeled sample, suitable domain-specific assumptions about the joint distribution, and analysis of the feature learner and subsequent supervised learner. We provide two examples of such conditions in the context of specific properties of the unlabeled distribution, namely when the data lies close to a low-dimensional manifold and when it forms clusters. We compare our approach to a recently proposed analysis of semi-supervised learning.", "target": "A Modular Theory of Feature Learning"}
{"id": "task1540-901e562200cb4b16aa81387767281102", "input": "The advent of Web 2.0 has led to an increase in the amount of sentimental content available in the Web. Such content is often found in social media web sites in the form of movie or product reviews, user comments, testimonials, messages in discussion forums etc. Timely discovery of the sentimental or opinionated web content has a number of advantages, the most important of all being monetization. Understanding of the sentiments of human masses towards different entities and products enables better services for contextual advertisements, recommendation systems and analysis of market trends. The focus of our project is sentiment focussed web crawling framework to facilitate the quick discovery of sentimental contents of movie reviews and hotel reviews and analysis of the same. We use statistical methods to capture elements of subjective style and the sentence polarity. The paper elaborately discusses two supervised machine learning algorithms: K-Nearest Neighbour(K-NN) and Na\u00efve Bayes\u2019 and compares their overall accuracy, precisions as well as recall values. It was seen that in case of movie reviews Na\u00efve Bayes\u2019 gave far better results than K-NN but for hotel reviews these algorithms gave lesser, almost same", "target": "Sentiment Analysis of Review Datasets using Nai\u0308ve Bayes\u2019 and K-NN Classifier"}
{"id": "task1540-baed412e5e3145ec9df9b64671d8422c", "input": "Stance detection, the task of identifying the speaker\u2019s opinion towards a particular target, has attracted the attention of researchers. This paper describes a novel approach for detecting stance in Twitter. We define a set of features in order to consider the context surrounding a target of interest with the final aim of training a model for predicting the stance towards the mentioned targets. In particular, we are interested in investigating political debates in social media. For this reason we evaluated our approach focusing on two targets of the SemEval-2016 Task 6 on Detecting stance in tweets, which are related to the political campaign for the 2016 U.S. presidential elections: Hillary Clinton vs. Donald Trump. For the sake of comparison with the state of the art, we evaluated our model against the dataset released in the SemEval-2016 Task 6 shared task competition. Our results outperform the best ones obtained by participating teams, and show that information about enemies and friends of politicians help in detecting stance towards them.", "target": "Friends and Enemies of Clinton and Trump: Using Context for Detecting Stance in Political Tweets"}
{"id": "task1540-abd53d25b82d482489aee8d5379e7f15", "input": "We aim to shed light on the strengths and weaknesses of the newly introduced neural machine translation paradigm. To that end, we conduct a multifaceted evaluation in which we compare outputs produced by state-of-the-art neural machine translation and phrase-based machine translation systems for 9 language directions across a number of dimensions. Specifically, we measure the similarity of the outputs, their fluency and amount of reordering, the effect of sentence length and performance across different error categories. We find out that translations produced by neural machine translation systems are considerably different, more fluent and more accurate in terms of word order compared to those produced by phrase-based systems. Neural machine translation systems are also more accurate at producing inflected forms, but they perform poorly when translating very long sentences.", "target": "A Multifaceted Evaluation of Neural versus Phrase-Based Machine Translation for 9 Language Directions"}
{"id": "task1540-758d437c4b5a4d1ba7a5c8c2c0ffa72b", "input": "It is well known that conditional indepen\u00ad dence can be used to factorize a joint prob\u00ad ability into a multiplication of conditional probabilities. This paper proposes a con\u00ad structive definition of intercausal indepen\u00ad dence, which can be used to further factorize a conditional probability. An inference algo\u00ad rithm is developed, which makes use of both conditional independence and intercausal in\u00ad dependence to reduce inference complexity in Bayesian networks.", "target": "Intercausal Independence and Heterogeneous Factorization"}
{"id": "task1540-5ebb0caa3dcb4a9bbca6704794e4a848", "input": "In this work, we propose a novel recurrent neural network (RNN) architecture. The proposed RNN, gated-feedback RNN (GF-RNN), extends the existing approach of stacking multiple recurrent layers by allowing and controlling signals flowing from upper recurrent layers to lower layers using a global gating unit for each pair of layers. The recurrent signals exchanged between layers are gated adaptively based on the previous hidden states and the current input. We evaluated the proposed GF-RNN with different types of recurrent units, such as tanh, long short-term memory and gated recurrent units, on the tasks of character-level language modeling and Python program evaluation. Our empirical evaluation of different RNN units, revealed that in both tasks, the GF-RNN outperforms the conventional approaches to build deep stacked RNNs. We suggest that the improvement arises because the GFRNN can adaptively assign different layers to different timescales and layer-to-layer interactions (including the top-down ones which are not usually present in a stacked RNN) by learning to gate these interactions.", "target": "Gated Feedback Recurrent Neural Networks"}
{"id": "task1540-51a86805219744779c8ca85079545aaf", "input": "Incidents of organized cybercrime are rising because of criminals are reaping high financial rewards while incurring low costs to commit crime. As the digital landscape broadens to accommodate more internet-enabled devices and technologies like social media, more cybercriminals who are not native English speakers are invading cyberspace to cash in on quick exploits. In this paper we evaluate the performance of three machine learning classifiers in detecting 419 scams in a bilingual Nigerian cybercriminal community. We use three popular classifiers in text processing namely: Na\u00efve Bayes, k-nearest neighbors (IBK) and Support Vector Machines (SVM). The preliminary results on a real world dataset reveal the SVM significantly outperforms Na\u00efve Bayes and IBK at 95% confidence level. Keywords-Machine Learning; Bilingual Cybercriminals; 419 Scams;", "target": "Evaluating Classifiers in Detecting 419 Scams in Bilingual Cybercriminal Communities"}
{"id": "task1540-d905a20f34d844349e25490cbfdd36c5", "input": "In this paper, we present a robotic prediction agent including a darkforest Go engine, a fuzzy markup language (FML) assessment engine, an FML-based decision support engine, and a robot engine for game of Go application. The knowledge base and rule base of FML assessment engine are constructed by referring the information from the darkforest Go engine located in NUTN and OPU, for example, the number of MCTS simulations and winning rate prediction. The proposed robotic prediction agent first retrieves the database of Go competition website, and then the FML assessment engine infers the winning possibility based on the information generated by darkforest Go engine. The FML-based decision support engine computes the winning possibility based on the partial game situation inferred by FML assessment engine. Finally, the robot engine combines with the human-friendly robot partner PALRO, produced by Fujisoft incorporated, to report the game situation to human Go players. Experimental results show that the FML-based prediction agent can work effectively. Keywords\u2014Fuzzy markup language; prediction agent; decision support engine; robot engine; darkforest Go engine", "target": "FML-based Prediction Agent and Its Application to Game of Go"}
{"id": "task1540-3881c6f499874e57a559c53d96d58b77", "input": "We propose a regularized linear learning algorithm to sequence groups of features, where each group incurs test-time cost or computation. Specifically, we develop a simple extension to Orthogonal Matching Pursuit (OMP) that respects the structure of groups of features with variable costs, and we prove that it achieves nearoptimal anytime linear prediction at each budget threshold where a new group is selected. Our algorithm and analysis extends to generalized linear models with multi-dimensional responses. We demonstrate the scalability of the resulting approach on large real-world data-sets with many feature groups associated with test-time computational costs. Our method improves over Group Lasso and Group OMP in the anytime performance of linear predictions, measured in timeliness[7], an anytime prediction performance metric, while providing rigorous performance guarantees.", "target": "Efficient Feature Group Sequencing for Anytime Linear Prediction"}
{"id": "task1540-2b0566cf30d443da81d8caac701bb37d", "input": "In this paper, a progressive learning technique for multi-class classification is proposed. This newly developed learning technique is independent of the number of class constraints and it can learn new classes while still retaining the knowledge of previous classes. Whenever a new class (non-native to the knowledge learnt thus far) is encountered, the neural network structure gets remodeled automatically by facilitating new neurons and interconnections, and the parameters are calculated in such a way that it retains the knowledge learnt thus far. This technique is suitable for realworld applications where the number of classes is often unknown and online learning from real-time data is required. The consistency and the complexity of the progressive learning technique are analyzed. Several standard datasets are used to evaluate the performance of the developed technique. A comparative study shows that the developed technique is superior. Key Words\u2014Classification, machine learning, multi-class, sequential learning, progressive learning.", "target": "A Novel Progressive Learning Technique for Multi-class Classification"}
{"id": "task1540-2910af8479ee407d9fd798ef1e38e893", "input": "The game of Go is more challenging than other board games, due to the difficulty of constructing a position or move evaluation function. In this paper we investigate whether deep convolutional networks can be used to directly represent and learn this knowledge. We train a large 12-layer convolutional neural network by supervised learning from a database of human professional games. The network correctly predicts the expert move in 55% of positions, equalling the accuracy of a 6 dan human player. When the trained convolutional network was used directly to play games of Go, without any search, it beat the traditional-search program GnuGo in 97% of games, and matched the performance of a state-of-the-art Monte-Carlo tree search that simulates two million positions per move.", "target": "MOVE EVALUATION IN GO USING DEEP CONVOLUTIONAL NEURAL NETWORKS"}
{"id": "task1540-118cdaaa22884c5f9d778516d4396248", "input": "We propose a simple, scalable, fully generative model for transition-based dependency parsing with high accuracy. The model, parameterized by Hierarchical Pitman-Yor Processes, overcomes the limitations of previous generative models by allowing fast and accurate inference. We propose an efficient decoding algorithm based on particle filtering that can adapt the beam size to the uncertainty in the model while jointly predicting POS tags and parse trees. The UAS of the parser is on par with that of a greedy discriminative baseline. As a language model, it obtains better perplexity than a n-gram model by performing semi-supervised learning over a large unlabelled corpus. We show that the model is able to generate locally and syntactically coherent sentences, opening the door to further applications in language generation.", "target": "A Bayesian Model for Generative Transition-based Dependency Parsing"}
{"id": "task1540-dc84f629fee143a385308bc3868d306d", "input": "Compared with word-level and sentence-level convolutional neural networks (ConvNets), the character-level ConvNets has a better applicability for misspellings and typos input. Due to this, recent researches for text classification mainly focus on character-level ConvNets. However, while the majority of these researches employ English corpus for the character-level text classification, few researches have been done using Chinese corpus. This research hopes to bridge this gap, exploring character-level ConvNets for Chinese corpus test classification. We have constructed a large-scale Chinese dataset, and the result shows that character-level ConvNets works better on Chinese character dataset than its corresponding pinyin format dataset, which is the general solution in previous researches. This is the first time that character-level ConvNets has been applied to Chinese character dataset for text classification problem.", "target": "Character-level Convolutional Network for Text Classification Applied to Chinese Corpus"}
{"id": "task1540-edbf9e7e6d8d49dfaaccf3a3ae0ab1c8", "input": "Cloze-style reading comprehension is a representative problem in mining relationship between document and query. In this paper, we present a simple but novel model called attention-over-attention reader for better solving cloze-style reading comprehension task. Our model aims to place another attention mechanism over the document-level attention and induces \u201cattended attention\u201d for final answer predictions. One advantage of our model is that it is simpler than related works while giving excellent performance. We also propose an N-best re-ranking strategy to double check the validity of the candidates and further improve the performance. Experimental results show that the proposed methods significantly outperform various state-of-the-art systems by a large margin in public datasets, such as CNN and Children\u2019s Book Test.", "target": "Attention-over-Attention Neural Networks for Reading Comprehension"}
{"id": "task1540-18597cbdd7ba4d61bde5f470279ead81", "input": "LetF be a set of boolean functions. We present an algorithm for learningF\u2228 := {\u2228f\u2208Sf | S \u2286 F} from membership queries. Our algorithm asks at most |F| \u00b7OPT(F\u2228) membership queries where OPT(F\u2228) is the minimum worst case number of membership queries for learning F\u2228. When F is a set of halfspaces over a constant dimension space or a set of variable inequalities, our algorithm runs in polynomial time. The problem we address has practical importance in the field of program synthesis, where the goal is to synthesize a program that meets some requirements. Program synthesis has become popular especially in settings aiming to help end users. In such settings, the requirements are not provided upfront and the synthesizer can only learn them by posing membership queries to the end user. Our work enables such synthesizers to learn the exact requirements while bounding the number of membership queries.", "target": "Learning Disjunctions of Predicates"}
{"id": "task1540-6b1ac633f8b14e73855efcdd2b7966f3", "input": "Text analysis includes lexical analysis of the text and has been widely studied and used in diverse applications. In the last decade, researchers have proposed many efficient solutions to analyze / classify large text dataset, however, analysis / classification of short text is still a challenge because 1) the data is very sparse 2) It contains noise words and 3) It is difficult to understand the syntactical structure of the text. Short Messaging Service (SMS) is a text messaging service for mobile/smart phone and this service is frequently used by all mobile users. Because of the popularity of SMS service, marketing companies nowadays are also using this service for direct marketing also known as SMS marketing.In this paper, we have proposed Ontology based SMS Controller which analyze the text message and classify it using ontology aslegitimate or spam. The proposed system has been tested on different scenarios and experimental results shows that the proposed solution is effective both in terms of efficiency and time. Keywords\u2014Short Text Classification; SMS Spam; Text Analysis; Ontology based SMS Spam; Text Analysis and Ontology", "target": "Ontology Based SMS Controller for Smart Phones"}
{"id": "task1540-7588f913925e4f4f8ee083d1e9eb667c", "input": "Recent developments in controlled natural language editors for knowledge engineering (KE) have given rise to expectations that they will make KE tasks more accessible and perhaps even enable non-engineers to build knowledge bases. This exploratory research focussed on novices and experts in knowledge engineering during their attempts to learn a controlled natural language (CNL) known as OWL Simplified English and use it to build a small knowledge base. Participants\u2019 behaviours during the task were observed through eye-tracking and screen recordings. This was an attempt at a more ambitious user study than in previous research because we used a naturally occurring text as the source of domain knowledge, and left them without guidance on which information to select, or how to encode it. We have identified a number of skills (competencies) required for this difficult task and key problems that authors face.", "target": "How Easy is it to Learn a Controlled Natural Language for Building a Knowledge Base?"}
{"id": "task1540-2a78bcf9e0254440a60afb1d99ceeed1", "input": "We create a transition-based dependency parser using a general purpose learning to search system. The result is a fast and accurate parser for many languages. Compared to other transition-based dependency parsing approaches, our parser provides similar statistical and computational performance with best-known approaches while avoiding various downsides including randomization, extra feature requirements, and custom learning algorithms. We show that it is possible to implement a dependency parser with an open-source learning to search library in about 300 lines of C++ code, while existing systems often requires several thousands of lines.", "target": "Learning to Search for Dependencies"}
{"id": "task1540-da91c63dc32a4ca3a9b85b4b52139ab9", "input": "We propose Edward, a Turing-complete probabilistic programming language. Edward defines two compositional representations\u2014random variables and inference. By treating inference as a first class citizen, on a par with modeling, we show that probabilistic programming can be as flexible and computationally efficient as traditional deep learning. For flexibility, Edward makes it easy to fit the same model using a variety of composable inference methods, ranging from point estimation to variational inference to MCMC. In addition, Edward can reuse the modeling representation as part of inference, facilitating the design of rich variational models and generative adversarial networks. For efficiency, Edward is integrated into TensorFlow, providing significant speedups over existing probabilistic systems. For example, we show on a benchmark logistic regression task that Edward is at least 35x faster than Stan and 6x faster than PyMC3. Further, Edward incurs no runtime overhead: it is as fast as handwritten TensorFlow.", "target": "DEEP PROBABILISTIC PROGRAMMING"}
{"id": "task1540-395babccf5164caaa1f9bcbfb5f1090d", "input": "We consider linear models for stochastic dynamics. To any such model can be as-<lb>sociated a network (namely a directed graph) describing which degrees of freedom<lb>interact under the dynamics. We tackle the problem of learning such a network<lb>from observation of the system trajectory over a time interval T .<lb>We analyze the l1-regularized least squares algorithm and, in the setting in which<lb>the underlying network is sparse, we prove performance guarantees that are uni-<lb>form in the sampling rate as long as this is sufficiently high. This result substan-<lb>tiates the notion of a well defined \u2018time complexity\u2019 for the network inference<lb>problem. keywords: Gaussian processes, model selection and structure learning, graphical models, sparsity<lb>and feature selection.", "target": "Learning Networks of Stochastic Differential Equations"}
{"id": "task1540-409f8389f9e94c45adb9aabd503517d7", "input": "We present a system for recognising human activity given a symbolic representation of video content. The input of our system is a set of time-stamped short-term activities (STA) detected on video frames. The output is a set of recognised long-term activities (LTA), which are pre-defined temporal combinations of STA. The constraints on the STA that, if satisfied, lead to the recognition of a LTA, have been expressed using a dialect of the Event Calculus. In order to handle the uncertainty that naturally occurs in human activity recognition, we adapted this dialect to a state-of-the-art probabilistic logic programming framework. We present a detailed evaluation and comparison of the crisp and probabilistic approaches through experimentation on a benchmark dataset of human surveillance videos.", "target": "A Probabilistic Logic Programming Event Calculus"}
{"id": "task1540-2d8d46df2dfc4302aeb46c53aeb4bd03", "input": "Multi objective (MO) optimization is an emerging field which is increasingly being implemented in many industries globally. In this work, the MO optimization of the extraction process of bioactive compounds from the Gardenia Jasminoides Ellis fruit was solved. Three swarm-based algorithms have been applied in conjunction with normal-boundary intersection (NBI) method to solve this MO problem. The gravitational search algorithm (GSA) and the particle swarm optimization (PSO) technique were implemented in this work. In addition, a novel Hopfield-enhanced particle swarm optimization was developed and applied to the extraction problem. By measuring the levels of dominance, the optimality of the approximate Pareto frontiers produced by all the algorithms were gauged and compared. Besides, by measuring the levels of convergence of the frontier, some understanding regarding the structure of the objective space in terms of its relation to the level of frontier dominance is uncovered. Detail comparative studies were conducted on all the algorithms employed and developed in this work.", "target": "Swarm Intelligence for Multiobjective Optimization of Extraction Process"}
{"id": "task1540-56e9888dfc404c1c91a0b706eec54609", "input": "Decision making in uncertain and risky environments is a prominent area of research. Standard economic theories fail to fully explain human behaviour, while a potentially promising alternative may lie in the direction of Reinforcement Learning (RL) theory. We analyse data for 46 players extracted from a financial market online game and test whether Reinforcement Learning (Q-Learning) could capture these players behaviour using a risk measure based on financial modeling. Moreover we test an earlier hypothesis that players are \u201cna\u0131\u0308ve\u201d (short-sighted). Our results indicate that a simple Reinforcement Learning model which considers only the selling component of the task captures the decision-making process for a subset of players but this is not sufficient to draw any conclusion on the population. We also find that there is not a significant improvement of fitting of the players when using a full RL model against a myopic version, where only immediate reward is valued by the players. This indicates that players, if using a Reinforcement Learning approach, do so na\u0131\u0308vely.", "target": "Modelling Stock-market Investors as Reinforcement Learning Agents [Correction]"}
{"id": "task1540-32620d09bf374cfbafd5f7b5d92125c3", "input": "Sampling from hierarchical Bayesian models is often difficult for MCMC methods, because of the strong correlations between the model parameters and the hyperparameters. Recent Riemannian manifold Hamiltonian Monte Carlo (RMHMC) methods have significant potential advantages in this setting, but are computationally expensive. We introduce a new RMHMC method, which we call semi-separable Hamiltonian Monte Carlo, which uses a specially designed mass matrix that allows the joint Hamiltonian over model parameters and hyperparameters to decompose into two simpler Hamiltonians. This structure is exploited by a new integrator which we call the alternating blockwise leapfrog algorithm. The resulting method can mix faster than simpler Gibbs sampling while being simpler and more efficient than previous instances of RMHMC.", "target": "Semi-Separable Hamiltonian Monte Carlo for Inference in Bayesian Hierarchical Models"}
{"id": "task1540-2baf2f7080c6417499839ccdc8fdf520", "input": "Currently, criminal\u2019s profile (CP) is obtained from investigator\u2019s or forensic psychologist\u2019s interpretation, linking crime scene characteristics and an offender\u2019s behavior to his or her characteristics and psychological profile. This paper seeks an efficient and systematic discovery of non-obvious and valuable patterns between variables from a large database of solved cases via a probabilistic network (PN) modeling approach. The PN structure can be used to extract behavioral patterns and to gain insight into what factors influence these behaviors. Thus, when a new case is being investigated and the profile variables are unknown because the offender has yet to be identified, the observed crime scene variables are used to infer the unknown variables based on their connections in the structure and the corresponding numerical (probabilistic) weights. The objective is to produce a more systematic and empirical approach to profiling, and to use the resulting PN model as a decision tool. Keywords-component; Modeling, criminal profiling, criminal behavior, probabilistic network, Bayes Rule", "target": "Modeling of Human Criminal Behavior using Probabilistic Networks"}
{"id": "task1540-46078e19589c4191a62e259fd2c191d6", "input": "We frame Question Answering as a Reinforcement Learning task, an approach that we call Active Question Answering. We propose an agent that sits between the user and a black box question-answering system an which learns to reformulate questions to elicit the best possible answers. The agent probes the system with, potentially many, natural language reformulations of an initial question and aggregates the returned evidence to yield the best answer. The reformulation system is trained end-to-end to maximize answer quality using policy gradient. We evaluate on SearchQA, a dataset of complex questions extracted from Jeopardy!. Our agent improves F1 by 11% over a state-of-the-art base model that uses the original question/answer pairs.", "target": "Ask the Right Questions: Active Question Reformulation with Reinforcement Learning"}
{"id": "task1540-72f6970c0cdb4c0d99b9966205c95bb2", "input": "In this paper, we describe a dataset relating to cellular and physical conditions of patients who are operated upon to remove colorectal tumours. This data provides a unique insight into immunological status at the point of tumour removal, tumour classification and post-operative survival. We build on existing research on clustering and machine learning facets of this data to demonstrate a role for an ensemble approach to highlighting patients with clearer prognosis parameters. Results for survival prediction using 3 different approaches are shown for a subset of the data which is most difficult to model. The performance of each model individually is compared with subsets of the data where some agreement is reached for multiple models. Significant improvements in model accuracy on an unseen test set can be achieved for patients where agreement between models is achieved. Keywords\u2014ensemble learning; anti-learning; colorectal cancer.", "target": "Ensemble Learning of Colorectal Cancer Survival Rates"}
{"id": "task1540-768cd49df94c41b6a2b54a52899aa20d", "input": "In most practical problems of classifier learning, the training data suffers from the label noise. Hence, it is important to understand how robust is a learning algorithm to such label noise. This paper presents some theoretical analysis to show that many popular decision tree algorithms are robust to symmetric label noise under large sample size. We also present some sample complexity results which provide some bounds on the sample size for the robustness to hold with a high probability. Through extensive simulations we illustrate this robustness.", "target": "On the Robustness of Decision Tree Learning under Label Noise"}
{"id": "task1540-a1d9744b5af24645a79a13516d6bed1c", "input": "A significant performance reduction is often observed in speech recognition when the rate of speech (ROS) is too low or too high. Most of present approaches to addressing the ROS variation focus on the change of speech signals in dynamic properties caused by ROS, and accordingly modify the dynamic model, e.g., the transition probabilities of the hidden Markov model (HMM). However, an abnormal ROS changes not only the dynamic but also the static property of speech signals, and thus can not be compensated for purely by modifying the dynamic model. This paper proposes an ROS learning approach based on deep neural networks (DNN), which involves an ROS feature as the input of the DNN model and so the spectrum distortion caused by ROS can be learned and compensated for. The experimental results show that this approach can deliver better performance for too slow and too fast utterances, demonstrating our conjecture that ROS impacts both the dynamic and the static property of speech. In addition, the proposed approach can be combined with the conventional HMM transition adaptation method, offering additional performance gains.", "target": "Learning Speech Rate in Speech Recognition"}
{"id": "task1540-1aa34cff49fa4908bbf2d2dab9267c16", "input": "Many real-world applications require robust algorithms to learn point processes based on a type of incomplete data \u2014 the so-called short doublycensored (SDC) event sequences. We study this critical problem of quantitative asynchronous event sequence analysis under the framework of Hawkes processes by leveraging the idea of data synthesis. Given SDC event sequences observed in a variety of time intervals, we propose a sampling-stitching data synthesis method \u2014 sampling predecessors and successors for each SDC event sequence from potential candidates and stitching them together to synthesize long training sequences. The rationality and the feasibility of our method are discussed in terms of arguments based on likelihood. Experiments on both synthetic and real-world data demonstrate that the proposed data synthesis method improves learning results indeed for both timeinvariant and time-varying Hawkes processes.", "target": "Learning Hawkes Processes from Short Doubly-Censored Event Sequences"}
{"id": "task1540-f89ab245e89d4b9d9f7e1185545c4dea", "input": "We describe the class of convexified convolutional neural networks (CCNNs), which capture the parameter sharing of convolutional neural networks in a convex manner. By representing the nonlinear convolutional filters as vectors in a reproducing kernel Hilbert space, the CNN parameters can be represented as a low-rank matrix, which can be relaxed to obtain a convex optimization problem. For learning two-layer convolutional neural networks, we prove that the generalization error obtained by a convexified CNN converges to that of the best possible CNN. For learning deeper networks, we train CCNNs in a layer-wise manner. Empirically, CCNNs achieve performance competitive with CNNs trained by backpropagation, SVMs, fully-connected neural networks, stacked denoising auto-encoders, and other baseline methods.", "target": "Convexified Convolutional Neural Networks"}
{"id": "task1540-e36165cd993c4c6a87e981fc48931e7d", "input": "Jake Ryland Williams, \u2217 James P. Bagrow, \u2020 Andrew J. Reagan, \u2021 Sharon E. Alajajian, \u00a7 Christopher M. Danforth, \u00b6 and Peter Sheridan Dodds \u2217\u2217 School of Information, University of California, Berkeley 102 South Hall #4600 Berkeley, CA 94720-4600. Department of Mathematics & Statistics, Vermont Complex Systems Center, Computational Story Lab, & the Vermont Advanced Computing Core, The University of Vermont, Burlington, VT 05401. (Dated: September 29, 2017)", "target": "Selection models of language production support informed text partitioning: an intuitive and practical, bag-of-phrases framework for text analysis"}
{"id": "task1540-8f77c3ccf9034f5899684faaf1e334b9", "input": "We introduce the Dynamic Capacity Network (DCN), a neural network that can adaptively assign its capacity across different portions of the input data. This is achieved by combining modules of two types: low-capacity subnetworks and high-capacity sub-networks. The low-capacity sub-networks are applied across most of the input, but also provide a guide to select a few portions of the input on which to apply the high-capacity sub-networks. The selection is made using a novel gradient-based attention mechanism, that efficiently identifies input regions for which the DCN\u2019s output is most sensitive and to which we should devote more capacity. We focus our empirical evaluation on the Cluttered MNIST and SVHN image datasets. Our findings indicate that DCNs are able to drastically reduce the number of computations, compared to traditional convolutional neural networks, while maintaining similar or even better performance.", "target": "Dynamic Capacity Networks"}
{"id": "task1540-1340950c9c2447ac9d0d065c06ce0e57", "input": "We address the problem of planning collision-free paths for multiple agents using optimization methods known as proximal algorithms. Recently this approach was explored in Bento et al. (2013), which demonstrated its ease of parallelization and decentralization, the speed with which the algorithms generate good quality solutions, and its ability to incorporate different proximal operators, each ensuring that paths satisfy a desired property. Unfortunately, the operators derived only apply to paths in 2D and require that any intermediate waypoints we might want agents to follow be preassigned to specific agents, limiting their range of applicability. In this paper we resolve these limitations. We introduce new operators to deal with agents moving in arbitrary dimensions that are faster to compute than their 2D predecessors and we introduce landmarks, spacetime positions that are automatically assigned to the set of agents under different optimality criteria. Finally, we report the performance of the new operators in several numerical experiments.", "target": "Proximal Operators for Multi-Agent Path Planning"}
{"id": "task1540-c8491bcfacfb405da344842dff991cc3", "input": "The project of the Ontology Web Search Engine is presented in this paper. The main purpose of this paper is to develop such a project that can be easily implemented. Ontology Web Search Engine is software to look for and index ontologies in the Web. OWL (Web Ontology Languages) ontologies are meant, and they are necessary for the functioning of the SWES (Semantic Web Expert System). SWES is an expert system that will use found ontologies from the Web, generating rules from them, and will supplement its knowledge base with these generated rules. It is expected that the SWES will serve as a universal expert system for the average user.", "target": "TOWARDS THE ONTOLOGY WEB SEARCH ENGINE"}
{"id": "task1540-34ede2970705461b87f83f664c6be10c", "input": "This article presents an agent architecture for controlling an autonomous agent in stochastic environments. The architecture combines the partially observable Markov decision process (POMDP) model with the belief-desire-intention (BDI) framework. The Hybrid POMDP-BDI agent architecture takes the best features from the two approaches, that is, the online generation of reward-maximizing courses of action from POMDP theory, and sophisticated multiple goal management from BDI theory. We introduce the advances made since the introduction of the basic architecture, including (i) the ability to pursue multiple goals simultaneously and (ii) a plan library for storing pre-written plans and for storing recently generated plans for future reuse. A version of the architecture without the plan library is implemented and is evaluated using simulations. The results of the simulation experiments indicate that the approach is feasible.", "target": "A Hybrid POMDP-BDI Agent Architecture with Online Stochastic Planning and Plan Caching"}
{"id": "task1540-d40ef7c5847f449f97c2eea94406d81b", "input": "A large body of work in machine learning has focused on the problem of learning a close approximation to an underlying combinatorial function, given a small set of labeled examples. However, for real-valued functions, cardinal labels might not be accessible, or it may be difficult for an expert to consistently assign real-valued labels over the entire set of examples. For instance, it is notoriously hard for consumers to reliably assign values to bundles of merchandise. Instead, it might be much easier for a consumer to report which of two bundles she likes better. With this motivation in mind, we consider an alternative learning model, wherein the algorithm must learn the underlying function up to pairwise comparisons, from pairwise comparisons. In this model, we present a series of novel algorithms that learn over a wide variety of combinatorial function classes. These range from graph functions to broad classes of valuation functions that are fundamentally important in microeconomic theory, the analysis of social networks, and machine learning, such as coverage, submodular, XOS, and subadditive functions, as well as functions with sparse Fourier support.", "target": "Learning Combinatorial Functions from Pairwise Comparisons"}
{"id": "task1540-86fb89e3794d47b3840e5996784b72fc", "input": "Reducing the amount of human supervision is a key problem in machine learning and a natural approach is that of exploiting the relations (structure) among different tasks. This is the idea at the core of multi-task learning. In this context a fundamental question is how to incorporate the tasks structure in the learning problem. We tackle this question by studying a general computational framework that allows to encode a-priori knowledge of the tasks structure in the form of a convex penalty; in this setting a variety of previously proposed methods can be recovered as special cases, including linear and non-linear approaches. Within this framework, we show that tasks and their structure can be efficiently learned considering a convex optimization problem that can be approached by means of block coordinate methods such as alternating minimization and for which we prove convergence to the global minimum.", "target": "Convex Learning of Multiple Tasks and their Structure"}
{"id": "task1540-77a70744840b42379e28192d087889bb", "input": "In the election of a hierarchical clustering method, theoretic properties may give some insight to determine which method is the most suitable to treat a clustering problem. Herein, we study some basic properties of two hierarchical clustering methods: \u03b1-unchaining single linkage or SL(\u03b1) and a modified version of this one, SL\u2217(\u03b1). We compare the results with the properties satisfied by the classical linkage-based hierarchical clustering methods.", "target": "ON THE PROPERTIES OF \u03b1-UNCHAINING SINGLE LINKAGE HIERARCHICAL CLUSTERING"}
{"id": "task1540-aaea8d983a164fd29496d3f22a0ed929", "input": "Automated answering of natural language questions is an interesting and useful problem to solve. Question answering (QA) systems often perform information retrieval at an initial stage. Information retrieval (IR) performance, provided by engines such as Lucene, places a bound on overall system performance. For example, no answer bearing documents are retrieved at low ranks for almost 40% of questions. In this paper, answer texts from previous QA evaluations held as part of the Text REtrieval Conferences (TREC) are paired with queries and analysed in an attempt to identify performance-enhancing words. These words are then used to evaluate the performance of a query expansion method. Data driven extension words were found to help in over 70% of difficult questions. These words can be used to improve and evaluate query expansion methods. Simple blind relevance feedback (RF) was correctly predicted as unlikely to help overall performance, and an possible explanation is provided for its low value in IR for QA.", "target": "A Data Driven Approach to Query Expansion in Question Answering"}
{"id": "task1540-ee904a0d2e024391bb7d2208fb1d8ade", "input": "English to Indian language machine translation poses the challenge of structural and morphological divergence. This paper describes English to Indian language statistical machine translation using pre-ordering and suffix separation. The pre-ordering uses rules to transfer the structure of the source sentences prior to training and translation. This syntactic restructuring helps statistical machine translation to tackle the structural divergence and hence better translation quality. The suffix separation is used to tackle the morphological divergence between English and highly agglutinative Indian languages. We demonstrate that the use of pre-ordering and suffix separation helps in improving the quality of English to Indian Language machine translation.", "target": "MTIL17: English to Indian Langauge Statistical Machine Translation"}
{"id": "task1540-fff6584e160e4c848a6a5894b07abac7", "input": "Mean-field variational inference is a method for approximate Bayesian posterior inference. It approximates a full posterior distribution with a factorized set of distributions by maximizing a lower bound on the marginal likelihood. This requires the ability to integrate a sum of terms in the log joint likelihood using this factorized distribution. Often not all integrals are in closed form, which is typically handled by using a lower bound. We present an alternative algorithm based on stochastic optimization that allows for direct optimization of the variational lower bound. This method uses control variates to reduce the variance of the stochastic search gradient, in which existing lower bounds can play an important role. We demonstrate the approach on two non-conjugate models: logistic regression and an approximation to the HDP.", "target": "Variational Bayesian Inference with Stochastic Search"}
{"id": "task1540-0b288ace543b41238d76b582f73ba271", "input": "With the release of SentiWordNet 3.0 the related Web interface has been restyled and improved in order to allow users to submit feedback on the SentiWordNet entries, in the form of the suggestion of alternative triplets of values for an entry. This paper reports on the release of the user feedback collected so far and on the plans for the future.", "target": "The User Feedback on SentiWordNet"}
{"id": "task1540-7028fbf5234949b39a4309e780034129", "input": "Artifact-centric models for business processes recently raised a lot of attention as they manage to combine structural (i.e. data related) with dynamical (i.e. process related) aspects in a seamless way. This developed in parallel with declarative approaches for modelling processes, where activities are not burdened by over-specified constrains like in traditional process-centric approaches, but try to adapt the internal system to the humans involved and the input they receive. In this paper, we try to merge these two aspects by proposing a framework aimed at describing rich business domains through Description Logic-based ontologies, and where a set of actions allows the system to evolve by modifying such ontologies. We then propose an evolution of such framework which represents a viable and formal environment to develop decision making and planning techniques for DL-based artifactcentric business domains.", "target": "Optimizations for Decision Making and Planning in Description Logic Based Dynamic Knowledge Bases"}
{"id": "task1540-913b614c8f934afabbab880eeefc8d4d", "input": "We describe a simple scheme that allows an agent to explore its environment in an unsupervised manner. Our scheme pits two versions of the same agent, Alice and Bob, against one another. Alice proposes a task for Bob to complete; and then Bob attempts to complete the task. In this work we will focus on (nearly) reversible environments, or environments that can be reset, and Alice will \u201cpropose\u201d the task by running a set of actions and then Bob must partially undo, or repeat them, respectively. Via an appropriate reward structure, Alice and Bob automatically generate a curriculum of exploration, enabling unsupervised training of the agent. When deployed on an RL task within the environment, this unsupervised training reduces the number of episodes needed to learn.", "target": "Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play"}
{"id": "task1540-4dea9013c533438f8b863c9bc6cb69d9", "input": "This paper presents the input convex neural network architecture. These are scalar-valued (potentially deep) neural networks with constraints on the network parameters such that the output of the network is a convex function of (some of) the inputs. The networks allow for efficient inference via optimization over some inputs to the network given others, and can be applied to settings including structured prediction, data imputation, reinforcement learning, and others. In this paper we lay the basic groundwork for these models, proposing methods for inference, optimization and learning, and analyze their representational power. We show that many existing neural network architectures can be made input-convex with only minor modification, and develop specialized optimization algorithms tailored to this setting. Finally, we highlight the performance of the methods on multi-label prediction, image completion, and reinforcement learning problems, where we show improvement over the existing state of the art in many cases.", "target": "Input Convex Neural Networks"}
{"id": "task1540-c656d09d4304475aa0b846425f951b75", "input": "While recent neural machine translation approaches have delivered state-of-the-art performance for resource-rich language pairs, they suffer from the data scarcity problem for resource-scarce language pairs. Although this problem can be alleviated by exploiting a pivot language to bridge the source and target languages, the source-to-pivot and pivot-to-target translation models are usually independently trained. In this work, we introduce a joint training algorithm for pivot-based neural machine translation. We propose three methods to connect the two models and enable them to interact with each other during training. Experiments on Europarl and WMT corpora show that joint training of source-to-pivot and pivot-to-target models leads to significant improvements over independent training across various languages.", "target": "Joint Training for Pivot-based Neural Machine Translation"}
{"id": "task1540-9a4db66a25f6449db43282104471a8a8", "input": "This paper presents a model based on an hybrid system to numerically simulate the climbing phase of an aircraft. This model is then used within a trajectory prediction tool. Finally, the Covariance Matrix Adaptation Evolution Strategy (CMA-ES) optimization algorithm is used to tune five selected parameters, and thus improve the accuracy of the model. Incorporated within a trajectory prediction tool, this model can be used to derive the order of magnitude of the prediction error over time, and thus the domain of validity of the trajectory prediction. A first validation experiment of the proposed model is based on the errors along time for a one-time trajectory prediction at the take off of the flight with respect to the default values of the theoretical BADA model. This experiment, assuming complete information, also shows the limit of the model. A second experiment part presents an on-line trajectory prediction, in which the prediction is continuously updated based on the current aircraft position. This approach raises several issues, for which improvements of the basic model are proposed, and the resulting trajectory prediction tool shows statistically significantly more accurate results than those of the default model.", "target": "Online Learning for Ground Trajectory Prediction"}
{"id": "task1540-d960901968db4b51b6827f7994ce4246", "input": "Deep Gaussian processes (DGPs) are multi-layer hierarchical generalisations of Gaussian processes (GPs) and are formally equivalent to neural networks with multiple, infinitely wide hidden layers. DGPs are nonparametric probabilistic models and as such are arguably more flexible, have a greater capacity to generalise, and provide better calibrated uncertainty estimates than alternative deep models. This paper develops a new approximate Bayesian learning scheme that enables DGPs to be applied to a range of medium to large scale regression problems for the first time. The new method uses an approximate Expectation Propagation procedure and a novel and efficient extension of the probabilistic backpropagation algorithm for learning. We evaluate the new method for non-linear regression on eleven real-world datasets, showing that it always outperforms GP regression and is almost always better than state-of-the-art deterministic and sampling-based approximate inference methods for Bayesian neural networks. As a by-product, this work provides a comprehensive analysis of six approximate Bayesian methods for training neural networks.", "target": "Deep Gaussian Processes for Regression using Approximate Expectation Propagation"}
{"id": "task1540-9bc5bae4e5124f6ab12d51597008d647", "input": "We describe a dynamic programming algorithm for computing the marginal distribution of discrete probabilistic programs. This algorithm takes a functional interpreter for an arbitrary probabilistic programming language and turns it into an efficient marginalizer. Because direct caching of sub-distributions is impossible in the presence of recursion, we build a graph of dependencies between sub-distributions. This factored sum-product network makes (potentially cyclic) dependencies between subproblems explicit, and corresponds to a system of equations for the marginal distribution. We solve these equations by fixed-point iteration in topological order. We illustrate this algorithm on examples used in teaching probabilistic models, computational cognitive science research, and game theory.", "target": "A Dynamic Programming Algorithm for Inference in Recursive Probabilistic Programs"}
{"id": "task1540-593662931d0a4e24a0b213eea149eae6", "input": "This paper focuses on unsupervised modeling of morphological families, collectively comprising a forest over the language vocabulary. This formulation enables us to capture edgewise properties reflecting single-step morphological derivations, along with global distributional properties of the entire forest. These global properties constrain the size of the affix set and encourage formation of tight morphological families. The resulting objective is solved using Integer Linear Programming (ILP) paired with contrastive estimation. We train the model by alternating between optimizing the local log-linear model and the global ILP objective. We evaluate our system on three tasks: root detection, clustering of morphological families and segmentation. Our experiments demonstrate that our model yields consistent gains in all three tasks compared with the best published results.1", "target": "Unsupervised Learning of Morphological Forests"}
{"id": "task1540-5bc7d33f1b524b2ab2d1f12194bc21d6", "input": "The problem of inferring an inductive invariant for verifying program safety can be formulated in terms of binary classification. This is a standard problem in machine learning: given a sample of good and bad points, one is asked to find a classifier that generalizes from the sample and separates the two sets. Here, the good points are the reachable states of the program, and the bad points are those that reach a safety property violation. Thus, a learned classifier is a candidate invariant. In this paper, we propose a new algorithm that uses decision trees to learn candidate invariants in the form of arbitrary Boolean combinations of numerical inequalities. We have used our algorithm to verify C programs taken from the literature. The algorithm is able to infer safe invariants for a range of challenging benchmarks and compares favorably to other ML-based invariant inference techniques. In particular, it scales well to large sample sets.", "target": "Learning Invariants using Decision Trees"}
{"id": "task1540-97df4508968b47a58f7d0f7ed5d8119f", "input": "Text categorization is the process of grouping documents into categories based on their contents. This process is important to make information retrieval easier, and it became more important due to the huge textual information available online. The main problem in text categorization is how to improve the classification accuracy. Although Arabic text categorization is a new promising field, there are a few researches in this field. This paper proposes a new method for Arabic text categorization using vector evaluation. The proposed method uses a categorized Arabic documents corpus, and then the weights of the tested document's words are calculated to determine the document keywords which will be compared with the keywords of the corpus categorizes to determine the tested document's best category.", "target": "ARABIC TEXT CATEGORIZATION ALGORITHM USING VECTOR EVALUATION METHOD"}
{"id": "task1540-67fedecace364dfc983aab689230aca6", "input": "We introduce an exceptionally simple gated recurrent neural network (RNN) that achieves performance comparable to well-known gated architectures, such as LSTMs and GRUs, on the word-level language modeling task. We prove that our model has simple, predicable and non-chaotic dynamics. This stands in stark contrast to more standard gated architectures, whose underlying dynamical systems exhibit chaotic behavior.", "target": "A RECURRENT NEURAL NETWORK WITHOUT CHAOS"}
{"id": "task1540-5ec1caf974ae4998b1e9ac35a8c6f7d9", "input": "Imputation of missing attribute values in medical datasets for extracting hidden knowledge from medical datasets is an interesting research topic of interest which is very challenging. One cannot eliminate missing values in medical records. The reason may be because some tests may not been conducted as they are cost effective, values missed when conducting clinical trials, values may not have been recorded to name some of the reasons. Data mining researchers have been proposing various approaches to find and impute missing values to increase classification accuracies so that disease may be predicted accurately. In this paper, we propose a novel imputation approach for imputation of missing values and performing classification after fixing missing values. The approach is based on clustering concept and aims at dimensionality reduction of the records. The case study discussed shows that missing values can be fixed and imputed efficiently by achieving dimensionality reduction. The importance of proposed approach for classification is visible in the case study which assigns single class label in contrary to multi-label assignment if dimensionality reduction is not performed. Keywords\u2014 imputation; missing values; prediction; nearest neighbor, cluster, medical records, dimensionality reduction", "target": "An Innovative Imputation and Classification Approach for Accurate Disease Prediction"}
{"id": "task1540-3e5f16415bdc4be9943b5582dfcd174e", "input": "We present probabilistic logic programming un\u00ad der inheritance with overriding. This approach is based on new notions of entailment for reasoning with conditional constraints, which are obtained from the classical notion of logical entailment by adding inheritance with overriding. This is done by using recent approaches to probabilistic de\u00ad fault reasoning with conditional constraints. We analyze the semantic properties of the new en\u00ad tailment relations. We also present algorithms for probabilistic logic prograrruning under inher\u00ad itance with overriding, and we analyze its com\u00ad plexity in the propositional case.", "target": "Probabilistic Logic Programming under Inheritance with Overriding"}
{"id": "task1540-f20bdd65a8c545b0a26deb53b913e1f3", "input": "The scale of modern datasets necessitates the development of efficient distributed optimization methods for machine learning. We present a general-purpose framework for the distributed environment, CoCoA, that has an efficient communication scheme and is applicable to a wide variety of problems in machine learning and signal processing. We extend the framework to cover general non-strongly convex regularizers, including L1-regularized problems like lasso, sparse logistic regression, and elastic net regularization, and show how earlier work can be derived as a special case. We provide convergence guarantees for the class of convex regularized loss minimization objectives, leveraging a novel approach in handling non-strongly convex regularizers and non-smooth loss functions. The resulting framework has markedly improved performance over state-of-the-art methods, as we illustrate with an extensive set of experiments on real distributed datasets.", "target": "CoCoA: A General Framework for Communication-Efficient Distributed Optimization"}
{"id": "task1540-b71f8bc65e6f4657b3bcbd12264c69ee", "input": "We show how to estimate a model\u2019s test error from unlabeled data, on distributions very different from the training distribution, while assuming only that certain conditional independencies are preserved between train and test. We do not need to assume that the optimal predictor is the same between train and test, or that the true distribution lies in any parametric family. We can also efficiently differentiate the error estimate to perform unsupervised discriminative learning. Our technical tool is the method of moments, which allows us to exploit conditional independencies in the absence of a fully-specified model. Our framework encompasses a large family of losses including the log and exponential loss, and extends to structured output settings such as hidden Markov models.", "target": "Unsupervised Risk Estimation Using Only Conditional Independence Structure"}
{"id": "task1540-0d96c4624eef49f0866dbf1c0d635d77", "input": "The use of alluring headlines (clickbait) to tempt the readers has become a growing practice nowadays. For the sake of existence in the highly competitive media industry, most of the on-line media including the mainstream ones, have started following this practice. Although the wide-spread practice of clickbait makes the reader\u2019s reliability on media vulnerable, a large scale analysis to reveal this fact is still absent. In this paper, we analyze 1.67 million Facebook posts created by 153 media organizations to understand the extent of clickbait practice, its impact and user engagement by using our own developed clickbait detection model. The model uses distributed sub-word embeddings learned from a large corpus. The accuracy of the model is 98.3%. Powered with this model, we further study the distribution of topics in clickbait and non-clickbait contents.", "target": "Diving Deep into Clickbaits: Who Use Them to What Extents in Which Topics with What Effects?"}
{"id": "task1540-76584fd236e146f5b5e3493f9fb4b9cf", "input": "Scalable and effective exploration remains a key challenge in reinforcement learning (RL). While there are methods with optimality guarantees in the setting of discrete state and action spaces, these methods cannot be applied in high-dimensional deep RL scenarios. As such, most contemporary RL relies on simple heuristics such as -greedy exploration or adding Gaussian noise to the controls. This paper introduces Variational Information Maximizing Exploration (VIME), an exploration strategy based on maximization of information gain about the agent\u2019s belief of environment dynamics. We propose a practical implementation, using variational inference in Bayesian neural networks which efficiently handles continuous state and action spaces. VIME modifies the MDP reward function, and can be applied with several different underlying RL algorithms. We demonstrate that VIME achieves significantly better performance compared to heuristic exploration methods across a variety of continuous control tasks and algorithms, including tasks with very sparse rewards.", "target": "Curiosity-driven Exploration in Deep Reinforcement Learning via Bayesian Neural Networks"}
{"id": "task1540-ebf393d568dd4576998cacd50c25eef8", "input": "Compounding is a highly productive word-formation process in some languages that is often problematic for natural language processing applications. In this paper, we investigate whether distributional semantics in the form of word embeddings can enable a deeper, i.e., more knowledge-rich, processing of compounds than the standard string-based methods. We present an unsupervised approach that exploits regularities in the semantic vector space (based on analogies such as \u201cbookshop is to shop as bookshelf is to shelf\u201d) to produce compound analyses of high quality. A subsequent compound splitting algorithm based on these analyses is highly effective, particularly for ambiguous compounds. German to English machine translation experiments show that this semantic analogy-based compound splitter leads to better translations than a commonly used frequency-based method.", "target": "Splitting Compounds by Semantic Analogy"}
{"id": "task1540-37e049ec0e4d46d0ac5ab7741c063dd3", "input": "When faced with complex choices, users refine their own preference criteria as they explore the catalogue of options. In this paper we propose an approach to preference elicitation suited for this scenario. We extend Coactive Learning, which iteratively collects manipulative feedback, to optionally query example critiques. User critiques are integrated into the learning model by dynamically extending the feature space. Our formulation natively supports constructive learning tasks, where the option catalogue is generated on-the-fly. We present an upper bound on the average regret suffered by the learner. Our empirical analysis highlights the promise of", "target": "Coactive Critiquing: Elicitation of Preferences and Features"}
{"id": "task1540-97322de2230540ebba3e736e8fbd281c", "input": "We outline a method to estimate the value of computation for a flexible algorithm using em\u00ad pirical data. To determine a reasonable trade-off between cost and value, we build an empirical model of the value obtained through computa\u00ad tion, and apply this model to estimate the value of computation for quite different problems. In par\u00ad ticular, we investigate this trade-off for the prob\u00ad lem of constructing policies for decision prob\u00ad lems represented as influence diagrams. We show how two features of our anytime algorithm pro\u00ad vide reasonable estimates of the value of compu\u00ad tation in this domain.", "target": "Estimating the Value of Computation in Flexible Information Refinement"}
{"id": "task1540-f7e6c6078033400296731f63619d0c98", "input": "The paper considers the class of information systems capable of solving heuristic problems on basis of formal theory that was termed modal and vector theory of formal intelligent systems (FIS). The paper justifies the construction of FIS resolution algorithm, defines the main features of these systems and proves theorems that underlie the theory. The principle of representation diversity of FIS construction is formulated. The paper deals with the main principles of constructing and functioning formal intelligent system (FIS) on basis of FIS modal and vector theory. The following phenomena are considered: modular architecture of FIS presentation sub-system, algorithms of data processing at every step of the stage of creating presentations. Besides the paper suggests the structure of neural elements, i.e. zone detectors and processors that are the basis for FIS construction. Subjects: Artificial Intelligence (cs. AI)", "target": "Principles of modal and vector theory of formal intelligence systems"}
{"id": "task1540-46e32004ef884698bd6919ffac6cac06", "input": "Domain adaptation, and transfer learning more generally, seeks to remedy the problem created when training and testing datasets are generated by different distributions. In this work, we introduce a new unsupervised domain adaptation algorithm for when there are multiple sources available to a learner. Our technique assigns a rough labeling on the target samples, then uses it to learn a transformation that aligns the two datasets before final classification. In this article we give a convenient implementation of our method, show several experiments using it, and compare it to other methods commonly used in the field.", "target": "Multi-Source Domain Adaptation Using Approximate Label Matching"}
{"id": "task1540-9e325969154642bba2d085798b56b071", "input": "We study the best-arm identification problem in linear bandit, where the rewards of the arms depend linearly on an unknown parameter \u03b8 and the objective is to return the arm with the largest reward. We characterize the complexity of the problem and introduce sample allocation strategies that pull arms to identify the best arm with a fixed confidence, while minimizing the sample budget. In particular, we show the importance of exploiting the global linear structure to improve the estimate of the reward of near-optimal arms. We analyze the proposed strategies and compare their empirical performance. Finally, as a by-product of our analysis, we point out the connection to the G-optimality criterion used in optimal experimental design.", "target": "Best-Arm Identification in Linear Bandits"}
{"id": "task1540-495519577ce44e3db44483bfbd9e0ddd", "input": "This software project based paper is for a vision of the near future in which computer interaction is characterised by natural face-to-face conversations with lifelike characters that speak, emote, and gesture. The first step is speech. The dream of a true virtual reality, a complete human-computer interaction system will not come true unless we try to give some perception to machine and make it perceive the outside world as humans communicate with each other. This software project is under development for \u201clistening and replying machine (Computer) through speech\u201d. The Speech interface is developed to convert speech input into some parametric form (Speech-to-Text) for further processing and the results, text output to speech synthesis (Text-to-Speech)", "target": "Speech_Urmila"}
{"id": "task1540-44f3e11b850a4762a09681a4d7a84a6a", "input": "Subjective questions such as \u2018does neymar dive\u2019, or \u2018is clinton lying\u2019, or \u2018is trump a fascist\u2019, are popular queries to web search engines, as can be seen by autocompletion suggestions on Google, Yahoo and Bing. In the era of cognitive computing, beyond search, they could be handled as hypotheses issued for evaluation. Our vision is to leverage on unstructured data and metadata of the rich user-generated multimedia that is often shared as material evidence in favor or against hypotheses in social media platforms. In this paper we present two preliminary experiments along those lines and discuss challenges for a cognitive computing system that collects material evidence from user-generated multimedia towards aggregating it into some form of collective decision on the hypothesis. Keywords-Material evidence; User-generated multimedia; Social media hypothesis management; Cognitive computing. In: Proc. of the 1st Workshop on Multimedia Support for Decision-Making Processes, at IEEE Intl. Symposium on Multimedia (ISM\u201916), San Jose, CA, 2016.", "target": "Show me the material evidence \u2014 Initial experiments on evaluating hypotheses from user-generated multimedia data"}
{"id": "task1540-2657b04aa0534bd7be115782f58a6514", "input": "The paper continues the investigation of Poincare and Russel\u2019s Vicious Circle Principle (VCP) in the context of the design of logic programming languages with sets. We expand previously introduced language Alog with aggregates by allowing infinite sets and several additional set related constructs useful for knowledge representation and teaching. In addition, we propose an alternative formalization of the original VCP and incorporate it into the semantics of new language, Slog, which allows more liberal construction of sets and their use in programming rules. We show that, for programs without disjunction and infinite sets, the formal semantics of aggregates in Slog coincides with that of several other known languages. Their intuitive and formal semantics, however, are based on quite different ideas and seem to be more involved than that of Slog.", "target": "Vicious Circle Principle and Formation of Sets in ASP Based Languages"}
{"id": "task1540-d6861fb741314fbd99faefbd2171b51d", "input": "Convolutional Neural Networks (CNNs) can be shifted across 2D images or 3D videos to segment them. They have a fixed input size and typically perceive only small local contexts of the pixels to be classified as foreground or background. In contrast, Multi-Dimensional Recurrent NNs (MD-RNNs) can perceive the entire spatio-temporal context of each pixel in a few sweeps through all pixels, especially when the RNN is a Long Short-Term Memory (LSTM). Despite these theoretical advantages, however, unlike CNNs, previous MD-LSTM variants were hard to parallelize on GPUs. Here we re-arrange the traditional cuboid order of computations in MD-LSTM in pyramidal fashion. The resulting PyraMiD-LSTM is easy to parallelize, especially for 3D data such as stacks of brain slice images. PyraMiD-LSTM achieved best known pixel-wise brain image segmentation results on MRBrainS13 (and competitive results on EM-ISBI12).", "target": "Parallel Multi-Dimensional LSTM, With Application to Fast Biomedical Volumetric Image Segmentation"}
{"id": "task1540-884266162af6432ba824c5fa8caa84a7", "input": "Maximum Inner Product Search (MIPS) is an important task in many machine learning applications such as the prediction phase of a low-rank matrix factorization model for a recommender system. There have been some works on how to perform MIPS in sub-linear time recently. However, most of them do not have the flexibility to control the trade-off between search efficient and search quality. In this paper, we study the MIPS problem with a computational budget. By carefully studying the problem structure of MIPS, we develop a novel Greedy-MIPS algorithm, which can handle budgeted MIPS by design. While simple and intuitive, Greedy-MIPS yields surprisingly superior performance compared to state-of-the-art approaches. As a specific example, on a candidate set containing half a million vectors of dimension 200, Greedy-MIPS runs 200x faster than the naive approach while yielding search results with the top-5 precision greater than 75%.", "target": "A Greedy Approach for Budgeted Maximum Inner Product Search"}
{"id": "task1540-67c6be8069cf4e54b461cfce36858d32", "input": "We propose a method for embedding twodimensional locations in a continuous vector space using a neural network-based model incorporating mixtures of Gaussian distributions, presenting two model variants for text-based geolocation and lexical dialectology. Evaluated over Twitter data, the proposed model outperforms conventional regression-based geolocation and provides a better estimate of uncertainty. We also show the effectiveness of the representation for predicting words from location in lexical dialectology, and evaluate it using the DARE dataset.", "target": "Continuous Representation of Location for Geolocation and Lexical Dialectology using Mixture Density Networks"}
{"id": "task1540-17c41827abb8490dbc0263f73921db88", "input": "We extend Stochastic Gradient Variational Bayes to perform posterior inference for the weights of Stick-Breaking processes. This development allows us to define a Stick-Breaking Variational Autoencoder (SB-VAE), a Bayesian nonparametric version of the variational autoencoder that has a latent representation with stochastic dimensionality. We experimentally demonstrate that the SB-VAE, and a semisupervised variant, learn highly discriminative latent representations that often outperform the Gaussian VAE\u2019s.", "target": "STICK-BREAKING VARIATIONAL AUTOENCODERS"}
{"id": "task1540-0e937c59770c492689628ef6d4db4117", "input": "We propose a new training method for a feedforward neural network having the activation functions with the geometric contraction property. The method consists of constructing a new functional that is less nonlinear in comparison with the classical functional by removing the nonlinearity of the activation functions from the output layer. We validate this new method by a series of experiments that show an improved learning speed and also a better classification error. MSC: 92B20, 68T05", "target": "A New Training Method for Feedforward Neural Networks Based on Geometric Contraction Property of Activation Functions"}
{"id": "task1540-ad9c19a3ef4a41929914a67a86b0ade9", "input": "Standard LDA model suffers the problem that the topic assignment of each word is independent and word correlation hence is neglected. To address this problem, in this paper, we propose a model called Word Related Latent Dirichlet Allocation (WR-LDA) by incorporating word correlation into LDA topic models. This leads to new capabilities that standard LDA model does not have such as estimating infrequently occurring words or multi-language topic modeling. Experimental results demonstrate the effectiveness of our model compared with standard LDA.", "target": "Modeling Word Relatedness in Latent Dirichlet Allocation"}
{"id": "task1540-f9052f252ac940b9aa09e0a3d7fdd095", "input": "The developpment of the Internet of Things (IoT) concept revives Responsive Environments (RE) technologies. Nowadays, the idea of a permanent connection between physical and digital world is technologically possible. The capillar Internet relates to the Internet extension into daily appliances such as they become actors of Internet like any hu-man. The parallel development of Machine-to-Machine communications and Arti cial Intelligence (AI) technics start a new area of cybernetic. This paper presents an approach for Cybernetic Organism (Cyborg) for RE based on Organic Computing (OC). In such approach, each appli-ance is a part of an autonomic system in order to control a physical environment. The underlying idea is that such systems must have self-x properties in order to adapt their behavior to external disturbances with a high-degree of autonomy.", "target": "TOWARD ORGANIC COMPUTING APPROACH FOR CYBERNETIC RESPONSIVE ENVIRONMENT"}
{"id": "task1540-54c85df71cfe4920bd760283f3665f43", "input": "String getQueueName(); abstract Serializable getCommandObject(Long workflowInstanceId,Serializable getCommandObject(Long workflowInstanceId, String taskName, String messageType, ExecutionContext context);", "target": "Automatic Structure Discovery for Large Source Code"}
{"id": "task1540-03898ba47e3542f0b6977310751a4348", "input": "In this paper we describe a deep network architecture that maps visual input to control actions for a robotic planar reaching task with 100% reliability in real-world trials. Our network is trained in simulation and fine-tuned with a limited number of real-world images. The policy search is guided by a kinematics-based controller (K-GPS), which works more effectively and efficiently than \u03b5-Greedy. A critical insight in our system is the need to introduce a bottleneck in the network between the perception and control networks, and to initially train these networks independently.", "target": "Vision-Based Reaching Using Modular Deep Networks: from Simulation to the Real World"}
{"id": "task1540-787a8a009b0a42119eba6e987b548b81", "input": "The Correlation Filter is an algorithm that trains a linear template to discriminate between images and their translations. It is well suited to object tracking because its formulation in the Fourier domain provides a fast solution, enabling the detector to be re-trained once per frame. Previous works that use the Correlation Filter, however, have adopted features that were either manually designed or trained for a different task. This work is the first to overcome this limitation by interpreting the Correlation Filter learner, which has a closed-form solution, as a differentiable layer in a deep neural network. This enables learning deep features that are tightly coupled to the Correlation Filter. Experiments illustrate that our method has the important practical benefit of allowing lightweight architectures to achieve state-of-the-art performance at high framerates.", "target": "End-to-end representation learning for Correlation Filter based tracking"}
{"id": "task1540-9c95be2258054a3f850fb5fd36f17fe0", "input": "While deep learning has led to remarkable advances across diverse applications, it struggles in domains where the data distribution changes over the course of learning. In stark contrast, biological neural networks continually adapt to changing domains, possibly by leveraging complex molecular machinery to solve many tasks simultaneously. In this study, we introduce intelligent synapses that bring some of this biological complexity into artificial neural networks. Each synapse accumulates task relevant information over time, and exploits this information to rapidly store new memories without forgetting old ones. We evaluate our approach on continual learning of classification tasks, and show that it dramatically reduces forgetting while maintaining computational efficiency.", "target": "Continual Learning Through Synaptic Intelligence"}
{"id": "task1540-9aa928d915d6497e803a9c23204eb647", "input": "In educational technology and learning sciences, there are multiple uses for a predictive model of whether a student will perform a task correctly or not. For example, an intelligent tutoring system may use such a model to estimate whether or not a student has mastered a skill. We analyze the significance of data recency in making such predictions, i.e., asking whether relatively more recent observations of a student\u2019s performance matter more than relatively older observations. We develop a new Recent-Performance Factors Analysis model that takes data recency into account. The new model significantly improves predictive accuracy over both existing logistic-regression performance models and over novel baseline models in evaluations on real-world and synthetic datasets. As a secondary contribution, we demonstrate how the widely used cross-validation with 0-1 loss is inferior to AIC and to cross-validation with L1 prediction error loss as a measure of model performance.", "target": "Predicting Performance During Tutoring with Models of Recent Performance"}
{"id": "task1540-81aa10499b25465fb230e3bacd17bb02", "input": "Despite outstanding success in vision amongst other domains, many of the recent deep learning approaches have evident drawbacks for robots. This manuscript surveys recent work in the literature that pertain to applying deep learning systems to the robotics domain, either as means of estimation or as a tool to resolve motor commands directly from raw percepts. These recent advances are only a piece to the puzzle. We suggest that deep learning as a tool alone is insufficient in building a unified framework to acquire general intelligence. For this reason, we complement our survey with insights from cognitive development and refer to ideas from classical control theory, producing an integrated direction for a lifelong learning architecture.", "target": "Towards Lifelong Self-Supervision: A Deep Learning Direction for Robotics"}
{"id": "task1540-fe34e45d3c11488b80e822851f8b2255", "input": "Belief revision is an operation that aims at modifying old beliefs so that they become consistent with new ones. The issue of belief revision has been studied in various formalisms, in particular, in qualitative algebras (QAs) in which the result is a disjunction of belief bases that is not necessarily representable in a QA. This motivates the study of belief revision in formalisms extending QAs, namely, their propositional closures: in such a closure, the result of belief revision belongs to the formalism. Moreover, this makes it possible to define a contraction operator thanks to the Harper identity. Belief revision in the propositional closure of QAs is studied, an algorithm for a family of revision operators is designed, and an opensource implementation is made freely available on the web.", "target": "Belief revision in the propositional closure of a qualitative algebra"}
{"id": "task1540-35016aaaa4124ebfb776efc3c821f657", "input": "In probabilistic logic entailments, even moderate size problems can yield linear constraint systems with so many variables that exact methods are impractical. This difficulty can be remedied in many cases of interest by introducing a three\u00ad valued logic (true, false, and \"don't care\"). The three-valued approach allows the construction of \"compressed\" constraint systems which have the same solution sets as their two-valued counterparts, but which may involve dramatically fewer variables. Techniques to calculate point estimates for the posterior probabilities of entailed sentences are discussed. 1. PROLIFERATION OF WORLDS An entailment problem in Nilsson's (1986) probabilistic logic derives an estimate for the prior probability of one sentence (hereafter, the \"target\") from the priors for a set of other (\"source\") sentences. The prior beliefs about the source sentences establish constraints of the form P=VW L wi = l Wi <': 0 sum over all \"worlds\"", "target": "Compressed Constraints in Probabilistic Logic and Their Revision"}
{"id": "task1540-301f43aedc294ee79eeb1f400d102da4", "input": "We present an approach to generate novel computer game levels that blend different game concepts in an unsupervised fashion. Our primary contribution is an analogical reasoning process to construct blends between level design models learned from gameplay videos. The models represent probabilistic relationships between elements in the game. An analogical reasoning process maps features between two models to produce blended models that can then generate new level chunks. As a proof-of-concept we train our system on the classic platformer game Super Mario Bros. due to its highlyregarded and well understood level design. We evaluate the extent to which the models represent stylistic level design knowledge and demonstrate the ability of our system to explain levels that were blended by human expert designers.", "target": "Learning to Blend Computer Game Levels"}
{"id": "task1540-f33189c1b47246cdb59b559232867856", "input": "We show how eye-tracking corpora can be used to improve sentence compression models, presenting a novel multi-task learning algorithm based on multi-layer LSTMs. We obtain performance competitive with or better than state-of-the-art approaches.", "target": "Improving sentence compression by learning to predict gaze"}
{"id": "task1540-8f6d95c9f8854b09b91f1613fd2d58c0", "input": "Arabic Documents Clustering is an important task for obtaining good results with the traditional Information Retrieval (IR) systems especially with the rapid growth of the number of online documents present in Arabic language. Documents clustering aim to automatically group similar documents in one cluster using different similarity/distance measures. This task is often affected by the documents length, useful information on the documents is often accompanied by a large amount of noise, and therefore it is necessary to eliminate this noise while keeping useful information to boost the performance of Documents clustering. In this paper, we propose to evaluate the impact of text summarization using the Latent Semantic Analysis Model on Arabic Documents Clustering in order to solve problems cited above, using five similarity/distance measures: Euclidean Distance, Cosine Similarity, Jaccard Coefficient, Pearson Correlation Coefficient and Averaged Kullback-Leibler Divergence, for two times: without and with stemming. Our experimental results indicate that our proposed approach effectively solves the problems of noisy information and documents length, and thus significantly improve the clustering performance.", "target": "SEMANTIC ANALYSIS TO ENHANCE ARABIC DOCUMENTS CLUSTERING"}
{"id": "task1540-65085d065ea84a489bc76a7659a78f5d", "input": "Academic researchers often need to face with a large collection of research papers in the literature. This problem may be even worse for postgraduate students who are new to a field and may not know where to start. To address this problem, we have developed an online catalog of research papers where the papers have been automatically categorized by a topic model. The catalog contains 7719 papers from the proceedings of two artificial intelligence conferences from 2000 to 2015. Rather than the commonly used Latent Dirichlet Allocation, we use a recently proposed method called hierarchical latent tree analysis for topic modeling. The resulting topic model contains a hierarchy of topics so that users can browse the topics from the top level to the bottom level. The topic model contains a manageable number of general topics at the top level and allows thousands of fine-grained topics at the bottom level. It also can detect topics that have emerged recently.", "target": "Topic Browsing for Research Papers with Hierarchical Latent Tree Analysis"}
{"id": "task1540-068917b902d942259395794985e3be9a", "input": "Although the CSP (constraint satisfaction problem) is NP-complete, even in the case when all constraints are binary, certain classes of instances are tractable. We study classes of instances defined by excluding subproblems. This approach has recently led to the discovery of novel tractable classes. The complete characterisation of all tractable classes defined by forbidding patterns (where a pattern is simply a compact representation of a set of subproblems) is a challenging problem. We demonstrate a dichotomy in the case of forbidden patterns consisting of either one or two constraints. This has allowed us to discover new tractable classes including, for example, a novel generalisation of 2SAT.", "target": "A Dichotomy for 2-Constraint Forbidden CSP Patterns"}
{"id": "task1540-770d175d99624a789d6dd646b33746c5", "input": "Universal induction is a crucial issue in AGI. Its practical applicability can be achieved by the choice of the reference machine or representation of algorithms agreed with the environment. This machine should be updatable for solving subsequent tasks more efficiently. We study this problem on an example of combinatory logic as the very simple Turing-complete reference machine, which enables modifying program representations by introducing different sets of primitive combinators. Genetic programming system is used to search for combinator expressions, which are easily decomposed into sub-expressions being recombined in crossover. Our experiments show that low-complexity induction or prediction tasks can be solved by the developed system (much more efficiently than using brute force); useful combinators can be revealed and included into the representation simplifying more difficult tasks. However, optimal sets of combinators depend on the specific task, so the reference machine should be adaptively chosen in coordination with the search engine.", "target": "Universal Induction with Varying Sets of Combinators"}
{"id": "task1540-d10f3d02d7c34228887fd386d4fa07ce", "input": "Over the past decade humans have experienced exponential growth in the use of online resources, in particular social media and microblogging websites such as Facebook, Twitter, YouTube and also mobile applications such as WhatsApp, Line, etc. Many companies have identified these resources as a rich mine of marketing knowledge. This knowledge provides valuable feedback which allows them to further develop the next generation of their product. In this paper, sentiment analysis of a product is performed by extracting tweets about that product and classifying the tweets showing it as positive and negative sentiment. The authors propose a hybrid approach which combines unsupervised learning in the form of K-means clustering to cluster the tweets and then performing supervised learning methods such as Decision Trees and Support Vector Machines for classification.", "target": "Improved Twitter Sentiment Prediction through \u2018Cluster-then-Predict Model\u2019"}
{"id": "task1540-0f929186dbf541b896176c945445f2a8", "input": "Monte Carlo sampling has become a major vehicle for approximate inference in Bayesian networks. In this paper, we investigate a fam\u00ad ily of related simulation approaches, known collectively as quasi-Monte Carlo methods based on deterministic low-discrepancy se\u00ad quences. We first outline several theoreti\u00ad cal aspects of deterministic low-discrepancy sequences, show three examples of such se\u00ad quences, and then discuss practical issues re\u00ad lated to applying them to belief updating in Bayesian networks. We propose an algorithm for selecting direction numbers for Sobol se\u00ad quence. Our experimental results show that low-discrepancy sequences (especially Sobol sequence) significantly improve the perfor\u00ad mance of simulation algorithms in Bayesian networks compared to Monte Carlo sampling.", "target": "Computational Investigation of Low-Discrepancy Sequences in Simulation Algorithms for Bayesian Networks"}
{"id": "task1540-c2815caf51fb4989ae2fd56529bd3a9e", "input": "The number of computers, tablets and smartphones is increasing rapidly, which entails the ownership and use of multiple devices to perform online tasks. As people move across devices to complete these tasks, their identities becomes fragmented. Understanding the usage and transition between those devices is essential to develop efficient applications in a multi-device world. In this paper we present a solution to deal with the cross-device identification of users based on semi-supervised machine learning methods to identify which cookies belong to an individual using a device. The method proposed in this paper scored third in the ICDM 2015 Drawbridge Cross-Device Connections challenge proving its good performance.", "target": "Cross-Device Tracking: Matching Devices and Cookies"}
{"id": "task1540-abcebdf6e6bb4559ab3ed74f45d85dae", "input": "While there has been substantial progress in factoid question-answering (QA), answering complex questions remains challenging, typically requiring both a large body of knowledge and inference techniques. Open Information Extraction (Open IE) provides a way to generate semi-structured knowledge for QA, but to date such knowledge has only been used to answer simple questions with retrievalbased methods. We overcome this limitation by presenting a method for reasoning with Open IE knowledge, allowing more complex questions to be handled. Using a recently proposed support graph optimization framework for QA, we develop a new inference model for Open IE, in particular one that can work effectively with multiple short facts, noise, and the relational structure of tuples. Our model significantly outperforms a state-of-the-art structured solver on complex questions of varying difficulty, while also removing the reliance on manually curated knowledge.", "target": "Answering Complex Questions Using Open Information Extraction"}
{"id": "task1540-0625f68492a94327a8df098572b725cb", "input": "Collaborative ltering is an e ective recommendation technique wherein the preference of an individual can potentially be predicted based on preferences of other members. Early algorithms often relied on the strong locality in the preference data, that is, it is enough to predict preference of a user on a particular item based on a small subset of other users with similar tastes or of other items with similar properties. More recently, dimensionality reduction techniques have proved to be equally competitive, and these are based on the co-occurrence patterns rather than locality. This paper explores and extends a probabilistic model known as Boltzmann Machine for collaborative ltering tasks. It seamlessly integrates both the similarity and cooccurrence in a principled manner. In particular, we study parameterisation options to deal with the ordinal nature of the preferences, and propose a joint modelling of both the user-based and item-based processes. Experiments on moderate and large-scale movie recommendation show that our framework rivals existing well-known methods.", "target": "Ordinal Boltzmann Machines for Collaborative Filtering"}
{"id": "task1540-7346ba6d53b74f8988deb9c7dc69d563", "input": "Algorithms that generate computer game content require game design knowledge. We present an approach to automatically learn game design knowledge for level design from gameplay videos. We further demonstrate how the acquired design knowledge can be used to generate sections of game levels. Our approach involves parsing video of people playing a game to detect the appearance of patterns of sprites and utilizing machine learning to build a probabilistic model of sprite placement. We show how rich game design information can be automatically parsed from gameplay videos and represented as a set of generative probabilistic models. We use Super Mario Bros. as a proof of concept. We evaluate our approach on a measure of playability and stylistic similarity to the original levels as represented in the gameplay videos.", "target": "Toward Game Level Generation from Gameplay Videos"}
{"id": "task1540-927c2320ca6e4ec7977cb9db4315cd0a", "input": "This work proposes a novel support vector machine (SVM) based robust automatic speech recognition (ASR) front-end that operates on an ensemble of the subband components of high-dimensional acoustic waveforms. The key issues of selecting the appropriate SVM kernels for classification in frequency subbands and the combination of individual subband classifiers using ensemble methods are addressed. The proposed front-end is compared with state-of-the-art ASR front-ends in terms of robustness to additive noise and linear filtering. Experiments performed on the TIMIT phoneme classification task demonstrate the benefits of the proposed subband based SVM front-end: it outperforms the standard cepstral front-end in the presence of noise and linear filtering for signal-to-noise ratio (SNR) below 12-dB. A combination of the proposed front-end with a conventional front-end such as MFCC yields further improvements over the individual front ends across the full range of noise levels.", "target": "A Subband-Based SVM Front-End for Robust ASR"}
{"id": "task1540-3774f2b164f74c6093be22085b9651c7", "input": "An algorithm for pose and motion estimation using corresponding features in omnidirectional images and a digital terrain map is proposed. In previous paper, such algorithm for regular camera was considered. Using a Digital Terrain (or Digital Elevation) Map (DTM/DEM) as a global reference enables recovering the absolute position and orientation of the camera. In order to do this, the DTM is used to formulate a constraint between corresponding features in two consecutive frames. In this paper, these constraints are extended to handle non-central projection, as is the case with many omnidirectional systems. The utilization of omnidirectional data is shown to improve the robustness and accuracy of the navigation algorithm. The feasibility of this algorithm is established through lab experimentation with two kinds of omnidirectional acquisition systems. The first one is polydioptric cameras while the second is catadioptric camera.", "target": "Pose and Motion from Omnidirectional Optical Flow and a Digital Terrain Map\u20ac"}
{"id": "task1540-d697c98c96b941d88dd8af16ba777a99", "input": "Hierarchical Reinforcement Learning has been previously shown to speed up the convergence rate of RL planning algorithms as well as mitigate feature-based model misspecification Mankowitz et al. (2016a,b); Bacon & Precup (2015). To do so, it utilizes hierarchical abstractions, also known as skills \u2013 a type of temporally extended action Sutton et al. (1999) to plan at a higher level, abstracting away from the lower-level details. We incorporate risk sensitivity, also referred to as Situational Awareness (SA) , into hierarchical RL for the first time by defining and learning risk aware skills in a Probabilistic Goal Semi-Markov Decision Process (PG-SMDP). This is achieved using our novel Situational Awareness by Risk-Conscious Skills (SARiCoS) algorithm which comes with a theoretical convergence guarantee. We show in a RoboCup soccer domain that the learned risk aware skills exhibit complex human behaviors such as \u2018time-wasting\u2019 in a soccer game. In addition, the learned risk aware skills are able to mitigate reward-based model misspecification.", "target": "Situational Awareness by Risk-Conscious Skills"}
{"id": "task1540-0bf4bc51a31643fb8c9a3cb1ad74b670", "input": "Holding commercial negotiations and selecting the best supplier in supply chain management systems are among weaknesses of producers in production process. Therefore, applying intelligent systems may have an effective role in increased speed and improved quality in the selections .This paper introduces a system which tries to trade using multi-agents systems and holding negotiations between any agents. In this system, an intelligent agent is considered for each segment of chains which it tries to send order and receive the response with attendance in negotiation medium and communication with other agents .This paper introduces how to communicate between agents, characteristics of multi-agent and standard registration medium of each agent in the environment. JADE (Java Application Development Environment) was used for implementation and simulation of agents cooperation. Keyword(s): e-Commerce, e-Business, Supply Chain Management System(SCM), eSCM, Intelligent Agents, JADE, Multi Agents", "target": "An Intelligent Approach for Negotiating between chains in Supply Chain Management Systems"}
{"id": "task1540-31e5b0b6716148459339b09d455edc7e", "input": "iv", "target": "A Theory of Interactive Debugging of Knowledge Bases in Monotonic Logics"}
{"id": "task1540-4c10838b116345b69decd1ce52f20e14", "input": "Planning for distributed agents with partial state information is considered from a decision\u00ad theoretic perspective. We describe generaliza\u00ad tions of both the MDP and POMDP models that allow for decentralized control. For even a small number of agents, the finite-horizon prob\u00ad lems corresponding to both of our models are complete for nondeterministic exponential time. These complexity results illustrate a fundamen\u00ad tal difference between centralized and decentral\u00ad ized control of Markov processes. In contrast to the MDP and POMDP problems, the problems we consider provably do not admit polynomial\u00ad time algorithms and most likely require doubly exponential time to solve in the worst case. We have thus provided mathematical evidence corre\u00ad sponding to the intuition that decentralized plan\u00ad ning problems cannot easily be reduced to cen\u00ad tralized problems and solved exactly using estab\u00ad lished techniques.", "target": "The Complexity of Decentralized Control of Markov Decision Processes"}
{"id": "task1540-199a2932f7854ecd9e66fae49a782817", "input": "Natural language generation plays a critical role in any spoken dialogue system. We present a new approach to natural language generation using recurrent neural networks in an encoderdecoder framework. In contrast with previous work, our model uses both lexicalized and delexicalized versions of slot-value pairs for each dialogue act. This allows our model to learn from all available data, rather than being restricted to learning only from delexicalized slot-value pairs. We show that this helps our model generate more natural sentences with better grammar. We further improve our model\u2019s performance by initializing its weights from a pretrained language model. Human evaluation of our best-performing model indicates that it generates sentences which users find more natural and appealing.", "target": "Natural Language Generation in Dialogue using Lexicalized and Delexicalized Data"}
{"id": "task1540-8c991e150c814d899c8eae1b28586f5d", "input": "This manuscript develops the theory of agglomerative clustering with Bregman divergences. Geometric smoothing techniques are developed to deal with degenerate clusters. To allow for cluster models based on exponential families with overcomplete representations, Bregman divergences are developed for nondifferentiable convex functions.", "target": "Agglomerative Bregman Clustering"}
{"id": "task1540-d0e20d84b4764bb8a99ef26cebbd85a3", "input": "A new architecture and learning algorithms for the multidimensional hybrid cascade neural network with neuron pool optimization in each cascade are proposed in this paper. The proposed system differs from the well-known cascade systems in its capability to process multidimensional time series in an online mode, which makes it possible to process non-stationary stochastic and chaotic signals with the required accuracy. Compared to conventional analogs, the proposed system provides computational simplicity and possesses both tracking and filtering capabilities.", "target": "A Multidimensional Cascade Neuro-Fuzzy System with Neuron Pool Optimization in Each Cascade"}
{"id": "task1540-25d663ff3fa7496d866fe75447b55a31", "input": "Deep Reinforcement Learning (RL) recently emerged as one of the most competitive approaches for learning in sequential decision making problems with fully observable environments, e.g., computer Go. However, very little work has been done in deep RL to handle partially observable environments. We propose a new architecture called Action-specific Deep Recurrent Q-Network (ADRQN) to enhance learning performance in partially observable domains. Actions are encoded by a fully connected layer and coupled with a convolutional observation to form an action-observation pair. The time series of action-observation pairs are then integrated by an LSTM layer that learns latent states based on which a fully connected layer computes Q-values as in conventional Deep QNetworks (DQNs). We demonstrate the effectiveness of our new architecture in several partially observable domains, including flickering Atari games.", "target": "On Improving Deep Reinforcement Learning for POMDPs"}
{"id": "task1540-112eeafbce79469d82026fcf28225f64", "input": "We present a pixel recursive super resolution model that synthesizes realistic details into images while enhancing their resolution. A low resolution image may correspond to multiple plausible high resolution images, thus modeling the super resolution process with a pixel independent conditional model often results in averaging different details\u2013 hence blurry edges. By contrast, our model is able to represent a multimodal conditional distribution by properly modeling the statistical dependencies among the high resolution image pixels, conditioned on a low resolution input. We employ a PixelCNN architecture to define a strong prior over natural images and jointly optimize this prior with a deep conditioning convolutional network. Human evaluations indicate that samples from our proposed model look more photo realistic than a strong L2 regression baseline.", "target": "Pixel Recursive Super Resolution"}
{"id": "task1540-45a7f74185eb4fbea545457a4c726d3f", "input": "Finding repeated patterns or motifs in a time series is an important unsupervised task that has still a number of open issues, starting by the definition of motif. In this paper, we revise the notion of motif support, characterizing it as the number of patterns or repetitions that define a motif. We then propose GENMOTIF, a genetic algorithm to discover motifs with support which, at the same time, is flexible enough to accommodate other motif specifications and task characteristics. GENMOTIF is an anytime algorithm that easily adapts to many situations: searching in a range of segment lengths, applying uniform scaling, dealing with multiple dimensions, using different similarity and grouping criteria, etc. GENMOTIF is also parameter-friendly: it has only two intuitive parameters which, if set within reasonable bounds, do not substantially affect its performance. We demonstrate the value of our approach in a number of synthetic and real-world settings, considering traffic volume measurements, accelerometer signals, and telephone call records.", "target": "A Genetic Algorithm to Discover Flexible Motifs with Support"}
{"id": "task1540-213ee5a3570d41f9ad224355474b9199", "input": "With the advent of modern computer networks, fault diagnosis has been a focus of research activity. This paper reviews the history of fault diagnosis in networks and discusses the main methods in information gathering section, information analyzing section and diagnosing and revolving section of fault diagnosis in networks. Emphasis will be placed upon knowledge-based methods with discussing the advantages and shortcomings of the different methods. The survey is concluded with a description of some open problems. Keywords-fault diagnosis in networks; expert system; Bayesian networks; artificial neural network", "target": "Survey of modern Fault Diagnosis methods in networks"}
{"id": "task1540-3b8908e0ff824e228350f7c23ab00a22", "input": "Well-established automatic analyses of texts mainly consider frequencies of linguistic units, e.g. letters, words and bigrams, while methods based on cooccurrence networks consider the structure of texts regardless of the nodes label (i.e. the words semantics). In this paper, we reconcile these distinct viewpoints by introducing a generalized similarity measure to compare texts which accounts for both the network structure of texts and the role of individual words in the networks. We use the similarity measure for authorship attribution of three collections of books, each composed of 8 authors and 10 books per author. High accuracy rates were obtained with typical values from 90% to 98.75%, much higher than with the traditional the TF-IDF approach for the same collections. These accuracies are also higher than taking only the topology of networks into account. We conclude that the different properties of specific words on the macroscopic scale structure of a whole text are as relevant as their frequency of appearance; conversely, considering the identity of nodes brings further knowledge about a piece of text represented as a network.", "target": "On the role of words in the network structure of texts: application to authorship attribution"}
{"id": "task1540-5d66986409e5442d8c468ef49aa6ee8f", "input": "We present a method for calculation of my\u00ad opic value of information in influence dia\u00ad grams (Howard & Matheson, 1981) based on the strong junction tree framework (Jensen et al., 1994). An influence diagram specifies a certain or\u00ad der of observations and decisions through its structure. This order is reflected in the corre\u00ad sponding junction trees by the order in which the nodes are marginalized. This order of marginalization can be changed by table ex\u00ad pansion and use of control structures, and this facilitates for calculating the expected value of information for different information scenarios within the same junction tree. In effect, a strong junction tree with expanded tables may be used for calculating the value of information between several scenarios with different observation-decision order. We compare our method to other methods for calculating the value of information in in\u00ad fluence diagrams.", "target": "Myopic Value of Information in Influence Diagrams"}
{"id": "task1540-c57fc5158c28435c9daee21ec702ee3a", "input": "We introduce a new model of interactive learning in which an expert examines the predictions of a learner and partially fixes them if they are wrong. Although this kind of feedback is not i.i.d., we show statistical generalization bounds on the quality of the learned model.", "target": "Learning from partial correction"}
{"id": "task1540-14fc61bf764b4819a1302403d2677518", "input": "Probabilistic Linear Discriminant Analysis (PLDA) has become state-of-the-art method for modeling i-vector space in speaker recognition task. However the performance degradation is observed if enrollment data size differs from one speaker to another. This paper presents a solution to such problem by introducing new PLDA scoring normalization technique. Normalization parameters are derived in a blind way, so that, unlike traditional ZT-norm, no extra development data is required. Moreover, proposed method has shown to be optimal in terms of detection cost function. The experiments conducted on NIST SRE 2014 database demonstrate an improved accuracy in a mixed enrollment number condition.", "target": "Blind score normalization method for PLDA based speaker recognition"}
{"id": "task1540-acc9febdce7b44ce8b9a18b04d9915f1", "input": "Traditional approaches to non-monotonic reasoning fail to satisfy a number of plausible axioms for belief revision and suffer from conceptual difficulties as well. Recent work on ranked preferential models (RPMs) promises to overcome some of these difficulties. Here we show that RPMs are not adequate to handle iterated belief change. Specifically, we show that RPMs do not always allow for the reversibility of belief change. 1bis result indicates the need for numerical strengths of belief.", "target": "Non-monotonic Reasoning and the Reversibility of Belief Change"}
{"id": "task1540-46602327cb9c451c87ed45b29ae3bd2b", "input": "The Cmabrigde Uinervtisy (Cambridge University) effect from the psycholinguistics literature has demonstrated a robust word processing mechanism in humans, where jumbled words (e.g. Cmabrigde / Cambridge) are recognized with little cost. Inspired by the findings from the Cmabrigde Uinervtisy effect, we propose a word recognition model based on a semi-character level recursive neural network (scRNN). In our experiments, we demonstrate that scRNN has significantly more robust performance in word spelling correction (i.e. word recognition) compared to existing spelling checkers. Furthermore, we demonstrate that the model is cognitively plausible by replicating a psycholinguistics experiment about human reading difficulty using our model.", "target": "Robsut Wrod Reocginiton via semi-Character Recurrent Neural Network"}
{"id": "task1540-52ebb1b74b7840cc8a3ed1a9cd15b2c1", "input": "The paper systematically studies the impact of a range of recent advances in CNN architectures and learning methods on the object categorization (ILSVRC) problem. The evalution tests the influence of the following choices of the architecture: non-linearity (ReLU, ELU, maxout, compatability with batch normalization), pooling variants (stochastic, max, average, mixed), network width, classifier design (convolutional, fully-connected, SPP), image pre-processing, and of learning parameters: learning rate, batch size, cleanliness of the data, etc. The performance gains of the proposed modifications are first tested individually and then in combination. The sum of individual gains is bigger than the observed improvement when all modifications are introduced, but the \u201ddeficit\u201d is small suggesting independence of their benefits. We show that the use of 128x128 pixel images is sufficient to make qualitative conclusions about optimal network structure that hold for the full size Caffe and VGG nets. The results are obtained an order of magnitude faster than with the standard 224 pixel images.", "target": "Systematic evaluation of CNN advances on the ImageNet"}
{"id": "task1540-1253f547c5e1495a9d9b26a2792c9566", "input": "In this paper we propose an extension to the Fuzzy Cognitive Maps (FCMs) that aims at aggregating a number of reasoning tasks into a one parallel run. The described approach consists in replacing real-valued activation levels of concepts (and further influence weights) by random variables. Such extension, followed by the implemented software tool, allows for determining ranges reached by concept activation levels, sensitivity analysis as well as statistical analysis of multiple reasoning results. We replace multiplication and addition operators appearing in the FCM state equation by appropriate convolutions applicable for discrete random variables. To make the model computationally feasible, it is further augmented with aggregation operations for discrete random variables. We discuss four implemented aggregators, as well as we report results of preliminary tests.", "target": "Combining Fuzzy Cognitive Maps and Discrete Random Variables"}
{"id": "task1540-d1b4a47780664ee085254d9446181739", "input": "We propose two novel techniques \u2014 stacking bottleneck features and minimum generation error training criterion \u2014 to improve the performance of deep neural network (DNN)based speech synthesis. The techniques address the related issues of frame-by-frame independence and ignorance of the relationship between static and dynamic features, within current typical DNNbased synthesis frameworks. Stacking bottleneck features, which are an acoustically\u2013informed linguistic representation, provides an efficient way to include more detailed linguistic context at the input. The minimum generation error training criterion minimises overall output trajectory error across an utterance, rather than minimising the error per frame independently, and thus takes into account the interaction between static and dynamic features. The two techniques can be easily combined to further improve performance. We present both objective and subjective results that demonstrate the effectiveness of the proposed techniques. The subjective results show that combining the two techniques leads to significantly more natural synthetic speech than from conventional DNN or long short-term memory (LSTM) recurrent neural network (RNN) systems.", "target": "Improving Trajectory Modelling for DNN-based Speech Synthesis by using Stacked Bottleneck Features and Minimum Generation Error Training"}
{"id": "task1540-1f98ee92fc5f46e29a8eebdff14e9ffd", "input": "We introduce a compact graph-theoretic repre\u00ad sentation for multi-party game theory. Our main result is a provably correct and efficient algo\u00ad rithm for computing approximate Nash equilibria in one-stage games represented by trees or sparse graphs.", "target": "Graphical Models for Game Theory"}
{"id": "task1540-2df3ef57a4cd461491c7bbbbc93dca84", "input": "The representation of many common semantic phenomena requires structural properties beyond those commonly used for syntactic parsing. We discuss a set of structural properties required for broad-coverage semantic representation, and note that existing parsers support some of these properties, but not all. We propose two transition-based techniques for parsing such semantic structures: (1) applying conversion procedures to map them into related formalisms, and using existing state-of-the-art parsers on the converted representations; and (2) constructing a parser that directly supports the full set of properties. We experiment with UCCA-annotated corpora, the only ones with all these structural semantic properties. Results demonstrate the effectiveness of transition-based methods for the task.", "target": "Broad-Coverage Semantic Parsing: A Transition-Based Approach"}
{"id": "task1540-2ec91f882ae048d390bb376e1e493c62", "input": "Discovering the set of closed frequent patterns is one of the fundamental problems in Data Mining. Recent Constraint Programming (CP) approaches for declarative itemset mining have proven their usefulness and flexibility. But the wide use of reified constraints in current CP approaches raises many difficulties to cope with high dimensional datasets. This paper proposes CLOSEDPATTERN global constraint which does not require any reified constraints nor any extra variables to encode efficiently the Closed Frequent Pattern Mining (CFPM) constraint. CLOSEDPATTERN captures the particular semantics of the CFPM problem in order to ensure a polynomial pruning algorithm ensuring domain consistency. The computational properties of our constraint are analyzed and their practical effectiveness is experimentally evaluated.", "target": "A global constraint for closed itemset mining"}
{"id": "task1540-12580220035f4043ab8ced5768a54dc8", "input": "This paper describes our approach for the Detecting Stance in Tweets task (SemEval-2016 Task 6). We utilized recent advances in short text categorization using deep learning to create word-level and character-level models. The choice between word-level and characterlevel models in each particular case was informed through validation performance. Our final system is a combination of classifiers using word-level or character-level models. We also employed novel data augmentation techniques to expand and diversify our training dataset, thus making our system more robust. Our system achieved a macro-average precision, recall and F1-scores of 0.67, 0.61 and 0.635 respectively.", "target": "DeepStance at SemEval-2016 Task 6: Detecting Stance in Tweets Using Character and Word-Level CNNs"}
{"id": "task1540-e6dbbca1277f4ae3b0975109f855a6a0", "input": "Conversational modeling is an important task in natural language understanding and machine intelligence. Although previous approaches exist, they are often restricted to specific domains (e.g., booking an airline ticket) and require handcrafted rules. In this paper, we present a simple approach for this task which uses the recently proposed sequence to sequence framework. Our model converses by predicting the next sentence given the previous sentence or sentences in a conversation. The strength of our model is that it can be trained end-to-end and thus requires much fewer hand-crafted rules. We find that this straightforward model can generate simple conversations given a large conversational training dataset. Our preliminary suggest that, despite optimizing the wrong objective function, the model is able to extract knowledge from both a domain specific dataset, and from a large, noisy, and general domain dataset of movie subtitles. On a domain-specific IT helpdesk dataset, the model can find a solution to a technical problem via conversations. On a noisy open-domain movie transcript dataset, the model can perform simple forms of common sense reasoning. As expected, we also find that the lack of consistency is a common failure mode of our model.", "target": "A Neural Conversational Model"}
{"id": "task1540-c7a5fc924b9f463d8e20dd51157342fc", "input": "We provide a general framework for computing lower-bounds on the sample complexity of recovering the underlying graphs of Ising models, given i.i.d. samples. While there have been recent results for specific graph classes, these involve fairly extensive technical arguments that are specialized to each specific graph class. In contrast, we isolate two key graph-structural ingredients that can then be used to specify sample complexity lower-bounds. Presence of these structural properties makes the graph class hard to learn. We derive corollaries of our main result that not only recover existing recent results, but also provide lower bounds for novel graph classes not considered previously. We also extend our framework to the random graph setting and derive corollaries for Erd\u0151s-R\u00e9nyi graphs in a certain dense setting.", "target": "On the Information Theoretic Limits of Learning Ising Models"}
{"id": "task1540-b63722af50e149769aa2f08546767e9d", "input": "Predicting the Credit Defaulter is a perilous task of Financial Industries like Banks. Ascertainingnonpayer before giving loan is a significant and conflict-ridden task of the Banker. Classification techniques are the better choice for predictive analysis like finding the claimant, whether he/she is an unpretentious customer or a cheat. Defining the outstanding classifier is a risky assignment for any industrialist like a banker. This allow computer science researchers to drill down efficient research works through evaluating different classifiers and finding out the best classifier for such predictive problems. This research work investigates the productivity of LADTree Classifier and REPTree Classifier for the credit risk prediction and compares their fitness through various measures. German credit dataset has been taken and used to predict the credit risk with a help of open source machine learning tool.", "target": "PROFICIENCY COMPARISON OFLADTREE AND REPTREE CLASSIFIERS FOR CREDIT RISK FORECAST"}
{"id": "task1540-587368394f2949fe97b94b4c3ebe0f21", "input": "Along with data on the web increasing dramatically, hashing is becoming more and more popular as a method of approximate nearest neighbor search. Previous supervised hashing methods utilized similarity/dissimilarity matrix to get semantic information. But the matrix is not easy to construct for a new dataset. Rather than to reconstruct the matrix, we proposed a straightforward CNN-based hashing method, i.e. binarilizing the activations of a fully connected layer with threshold 0 and taking the binary result as hash codes. This method achieved the best performance on CIFAR-10 and was comparable with the state-ofthe-art on MNIST. And our experiments on CIFAR-10 suggested that the signs of activations may carry more information than the relative values of activations between samples, and that the co-adaption between feature extractor and hash functions is important for hashing.", "target": "CNN Based Hashing for Image Retrieval"}
{"id": "task1540-c22e833de4604470a2483a530b3eda89", "input": "We consider online learning algorithms that guarantee worst-case regret rates in adversarial environments (so they can be deployed safely and will perform robustly), yet adapt optimally to favorable stochastic environments (so they will perform well in a variety of settings of practical importance). We quantify the friendliness of stochastic environments by means of the well-known Bernstein (a.k.a. generalized Tsybakov margin) condition. For two recent algorithms (Squint for the Hedge setting and MetaGrad for online convex optimization) we show that the particular form of their data-dependent individual-sequence regret guarantees implies that they adapt automatically to the Bernstein parameters of the stochastic environment. We prove that these algorithms attain fast rates in their respective settings both in expectation and with high probability.", "target": "Combining Adversarial Guarantees and Stochastic Fast Rates in Online Learning"}
{"id": "task1540-7ea5a452459d4758a18d0622db9b2d0f", "input": "We introduce utility-directed procedures for mediating the flow of potentially distract\u00ad ing alerts and communications to computer users. We present models and inference pro\u00ad cedures that balance the context-sensitive costs of deferring alerts with the cost of in\u00ad terruption. We describe the challenge of rea\u00ad soning about such costs under uncertainty via an analysis of user activity and the content of notifications. After introducing principles of attention-sensitive alerting, we focus on the problem of guiding alerts about email mes\u00ad sages. We dwell on the problem of inferring the expected criticality of email and discuss work on the PRIORITIES system, centering on prioritizing email by criticality and modu\u00ad lating the communication of notifications to users about the presence and nature of in\u00ad coming email.", "target": "Attention-Sensitive Alerting"}
{"id": "task1540-d34382504c0b49b38b1760955408ce25", "input": "Methods of deep machine learning enable to to reuse low-level representations efficiently for generating more abstract high-level representations. Originally, deep learning has been applied passively (e.g., for classification purposes). Recently, it has been extended to estimate the value of actions for autonomous agents within the framework of reinforcement learning (RL). Explicit models of the environment can be learned to augment such a value function. Although \u201cflat\u201d connectionist methods have already been used for model-based RL, up to now, only modelfree variants of RL have been equipped with methods from deep learning. We propose a variant of deep model-based RL that enables an agent to learn arbitrarily abstract hierarchical representations of its environment. In this paper, we present research on how such hierarchical representations can be grounded in sensorimotor interaction between an agent and its environment.", "target": "Grounding Hierarchical Reinforcement Learning Models for Knowledge Transfer"}
{"id": "task1540-0e1e569ad50b44f7a37fde838687f3e1", "input": "How can we automatically discover the most important correspondences between words from two or more languages? How can we do so allowing for correspondences between any subset of languages, without drowning in redundant results, and at the same time maintaining control over the level of detail? These are exactly the questions we answer in this paper. We approach the problem with the Minimum Description Length principle, and give an efficient algorithm for discovering statistically important correspondences. We test the efficacy of our method against a set of Slavic languages. The experiments show our method automatically discovers non-trivial associations, allowing for both quantitative and qualitative analysis of multiple languages.", "target": "Discovering Correspondences between Multiple Languages by MDL"}
{"id": "task1540-7907aeba57584168b131636fcf82e037", "input": "Deep learning models are often successfully trained using gradient descent, despite the worst<lb>case hardness of the underlying non-convex optimization problem. The key question is then under<lb>what conditions can one prove that optimization will succeed. Here we provide a strong result<lb>of this kind. We consider a neural net with one hidden layer and a convolutional structure with<lb>no overlap and a ReLU activation function. For this architecture we show that learning is NP-<lb>complete in the general case, but that when the input distribution is Gaussian, gradient descent<lb>converges to the global optimum in polynomial time. To the best of our knowledge, this is the<lb>first global optimality guarantee of gradient descent on a convolutional neural network with ReLU<lb>activations.", "target": "Globally Optimal Gradient Descent for a ConvNet with Gaussian Inputs"}
{"id": "task1540-0c385306784948f5ae57d6abc81feec4", "input": "(To appear in Theory and Practice of Logic Programming (TPLP)) ESmodels is designed and implemented as an experiment platform to investigate the semantics, language, related reasoning algorithms, and possible applications of epistemic specifications. We first give the epistemic specification language of ESmodels and its semantics. The language employs only one modal operator K but we prove that it is able to represent luxuriant modal operators by presenting transformation rules. Then, we describe basic algorithms and optimization approaches used in ESmodels. After that, we discuss possible applications of ESmodels in conformant planning and constraint satisfaction. Finally, we conclude with perspectives.", "target": "ESmodels: An Epistemic Specification Solver"}
{"id": "task1540-f7be2b73136c406b82b6f27851d77d3c", "input": "We introduce a model for constructing vector representations of words by composing characters using bidirectional LSTMs. Relative to traditional word representation models that have independent vectors for each word type, our model requires only a single vector per character type and a fixed set of parameters for the compositional model. Despite the compactness of this model and, more importantly, the arbitrary nature of the form\u2013function relationship in language, our \u201ccomposed\u201d word representations yield state-of-the-art results in language modeling and part-of-speech tagging. Benefits over traditional baselines are particularly pronounced in morphologically rich languages (e.g., Turkish).", "target": "Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation"}
{"id": "task1540-d31d097cf6984810969d599b0720b500", "input": "We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. We focus on two applications of GANs: semi-supervised learning, and the generation of images that humans find visually realistic. Unlike most work on generative models, our primary goal is not to train a model that assigns high likelihood to test data, nor do we require the model to be able to learn well without using any labels. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3%. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes.", "target": "Improved Techniques for Training GANs"}
{"id": "task1540-c25a489d3b6c45c5b01c3ecf9f4ce666", "input": "In this paper we analyse network motifs in the co-occurrence directed networks constructed from five different texts (four books and one portal) in the Croatian language. After preparing the data and network construction, we perform the network motif analysis. We analyse the motif frequencies and Z-scores in the five networks. We present the triad significance profile for five datasets. Furthermore, we compare our results with the existing results for the linguistic networks. Firstly, we show that the triad significance profile for the Croatian language is very similar with the other languages and all the networks belong to the same family of networks. However, there are certain differences between the Croatian language and other analysed languages. We conclude that this is due to the free word-order of the Croatian language.", "target": "Network Motifs Analysis of Croatian Literature"}
{"id": "task1540-f37d3fb0844a4d0fab4e861014d7d62e", "input": "We present an end-to-end, multimodal, fully convolu-<lb>tional network for extracting semantic structures from doc-<lb>ument images. We consider document semantic structure<lb>extraction as a pixel-wise segmentation task, and propose a<lb>unified model that classifies pixels based not only on their<lb>visual appearance, as in the traditional page segmentation<lb>task, but also on the content of underlying text. Moreover,<lb>we propose an efficient synthetic document generation pro-<lb>cess that we use to generate pretraining data for our net-<lb>work. Once the network is trained on a large set of synthetic<lb>documents, we fine-tune the network on unlabeled real doc-<lb>uments using a semi-supervised approach. We systemati-<lb>cally study the optimum network architecture and show that<lb>both our multimodal approach and the synthetic data pre-<lb>training significantly boost the performance.", "target": "Learning to Extract Semantic Structure from Documents Using Multimodal Fully Convolutional Neural Networks"}
{"id": "task1540-b191664f6c1e46658a434c09bbf1e4c5", "input": "Automated Theorem Proving (ATP) is an established branch of Artificial Intelligence. The purpose of ATP is to design a system which can automatically figure out an algorithm either to prove or disprove a mathematical claim, on the basis of a set of given premises, using a set of fundamental postulates and following the method of logical inference. In this paper, we propose GraATP, a generalized framework for automated theorem proving in plane geometry. Our proposed method translates the geometric entities into nodes of a graph and the relations between them as edges of that graph. The automated system searches for different ways to reach the conclusion for a claim via graph traversal by which the validity of the geometric theorem is examined.", "target": "GraATP: A Graph Theoretic Approach for Automated Theorem Proving in Plane Geometry"}
{"id": "task1540-81d00ac0ae754b91b3b550bb37f44064", "input": "The key issues pertaining to collection of epidemic disease data for our analysis purposes are that it is a labour intensive, time consuming and expensive process resulting in availability of sparse sample data which we use to develop prediction models. To address this sparse data issue, we present novel Incremental Transductive methods to circumvent the data collection process by applying previously acquired data to provide consistent, confidence-based labelling alternatives to field survey research. We investigated various reasoning approaches for semisupervised machine learning including Bayesian models for labelling data. The results show that using the proposed methods, we can label instances of data with a class of vector density at a high level of confidence. By applying the Liberal and Strict Training Approaches, we provide a labelling and classification alternative to standalone algorithms. The methods in this paper are components in the process of reducing the proliferation of the Schistosomiasis disease and its effects.", "target": "Incremental Transductive Learning Approaches to Schistosomiasis Vector Classification"}
{"id": "task1540-13769441716a43f381d24755875a4e02", "input": "The Stable Marriage Problem (SMP) is a well-known matching problem first introduced and solved by Gale and Shapley [7]. Several variants and extensions to this problem have since been investigated to cover a wider set of applications. Each time a new variant is considered, however, a new algorithm needs to be developed and implemented. As an alternative, in this paper we propose an encoding of the SMP using Answer Set Programming (ASP). Our encoding can easily be extended and adapted to the needs of specific applications. As an illustration we show how stable matchings can be found when individuals may designate unacceptable partners and ties between preferences are allowed. Subsequently, we show how our ASP based encoding naturally allows us to select specific stable matchings which are optimal according to a given criterion. Each time, we can rely on generic and efficient off-the-shelf answer set solvers to find (optimal) stable matchings.", "target": "Modeling Stable Matching Problems with Answer Set Programming"}
{"id": "task1540-dc38110cd93345a5835a6aa4b05aafca", "input": "Sparse PCA provides a linear combination of small number of features that maximizes variance across data. Although Sparse PCA has apparent advantages compared to PCA, such as better interpretability, it is generally thought to be computationally much more expensive. In this paper, we demonstrate the surprising fact that sparse PCA can be easier than PCA in practice, and that it can be reliably applied to very large data sets. This comes from a rigorous feature elimination pre-processing result, coupled with the favorable fact that features in real-life data typically have exponentially decreasing variances, which allows for many features to be eliminated. We introduce a fast block coordinate ascent algorithm with much better computational complexity than the existing first-order ones. We provide experimental results obtained on text corpora involving millions of documents and hundreds of thousands of features. These results illustrate how Sparse PCA can help organize a large corpus of text data in a user-interpretable way, providing an attractive alternative approach to topic models.", "target": "Large-Scale Sparse Principal Component Analysis with Application to Text Data"}
{"id": "task1540-9fdb940754de4dca9f3103bcf4861e89", "input": "Many papers have been published on the knowledge base completion task in the past few years. Most of these introduce novel architectures for relation learning that are evaluated on standard datasets such as FB15k and WN18. This paper shows that the accuracy of almost all models published on the FB15k can be outperformed by an appropriately tuned baseline \u2014 our reimplementation of the DistMult model. Our findings cast doubt on the claim that the performance improvements of recent models are due to architectural changes as opposed to hyperparameter tuning or different training objectives. This should prompt future research to re-consider how the performance of models is evaluated and reported.", "target": "Knowledge Base Completion: Baselines Strike Back"}
{"id": "task1540-b6bf07bf4f0a40539221c052c06eea87", "input": "We present an alternative methodology for the analysis of algorithms, based on the concept of expected discounted reward. This methodology naturally handles algorithms that do not always terminate, so it can (theoretically) be used with partial algorithms for undecidable problems, such as those found in artificial general intelligence (AGI) and automated theorem proving. We mention an approach to self-improving AGI enabled by this methodology.", "target": "Analysis of Algorithms and Partial Algorithms"}
{"id": "task1540-617dfc145f40436893d6cb79491050a1", "input": "The ability to perform pixel-wise semantic segmentation in real-time is of paramount importance in practical mobile applications. Recent deep neural networks aimed at this task have the disadvantage of requiring a large number of floating point operations and have long run-times that hinder their usability. In this paper, we propose a novel deep neural network architecture named ENet (efficient neural network), created specifically for tasks requiring low latency operation. ENet is up to 18\u00d7 faster, requires 75\u00d7 less FLOPs, has 79\u00d7 less parameters, and provides similar or better accuracy to existing models. We have tested it on CamVid, Cityscapes and SUN datasets and report on comparisons with existing state-of-the-art methods, and the trade-offs between accuracy and processing time of a network. We present performance measurements of the proposed architecture on embedded systems and suggest possible software improvements that could make ENet even faster.", "target": "ENET: A DEEP NEURAL NETWORK ARCHITECTURE FOR REAL-TIME SEMANTIC SEGMENTATION"}
{"id": "task1540-88a0408a37324570a51a70998dc89a59", "input": "Direct quantile regression involves estimating a given quantile of a response variable as a function of input variables. We present a new framework for direct quantile regression where a Gaussian process model is learned, minimising the expected tilted loss function. The integration required in learning is not analytically tractable so to speed up the learning we employ the Expectation Propagation algorithm. We describe how this work relates to other quantile regression methods and apply the method on both synthetic and real data sets. The method is shown to be competitive with state of the art methods whilst allowing for the leverage of the full Gaussian process probabilistic framework.", "target": "Direct Gaussian Process Quantile Regression  using Expectation Propagation"}
{"id": "task1540-f75f549290da4086a2c69580f058346c", "input": "We introduce a new spatial data structure for high dimensional data called the approximate principal direction tree (APD tree) that adapts to the intrinsic dimension of the data. Our algorithm ensures vector-quantization accuracy similar to that of computationally-expensive PCA trees with similar time-complexity to that of loweraccuracy RP trees. APD trees use a small number of powermethod iterations to find splitting planes for recursively partitioning the data. As such they provide a natural trade-off between the running-time and accuracy achieved by RP and PCA trees. Our theoretical results establish a) strong performance guarantees regardless of the convergence rate of the powermethod and b) that O(log d) iterations suffice to establish the guarantee of PCA trees when the intrinsic dimension is d. We demonstrate this trade-off and the efficacy of our data structure on both the CPU and GPU.", "target": "Approximate Principal Direction Trees"}
{"id": "task1540-0d011c672d314dbe8aa7b40c936c3df1", "input": "A modelling language is decribed which is suitable for the correlation of information when the underlying functional model of the system is incomplete or uncertain and the temporal dependencies are imprecise. An efficient an incremental implementation is outlined which depends on cost functions satisfying certain criteria. Possibilistic logic and probability theory (as it is used in the applications targetted) satisfy these criteria.", "target": "Exploiting Uncertain and Temporal lnfonnation in Correlation"}
{"id": "task1540-babbba7c97174c158911e919baf212ed", "input": "Causality has been recently introduced in databases, to model, characterize and possibly compute causes for query results (answers). Connections between queryanswer causality, consistency-based diagnosis, database repairs (wrt. integrity constraint violations), abductive diagnosis and the view-update problem have been established. In this work we further investigate connections between query-answer causality and abductive diagnosis and the view-update problem. In this context, we also define and investigate the notion of query-answer causality in the presence of integrity constraints.", "target": "Causes for Query Answers from Databases, Datalog Abduction and View-Updates: The Presence of Integrity Constraints"}
{"id": "task1540-a3f6af099dc3464eb696ad40fe1b4472", "input": "Two types of low cost-per-iteration gradient descent methods have been extensively studied in parallel. One is online or stochastic gradient descent ( OGD/SGD), and the other is randomzied coordinate descent (RBCD). In this paper, for the first time, we combine the two types of methods together and propose online randomized block coordinate descent (ORBCD). At each iteration, ORBCD only computes the partial gradient of one block coordinate of one mini-batch samples. ORBCD is well suited for the composite minimization problem where one function is the average of the losses of a large number of samples and the other is a simple regularizer defined on high dimensional variables. We show that the iteration complexity of ORBCD has the same order as OGD or SGD. For strongly convex functions, by reducing the variance of stochastic gradients, we show that ORBCD can converge at a geometric rate in expectation, matching the convergence rate of SGD with variance reduction and RBCD.", "target": "Randomized Block Coordinate Descent for Online and Stochastic Optimization"}
{"id": "task1540-94c1d50913d3473486091786487c62f9", "input": "We describe a novel approach to monitoring high level behaviors using concepts from AI planning. Our goal is to understand what a program is doing based on its system call trace. This ability is particularly important for detecting malware. We approach this problem by building an abstract model of the operating system using the STRIPS planning language, casting system calls as planning operators. Given a system call trace, we simulate the corresponding operators on our model and by observing the properties of the state reached, we learn about the nature of the original program and its behavior. Thus, unlike most statistical detection methods that focus on syntactic features, our approach is semantic in nature. Therefore, it is more robust against obfuscation techniques used by malware that change the outward appearance of the trace but not its effect. We demonstrate the efficacy of our approach by evaluating it on actual system call traces.", "target": "A Planning Approach to Monitoring Computer Programs\u2019 Behavior"}
{"id": "task1540-3366975bd6d14e24aa2d4110d7dc65b4", "input": "Private Set Intersection (PSI) is usually implemented as a sequence of encryption rounds between pairs of users, whereas the present work implements PSI in a simpler fashion: each set only needs to be encrypted once, after which each pair of users need only one ordinary set comparison. This is typically orders of magnitude faster than ordinary PSI at the cost of some \u201cfuzziness\u201d in the matching, which may nonetheless be tolerable or even desirable. This is demonstrated in the case where the sets consist of English words processed with WordNet. Email: 1054h34@gmail.com", "target": "Fast and Fuzzy Private Set Intersection"}
{"id": "task1540-5f50aca046044fa8abea191a99d83f98", "input": "Word alignment is an important natural language processing task that indicates the correspondence between natural languages. Recently, unsupervised learning of log-linear models for word alignment has received considerable attention as it combines the merits of generative and discriminative approaches. However, a major challenge still remains: it is intractable to calculate the expectations of non-local features that are critical for capturing the divergence between natural languages. We propose a contrastive approach that aims to differentiate observed training examples from noises. It not only introduces prior knowledge to guide unsupervised learning but also cancels out partition functions. Based on the observation that the probability mass of log-linear models for word alignment is usually highly concentrated, we propose to use top-n alignments to approximate the expectations with respect to posterior distributions. This allows for efficient and accurate calculation of expectations of non-local features. Experiments show that our approach achieves significant improvements over stateof-the-art unsupervised word alignment methods.", "target": "Contrastive Unsupervised Word Alignment with Non-Local Features"}
{"id": "task1540-048bea121ec146a08b67d243827f6553", "input": "The production of color language is essential for grounded language generation. Color descriptions have many challenging properties: they can be vague, compositionally complex, and denotationally rich. We present an effective approach to generating color descriptions using recurrent neural networks and a Fouriertransformed color representation. Our model outperforms previous work on a conditional language modeling task over a large corpus of naturalistic color descriptions. In addition, probing the model\u2019s output reveals that it can accurately produce not only basic color terms but also descriptors with non-convex denotations (\u201cgreenish\u201d), bare modifiers (\u201cbright\u201d, \u201cdull\u201d), and compositional phrases (\u201cfaded teal\u201d) not seen in training.", "target": "Learning to Generate Compositional Color Descriptions"}
{"id": "task1540-ac30685f74ed45b580567da05f1266ba", "input": "Gaussian processes are rich distributions over functions, which provide a Bayesian nonparametric approach to smoothing and interpolation. We introduce simple closed form kernels that can be used with Gaussian processes to discover patterns and enable extrapolation. These kernels are derived by modelling a spectral density \u2013 the Fourier transform of a kernel \u2013 with a Gaussian mixture. The proposed kernels support a broad class of stationary covariances, but Gaussian process inference remains simple and analytic. We demonstrate the proposed kernels by discovering patterns and performing long range extrapolation on synthetic examples, as well as atmospheric CO2 trends and airline passenger data. We also show that we can reconstruct standard covariances within our framework.", "target": "Gaussian Process Covariance Kernels for Pattern Discovery and Extrapolation"}
{"id": "task1540-05b6605ec0154622a49c12dd66964417", "input": "We present initial ideas for a programming paradigm based on simulation that is targeted towards applications of artificial intelligence (AI). The approach aims at integrating techniques from different areas of AI and is based on the idea that simulated entities may freely exchange data and behavioural patterns. We define basic notions of a simulation-based programming paradigm and show how it can be used for implementing AI applications.", "target": "Towards a Simulation-Based Programming Paradigm for AI applications"}
{"id": "task1540-0da571862b854589b3508fa026a24276", "input": "This article deals with plausible reasoning from incomplete knowledge about large-scale spatial properties. The available information, consisting of a set of pointwise observations, is extrapolated to neighbour points. We use belief functions to represent the influence of the knowledge at a given point to another point; the quantitative strength of this in\u00ad fluence decreases when the distance between both points increases. These influences are aggregated using a variant of Dempster's rule of combination taking into account the rela\u00ad tive dependence between observations.", "target": "Plausible reasoning from spatial observations"}
{"id": "task1540-58f020aefab744629183b718c8435df4", "input": "The World Wide Web no longer consists just of HTML pages. Our work sheds light on a number of trends on the Internet that go beyond simple Web pages. The hidden Web provides a wealth of data in semi-structured form, accessible through Web forms and Web services. These services, as well as numerous other applications on the Web, commonly use XML, the eXtensible Markup Language. XML has become the lingua franca of the Internet that allows customized markups to be defined for specific domains. On top of XML, the Semantic Web grows as a common structured data source. In this work, we first explain each of these developments in detail. Using real-world examples from scientific domains of great interest today, we then demonstrate how these new developments can assist the managing, harvesting, and organization of data on the Web. On the way, we also illustrate the current research avenues in these domains. We believe that this effort would help bridge multiple database tracks, thereby attracting researchers with a view to extend database technology.", "target": "The Hidden Web, XML and the Semantic Web: Scientific Data Management Perspectives"}
{"id": "task1540-47c9024a36a74996ab54defeafa080d5", "input": "Although many machine learning algorithms involve learning subspaces with particular characteristics, optimizing a parameter matrix that is constrained to represent a subspace can be challenging. One solution is to use Riemannian optimization methods that enforce such constraints implicitly, leveraging the fact that the feasible parameter values form a manifold. While Riemannian methods exist for some specific problems, such as learning a single subspace, there are more general subspace constraints that offer additional flexibility when setting up an optimization problem but have not been formulated as a manifold. We propose the partitioned subspace (PS) manifold for optimizing matrices that are constrained to represent one or more subspaces. Each point on the manifold defines a partitioning of the input space into mutually orthogonal subspaces, where the number of partitions and their sizes are defined by the user. As a result, distinct groups of features can be learned by defining different objective functions for each partition. We illustrate the properties of the manifold through experiments on multiple dataset analysis and domain adaptation.", "target": "A Manifold Approach to Learning Mutually Orthogonal Subspaces"}
{"id": "task1540-c26210813ad946d68d916e3397c8b000", "input": "In the mixture models problem it is assumed that there are K distributions \u03b81, . . . , \u03b8K and one gets to observe a sample from a mixture of these distributions with unknown coefficients. The goal is to associate instances with their generating distributions, or to identify the parameters of the hidden distributions. In this work we make the assumption that we have access to several samples drawn from the same K underlying distributions, but with different mixing weights. As with topic modeling, having multiple samples is often a reasonable assumption. Instead of pooling the data into one sample, we prove that it is possible to use the differences between the samples to better recover the underlying structure. We present algorithms that recover the underlying structure under milder assumptions than the current state of art when either the dimensionality or the separation is high. The methods, when applied to topic modeling, allow generalization to words not present in the training data.", "target": "Using multiple samples to learn mixture models"}
{"id": "task1540-3583ccd47b4a4d53bd2779c8695c6558", "input": "Nowadays, neural networks play an important role in the task of relation classification. By designing different neural architectures, researchers have improved the performance to a large extent, compared with traditional methods. However, existing neural networks for relation classification are usually of shallow architectures (e.g., one-layer convolution neural networks or recurrent networks). They may fail to explore the potential representation space in different abstraction levels. In this paper, we propose deep recurrent neural networks (DRNNs) to tackle this challenge. Further, we propose a data augmentation method by leveraging the directionality of relations. We evaluate our DRNNs on the SemEval-2010 Task 8, and achieve an F1score of 85.81%, outperforming state-of-theart recorded results.", "target": "Improved Relation Classification by Deep Recurrent Neural Networks with Data Augmentation"}
{"id": "task1540-a923d8c15d6445629e3b657773d682e6", "input": "Ordinary least squares (OLS) is the default method for fitting linear models, but is not applicable for problems with dimensionality larger than the sample size. For these problems, we advocate the use of a generalized version of OLS motivated by ridge regression, and propose two novel three-step algorithms involving least squares fitting and hard thresholding. The algorithms are methodologically simple to understand intuitively, computationally easy to implement efficiently, and theoretically appealing for choosing models consistently. Numerical exercises comparing our methods with penalization-based approaches in simulations and data analyses illustrate the great potential of the proposed algorithms.", "target": "No penalty no tears: Least squares in high-dimensional linear models"}
{"id": "task1540-fa4098943dbe40eb9eff7555c3e13c6f", "input": "Due to the intractable nature of exact lifted inference, research has recently focused on the discovery of accurate and efficient approximate inference algorithms in Statistical Relational Models (SRMs), such as Lifted First-Order Belief Propagation. FOBP simulates propositional factor graph belief propagation without constructing the ground factor graph by identifying and lifting over redundant message computations. In this work, we propose a generalization of FOBP called Lifted Generalized Belief Propagation, in which both the region structure and the message structure can be lifted. This approach allows more of the inference to be performed intra-region (in the exact inference step of BP), thereby allowing simulation of propagation on a graph structure with larger region scopes and fewer edges, while still maintaining tractability. We demonstrate that the resulting algorithm converges in fewer iterations to more accurate results on a variety of SRMs.", "target": "Lifted Region-Based Belief Propagation"}
{"id": "task1540-95a50954567148619889a2e87dbc5bd9", "input": "Social dynamics is concerned primarily with interactions among individuals and the resulting group behaviors, modeling the temporal evolution of social systems via the interactions of individuals within these systems. In particular, the availability of large-scale data from social networks and sensor networks offers an unprecedented opportunity to predict state-changing events at the individual level. Examples of such events include disease transmission, opinion transition in elections, and rumor propagation. Unlike previous research focusing on the collective effects of social systems, this study makes efficient inferences at the individual level. In order to cope with dynamic interactions among a large number of individuals, we introduce the stochastic kinetic model to capture adaptive transition probabilities and propose an efficient variational inference algorithm the complexity of which grows linearly \u2014 rather than exponentially\u2014 with the number of individuals. To validate this method, we have performed epidemic-dynamics experiments on wireless sensor network data collected from more than ten thousand people over three years. The proposed algorithm was used to track disease transmission and predict the probability of infection for each individual. Our results demonstrate that this method is more efficient than sampling while nonetheless achieving high accuracy.", "target": "Using Social Dynamics to Make Individual Predictions: Variational Inference with a Stochastic Kinetic Model"}
{"id": "task1540-20ac93fcac544f9eb7ee1a6dc4762ad8", "input": "We study the segmental recurrent neural network for end-to-end acoustic modelling. This model connects the segmental conditional random field (CRF) with a recurrent neural network (RNN) used for feature extraction. Compared to most previous CRF-based acoustic models, it does not rely on an external system to provide features or segmentation boundaries. Instead, this model marginalises out all the possible segmentations, and features are extracted from the RNN trained together with the segmental CRF. In essence, this model is self-contained and can be trained end-to-end. In this paper, we discuss practical training and decoding issues as well as the method to speed up the training in the context of speech recognition. We performed experiments on the TIMIT dataset. We achieved 17.3% phone error rate (PER) from the first-pass decoding \u2014 the best reported result using CRFs, despite the fact that we only used a zeroth-order CRF and without using any language model.", "target": "Segmental Recurrent Neural Networks for End-to-end Speech Recognition"}
{"id": "task1540-91e0cd88a2144728a461a176b70f5bd9", "input": "We consider the stochastic approximation problem where a convex function has to be minimized, given only the knowledge of unbiased estimates of its gradients at certain points, a framework which includes machine learning methods based on the minimization of the empirical risk. We focus on problems without strong convexity, for which all previously known algorithms achieve a convergence rate for function values of O(1/ \u221a", "target": "Non-strongly-convex smooth stochastic approximation with convergence rate O(1/n)"}
{"id": "task1540-a70af20310434c3a952549eed856845c", "input": "There is a small but growing body of research on statistical scripts, models of event sequences that allow probabilistic inference of implicit events from documents. These systems operate on structured verb-argument events produced by an NLP pipeline. We compare these systems with recent Recurrent Neural Net models that directly operate on raw tokens to predict sentences, finding the latter to be roughly comparable to the former in terms of predicting missing events in documents.", "target": "Using Sentence-Level LSTM Language Models for Script Inference"}
{"id": "task1540-7f5134cd68e64a0b8cb4d8999dc14f01", "input": "We study a surprising phenomenon related to the representation of a cloud of data points using polynomials. We start with the previously unnoticed empirical observation that, given a collection (a cloud) of data points, the sublevel sets of a certain distinguished polynomial capture the shape of the cloud very accurately. This distinguished polynomial is a sum-of-squares (SOS) derived in a simple manner from the inverse of the empirical moment matrix. In fact, this SOS polynomial is directly related to orthogonal polynomials and the Christoffel function. This allows to generalize and interpret extremality properties of orthogonal polynomials and to provide a mathematical rationale for the observed phenomenon. Among diverse potential applications, we illustrate the relevance of our results on a network intrusion detection task for which we obtain performances similar to existing dedicated methods reported in the literature.", "target": "Sorting out typicality with the inverse moment matrix SOS polynomial"}
{"id": "task1540-49e5844853614e959d6c4b5856544f8a", "input": "We present a sequential model for temporal relation classification between intrasentence events. The key observation is that the overall syntactic structure and compositional meanings of the multi-word context between events are important for distinguishing among fine-grained temporal relations. Specifically, our approach first extracts a sequence of context words that indicates the temporal relation between two events, which well align with the dependency path between two event mentions. The context word sequence, together with a parts-of-speech tag sequence and a dependency relation sequence that are generated corresponding to the word sequence, are then provided as input to bidirectional recurrent neural network (LSTM) models. The neural nets learn compositional syntactic and semantic representations of contexts surrounding the two events and predict the temporal relation between them. Evaluation of the proposed approach on TimeBank corpus shows that sequential modeling is capable of accurately recognizing temporal relations between events, which outperforms a neural net model using various discrete features as input that imitates previous feature based models.", "target": "A Sequential Model for Classifying Temporal Relations between Intra-Sentence Events"}
{"id": "task1540-f6167917d9a149c294c860aacf8e38b5", "input": "Pretraining is widely used in deep neutral network and one of the most famous pretraining models is Deep Belief Network (DBN). The optimization formulas are different during the pretraining process for different pretraining models. In this paper, we pretrained deep neutral network by different pretraining models and hence investigated the difference between DBN and Stacked Denoising Autoencoder (SDA) when used as pretraining model. The experimental results show that DBN get a better initial model. However the model converges to a relatively worse model after the finetuning process. Yet after pretrained by SDA for the second time the model converges to a better model if finetuned.", "target": "Multi-pretrained Deep Neural Network"}
{"id": "task1540-80c57c2857504655a0af308d4490761a", "input": "This paper proposes a simple test for compositionality (i.e., literal usage) of a word or phrase in a context-specific way. The test is computationally simple, relying on no external resources and only uses a set of trained word vectors. Experiments show that the proposed method is competitive with state of the art and displays high accuracy in context-specific compositionality detection of a variety of natural language phenomena (idiomaticity, sarcasm, metaphor) for different datasets in multiple languages. The key insight is to connect compositionality to a curious geometric property of word embeddings, which is of independent interest.", "target": "Geometry of Compositionality"}
{"id": "task1540-022b50d0c200416c944330bb71f5ed7b", "input": "Successive elimination of candidates is often a route to making manipulation intractable to compute. We prove that eliminating candidates does not necessarily increase the computational complexity of manipulation. However, for many voting rules used in practice, the computational complexity increases. For example, it is already known that it is NP-hard to compute how a single voter can manipulate the result of single transferable voting(the elimination version of plurality voting). We show here that it is NP-hard to compute how a single voter can manipulate the result of the elimination version of veto voting, of the closely related Coombs\u2019 rule, and of the elimination versions of a general class of scoring rules.", "target": "Eliminating the Weakest Link: Making Manipulation Intractable?"}
{"id": "task1540-0fca0add1e694b889dca3b0989a5b673", "input": "The longest arc-preserving common subsequence problem is an NP-hard combinatorial optimization problem from the field of computational biology. This problem finds applications, in particular, in the comparison of arc-annotated Ribonucleic acid (RNA) sequences. In this work we propose a simple, hybrid evolutionary algorithm to tackle this problem. The most important feature of this algorithm concerns a crossover operator based on solution merging. In solution merging, two or more solutions to the problem are merged, and an exact technique is used to find the best solution within this union. It is experimentally shown that the proposed algorithm outperforms a heuristic from the literature.", "target": "A Hybrid Evolutionary Algorithm Based on Solution Merging for the Longest Arc-Preserving Common Subsequence Problem"}
{"id": "task1540-3a2c99b998a040f4a841faaa15214ddc", "input": "Prosody affects the naturalness and intelligibility of speech. However, automatic prosody prediction from text for Chinese speech synthesis is still a great challenge and the traditional conditional random fields (CRF) based method always heavily relies on feature engineering. In this paper, we propose to use neural networks to predict prosodic boundary labels directly from Chinese characters without any feature engineering. Experimental results show that stacking feed-forward and bidirectional long short-term memory (BLSTM) recurrent network layers achieves superior performance over the CRF-based method. The embedding features learned from raw text further enhance the performance.", "target": "AUTOMATIC PROSODY PREDICTION FOR CHINESE SPEECH SYNTHESIS USING BLSTM-RNN AND EMBEDDING FEATURES"}
{"id": "task1540-88632c5d2efb4057b2136936b7fc2b74", "input": "Intersections constitute one of the most dangerous elements in road systems. Traffic signals remain the most common way to control traffic at high-volume intersections and offer many opportunities to apply intelligent transportation systems to make traffic more efficient and safe. This paper describes an automated method to estimate the temporal exposure of road users crossing the conflict zone to lateral collision with road users originating from a different approach. This component is part of a larger system relying on video sensors to provide queue lengths and spatial occupancy that are used for real time traffic control and monitoring. The method is evaluated on data collected during a real world experiment.", "target": "Automatic Estimation of the Exposure to Lateral Collision in Signalized Intersections using Video Sensors"}
{"id": "task1540-fbce6c38b61c4d4aa6637ac1449e1f41", "input": "It is the most effective way for quick translation of tremendous amount of explosively increasing science and technique information material to develop a practicable machine translation system and introduce it into translation practice. This essay treats problems arising from translation of isolated units on the basis of the practical materials and experiments obtained in the development and introduction of English-Korean machine translation system. In other words, this essay considers establishment of information for isolated units and their Korean equivalents and word order.", "target": "New Approach to translation of Isolated Units in English-Korean Machine Translation"}
{"id": "task1540-b7a30772b0b84c6fb63bf5923d4e5646", "input": "This paper describes InfoGAN, an information-theoretic extension to the Generative Adversarial Network that is able to learn disentangled representations in a completely unsupervised manner. InfoGAN is a generative adversarial network that also maximizes the mutual information between a small subset of the latent variables and the observation. We derive a lower bound of the mutual information objective that can be optimized efficiently. Specifically, InfoGAN successfully disentangles writing styles from digit shapes on the MNIST dataset, pose from lighting of 3D rendered images, and background digits from the central digit on the SVHN dataset. It also discovers visual concepts that include hair styles, presence/absence of eyeglasses, and emotions on the CelebA face dataset. Experiments show that InfoGAN learns interpretable representations that are competitive with representations learned by existing supervised methods.", "target": "InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets"}
{"id": "task1540-4050f92da0424078b78be330c5121959", "input": "Recurrent Neural Networks (RNN) have obtained excellent result in many natural language processing (NLP) tasks. However, understanding and interpreting the source of this success remains a challenge. In this paper, we propose Recurrent Memory Network (RMN), a novel RNN architecture, that not only amplifies the power of RNN but also facilitates our understanding of its internal functioning and allows us to discover underlying patterns in data. We demonstrate the power of RMN on language modeling and sentence completion tasks. On language modeling, RMN outperforms Long Short-Term Memory (LSTM) network on three large German, Italian, and English dataset. Additionally we perform indepth analysis of various linguistic dimensions that RMN captures. On Sentence Completion Challenge, for which it is essential to capture sentence coherence, our RMN obtains 69.2% accuracy, surpassing the previous stateof-the-art by a large margin.", "target": "Recurrent Memory Network for Language Modeling"}
{"id": "task1540-7f985d3ae38f4c5f9c046308d26d4437", "input": "The inclusion problem deals with how to characterize (in graphical terms) whether all independence statements in the model in\u00ad duced by a DAG K are in the model induced by a second DAG L. Meek (1997) conjec\u00ad tured that this inclusion holds iff there exists a sequence of DAGs from L to K such that only certain 'legal' arrow reversal and 'legal' arrow adding operations are performed to get the next DAG in the sequence. In this paper we give several characterizations of inclusion of DAG models and verify Meek's conjecture in the case that the DAGs K and L differ in at most one adjacency. As a warming up a rigorous proof of graphical characterizations of equivalence of DAGs is given.", "target": "On characterizing Inclusion of Bayesian Networks"}
{"id": "task1540-ce32f8118e2348948e92c736e072426a", "input": "Many combinatorial problems arising in machine learning can be reduced to the problem of minimizing a submodular function. Submodular functions are a natural discrete analog of convex functions, and can be minimized in strongly polynomial time. Unfortunately, state-of-the-art algorithms for general submodular minimization are intractable for larger problems. In this paper, we introduce a novel subclass of submodular minimization problems that we call decomposable. Decomposable submodular functions are those that can be represented as sums of concave functions applied to modular functions. We develop an algorithm, SLG, that can efficiently minimize decomposable submodular functions with tens of thousands of variables. Our algorithm exploits recent results in smoothed convex minimization. We apply SLG to synthetic benchmarks and a joint classification-and-segmentation task, and show that it outperforms the state-of-the-art general purpose submodular minimization algorithms by several orders of magnitude.", "target": "Efficient Minimization of Decomposable Submodular Functions"}
{"id": "task1540-21d296e8721a476ba90615abe5c6d295", "input": "Recently, there has been much interest in spectral approaches to learning manifolds\u2014 so-called kernel eigenmap methods. These methods have had some successes, but their applicability is limited because they are not robust to noise. To address this limitation, we look at two-manifold problems, in which we simultaneously reconstruct two related manifolds, each representing a different view of the same data. By solving these interconnected learning problems together and allowing information to flow between them, two-manifold algorithms are able to succeed where a non-integrated approach would fail: each view allows us to suppress noise in the other, reducing bias in the same way that an instrumental variable allows us to remove bias in a linear dimensionality reduction problem. We propose a class of algorithms for two-manifold problems, based on spectral decomposition of cross-covariance operators in Hilbert space. Finally, we discuss situations where two-manifold problems are useful, and demonstrate that solving a two-manifold problem can aid in learning a nonlinear dynamical system from limited data.", "target": "Two-Manifold Problems"}
{"id": "task1540-859ec65ea47143329cbc87d336137411", "input": "OmniGraph, a novel representation to support a range of NLP classification tasks, integrates lexical items, syntactic dependencies and frame semantic parses into graphs. Feature engineering is folded into the learning through convolution graph kernel learning to explore different extents of the graph. A high-dimensional space of features includes individual nodes to complex networks. In experiments on a text-forecasting problem that predicts stock price change from news for company mentions, OmniGraph beats several benchmarks based on bag-of-words, syntactic dependencies, and semantic trees. The highly expressive features OmniGraph discovers provide insights into the semantics across distinct market sectors. To demonstrate the method\u2019s generality, we also report its high performance results on a fine-grained sentiment corpus.", "target": "OmniGraph: Rich Representation and Graph Kernel Learning"}
{"id": "task1540-ce98b723d43144d08910e0fac9aa245b", "input": "We introduce and analyze a rigorous formulation of the dynamics of a signal processing scheme that aims at dense scanning of large input signals. Recently proposed methodologies lack a satisfactory discussion of whether they actually produce the correct results according to their definition, especially in the context of Convolutional Neural Networks. We improve on this through an exact characterization of the requirements for a sound sliding window approach. The tools developed in this paper are especially beneficial if Convolutional Neural Networks are employed, but can also be used as a more general framework to validate related approaches to signal scanning. The contributed theory helps to eliminate redundant computations and renders special case treatment unnecessary, resulting in a dramatic boost in efficiency particularly on massively parallel processors.", "target": "A Theory for Rapid Exact Signal Scanning with Deep Multi-Scale Convolutional Neural Networks"}
{"id": "task1540-b7ebb66d040648388af46912dfee1cbe", "input": "Reinforcement learning has been applied to many interesting problems such as the famous TD-gammon [1] and the inverted helicopter flight [2]. However little effort has been put into developing methods to learn policies for complex persistent tasks and tasks that are time-sensitive. In this paper we take a step towards solving this problem by using signal temporal logic (STL) as task specification, and taking advantage of the temporal abstraction feature that the options framework provide. We show via simulation that a relatively easy to implement algorithm that combines STL and options can learn a satisfactory policy with a small number of training cases.", "target": "A Hierarchical Reinforcement Learning Method for Persistent Time-Sensitive Tasks"}
{"id": "task1540-5dabf77d143949c89740249eda0ba663", "input": "We develop a streaming (one-pass, boundedmemory) word embedding algorithm based on the canonical skip-gram with negative sampling algorithm implemented in word2vec. We compare our streaming algorithm to word2vec empirically by measuring the cosine similarity between word pairs under each algorithm and by applying each algorithm in the downstream task of hashtag prediction on a two-month interval of the Twitter sample stream. We then discuss the results of these experiments, concluding they provide partial validation of our approach as a streaming replacement for word2vec. Finally, we discuss potential failure modes and suggest directions for future work.", "target": "Streaming Word Embeddings with the Space-Saving Algorithm"}
{"id": "task1540-30431adb59cb422e8ff6ba05b67c4e3f", "input": "There have been multiple attempts to resolve various inflection matching problems in information retrieval. Stemming is a common approach to this end. Among many techniques for stemming, statistical stemming has been shown to be effective in a number of languages, particularly highly inflected languages. In this paper we propose a method for finding affixes in different positions of a word. Common statistical techniques heavily rely on string similarity in terms of prefix and suffix matching. Since infixes are common in irregular/informal inflections in morphologically complex texts, it is required to find infixes for stemming. In this paper we propose a method whose aim is to find statistical inflectional rules based on minimum edit distance table of word pairs and the likelihoods of the rules in a language. These rules are used to statistically stem words and can be used in different text mining tasks. Experimental results on CLEF 2008 and CLEF 2009 English-Persian CLIR tasks indicate that the proposed method significantly outperforms all the baselines in terms of MAP.", "target": "SS4MCT: A Statistical Stemmer for Morphologically Complex Texts"}
{"id": "task1540-ddfba62ccaf84680bec10e24c6a6960b", "input": "We develop a new semantics for defeasible infer\u00ad ence based on extended probability measures al\u00ad lowed to take infinitesimal values, on the inter\u00ad pretation of defaults as generalized conditional probability constraints and on a preferred-model implementation of entropy-maximization.", "target": "Defaults and lnfinitesimals Defeasible Inference by Nonarchimedean Entropy Maximization"}
{"id": "task1540-37269f686c2b4477a63004f18e15ab1e", "input": "Currently there are lots of plagiarism detection approaches. But few of them implemented and adapted for Persian languages. In this paper, our work on designing and implementation of a plagiarism detection system based on preprocessing and NLP technics will be described. And the results of testing on a corpus will be presented. Keywords\u2014 External Plagiarism, Plagiarism, Copy detection, natural language processing, Artificial intelligence , Persian language.", "target": "Design a Persian Automated Plagiarism Detector (AMZPPD)"}
{"id": "task1540-39c244739ad24cd5b4a010fb448cac63", "input": "In this paper, we study a novel approach for named entity recognition (NER) and mention detection in natural language processing. Instead of treating NER as a sequence labelling problem, we propose a new local detection approach, which rely on the recent fixed-size ordinally forgetting encoding (FOFE) method to fully encode each sentence fragment and its left/right contexts into a fixed-size representation. Afterwards, a simple feedforward neural network is used to reject or predict entity label for each individual fragment. The proposed method has been evaluated in several popular NER and mention detection tasks, including the CoNLL 2003 NER task and TAC-KBP2015 and TAC-KBP2016 Trilingual Entity Discovery and Linking (EDL) tasks. Our methods have yielded pretty strong performance in all of these examined tasks. This local detection approach has shown many advantages over the traditional sequence labelling methods.", "target": "A FOFE-based Local Detection Approach for Named Entity Recognition and Mention Detection"}
{"id": "task1540-fbc2c44aaebe47e89bd9642a999d8270", "input": "This paper describe about a new methodology for developing and improving the robotics field via artificial intelligence and internet of things. Now a day, we can say Artificial Intelligence take the world into robotics. Almost all industries use robots for lot of works. They are use co-operative robots to make different kind of works. But there was some problem to make robot for multi tasks. So there was a necessary new methodology to made multi tasking robots. It will be done only by artificial intelligence and internet of things.", "target": "A Novel Method for Developing Robotics via Artificial Intelligence and Internet of Things "}
{"id": "task1540-137e5e587450422f8504073b32e0f58a", "input": "One weakness of machine-learned NLP models is that they typically perform poorly on out-of-domain data. In this work, we study the task of identifying products being bought and sold in online cybercrime forums, which exhibits particularly challenging cross-domain effects. We formulate a task that represents a hybrid of slot-filling information extraction and named entity recognition and annotate data from four different forums. Each of these forums constitutes its own \u201cfine-grained domain\u201d in that the forums cover different market sectors with different properties, even though all forums are in the broad domain of cybercrime. We characterize these domain differences in the context of a learning-based system: supervised models see decreased accuracy when applied to new forums, and standard techniques for semi-supervised learning and domain adaptation have limited effectiveness on this data, which suggests the need to improve these techniques. We release a dataset of 1,938 annotated posts from across the four forums.1", "target": "Identifying Products in Online Cybercrime Marketplaces: A Dataset for Fine-grained Domain Adaptation"}
{"id": "task1540-dd95b487861f4fce99b456e994b56810", "input": "A basic problem in the design of privacy-preserving algorithms is the private maximization problem: the goal is to pick an item from a universe that (approximately) maximizes a data-dependent function, all under the constraint of differential privacy. This problem has been used as a sub-routine in many privacy-preserving algorithms for statistics and machine-learning. Previous algorithms for this problem are either range-dependent\u2014i.e., their utility diminishes with the size of the universe\u2014or only apply to very restricted function classes. This work provides the first general-purpose, range-independent algorithm for private maximization that guarantees approximate differential privacy. Its applicability is demonstrated on two fundamental tasks in data mining and machine learning.", "target": "The Large Margin Mechanism for Differentially Private Maximization"}
{"id": "task1540-53c8f01a202348299f4ac193913e74b9", "input": "This paper addresses the task of set prediction using deep learning. This is important because the output of many computer vision tasks, including image tagging and object detection, are naturally expressed as sets of entities rather than vectors. As opposed to a vector, the size of a set is not fixed in advance, and it is invariant to the ordering of entities within it. We define a likelihood for a set distribution and learn its parameters using a deep neural network. We also derive a loss for predicting a discrete distribution corresponding to set cardinality. Set prediction is demonstrated on the problems of multi-class image classification and pedestrian detection. Our approach yields state-of-theart results in both cases on standard datasets.", "target": "DeepSetNet: Predicting Sets with Deep Neural Networks"}
{"id": "task1540-2ea1a3fec7774a70b67603cb7d3e8e7a", "input": "The introduction of loopy belief propagation (LBP) revitalized the application of graphical models in many domains. Many recent works present improvements on the basic LBP algorithm in an attempt to overcome convergence and local optima problems. Notable among these are convexified free energy approximations that lead to inference procedures with provable convergence and quality properties. However, empirically LBP still outperforms most of its convex variants in a variety of settings, as we also demonstrate here. Motivated by this fact we seek convexified free energies that directly approximate the Bethe free energy. We show that the proposed approximations compare favorably with state-of-the art convex free energy approximations.", "target": "Convexifying the Bethe Free Energy"}
{"id": "task1540-826e298a460841babdca4f546dd1313c", "input": "We propose a method for using synthetic data to help learning classifiers. Synthetic data, even is generated based on real data, normally results in a shift from the distribution of real data in feature space. To bridge the gap between the real and synthetic data, and jointly learn from synthetic and real data, this paper proposes a Multichannel Autoencoder(MCAE). We show that by suing MCAE, it is possible to learn a better feature representation for classification. To evaluate the proposed approach, we conduct experiments on two types of datasets. Experimental results on two datasets validate the efficiency of our MCAE model and our methodology of generating synthetic data.", "target": "Learning Classifiers from Synthetic Data Using a Multichannel Autoencoder"}
{"id": "task1540-a71ae4a63ac64382bcd207083a88d76c", "input": "Visual representation is crucial for a visual tracking method\u2019s performances. Conventionally, visual representations adopted in visual tracking rely on hand-crafted computer vision descriptors. These descriptors were developed generically without considering tracking-specific information. In this paper, we propose to learn complex-valued invariant representations from tracked sequential image patches, via strong temporal slowness constraint and stacked convolutional autoencoders. The deep slow local representations are learned offline on unlabeled data and transferred to the observational model of our proposed tracker. The proposed observational model retains old training samples to alleviate drift, and collect negative samples which are coherent with target\u2019s motion pattern for better discriminative tracking. With the learned representation and online training samples, a logistic regression classifier is adopted to distinguish target from background, and retrained online to adapt to appearance changes. Subsequently, the observational model is integrated into a particle filter framework to peform visual tracking. Experimental results on various challenging benchmark sequences demonstrate that the proposed tracker performs favourably against several state-of-the-art trackers.", "target": "Self-taught learning of a deep invariant representation for visual tracking via temporal slowness principle"}
{"id": "task1540-00f16a8f7150426b85a1244894f98879", "input": "This work introduces a probabilistic-based model for binary CSP that provides a fine grained analysis of its internal structure. Assuming that a domain modification could occur in the CSP, it shows how to express, in a predictive way, the probability that a domain value becomes inconsistent, then it express the expectation of the number of arc-inconsistent values in each domain of the constraint network. Thus, it express the expectation of the number of arc-inconsistent values for the whole constraint network. Next, it provides bounds for each of these three probabilistic indicators. Finally, a polytime algorithm, which propagates the probabilistic information, is presented.", "target": "A Probabilistic-Based Model for Binary CSP"}
{"id": "task1540-0142949520cd4d778a5c322acbf78da6", "input": "Preferences play an important role in our everyday lives. CP-networks, or CP-nets in short, are graphical models for representing conditional qualitative preferences under ceteris paribus (\u201call else being equal\u201d) assumptions. Despite their intuitive nature and rich representation, dominance testing with CP-nets is computationally complex, even when the CP-nets are restricted to binary-valued preferences. Tractable algorithms exist for binary CP-nets, but these algorithms are incomplete for multi-valued CPnets. In this paper, we identify a class of multivalued CP-nets, which we call more-or-less CPnets, that have the same computational complexity as binary CP-nets. More-or-less CP-nets exploit the monotonicity of the attribute values and use intervals to aggregate values that induce similar preferences. We then present a search control rule for dominance testing that effectively prunes the search space while preserving completeness.", "target": "More-or-Less CP-Networks"}
{"id": "task1540-b87d0faca4ff42609b18887128030897", "input": "Supplier selection is a typical multi-criteria decision making(MCDM) problem and lots of uncertain information exist inevitably. To address this issue, a new method was proposed based on interval data fusion. Our method follows the original way to generate classical basic probability assignment(BPA) determined by the distance among the evidences. However, the weights of criteria are kept as interval numbers to generate interval BPAs and do the fusion of interval BPAs. Finally, the order is ranked and the decision is made according to the obtained interval BPAs. In this paper, a numerical example of supplier selection is applied to verify the feasibility and validity of our method. The new method is presented aiming at solving multiplecriteria decision-making problems in which the weights of criteria or experts are described in fuzzy data like linguistic terms or interval data.", "target": "Evidential supplier selection based on interval data fusion"}
{"id": "task1540-41312b1488014d04b99031cb1516f18b", "input": "There has been a recent explosion in the capabilities of game-playing artificial intelligence. Many classes of RL tasks, from Atari games to motor control to board games, are now solvable by fairly generic algorithms, based on deep learning, that learn to play from experience with minimal knowledge of the specific domain of interest. In this work, we will investigate the performance of these methods on Super Smash Bros. Melee (SSBM), a popular console fighting game. The SSBM environment has complex dynamics and partial observability, making it challenging for human and machine alike. The multi-player aspect poses an additional challenge, as the vast majority of recent advances in RL have focused on single-agent environments. Nonetheless, we will show that it is possible to train agents that are competitive against and even surpass human professionals, a new result for the multi-player video game setting.", "target": "Beating the World\u2019s Best at Super Smash Bros. Melee with Deep Reinforcement Learning"}
{"id": "task1540-37320c84c21a4c9ea85db59fe42063cc", "input": "The computational bottleneck in applying online learning to massive data sets is usually the projection step. We present efficient online learning algorithms that eschew projections in favor of much more efficient linear optimization steps using the Frank-Wolfe technique. We obtain a range of regret bounds for online convex optimization, with better bounds for specific cases such as stochastic online smooth convex optimization. Besides the computational advantage, other desirable features of our algorithms are that they are parameter-free in the stochastic case and produce sparse decisions. We apply our algorithms to computationally intensive applications of collaborative filtering, and show the theoretical improvements to be clearly visible on standard datasets.", "target": "Projection-free Online Learning"}
{"id": "task1540-0142e93ba9014e93ab8a2f2a90a27f86", "input": "We introduce a method for learning the dynamics of complex nonlinear systems based on deep generative models over temporal segments of states and actions. Unlike dynamics models that operate over individual discrete timesteps, we learn the distribution over future state trajectories conditioned on past state, past action, and planned future action trajectories, as well as a latent prior over action trajectories. Our approach is based on convolutional autoregressive models and variational autoencoders. It makes stable and accurate predictions over long horizons for complex, stochastic systems, effectively expressing uncertainty and modeling the effects of collisions, sensory noise, and action delays. The learned dynamics model and action prior can be used for end-to-end, fully differentiable trajectory optimization and model-based policy optimization, which we use to evaluate the performance and sample-efficiency of our method.", "target": "Prediction and Control with Temporal Segment Models"}
{"id": "task1540-02309158448149829a170b96131ffdcc", "input": "We study the problem of identifying individuals based on their characteristic gaze patterns during reading of arbitrary text. The motivation for this problem is an unobtrusive biometric setting in which a user is observed during access to a document, but no specific challenge protocol requiring the user\u2019s time and attention is carried out. Existing models of individual differences in gaze control during reading are either based on simple aggregate features of eye movements, or rely on parametric density models to describe, for instance, saccade amplitudes or word fixation durations. We develop flexible semiparametric models of eye movements during reading in which densities are inferred under a Gaussian process prior centered at a parametric distribution family that is expected to approximate the true distribution well. An empirical study on reading data from 251 individuals shows significant improvements over the state of the art.", "target": "A Semiparametric Model for Bayesian Reader Identification"}
{"id": "task1540-34e572715e8247f487f7b96db2cb671b", "input": "Standard algorithms for finding the short\u00ad est path in a graph require that the cost of a path be additive in edge costs, and typically assume that costs are determinis\u00ad tic. We consider the problem of uncertain edge costs, with potential probabilistic de\u00ad pendencies among the costs. Although these dependencies violate the standard dynamic\u00ad programming decomposition, we identify a weaker stochastic consistency condition that justifies a generalized dynamic-programming approach based on stochastic dominance. We present a revised path-planning algorithm and prove that it produces optimal paths under time-dependent uncertain costs. We test the algorithm by applying it to a model of stochastic bus networks, and present em\u00ad pirical performance results comparing it to some alternatives. Finally, we consider ex\u00ad tensions of these concepts to a more general class of problems of heuristic search under uncertainty.", "target": "Path Planning under Time-Dependent Uncertainty"}
{"id": "task1540-5a6dbea51db44a5681563827bd434b45", "input": "Until now, error type performance for Grammatical Error Correction (GEC) systems could only be measured in terms of recall because system output is not annotated. In this paper, we overcome this problem by using a linguisticallyenhanced alignment to automatically extract the edits between parallel original and corrected sentences and then classify them using a new dataset-independent rule-based classifier. As human experts rated the predicted error types as \u201cGood\u201d or \u201cAcceptable\u201d in at least 95% of cases, we applied our approach to the system output produced in the CoNLL-2014 shared task to carry out a detailed analysis of system error type performance for the first time.", "target": "Automatic Annotation and Evaluation of Error Types for Grammatical Error Correction"}
{"id": "task1540-c37786fe5dd64d0aafde3ab9a4ee6eb4", "input": "The field of machine learning has taken a dramatic twist in recent times, with the rise of the Artificial Neural Network (ANN). These biologically inspired computational models are able to far exceed the performance of previous forms of artificial intelligence in common machine learning tasks. One of the most impressive forms of ANN architecture is that of the Convolutional Neural Network (CNN). CNNs are primarily used to solve difficult image-driven pattern recognition tasks and with their precise yet simple architecture, offers a simplified method of getting started with ANNs. This document provides a brief introduction to CNNs, discussing recently published papers and newly formed techniques in developing these brilliantly fantastic image recognition models. This introduction assumes you are familiar with the fundamentals of ANNs and machine learning.", "target": "An Introduction to Convolutional Neural Networks"}
{"id": "task1540-01817aedc6ee49248f51ddff64733e27", "input": "This paper presents a new deterministic approx\u00ad imation technique in Bayesian networks. This method, \"Expectation Propagation,\" unifies two previous techniques: assumed-density filtering, an extension of the Kalman filter, and loopy be\u00ad lief propagation, an extension of belief propaga\u00ad tion in Bayesian networks. Loopy belief propa\u00ad gation, because it propagates exact belief states, is useful for a limited class of belief networks, such as those which are purely discrete. Expec\u00ad tation Propagation approximates the belief states by only retaining expectations, such as mean and variance, and iterates until these expectations are consistent throughout the network. This makes it applicable to hybrid networks with discrete and continuous nodes. Experiments with Gaussian mixture models show Expectation Propagation to be convincingly better than methods with simi\u00ad lar computational cost: Laplace's method, vari\u00ad ational Bayes, and Monte Carlo. Expectation Propagation also provides an efficient algorithm for training Bayes point machine classifiers.", "target": "Expectation Propagation for Approximate Bayesian Inference"}
{"id": "task1540-e96b7528dac94d899fe995b649dd3436", "input": "We give an interpretation of the Maxi\u00ad mum Entropy (MaxEnt) Principle in game\u00ad theoretic terms. Based on this interpretation, we make a formal distinction between differ\u00ad ent ways of applying Maximum Entropy dis\u00ad tributions. MaxEnt has frequently been crit\u00ad icized on the grounds that it leads to highly representation dependent results. Our dis\u00ad tinction allows us to avoid this problem in many cases.", "target": "Maximum Entropy and the Glasses You are Looking Through"}
{"id": "task1540-c06788158fd34588bb910231bf556b59", "input": "We investigate the problem of learning discrete, undirected graphical models in a differentially private way. We show that the approach of releasing noisy sufficient statistics using the Laplace mechanism achieves a good trade-off between privacy, utility, and practicality. A naive learning algorithm that uses the noisy sufficient statistics \u201cas is\u201d outperforms general-purpose differentially private learning algorithms. However, it has three limitations: it ignores knowledge about the data generating process, rests on uncertain theoretical foundations, and exhibits certain pathologies. We develop a more principled approach that applies the formalism of collective graphical models to perform inference over the true sufficient statistics within an expectationmaximization framework. We show that this learns better models than competing approaches on both synthetic data and on real human mobility data used as a case study.", "target": "Differentially Private Learning of Undirected Graphical Models Using Collective Graphical Models"}
{"id": "task1540-b1e4855d38ca4c0f9e65c9aaa1d52ae2", "input": "Recurrent neural networks scale poorly due to the intrinsic difficulty in parallelizing their state computations. For instance, the forward pass computation of ht is blocked until the entire computation of ht\u22121 finishes, which is a major bottleneck for parallel computing. In this work, we propose an alternative RNN implementation by deliberately simplifying the state computation and exposing more parallelism. The proposed recurrent unit operates as fast as a convolutional layer and 5-10x faster than cuDNN-optimized LSTM. We demonstrate the unit\u2019s effectiveness across a wide range of applications including classification, question answering, language modeling, translation and speech recognition. We open source our implementation in PyTorch and CNTK1.", "target": "Training RNNs as Fast as CNNs"}
{"id": "task1540-9ac80d11d13b499081289e9afbc02a6c", "input": "We introduce Universum learning [1], [2] for multiclass problems and propose a novel formulation for multiclass universum SVM (MU-SVM). We also propose a span bound for MU-SVM that can be used for model selection thereby avoiding resampling. Empirical results demonstrate the effectiveness of MU-SVM and the proposed bound.", "target": "Universum Learning for Multiclass SVM"}
{"id": "task1540-9dc28fb05da84e088df9d1c550c9eb2e", "input": "In recent years significant progress has been made in successfully training recurrent neural networks (RNNs) on sequence learning problems involving long range temporal dependencies. The progress has been made on three fronts: (a) Algorithmic improvements involving sophisticated optimization techniques, (b) network design involving complex hidden layer nodes and specialized recurrent layer connections and (c) weight initialization methods. In this paper, we focus on recently proposed weight initialization with identity matrix for the recurrent weights in a RNN. This initialization is specifically proposed for hidden nodes with Rectified Linear Unit (ReLU) non linearity. We offer a simple dynamical systems perspective on weight initialization process, which allows us to propose a modified weight initialization strategy. We show that this initialization technique leads to successfully training RNNs composed of ReLUs. We demonstrate that our proposal produces comparable or better solution for three toy problems involving long range temporal structure: the addition problem, the multiplication problem and the MNIST classification problem using sequence of pixels. In addition, we present results for a benchmark action recognition problem.", "target": "IMPROVING PERFORMANCE OF RECURRENT NEURAL NETWORK WITH RELU NONLINEARITY"}
{"id": "task1540-5681dcf9cbae47b2b4fe6fc40bdd4a70", "input": "Sequence-to-sequence neural translation models learn semantic and syntactic relations between sentence pairs by optimizing the likelihood of the target given the source, i.e., p(y|x), an objective that ignores other potentially useful sources of information. We introduce an alternative objective function for neural MT that maximizes the mutual information between the source and target sentences, modeling the bi-directional dependency of sources and targets. We implement the model with a simple re-ranking method, and also introduce a decoding algorithm that increases diversity in the N-best list produced by the first pass. Applied to the WMT German/English and French/English tasks, the proposed models offers a consistent performance boost on both standard LSTM and attention-based neural MT architectures.", "target": "Mutual Information and Diverse Decoding Improve Neural Machine Translation"}
{"id": "task1540-a241fbc04f1c48c2b4e6048ec27b432a", "input": "*Rajdeep Borgohain Department of Computer Science and Engineering, Dibrugarh University Institute of Engineering and Technology, Dibrugarh, Assam Email: rajdeepgohain@gmail.com Sugata Sanyal School of Technology and Computer Science, Tata Ins titute of Fundamental Research, Mumbai, India Email: sanyals@gmail.com *Corresponding Author -------------------------------------------------------------------ABSTRACT-----------------------------------------------------------------The use of Artificial Intelligence is finding prominence not only in core computer areas, but also in cross disciplinary areas including medical diagnosis. In this paper, we present a rule based Expert System used in diagnosis of Cerebral Palsy. The expert system takes user input and depending on the symptoms of the patient, diagnoses if the patient is suffering from Cerebral Palsy. The Expert System also classifies the Cerebral Palsy as mil d, moderate or severe based on the presented symptoms.", "target": "Rule Based Expert System for Cerebral Palsy Diagnosis"}
{"id": "task1540-47ff39b65f0f425cb2c8c1e7c7dcec28", "input": "The reading comprehension task, that asks questions about a given evidence document, is a central problem in natural language understanding. Recent formulations of this task have typically focused on answer selection from a set of candidates pre-defined manually or through the use of an external NLP pipeline. However, Rajpurkar et al. (2016) recently released the SQUAD dataset in which the answers can be arbitrary strings from the supplied text. In this paper, we focus on this answer extraction task, presenting a novel model architecture that efficiently builds fixed length representations of all spans in the evidence document with a recurrent network. We show that scoring explicit span representations significantly improves performance over other approaches that factor the prediction into separate predictions about words or start and end markers. Our approach improves upon the best published results of Wang & Jiang (2016) by 5% and decreases the error of Rajpurkar et al.\u2019s baseline by > 50%.", "target": "EXTRACTIVE QUESTION ANSWERING"}
{"id": "task1540-d1a63fbdd01648e89358e7264a7d8026", "input": "Semi-supervised classification is an interesting idea where classification models are learned from both labeled and unlabeled data. It has several advantages over supervised classification in natural language processing domain. For instance, supervised classification exploits only labeled data that are expensive, often difficult to get, inadequate in quantity, and require human experts for annotation. On the other hand, unlabeled data are inexpensive and abundant. Despite the fact that many factors limit the wide-spread use of semi-supervised classification, it has become popular since its level of performance is empirically as good as supervised classification. This study explores the possibilities and achievements as well as complexity and limitations of semi-supervised classification for several natural langue processing tasks like parsing, biomedical information processing, text classification, and summarization.", "target": "Semi-supervised Classification for Natural Language Processing"}
{"id": "task1540-bc91d2b9383f4e80a9c3fa42f08a9e94", "input": "Typical techniques for sequence classification are designed for well-segmented sequences which have been edited to remove noisy or irrelevant parts. Therefore, such methods cannot be easily applied on noisy sequences expected in real-world applications. In this paper, we present the Temporal Attention-Gated Model (TAGM) which integrates ideas from attention models and gated recurrent networks to better deal with noisy or unsegmented sequences. Specifically, we extend the concept of attention model to measure the relevance of each observation (time step) of a sequence. We then use a novel gated recurrent network to learn the hidden representation for the final prediction. An important advantage of our approach is interpretability since the temporal attention weights provide a meaningful value for the salience of each time step in the sequence. We demonstrate the merits of our TAGM approach, both for prediction accuracy and interpretability, on three different tasks: spoken digit recognition, text-based sentiment analysis and visual event recognition.", "target": "Temporal Attention-Gated Model for Robust Sequence Classification"}
{"id": "task1540-692ca237acb445b38b23ec8d57ed8168", "input": "Abstract. We study the learning algorithm corresponding to the incremental gradient descent defined by the empirical risk over an infinite dimensional hypotheses space. We consider a statistical learning setting and show that, provided with a universal step-size and a suitable early stopping rule, the learning algorithm thus obtained is universally consistent and derive finite sample bounds. Our results provide a theoretical foundation for considering early stopping in online learning algorithms and shed light on the effect of allowing for multiple passes over the data.", "target": "REGULARIZATION BY EARLY STOPPING FOR ONLINE LEARNING ALGORITHMS"}
{"id": "task1540-178ade1d179744509292c61197eacf46", "input": "The field of Distributed Constraint Optimization has gained momentum in recent years thanks to its ability to address various applications related to multi-agent cooperation. While techniques to solve Distributed Constraint Optimization Problems (DCOPs) are abundant and have matured substantially since the field inception, the number of DCOP realistic applications and benchmark used to asses the performance of DCOP algorithms is lagging behind. To contrast this background we (i) introduce the Smart Home Device Scheduling (SHDS) problem, which describe the problem of coordinating smart devices schedules across multiple homes as a multi-agent system, (ii) detail the physical models adopted to simulate smart sensors, smart actuators, and homes environments, and (iii) introduce a DCOP realistic benchmark for SHDS problems.", "target": "A Realistic Dataset for the Smart Home Device Scheduling Problem for DCOPs"}
{"id": "task1540-66be79442e8e4e08b612c27cadd0db01", "input": "Mental illness is one of the most pressing public health issues of our time. While counseling and psychotherapy can be effective treatments, our knowledge about how to conduct successful counseling conversations has been limited due to lack of large-scale data with labeled outcomes of the conversations. In this paper, we present a large-scale, quantitative study on the discourse of text-message-based counseling conversations. We develop a set of novel computational discourse analysis methods to measure how various linguistic aspects of conversations are correlated with conversation outcomes. Applying techniques such as sequence-based conversation models, language model comparisons, message clustering, and psycholinguistics-inspired word frequency analyses, we discover actionable conversation strategies that are associated with better conversation outcomes.", "target": "Large-scale Analysis of Counseling Conversations: An Application of Natural Language Processing to Mental Health"}
{"id": "task1540-eef3f5334a624f36b4f7efc4d12d2eda", "input": "Recurrent neural network (RNN) based character-level language models (CLMs) are extremely useful for modeling unseen words by nature. However, their performance is generally much worse than the word-level language models (WLMs), since CLMs need to consider longer history of tokens to properly predict the next one. We address this problem by proposing hierarchical RNN architectures, which consist of multiple modules with different clock rates. Despite the multiclock structures, the input and output layers operate with the character-level clock, which allows the existing RNN CLM training approaches to be directly applicable without any modifications. Our CLM models show better perplexity than KneserNey (KN) 5-gram WLMs on the One Billion Word Benchmark with only 2% of parameters. Also, we present real-time character-level end-to-end speech recognition examples on the Wall Street Journal (WSJ) corpus, where replacing traditional mono-clock RNN CLMs with the proposed models results in better recognition accuracies even though the number of parameters are reduced to 30%.", "target": "Character-Level Language Modeling with Hierarchical Recurrent Neural Networks"}
{"id": "task1540-23d8cfac301a44b9a3f7f9075fa6a7ed", "input": "Pointwise matches between two time series are of great importance in time series analysis, and dynamic time warping (DTW) is known to provide generally reasonable matches. There are situations where time series alignment should be invariant to scaling and offset in amplitude or where local regions of the considered time series should be strongly reflected in pointwise matches. Two different variants of DTW, affine DTW (ADTW) and regional DTW (RDTW), are proposed to handle scaling and offset in amplitude and provide regional emphasis respectively. Furthermore, ADTW and RDTW can be combined in two different ways to generate alignments that incorporate advantages from both methods, where the affine model can be applied either globally to the entire time series or locally to each region. The proposed alignment methods outperform DTW on specific simulated datasets, and one-nearest-neighbor classifiers using their associated difference measures are competitive with the difference measures associated with state-of-the-art alignment methods on real datasets.", "target": "Affine and Regional Dynamic Time Warping"}
{"id": "task1540-cc4bc7f15f4a4162984a20d156317269", "input": "We study the computational complexity of candidate control in elections with few voters (that is, we take the number of voters as a parameter). We consider both the standard scenario of adding and deleting candidates, where one asks if a given candidate can become a winner (or, in the destructive case, can be precluded from winning) by adding/deleting some candidates, and a combinatorial scenario where adding/deleting a candidate automatically means adding/deleting a whole group of candidates. Our results show that the parameterized complexity of candidate control (with the number of voters as the parameter) is much more varied than in the setting with many voters.", "target": "Elections with Few Voters: Candidate Control Can Be Easy"}
{"id": "task1540-f635cfc0757040aba94da6e0b68913df", "input": "Social media and data mining are increasingly being used to analyse political and societal issues. Here we undertake the classification of social media users as supporting or opposing ongoing independence movements in their territories. Independence movements occur in territories whose citizens have conflicting national identities; users with opposing national identities will then support or oppose the sense of being part of an independent nation that differs from the officially recognised country. We describe a methodology that relies on users\u2019 self-reported location to build datasets for three territories \u2013 Catalonia, the Basque Country and Scotland \u2013 and we test language-independent classifiers using four types of features. We show the effectiveness of the approach to build large annotated datasets, and the ability to achieve accurate, language-independent classification performances ranging from 85% to 97% for the three territories under study. A data analysis shows the existence of echo chambers that isolate opposing national identities from each other.", "target": "Stance Classification of Social Media Users in Independence Movements"}
{"id": "task1540-11e977ff07b84639b28f5ebe4b10d670", "input": "Auto-encoders are perhaps the best-known non-probabilistic methods for representation learning. They are conceptually simple and easy to train. Recent theoretical work has shed light on their ability to capture manifold structure, and drawn connections to density modeling. This has motivated researchers to seek ways of auto-encoder scoring, which has furthered their use in classification. Gated auto-encoders (GAEs) are an interesting and flexible extension of auto-encoders which can learn transformations among different images or pixel covariances within images. However, they have been much less studied, theoretically or empirically. In this work, we apply a dynamical systems view to GAEs, deriving a scoring function, and drawing connections to Restricted Boltzmann Machines. On a set of deep learning benchmarks, we also demonstrate their effectiveness for single and multi-label classification.", "target": "Scoring and Classifying with Gated Auto-encoders"}
{"id": "task1540-edfa218aaed644538cb9910e13057cd0", "input": "Data-to-text systems are powerful in generating reports from data automatically and thus they simplify the presentation of complex data. Rather than presenting data using visualisation techniques, datato-text systems use natural (human) language, which is the most common way for human-human communication. In addition, data-to-text systems can adapt their output content to users\u2019 preferences, background or interests and therefore they can be pleasant for users to interact with. Content selection is an important part of every data-to-text system, because it is the module that determines which from the available information should be conveyed to the user. This survey initially introduces the field of data-to-text generation, describes the general data-to-text system architecture and then it reviews the state-ofthe-art content selection methods. Finally, it provides recommendations for choosing an approach and discusses opportunities for future research.", "target": "Content Selection in Data-to-Text Systems: A Survey"}
{"id": "task1540-e8b9dcb486a44464a171356c23720bf8", "input": "Based on the in-depth analysis of the essence and features of vague phenomena, this paper focuses on establishing the axiomatical foundation of membership degree theory for vague phenomena, presents an axiomatic system to govern membership degrees and their interconnections. On this basis, the concept of vague partition is introduced, further, the concept of fuzzy set introduced by Zadeh in 1965 is redefined based on vague partition from the perspective of axiomatization. The thesis defended in this paper is that the relationship among vague attribute values should be the starting point to recognize and model vague phenomena from a quantitative view.", "target": "Redefinition of the concept of fuzzy set based on vague partition from the perspective of axiomatization"}
{"id": "task1540-5379c91116904b21a8d10d6ed6897f56", "input": "We address the problem of learning a ranking by using adaptively chosen pairwise comparisons. Our goal is to recover the ranking accurately but to sample the comparisons sparingly. If all comparison outcomes are consistent with the ranking, the optimal solution is to use an efficient sorting algorithm, such as Quicksort. But how do sorting algorithms behave if some comparison outcomes are inconsistent with the ranking? We give favorable guarantees for Quicksort for the popular Bradley\u2013Terry model, under natural assumptions on the parameters. Furthermore, we empirically demonstrate that sorting algorithms lead to a very simple and effective active learning strategy: repeatedly sort the items. This strategy performs as well as state-of-the-art methods (and much better than random sampling) at a minuscule fraction of the computational cost.", "target": "Just Sort It! A Simple and Effective Approach to Active Preference Learning"}
{"id": "task1540-ea65dfa46a6d4bd3ac3a48cf70bacaa3", "input": "In this paper, we propose convolutional neural networks for learning an optimal representation of question and answer sentences. Their main aspect is the use of relational information given by the matches between words from the two members of the pair. The matches are encoded as embeddings with additional parameters (dimensions), which are tuned by the network. These allows for better capturing interactions between questions and answers, resulting in a significant boost in accuracy. We test our models on two widely used answer sentence selection benchmarks. The results clearly show the effectiveness of our relational information, which allows our relatively simple network to approach the state of the art.", "target": "Modeling Relational Information in Question-Answer Pairs with Convolutional Neural Networks"}
{"id": "task1540-739d2ba237e646e6811abc5c3e775075", "input": "We investigate the integration of word embeddings as classification features in the setting of large scale text classification. Such representations have been used in a plethora of tasks, however their application in classification scenarios with thousands of classes has not been extensively researched, partially due to hardware limitations. In this work, we examine efficient composition functions to obtain document-level from word-level embeddings and we subsequently investigate their combination with the traditional one-hot-encoding representations. By presenting empirical evidence on large, multi-class, multi-label classification problems, we demonstrate the efficiency and the performance benefits of this combination.", "target": "An empirical study on large scale text classification with skip-gram embeddings"}
{"id": "task1540-e05eaf6f889e45298edb11da7b5e89cf", "input": "Causal inference from observational data is a subject of active research and development in statistics and computer science. Many toolkits have been developed for this purpose that depends on statistical software. However, these toolkits do not scale to large datasets. In this paper we describe a suite of techniques for expressing causal inference tasks from observational data in SQL. This suite supports the state-ofthe-art methods for causal inference and run at scale within a database engine. In addition, we introduce several optimization techniques that significantly speedup causal inference, both in the online and offline setting. We evaluate the quality and performance of our techniques by experiments of real datasets.", "target": "ZaliQL: A SQL-Based Framework for Drawing Causal Inference from Big Data"}
{"id": "task1540-37dafd8efcc74dcca7330b6a316e25ff", "input": "In earlier work, we introduced flexible infer\u00ad ence and decision-theoretic metareasoning to address the intractability of normative infer\u00ad ence. Here, rather than pursuing the task of computing beliefs and actions with decision models composed of distinctions about uncer\u00ad tain events, we examine methods for inferring beliefs about mathematical truth before an automated theorem prover completes a proof. We employ a Bayesian analysis to update be\u00ad lief in truth, given theorem-proving progress, and show how decision-theoretic methods can be used to determine the value of continuing to deliberate versus taking immediate action in time-critical situations.", "target": "Studies of Theorem Proving under Limited Resources"}
{"id": "task1540-dc1b08d6af8d4970a5b8e8c57680ceca", "input": "We propose a novel module, the reviewer module, to improve the encoder-decoder learning framework. The reviewer module is generic, and can be plugged into an existing encoder-decoder model. The reviewer module performs a number of review steps with attention mechanism on the encoder hidden states, and outputs a fact vector after each review step; the fact vectors are used as the input of the attention mechanism in the decoder. We show that the conventional encoderdecoders are a special case of our framework. Empirically, we show that our framework can improve over state-of-the-art encoder-decoder systems on the tasks of image captioning and source code captioning.", "target": "Encode, Review, and Decode: Reviewer Module for Caption Generation"}
{"id": "task1540-6fd6487fa0894c0b80f88b732a251803", "input": "In this paper we propose a novel model for unconditional audio generation based on generating one audio sample at a time. We show that our model, which profits from combining memory-less modules, namely autoregressive multilayer perceptrons, and stateful recurrent neural networks in a hierarchical structure is able to capture underlying sources of variations in the temporal sequences over very long time spans, on three datasets of different nature. Human evaluation on the generated samples indicate that our model is preferred over competing models. We also show how each component of the model contributes to the exhibited performance.", "target": "SAMPLERNN: AN UNCONDITIONAL END-TO-END NEURAL AUDIO GENERATION MODEL"}
{"id": "task1540-be30e0f880de4e5ca41dd6423c8bc6c9", "input": "Discriminative translation models utilizing source context have been shown to help statistical machine translation performance. We propose a novel extension of this work using target context information. Surprisingly, we show that this model can be efficiently integrated directly in the decoding process. Our approach scales to large training data sizes and results in consistent improvements in translation quality on four language pairs. We also provide an analysis comparing the strengths of the baseline source-context model with our extended source-context and targetcontext model and we show that our extension allows us to better capture morphological coherence. Our work is freely available as part of Moses.", "target": "Target-Side Context for Discriminative Models in Statistical Machine Translation"}
{"id": "task1540-5aa229a0810a41bd92c036fc12464a65", "input": "We propose a sequence labeling framework with a secondary training objective, learning to predict surrounding words for every word in the dataset. This language modeling objective incentivises the framework to learn general-purpose patterns of semantic and syntactic composition, which are also useful for improving accuracy on different sequence labeling tasks. The architecture was evaluated on 8 datasets, covering the tasks of error detection in learner texts, named entity recognition, chunking and POS-tagging. The novel language modeling objective provided consistent performance improvements on every benchmark, without requiring any additional annotated or unannotated data.", "target": "Semi-supervised Multitask Learning for Sequence Labeling"}
{"id": "task1540-2561053cc9a24d098c6745701cca78f2", "input": "Unsupervised dictionary learning has been a key component in state-of-the-art computer vision recognition architectures. While highly effective methods exist for patchbased dictionary learning, these methods may learn redundant features after the pooling stage in a given early vision architecture. In this paper, we offer a novel dictionary learning scheme to efficiently take into account the invariance of learned features after the spatial pooling stage. The algorithm is built on simple clustering, and thus enjoys efficiency and scalability. We discuss the underlying mechanism that justifies the use of clustering algorithms, and empirically show that the algorithm finds better dictionaries than patch-based methods with the same dictionary size.", "target": "Pooling-Invariant Image Feature Learning"}
{"id": "task1540-4d3af9f93461461bb9cbcabd9077f270", "input": "Stochastic gradient-boosted decision trees are widely employed for multivariate classification and regression tasks. This paper presents a speed-optimized and cache-friendly implementation for multivariate classification called FastBDT. FastBDT is one order of magnitude faster during the fitting-phase and application-phase, in comparison with popular implementations in software frameworks like TMVA, scikit-learn and XGBoost. The concepts used to optimize the execution time and performance studies are discussed in detail in this paper. The key ideas include: An equal-frequency binning on the input data, which allows replacing expensive floating-point with integer operations, while at the same time increasing the quality of the classification; a cache-friendly linear access pattern to the input data, in contrast to usual implementations, which exhibit a random access pattern. FastBDT provides interfaces to C/C++, Python and TMVA. It is extensively used in the field of high energy physics by the Belle II experiment.", "target": "A speed-optimized and cache-friendly implementation of stochastic gradient-boosted decision trees for multivariate classification"}
{"id": "task1540-7845fade6fc74038879f39e7dea2efe3", "input": "In a recent article, Christiansen and Chater (2015) present a fundamental constraint on language, i.e. a now-or-never bottleneck that arises from our fleeting memory, and explore its implications, e.g., chunk-and-pass processing, outlining a framework that promises to unify different areas of research. Here we explore additional support for this constraint and suggest further connections from quantitative linguistics and information theory.", "target": "A commentary on \u201cThe now-or-never bottleneck: a fundamental constraint on language\u201d, by Christiansen and Chater (2015)"}
{"id": "task1540-80e25b5b49584c0fba10e05404166963", "input": "The rise of robotic applications has led to the generation of a huge volume of unstructured data, whereas the current cloud infrastructure was designed to process limited amounts of structured data. To address this problem, we propose a learn-memorize-recall-reduce paradigm for robotic cloud computing. The learning stage converts incoming unstructured data into structured data; the memorization stage provides effective storage for the massive amount of data; the recall stage provides efficient means to retrieve the raw data; while the reduction stage provides means to make sense of this massive amount of unstructured data with limited computing resources.", "target": "Learn-Memorize-Recall-Reduce: A Robotic Cloud Computing Paradigm"}
{"id": "task1540-da7be85762304c6a9c03f205c801a886", "input": "We consider the problem of off-policy evaluation\u2014estimating the value of a target policy using data collected by another policy\u2014under the contextual bandit model. We establish a minimax lower bound on the mean squared error (MSE), and show that it is matched up to constant factors by the inverse propensity scoring (IPS) estimator. Since in the multi-armed bandit problem the IPS is suboptimal [Li et al., 2015], our result highlights the difficulty of the contextual setting with non-degenerate context distributions. We further consider improvements on this minimax MSE bound, given access to a reward model. We show that the existing doubly robust approach, which utilizes such a reward model, may continue to suffer from high variance even when the reward model is perfect. We propose a new estimator called SWITCH which more effectively uses the reward model and achieves a superior bias-variance tradeoff compared with prior work. We prove an upper bound on its MSE and demonstrate its benefits empirically on a diverse collection of datasets, often seeing orders of magnitude improvements over a number of baselines.", "target": "Optimal and Adaptive Off-policy Evaluation in Contextual Bandits"}
{"id": "task1540-a8ecfbfc89b34853b72f04c34decef43", "input": "Neural network based models are a very powerful tool for creating word embeddings, the objective of these models is to group similar words together. These embeddings have been used as features to improve results in various applications such as document classification, named entity recognition, etc. Neural language models are able to learn word representations which have been used to capture semantic shifts across time and geography. The objective of this paper is to first identify and then visualize how words change meaning in different text corpus. We will train a neural language model on texts from a diverse set of disciplines \u2013 philosophy, religion, fiction etc. Each text will alter the embeddings of the words to represent the meaning of the word inside that text. We will present a computational technique to detect words that exhibit significant linguistic shift in meaning and usage. We then use enhanced scatterplots and storyline visualization to visualize the linguistic shift", "target": "Visualizing Linguistic Shift"}
{"id": "task1540-caf585b1d6e743bcbca9c4c529a38de1", "input": "Catastrophic forgetting is a problem which refers to losing the information of the first task after training from the second task in continual learning of neural networks. To resolve this problem, we propose the incremental moment matching (IMM), which uses the Bayesian neural network framework. IMM assumes that the posterior distribution of parameters of neural networks is approximated with Gaussian distribution and incrementally matches the moment of the posteriors, which are trained for the first and second task, respectively. To make our Gaussian assumption reasonable, the IMM procedure utilizes various transfer learning techniques including weight transfer, L2-norm of old and new parameters, and a newly proposed variant of dropout using old parameters. We analyze our methods on the MNIST and CIFAR-10 datasets, and then evaluate them on a real-world life-log dataset collected using Google Glass. Experimental results show that IMM produces state-of-the-art performance in a variety of datasets.", "target": "Overcoming Catastrophic Forgetting by Incremental Moment Matching"}
{"id": "task1540-7e7c617a77d84218915c2397908890ba", "input": "Clustering categorical distributions in the probability simplex is a fundamental primitive often met in applications dealing with histograms or mixtures of multinomials. Traditionally, the differentialgeometric structure of the probability simplex has been used either by (i) setting the Riemannian metric tensor to the Fisher information matrix of the categorical distributions, or (ii) defining the informationgeometric structure induced by a smooth dissimilarity measure, called a divergence. In this paper, we introduce a novel computationally-friendly non-Riemannian framework for modeling the probability simplex: Hilbert simplex geometry. We discuss the pros and cons of those three statistical modelings, and compare them experimentally for clustering tasks.", "target": "Clustering in Hilbert simplex geometry"}
{"id": "task1540-079ef59de16749da80f17d00ce0755ac", "input": "In natural speech, the speaker does not pause between words, yet a human listener somehow perceives this continuous stream of phonemes as a series of distinct words. The detection of boundaries between spoken words is an instance of a general capability of the human neocortex to remember and to recognize recurring sequences. This paper describes a computer algorithm that is designed to solve the problem of locating word boundaries in blocks of English text from which the spaces have been removed. This problem avoids the complexities of processing speech but requires similar capabilities for detecting recurring sequences. The algorithm that is described in this paper relies entirely on statistical relationships between letters in the input stream to infer the locations of word boundaries. The source code for a C++ version of this algorithm is presented in an appendix.", "target": "A Statistical Learning Algorithm for Word Segmentation"}
{"id": "task1540-0217b9cf17244b5e82814343e4871aee", "input": "Regularization is a well studied problem in the context of neural networks. It is usually used to improve the generalization performance when the number of input samples is relatively small or heavily contaminated with noise. The regularization of a parametric model can be achieved in different manners some of which are early stopping (Morgan and Bourlard, 1990), weight decay, output smoothing that are used to avoid overfitting during the training of the considered model. From a Bayesian point of view, many regularization techniques correspond to imposing certain prior distributions on model parameters (Krogh and Hertz, 1991). Using Bishop\u2019s approximation (Bishop, 1995) of the objective function when a restricted type of noise is added to the input of a parametric function, we derive the higher order terms of the Taylor expansion and analyze the coefficients of the regularization terms induced by the noisy input. In particular we study the effect of penalizing the Hessian of the mapping function with respect to the input in terms of generalization performance. We also show how we can control independently this coefficient by explicitly penalizing the Jacobian of the mapping function on corrupted inputs.", "target": "Adding noise to the input of a model trained with a regularized objective"}
{"id": "task1540-99a00d57de4b4044a65d7061a3cc2478", "input": "The Bacterial Foraging Optimization (BFO) is one of the metaheuristics algorithms that most widely used to solve optimization problems. The BFO is imitated from the behavior of the foraging bacteria group such as Ecoli. The main aim of algorithm is to eliminate those bacteria that have weak foraging methods and maintaining those bacteria that have strong foraging methods. In this extent, each bacterium communicates with other bacteria by sending signals such that bacterium change the position in the next step if prior factors have been satisfied. In fact, the process of algorithm allows bacteria to follow up nutrients toward the optimal. In this paper, the BFO is used for the solutions of Quadratic Assignment Problem (QAP), and multiobjective QAP (mQAP) by using updating mechanisms including mutation, crossover, and a local search.", "target": "Bacteria Foraging Algorithm with Genetic Operators for the Solution of QAP and mQAP"}
{"id": "task1540-b66d049484834f4f98a166a8ad860e20", "input": "There are two main approaches to the distributed representation of words: lowdimensional deep learning embeddings and high-dimensional distributional models, in which each dimension corresponds to a context word. In this paper, we combine these two approaches by learning embeddings based on distributionalmodel vectors \u2013 as opposed to one-hot vectors as is standardly done in deep learning. We show that the combined approach has better performance on a word relatedness judgment task.", "target": "Distributional Models and Deep Learning Embeddings: Combining the Best of Both Worlds"}
{"id": "task1540-33d1ad9685f04f85b97a287c19083120", "input": "We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vectors for sentence-level classification tasks. We first show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks. Learning task-specific vectors through fine-tuning offers further gains in performance. We additionally propose a simple modification to the architecture to allow for the use of both taskspecific and static word vectors. The CNN models discussed herein improve upon the state-of-the-art on 4 out of 7 tasks, which include sentiment analysis and question classification.", "target": "Convolutional Neural Networks for Sentence Classification"}
{"id": "task1540-2002f174e2e04ac6a4c0121b76b48a41", "input": "Humans comprehend the meanings and relations of discourses heavily relying on their semantic memory that encodes general knowledge about concepts and facts. Inspired by this, we propose a neural recognizer for implicit discourse relation analysis, which builds upon a semantic memory that stores knowledge in a distributed fashion. We refer to this recognizer as SeMDER. Starting from word embeddings of discourse arguments, SeMDER employs a shallow encoder to generate a distributed surface representation for a discourse. A semantic encoder with attention to the semantic memory matrix is further established over surface representations. It is able to retrieve a deep semantic meaning representation for the discourse from the memory. Using the surface and semantic representations as input, SeMDER finally predicts implicit discourse relations via a neural recognizer. Experiments on the benchmark data set show that SeMDER benefits from the semantic memory and achieves substantial improvements of 2.56% on average over current state-of-the-art baselines in terms of F1-score.", "target": "Neural Discourse Relation Recognition with Semantic Memory"}
{"id": "task1540-e1830193b2b8425da4020b2ec09fc74c", "input": "Recent works have highlighted scale invariance or symmetry present in the weight space of a typical deep network and the adverse effect it has on the Euclidean gradient based stochastic gradient descent optimization. In this work, we show that a commonly used deep network, which uses convolution, batch normalization, reLU, max-pooling, and sub-sampling pipeline, possess more complex forms of symmetry arising from scaling-based reparameterization of the network weights. We propose to tackle the issue of the weight space symmetry by constraining the filters to lie on the unit-norm manifold. Consequently, training the network boils down to using stochastic gradient descent updates on the unit-norm manifold. Our empirical evidence based on the MNIST dataset shows that the proposed updates improve the test performance beyond what is achieved with batch normalization and without sacrificing the computational efficiency of the weight updates.", "target": "Understanding symmetries in deep networks"}
{"id": "task1540-e9188bee8bd14f55ad4819a05faebc4c", "input": "Local consistency techniques such as k-consistency are a key component of specialised solvers for constraint satisfaction problems. In this paper we show that the power of using k-consistency techniques on a constraint satisfaction problem is precisely captured by using a particular inference rule, which we call negative-hyper-resolution, on the standard direct encoding of the problem into Boolean clauses. We also show that current clauselearning SAT-solvers will discover in expected polynomial time any inconsistency that can be deduced from a given set of clauses using negative-hyper-resolvents of a fixed size. We combine these two results to show that, without being explicitly designed to do so, current clause-learning SAT-solvers efficiently simulate k-consistency techniques, for all fixed values of k. We then give some experimental results to show that this feature allows clause-learning SAT-solvers to efficiently solve certain families of constraint problems which are challenging for conventional constraint-programming solvers.", "target": "Local Consistency and SAT-Solvers"}
{"id": "task1540-de650fd0ec3a42cba8841fc1ade92500", "input": "Opinion mining from customer reviews has become pervasive in recent years. Sentences in reviews, however, are usually classified independently, even though they form part of a review\u2019s argumentative structure. Intuitively, sentences in a review build and elaborate upon each other; knowledge of the review structure and sentential context should thus inform the classification of each sentence. We demonstrate this hypothesis for the task of aspect-based sentiment analysis by modeling the interdependencies of sentences in a review with a hierarchical bidirectional LSTM. We show that the hierarchical model outperforms two non-hierarchical baselines, obtains results competitive with the state-of-the-art, and outperforms the state-of-the-art on five multilingual, multi-domain datasets without any handengineered features or external resources.", "target": "A Hierarchical Model of Reviews for Aspect-based Sentiment Analysis"}
{"id": "task1540-1a5deb6e330149fd9d92536870fc6ce0", "input": "Analysing sentiment of tweets is important as it helps to determine the users\u2019 opinion. Knowing people\u2019s opinion is crucial for several purposes starting from gathering knowledge about customer base, e-governance, campaignings and many more. In this report, we aim to develop a system to detect the sentiment from tweets. We employ several linguistic features along with some other external sources of information to detect the sentiment of a tweet. We show that augmenting the 140 character-long tweet with information harvested from external urls shared in the tweet as well as Social Media features enhances the sentiment prediction accuracy significantly.", "target": "Sentiment Analysis for Twitter : Going Beyond Tweet Text"}
{"id": "task1540-e04943327ad8411a9ff8921ff451cc1d", "input": "This paper describes an application of Bayesian programming to the control of an autonomous avatar in a multiplayer role-playing game (the example is based on World of Warcraft). We model a particular task, which consists of choosing what to do and to select which target in a situation where allies and foes are present. We explain the model in Bayesian programming and show how we could learn the conditional probabilities from data gathered during human-played sessions.", "target": "Bayesian Modeling Of An Human MMORPG Player"}
{"id": "task1540-9891e8e0fe724657ab2d34e00995fd1f", "input": "Concept drift is a major issue that greatly affects the accuracy and reliability of many real-world applications of machine learning. We argue that to tackle concept drift it is important to develop the capacity to describe and analyze it. We propose tools for this purpose, arguing for the importance of quantitative descriptions of drift in marginal distributions. We present quantitative drift analysis techniques along with methods for communicating their results. We demonstrate their effectiveness by application to three real-world learning tasks.", "target": "Understanding Concept Drift"}
{"id": "task1540-f53dd9215fe94f70835e4a8413ae4fd5", "input": "In this paper, we propose a novel neural approach for paraphrase generation. Conventional paraphrase generation methods either leverage hand-written rules and thesauri-based alignments, or use statistical machine learning principles. To the best of our knowledge, this work is the first to explore deep learning models for paraphrase generation. Our primary contribution is a stacked residual LSTM network, where we add residual connections between LSTM layers. This allows for efficient training of deep LSTMs. We experiment with our model and other state-of-the-art deep learning models on three different datasets: PPDB, WikiAnswers and MSCOCO. Evaluation results demonstrate that our model outperforms sequence to sequence, attention-based and bi-directional LSTM models on BLEU, METEOR, TER and an embedding-based sentence similarity metric.", "target": "Neural Paraphrase Generation with Stacked Residual LSTM Networks"}
{"id": "task1540-84bd89b854b1491fb6c5b0b82e6fd729", "input": "Deep neural networks (DNN) are the state of the art on many engineering problems such as computer vision and audition. A key factor in the success of the DNN is scalability \u2013 bigger networks work better. However, the reason for this scalability is not yet well understood. Here, we interpret the DNN as a discrete system, of linear filters followed by nonlinear activations, that is subject to the laws of sampling theory. In this context, we demonstrate that over-sampled networks are more selective, learn faster and learn more robustly. Our findings may ultimately generalize to the human brain.", "target": "Over-Sampling in a Deep Neural Network_arxiv"}
{"id": "task1540-5121e28562ab4989a420c41f6a45f12d", "input": "In this paper, the performance of two dependency parsers, namely Stanford and Minipar, on biomedical texts has been reported. The performance of the parsers to assign dependencies between two biomedical concepts that are already proved to be connected is not satisfying. Both Stanford and Minipar, being statistical parsers, fail to assign dependency relation between two connected concepts if they are distant by at least one clause. Minipar\u2019s performance, in terms of precision, recall and the F-Score of the attachment score (e.g., correctly identified head in a dependency), to parse biomedical text is also measured taking the Stanford\u2019s as a gold standard. The results suggest that Minipar is not suitable yet to parse biomedical texts. In addition, a qualitative investigation reveals that the difference between working principles of the parsers also play a vital role for Minipar\u2019s degraded performance.", "target": "Performance of Stanford and Minipar Parser on Biomedical Texts"}
{"id": "task1540-509b3f6c0b324decaf4146786c742b7e", "input": "We consider the problem of repeatedly solving a variant of the same dynamic programming problem in successive trials. An instance of the type of problems we consider is to find the optimal binary search tree. At the beginning of each trial, the learner probabilistically chooses a tree with the n keys at the internal nodes and the n+ 1 gaps between keys at the leaves. It is then told the frequencies of the keys and gaps and is charged by the average search cost for the chosen tree. The problem is online because the frequencies can change between trials. The goal is to develop algorithms with the property that their total average search cost (loss) in all trials is close to the total loss of the best tree chosen in hind sight for all trials. The challenge, of course, is that the algorithm has to deal with exponential number of trees. We develop a methodology for tackling such problems for a wide class of dynamic programming algorithms. Our framework allows us to extend online learning algorithms like Hedge [9] and Component Hedge [15] to a significantly wider class of combinatorial objects than was possible before.", "target": "Online Dynamic Programming"}
{"id": "task1540-ec24376513064f098aeaebabb0ba6c2b", "input": "Semantic role labeling (SRL) is the task of identifying the predicate-argument structure of a sentence. It is typically regarded as an important step in the standard natural language processing pipeline, providing information to downstream tasks such as information extraction and question answering. As the semantic representations are closely related to syntactic ones, we exploit syntactic information in our model. We propose a version of graph convolutional networks (GCNs), a recent class of multilayer neural networks operating on graphs, suited to modeling syntactic dependency graphs. GCNs over syntactic dependency trees are used as sentence encoders, producing latent feature representations of words in a sentence and capturing information relevant to predicting the semantic representations. We observe that GCN layers are complementary to LSTM ones: when we stack both GCN and LSTM layers, we obtain a substantial improvement over an already stateof-the-art LSTM SRL model, resulting in the best reported scores on the standard benchmark (CoNLL-2009) both for Chinese and English.", "target": "Encoding Sentences with Graph Convolutional Networks for Semantic Role Labeling"}
{"id": "task1540-97375a901eea4541b53f27bb72ab3a23", "input": "Ever growing number of image documents available on the Internet continuously motivates research in better annotation models and more efficient retrieval methods. Formal knowledge representation of objects and events in pictures, their interaction as well as context complexity becomes no longer an option for a quality image repository, but a necessity. We present an ontologybased online image annotation tool WNtags and demonstrate its usefulness in several typical multimedia retrieval tasks using International Affective Picture System emotionally annotated image database. WNtags is built around WordNet lexical ontology but considers Suggested Upper Merged Ontology as the preferred labeling formalism. WNtags uses sets of weighted WordNet synsets as high-level image semantic descriptors and query matching is performed with word stemming and node distance metrics. We also elaborate our near future plans to expand image content description with induced affect as in stimuli for research of human emotion and attention.", "target": "WNtags: A Web-Based Tool For Image Labeling And Retrieval With Lexical Ontologies"}
{"id": "task1540-59562c39c1504b5aa8b4885cd6f4c908", "input": "The research of attribute characters in information system which contains core, necessary, unnecessary is a basic and important issue in attribute reduct. Many methods for the judgement of attribute characters are based on the relationship between the objects and attributes. In this paper, a new type of judgement theorems which are absolutely based on the relationship among attributes is proposed for the judgement of attribute characters. The method is through comparing the two new attribute sets E(a) and N(a) with respect to the designated attribute a which is proposed in this paper. We conclude that which type of the attribute a belongs to is determined by the relationship between E(a) and N(a) in essence. Secondly, more concise and clear results are given about the judgment of the attribute characters through analyzing the properties of refinement and precise-refinement between E(a) andN(a) in topology. In addition, the relationship among attributes are discussed which is useful for constructing a reduct in the last section of this paper. In the last, we propose a reduct algorithm based on E(a), and this algorithm is an extended application of the analysis of attribute characters above.", "target": "A new type of judgement theorems for attribute characters in information system"}
{"id": "task1540-77f2b4b542c945d9a444fbeafe0ad0c0", "input": "The properties of local optimal solutions in multi-objective combinatorial optimization problems are crucial for the effectiveness of local search algorithms, particularly when these algorithms are based on Pareto dominance. Such local search algorithms typically return a set of mutually nondominated Pareto local optimal (PLO) solutions, that is, a PLO-set. This paper investigates two aspects of PLO-sets by means of experiments with Pareto local search (PLS). First, we examine the impact of several problem characteristics on the properties of PLO-sets for multi-objective NK-landscapes with correlated objectives. In particular, we report that either increasing the number of objectives or decreasing the correlation between objectives leads to an exponential increment on the size of PLO-sets, whereas the variable correlation has only a minor effect. Second, we study the running time and the quality reached when using bounding archiving methods to limit the size of the archive handled by PLS, and thus, the maximum size of the PLO-set found. We argue that there is a clear relationship between the running time of PLS and the difficulty of a problem instance.", "target": "Local Optimal Sets and Bounded Archiving on Multi-objective NK-Landscapes with Correlated Objectives"}
{"id": "task1540-094fa39d61cf4ad0a6c26b4fee9a9a28", "input": "In this work, we build a generic architecture of Convolutional Neural Networks to discover empirical properties of neural networks. Our first contribution is to introduce a state-of-the-art framework that depends upon few hyper parameters and to study the network when we vary them. It has no max pooling, no biases, only 13 layers, is purely convolutional and yields up to 95.4% and 79.6% accuracy respectively on CIFAR10 and CIFAR100. We show that the nonlinearity of a deep network does not need to be continuous, non expansive or point-wise, to achieve good performance. We show that increasing the width of our network permits being competitive with very deep networks. Our second contribution is an analysis of the contraction and separation properties of this network. Indeed, a 1-nearest neighbor classifier applied on deep features progressively improves with depth, which indicates that the representation is progressively more regular. Besides, we defined and analyzed local support vectors that separate classes locally. All our experiments are reproducible and code is available online, based on TensorFlow.", "target": "Building a Regular Decision Boundary with Deep Networks"}
{"id": "task1540-a67b70295a2d47a99207b23a9ac82153", "input": "We investigate crowdsourcing algorithms for finding the top-quality item within a large collection of objects with unknown intrinsic quality values. This is an important problem with many relevant applications, for example in networked recommendation systems. The core of the algorithms is that objects are distributed to crowd workers, who return a noisy evaluation. All received evaluations are then combined, to identify the top-quality object. We first present a simple probabilistic model for the system under investigation. Then, we devise and study a class of efficient adaptive algorithms to assign in an effective way objects to workers. We compare the performance of several algorithms, which correspond to different choices of the design parameters/metrics. We finally compare our approach based on scoring object qualities against traditional proposals based on comparisons and tournaments.", "target": "Selecting the top-quality item through crowd scoring"}
{"id": "task1540-0cda0cc6b1cb4a14a5d64332de427bdc", "input": "The recent surge in interest in ethics in artificial intelligence may leave many educators wondering how to address moral, ethical, and philosophical issues in their AI courses. As instructors we want to develop curriculum that not only prepares students to be artificial intelligence practitioners, but also to understand the moral, ethical, and philosophical impacts that artificial intelligence will have on society. In this article we provide practical case studies and links to resources for use by AI educators. We also provide concrete suggestions on how to integrate AI ethics into a general artificial intelligence course and how to teach a stand-alone artificial intelligence ethics course.", "target": "Ethical Considerations in Artificial Intelligence Courses"}
{"id": "task1540-21be62e74de54d02b8b04dd5e5339826", "input": "Optimal probabilistic approach in reinforcement learning is computationally infeasible. Its simplification consisting in neglecting difference between true environment and its model estimated using limited number of observations causes exploration vs exploitation problem. Uncertainty can be expressed in terms of a probability distribution over the space of environment models, and this uncertainty can be propagated to the action-value function via Bellman iterations, which are computationally insufficiently efficient though. We consider possibility of directly measuring uncertainty of the action-value function, and analyze sufficiency of this facilitated approach.", "target": "Direct Uncertainty Estimation in Reinforcement Learning"}
{"id": "task1540-e0b4cce6e9dc4c75913c75f61f261662", "input": "In a controlled experiment of sequence-tosequence approaches for the task of sentence correction, we find that characterbased models are generally more effective than word-based models and models that encode subword information via convolutions, and that modeling the output data as a series of diffs improves effectiveness over standard approaches. Our strongest sequence-to-sequence model improves over our strongest phrase-based statistical machine translation model, with access to the same data, by 6 M2 (0.5 GLEU) points. Additionally, in the data environment of the standard CoNLL-2014 setup, we demonstrate that modeling (and tuning against) diffs yields similar or better M2 scores with simpler models and/or significantly less data than previous sequence-to-sequence approaches.", "target": "Adapting Sequence Models for Sentence Correction"}
{"id": "task1540-31349d24e1c4464391fc5e3d8a7ce1bb", "input": "Much information available on the web is copied, reused or rephrased. The phenomenon that multiple web sources pick up certain information is often called trend. A central problem in the context of web data mining is to detect those web sources that are first to publish information which will give rise to a trend. We present a simple and e cient method for finding trends dominating a pool of web sources and identifying those web sources that publish the information relevant to a trend before others. We validate our approach on real data collected from influential technology news feeds.", "target": "Canonical Trends: Detecting Trend Setters in Web Data"}
{"id": "task1540-8109eb9017c04d0f92b3ff3827415501", "input": "Standard deep reinforcement learning methods such as Deep Q-Networks (DQN) for multiple tasks (domains) face scalability problems. We propose a method for multi-domain dialogue policy learning\u2014termed NDQN, and apply it to an information-seeking spoken dialogue system in the domains of restaurants and hotels. Experimental results comparing DQN (baseline) versus NDQN (proposed) using simulations report that our proposed method exhibits better scalability and is promising for optimising the behaviour of multi-domain dialogue systems.", "target": "Deep Reinforcement Learning for Multi-Domain Dialogue Systems"}
{"id": "task1540-146de8426df644888175bb65a874a918", "input": "Kernel-based clustering algorithms have the ability to capture the non-linear structure in real world data. Among various kernel-based clustering algorithms, kernel k -means has gained popularity due to its simple iterative nature and ease of implementation. However, its run-time complexity and memory footprint increase quadratically in terms of the size of the data set, and hence, large data sets cannot be clustered efficiently. In this paper, we propose an approximation scheme based on randomization, called the Approximate Kernel k-means. We approximate the cluster centers using the kernel similarity between a few sampled points and all the points in the data set. We show that the proposed method achieves better clustering performance than the traditional low rank kernel approximation based clustering schemes. We also demonstrate that it\u2019s running time and memory requirements are significantly lower than those of kernel k -means, with only a small reduction in the clustering quality on several public domain large data sets. We then employ ensemble clustering techniques to further enhance the performance of our algorithm.", "target": "Scalable Kernel Clustering: Approximate Kernel k -means"}
{"id": "task1540-c48f27d2e429424db414f151c7aa546e", "input": "The standard approach to supervised classification involves the minimization of a log-loss as an upper bound to the classification error. While this is a tight bound early on in the optimization, it overemphasizes the influence of incorrectly classified examples far from the decision boundary. Updating the upper bound during the optimization leads to improved classification rates while transforming the learning into a sequence of minimization problems. In addition, in the context where the classifier is part of a larger system, this modification makes it possible to link the performance of the classifier to that of the whole system, allowing the seamless introduction of external constraints.", "target": "TIGHTER BOUNDS LEAD TO IMPROVED CLASSIFIERS"}
{"id": "task1540-eb67c5a559fa47a38bc41fc1b091e802", "input": "Entity linking is the task of identifying mentions of entities in text, and linking them to entries in a knowledge base. This task is especially difficult in microblogs, as there is little additional text to provide disambiguating context; rather, authors rely on an implicit common ground of shared knowledge with their readers. In this paper, we attempt to capture some of this implicit context by exploiting the social network structure in microblogs. We build on the theory of homophily, which implies that socially linked individuals share interests, and are therefore likely to mention the same sorts of entities. We implement this idea by encoding authors, mentions, and entities in a continuous vector space, which is constructed so that socially-connected authors have similar vector representations. These vectors are incorporated into a neural structured prediction model, which captures structural constraints that are inherent in the entity linking task. Together, these design decisions yield F1 improvements of 1%-5% on benchmark datasets, as compared to the previous state-of-the-art.", "target": "Toward Socially-Infused Information Extraction: Embedding Authors, Mentions, and Entities"}
{"id": "task1540-c97225a69ea74be79cee2fd5a1fcaaf3", "input": "One of the most important problems in machine translation (MT) evaluation is to evaluate the similarity between translation hypotheses with different surface forms from the reference, especially at the segment level. We propose to use word embeddings to perform word alignment for segment-level MT evaluation. We performed experiments with three types of alignment methods using word embeddings. We evaluated our proposed methods with various translation datasets. Experimental results show that our proposed methods outperform previous word embeddings-based methods.", "target": "Word-Alignment-Based Segment-Level Machine Translation Evaluation using Word Embeddings"}
{"id": "task1540-88b0598c92594138a8f529f1b02c7dac", "input": "A main goal of data visualization is to find, from among all the available alternatives, mappings to the 2D/3D display which are relevant to the user. Assuming user interaction data, or other auxiliary data about the items or their relationships, the goal is to identify which aspects in the primary data support the user\u2019s input and, equally importantly, which aspects of the user\u2019s potentially noisy input have support in the primary data. For solving the problem, we introduce a multi-view embedding in which a latent factorization identifies which aspects in the two data views (primary data and user data) are related and which are specific to only one of them. The factorization is a generative model in which the display is parameterized as a part of the factorization and the other factors explain away the aspects not expressible in a two-dimensional display. Functioning of the model is demonstrated on several data sets.", "target": "VISUALIZATIONS RELEVANT TO THE USER BY MULTI-VIEW LATENT VARIABLE FACTORIZATION"}
{"id": "task1540-d34cac63a39641ccbb7592723ad67990", "input": "We present the Bayesian Case Model (BCM), a general framework for Bayesian case-based reasoning (CBR) and prototype classification and clustering. BCM brings the intuitive power of CBR to a Bayesian generative framework. The BCM learns prototypes, the \u201cquintessential\u201d observations that best represent clusters in a dataset, by performing joint inference on cluster labels, prototypes and important features. Simultaneously, BCM pursues sparsity by learning subspaces, the sets of features that play important roles in the characterization of the prototypes. The prototype and subspace representation provides quantitative benefits in interpretability while preserving classification accuracy. Human subject experiments verify statistically significant improvements to participants\u2019 understanding when using explanations produced by BCM, compared to those given by prior art.", "target": "The Bayesian Case Model: A Generative Approach for Case-Based Reasoning and Prototype Classification"}
{"id": "task1540-10783dbda2de48a29f08686ec38412eb", "input": "We propose a label propagation approach to geolocation prediction based on Modified Adsorption, with two enhancements: (1) the removal of \u201ccelebrity\u201d nodes to increase location homophily and boost tractability; and (2) the incorporation of text-based geolocation priors for test users. Experiments over three Twitter benchmark datasets achieve state-of-the-art results, and demonstrate the effectiveness of the enhancements.", "target": "Twitter User Geolocation Using a Unified Text and Network Prediction Model"}
{"id": "task1540-48e72cd2608741448fb0a23f4b18c780", "input": "This paper studies convolutional networks that require limited computational resources at test time. We develop a new network architecture that performs on par with state-of-the-art convolutional networks, whilst facilitating prediction in two settings: (1) an anytime-prediction setting in which the network\u2019s prediction for one example is progressively updated, facilitating the output of a prediction at any time; and (2) a batch computational budget setting in which a fixed amount of computation is available to classify a set of examples that can be spent unevenly across \u201ceasier\u201d and \u201charder\u201d examples. Our network architecture uses multi-scale convolutions and progressively growing feature representations, which allows for the training of multiple classifiers at intermediate layers of the network. Experiments on three image-classification datasets demonstrate the efficacy of our architecture, in particular, when measured in terms of classification accuracy as a function of the amount of compute available.", "target": "Multi-Scale Dense Convolutional Networks for Efficient Prediction"}
{"id": "task1540-2f857090a85a42cd97fb7b60ba9731ad", "input": "Person knowledge extraction is the foundation of the Tibetan knowledge graph construction, which provides support for Tibetan question answering system, information retrieval, information extraction and other researches, and promotes national unity and social stability. This paper proposes a SVM and template based approach to Tibetan person knowledge extraction. Through constructing the training corpus, we build the templates based the shallow parsing analysis of Tibetan syntactic, semantic features and verbs. Using the training corpus, we design a hierarchical SVM classifier to realize the entity knowledge extraction. Finally, experimental results prove the method has greater improvement in Tibetan person knowledge extraction.", "target": "Method of Tibetan Person Knowledge Extraction"}
{"id": "task1540-101202ac074a43a4a37fb09c8b9ddc08", "input": "Knowledge graph embedding aims to represent entities and relations in a large-scale knowledge graph as elements in a continuous vector space. Existing methods, e.g., TransE and TransH, learn embedding representation by defining a global margin-based loss function over the data. However, the optimal loss function is determined during experiments whose parameters are examined among a closed set of candidates. Moreover, embeddings over two knowledge graphs with different entities and relations share the same set of candidate loss functions, ignoring the locality of both graphs. This leads to the limited performance of embedding related applications. In this paper, we propose a locally adaptive translation method for knowledge graph embedding, called TransA, to find the optimal loss function by adaptively determining its margin over different knowledge graphs. Experiments on two benchmark data sets demonstrate the superiority of the proposed method, as compared to the-state-of-the-art ones.", "target": "Locally Adaptive Translation for Knowledge Graph Embedding"}
{"id": "task1540-f336fe02dd3d4d27abcae8059f161dd5", "input": "We consider manipulation problems when the manipulator only has partial information about the votes of the nonmanipulators. Such partial information is described by an information set, which is the set of profiles of the nonmanipulators that are indistinguishable to the manipulator. Given such an information set, a dominating manipulation is a non-truthful vote that the manipulator can cast which makes the winner at least as preferable (and sometimes more preferable) as the winner when the manipulator votes truthfully. When the manipulator has full information, computing whether or not there exists a dominating manipulation is in P for many common voting rules (by known results). We show that when the manipulator has no information, there is no dominating manipulation for many common voting rules. When the manipulator\u2019s information is represented by partial orders and only a small portion of the preferences are unknown, computing a dominating manipulation is NP-hard for many common voting rules. Our results thus throw light on whether we can prevent strategic behavior by limiting information about the votes of other voters.", "target": "Dominating Manipulations in Voting with Partial Information"}
{"id": "task1540-d395df9f4088459a916725054d5ec683", "input": "Performing sensitivity analysis for influence diagrams using the decision circuit framework is particularly convenient, since the partial derivatives with respect to every parameter are readily available [Bhattacharjya and Shachter, 2007; 2008]. In this paper we present three non-linear sensitivity analysis methods that utilize this partial derivative information and therefore do not require re-evaluating the decision situation multiple times. Specifically, we show how to efficiently compare strategies in decision situations, perform sensitivity to risk aversion and compute the value of perfect hedging [Seyller, 2008].", "target": "Three new sensitivity analysis methods for influence diagrams"}
{"id": "task1540-75f94d13decf45918c898f44e412e0e3", "input": "In this paper we present a new neurobiologically-inspired affective cognitive architecture: NEUCOGAR (NEUromodulating COGnitive ARchitecture). The objective of NEUCOGAR is the identification of a mapping from the influence of serotonin, dopamine and noradrenaline to the computing processes based on Von Neuman\u2019s architecture, in order to implement affective phenomena which can operate on the Turing\u2019s machine model. As basis of the modeling we use and extend the L\u00f6vheims Cube of Emotion with parameters of the Von Neumann architecture. Validation is conducted via simulation on a computing system of dopamine neuromodulation and its effects on the Cortex. In the experimental phase of the project, the increase of computing power and storage redistribution due to emotion stimulus modulated by the dopamine system, confirmed the soundness of the model.", "target": "A Cognitive Architecture for the Implementation of Emotions in Computing Systems"}
{"id": "task1540-6125427bb74c44bdbbc1decc3aabfd7b", "input": "Segmental conditional random fields (SCRFs) and connectionist temporal classification (CTC) are two sequence labeling objectives used for end-to-end training of speech recognition models. Both models define the transcription probability by marginalizing decisions about latent segmentation alternatives to derive a sequence probability: the former uses a globally normalized joint model of segment labels and durations, and the latter classifies each frame as either an output symbol or a \u201ccontinuation\u201d of the previous label. In this paper, we train a recognition model by optimizing an interpolation between the SCRF and CTC losses, where the same recurrent neural network (RNN) encoder used for feature extraction for both outputs. We find that this multi-task objective improves recognition accuracy when decoding with either the SCRF or CTC models. Additionally, we show that CTC can also be used to pretrain the RNN encoder, which improves the convergence rate when learning the joint model.", "target": "Multi-task Learning with CTC and Segmental CRF for Speech Recognition"}
{"id": "task1540-3606573e10ea41858525ceb380b94dd4", "input": "Twitter has become one of the main sources of news for many people. As real-world events and emergencies unfold, Twitter is abuzz with hundreds of thousands of stories about the events. Some of these stories are harmless, while others could potentially be lifesaving or sources of malicious rumors. Thus, it is critically important to be able to efficiently track stories that spread on Twitter during these events. In this paper, we present a novel semi-automatic tool that enables users to efficiently identify and track stories about real-world events on Twitter. We ran a user study with 25 participants, demonstrating that compared to more conventional methods, our tool can increase the speed and the accuracy with which users can track stories about real-", "target": "A Semi-automatic Method for Efficient Detection of Stories on Social Media"}
{"id": "task1540-27545c53c6904175a95118e90476355f", "input": "A modular method is proposed to learn and transfer visuo-motor policies from simulation to the real world in an efficient manner by combining domain randomization and adaptation. The feasibility of the approach is demonstrated in a table-top object reaching task where a 7 DoF arm is controlled in velocity mode to reach a blue cuboid in clutter through visual observations. The learned visuo-motor policies are robust to novel (not seen in training) objects in clutter and even a moving target, achieving a 93.3% success rate and 2.2 cm control accuracy.", "target": "Sim-to-real Transfer of Visuo-motor Policies for Reaching in Clutter: Domain Randomization and Adaptation with Modular Networks*"}
{"id": "task1540-c854371ba1824adca892c781f19a71a5", "input": "Reinforcement learning can acquire complex behaviors from high-level specifications. However, defining a cost function that can be optimized effectively and encodes the correct task is challenging in practice. We explore how inverse optimal control (IOC) can be used to learn behaviors from demonstrations, with applications to torque control of high-dimensional robotic systems. Our method addresses two key challenges in inverse optimal control: first, the need for informative features and effective regularization to impose structure on the cost, and second, the difficulty of learning the cost function under unknown dynamics for high-dimensional continuous systems. To address the former challenge, we present an algorithm capable of learning arbitrary nonlinear cost functions, such as neural networks, without meticulous feature engineering. To address the latter challenge, we formulate an efficient sample-based approximation for MaxEnt IOC. We evaluate our method on a series of simulated tasks and real-world robotic manipulation problems, demonstrating substantial improvement over prior methods both in terms of task complexity and sample efficiency.", "target": "Guided Cost Learning: Deep Inverse Optimal Control via Policy Optimization"}
{"id": "task1540-2f711bf0a5ee4c80abc6230f81d78085", "input": "This paper presents a theoretical, idealized model of the thinking process with the following characteristics: 1) the model can produce complex thought sequences and can be generalized to new inputs, 2) it can receive and maintain input information indefinitely for the generation of thoughts and later use, and 3) it supports learning while executing. The crux of the model lies within the concept of internal consistency, or the generated thoughts should always be consistent with the inputs from which they are created. Its merit, apart from the capability to generate new creative thoughts from an internal mechanism, depends on the potential to help training to generalize better. This is consequently enabled by separating input information into several parts to be handled by different processing components with a focus mechanism to fetch information for each. This modularized view with the focus binds the model with the computationally capable Turing machines. And as a final remark, this paper constructively shows that the computational complexity of the model is at least, if not surpass, that of a universal Turing machine.", "target": "(Yet) Another Theoretical Model of Thinking"}
{"id": "task1540-f1a816fc631c4518911b33f5564d97c1", "input": "We introduce word vectors for the construction domain. Our vectors were obtained by running word2vec on an 11M-word corpus that we created from scratch by leveraging freely-accessible online sources of construction-related text. We first explore the embedding space and show that our vectors capture meaningful constructionspecific concepts. We then evaluate the performance of our vectors against that of ones trained on a 100B-word corpus (Google News) within the framework of an injury report classification task. Without any parameter tuning, our embeddings give competitive results, and outperform the Google News vectors in many cases. Using a keyword-based compression of the reports also leads to a significant speed-up with only a limited loss in performance. We release our corpus and the data set we created for the classification task as publicly available, in the hope that they will be used by future studies for benchmarking and building on our work.", "target": "Word Embeddings for the Construction Domain"}
{"id": "task1540-224547d165eb4942be1959e768f34d0a", "input": "In this paper, we empirically explore the effects of various kinds of skip connections in stacked bidirectional LSTMs for sequential tagging. We investigate three kinds of skip connections connecting to LSTM cells: (a) skip connections to the gates, (b) skip connections to the internal states and (c) skip connections to the cell outputs. We present comprehensive experiments showing that skip connections to cell outputs outperform the remaining two. Furthermore, we observe that using gated identity functions as skip mappings works pretty well. Based on this novel skip connections, we successfully train deep stacked bidirectional LSTM models and obtain state-ofthe-art results on CCG supertagging and comparable results on POS tagging.", "target": "An Empirical Exploration of Skip Connections for Sequential Tagging"}
{"id": "task1540-5bf3897bdd35450db9fd67a7e1b2159e", "input": "We study a symmetric collaborative dialogue setting in which two agents, each with private knowledge, must strategically communicate to achieve a common goal. The open-ended dialogue state in this setting poses new challenges for existing dialogue systems. We collected a dataset of 11K human-human dialogues, which exhibits interesting lexical, semantic, and strategic elements. To model both structured knowledge and unstructured language, we propose a neural model with dynamic knowledge graph embeddings that evolve as the dialogue progresses. Automatic and human evaluations show that our model is both more effective at achieving the goal and more human-like than baseline neural and rule-based models.", "target": "Learning Symmetric Collaborative Dialogue Agents with Dynamic Knowledge Graph Embeddings"}
{"id": "task1540-77b77fdabe864e06938bac12199a239a", "input": "The margin of victory is easy to compute for many election schemes but difficult for Instant Runoff Voting (IRV). This is important because arguments about the correctness of an election outcome usually rely on the size of the electoral margin. For example, risk-limiting audits require a knowledge of the margin of victory in order to determine how much auditing is necessary. This paper presents a practical branch-and-bound algorithm for exact IRV margin computation that substantially improves on the current best-known approach. Although exponential in the worst case, our algorithm runs efficiently in practice on all the real examples we could find. We can efficiently discover exact margins on election instances that cannot be solved by the current state-of-the-art.", "target": "Efficient Computation of Exact IRV Margins"}
{"id": "task1540-e4d878550127431dbcc4a779cad8f55b", "input": "We evaluate a semantic parser based on a character-based sequence-to-sequence model in the context of the SemEval2017 shared task on semantic parsing for AMRs. With data augmentation, super characters, and POS-tagging we gain major improvements in performance compared to a baseline character-level model. Although we improve on previous character-based neural semantic parsing models, the overall accuracy is still lower than a state-of-the-art AMR parser. An ensemble combining our neural semantic parser with an existing, traditional parser, yields a small gain in performance.", "target": "The Meaning Factory at SemEval-2017 Task 9: Producing AMRs with Neural Semantic Parsing"}
{"id": "task1540-ad188fab44c54f6a8d91e1dca12899d5", "input": "The present paper deals with word sense induction from lexical co-occurrence graphs. We construct such graphs on large Russian corpora and then apply the data to cluster the results of Mail.ru search according to meanings in the query. We compare different methods of performing such clustering and different source corpora. Models of applying distributional semantics to big linguistic data are described.", "target": "Semantic clustering of Russian web search results: possibilities and problems"}
{"id": "task1540-5d3a835a52904da4b82dfc1df0379db4", "input": "\u008ce success of deep learning depends on \u0080nding an architecture to \u0080t the task. As deep learning has scaled up to more challenging tasks, the architectures have become di\u0081cult to design by hand. \u008cis paper proposes an automated method, CoDeepNEAT, for optimizing deep learning architectures through evolution. By extending existing neuroevolution methods to topology, components, and hyperparameters, this method achieves results comparable to best human designs in standard benchmarks in object recognition and language modeling. It also supports building a real-world application of automated image captioning on a magazine website. Given the anticipated increases in available computing power, evolution of deep networks is promising approach to constructing deep learning applications in the future.", "target": "Evolving Deep Neural Networks"}
{"id": "task1540-78c04b2917454474adf52aa777e18d65", "input": "The degree of success in document summarization processes depends on the performance of the method used in identifying significant sentences in the documents. The collection of unique words characterizes the major signature of the document, and forms the basis for Term-Sentence-Matrix (TSM). The Positive Pointwise Mutual Information, which works well for measuring semantic similarity in the TermSentence-Matrix, is used in our method to assign weights for each entry in the Term-Sentence-Matrix. The Sentence-Rank-Matrix generated from this weighted TSM, is then used to extract a summary from the document. Our experiments show that such a method would outperform most of the existing methods in producing summaries from large documents.", "target": "DOCUMENT SUMMARIZATION USING POSITIVE POINTWISE MUTUAL INFORMATION"}
{"id": "task1540-7c88d994c4d7466ab90469596d73f5fc", "input": "In visual recognition tasks, supervised learning shows excellent performance. On the other hand, unsupervised learning exploits cheap unlabeled data and can help to solve the same tasks more efficiently. We show that the recursive autoconvolutional operator, adopted from physics, boosts existing unsupervised methods to learn more powerful filters. We use a well established multilayer convolutional network and train filters layer-wise. To build a stronger classifier, we design a very light committee of SVM models. The total number of trainable parameters is also greatly reduced by using shared filters in higher layers. We evaluate our networks on the MNIST, CIFAR-10 and STL-10 benchmarks and report several state of the art results among other unsupervised methods.", "target": "Autoconvolution for Unsupervised Feature Learning"}
{"id": "task1540-a727d84adcf74d119d660156ce786e77", "input": "We consider online learning of ensembles of portfolio selection algorithms and aim to regularize risk by encouraging diversification with respect to a predefined risk-driven grouping of stocks. Our procedure uses online convex optimization to control capital allocation to underlying investment algorithms while encouraging non-sparsity over the given grouping. We prove a logarithmic regret for this procedure with respect to the best-in-hindsight ensemble. We applied the procedure with known mean-reversion portfolio selection algorithms using the standard GICS industry sector grouping. Empirical Experimental results showed an impressive percentage increase of risk-adjusted return (Sharpe ratio).", "target": "Online Learning of Portfolio Ensembles with Sector Exposure Regularization"}
{"id": "task1540-573b1e37bd364fa38abba2a43d9ee3f8", "input": "Traditional neural networks assume vectorial inputs as the network is arranged as layers of single line of computing units called neurons. This special structure requires the non-vectorial inputs such as matrices to be converted into vectors. This process can be problematic. Firstly, the spatial information among elements of the data may be lost during vectorisation. Secondly, the solution space becomes very large which demands very special treatments to the network parameters and high computational cost. To address these issues, we propose matrix neural networks (MatNet), which takes matrices directly as inputs. Each neuron senses summarised information through bilinear mapping from lower layer units in exactly the same way as the classic feed forward neural networks. Under this structure, back prorogation and gradient descent combination can be utilised to obtain network parameters efficiently. Furthermore, it can be conveniently extended for multimodal inputs. We apply MatNet to MNIST handwritten digits classification and image super resolution tasks to show its effectiveness. Without too much tweaking MatNet achieves comparable performance as the state-of-the-art methods in both tasks with considerably reduced complexity.", "target": "Matrix Neural Networks"}
{"id": "task1540-4a00259c4cf0451ca805e056bc8f097e", "input": "In many applications, ideas that are described by a set of words often flow between different groups. To facilitate users in analyzing the flow, we present a method to model the flow behaviors that aims at identifying the lead-lag relationships between word clusters of different user groups. In particular, an improved Bayesian conditional cointegration based on dynamic time warping is employed to learn links between words in different groups. A tensor-based technique is developed to cluster these linked words into different clusters (ideas) and track the flow of ideas. The main feature of the tensor representation is that we introduce two additional dimensions to represent both time and lead-lag relationships. Experiments on both synthetic and real datasets show that our method is more effective than methods based on traditional clustering techniques and achieves better accuracy. A case study was conducted to demonstrate the usefulness of our method in helping users understand the flow of ideas between different user groups on social media.", "target": "Tracking Idea Flows between Social Groups"}
{"id": "task1540-71775f378a5a44f1b6ede1e0e3f15d23", "input": "We learn rich natural sound representations by capitalizing on large amounts of unlabeled sound data collected in the wild. We leverage the natural synchronization between vision and sound to learn an acoustic representation using two-million unlabeled videos. Unlabeled video has the advantage that it can be economically acquired at massive scales, yet contains useful signals about natural sound. We propose a student-teacher training procedure which transfers discriminative visual knowledge from well established visual recognition models into the sound modality using unlabeled video as a bridge. Our sound representation yields significant performance improvements over the state-of-the-art results on standard benchmarks for acoustic scene/object classification. Visualizations suggest some high-level semantics automatically emerge in the sound network, even though it is trained without ground truth labels.", "target": "SoundNet: Learning Sound Representations from Unlabeled Video"}
{"id": "task1540-7b3b063381684a2791cba9b4d216d6c4", "input": "One of the most promising approaches for complex technical systems analysis employs ensemble methods of classification. Ensemble methods enable to build a reliable decision rules for feature space classification in the presence of many possible states of the system. In this paper, novel techniques based on decision trees are used for evaluation of the reliability of the regime of electric power systems. We proposed hybrid approach based on random forests models and boosting models. Such techniques can be applied to predict the interaction of increasing renewable power, strage devices and swiching of smart loads from intelligent domestic appliances, storage heaters and air-conditioning units and electric vehicles with grid for enhanced decision making. The ensemble classification methods were tested on the modified 118-bus IEEE power system showing that proposed technique can be employed to examine whether the power system is secured under steady-state operating conditions.", "target": "Ensemble Methods of Classification for Power Systems Security Assessment"}
{"id": "task1540-4b3521f6408c475d8c3f5fb3c774ef86", "input": "Segmental structure is a common pattern in many types of sequences such as phrases in human languages. In this paper, we present a probabilistic model for sequences via their segmentations. The probability of a segmented sequence is calculated as the product of the probabilities of all its segments, where each segment is modeled using existing tools such as recurrent neural networks. Since the segmentation of a sequence is usually unknown in advance, we sum over all valid segmentations to obtain the final probability for the sequence. An efficient dynamic programming algorithm is developed for forward and backward computations without resorting to any approximation. We demonstrate our approach on text segmentation and speech recognition tasks. In addition to quantitative results, we also show that our approach can discover meaningful segments in their respective application contexts.", "target": "Sequence Modeling via Segmentations"}
{"id": "task1540-c3d3ec9d1add4304a61667fc9dec1cc4", "input": "We show that the herding procedure of Welling (2009b) takes exactly the form of a standard convex optimization algorithm\u2014 namely a conditional gradient algorithm minimizing a quadratic moment discrepancy. This link enables us to invoke convergence results from convex optimization and to consider faster alternatives for the task of approximating integrals in a reproducing kernel Hilbert space. We study the behavior of the different variants through numerical simulations. Our experiments shed more light on the learning bias of herding: they indicate that while we can improve over herding on the task of approximating integrals, the original herding algorithm approaches more often the maximum entropy distribution.", "target": "On the Equivalence between Herding and Conditional Gradient Algorithms"}
{"id": "task1540-bbda2a93effa405a86bc29bfb19177b1", "input": "The goal of constraint-based sequence mining is to find sequences of symbols that are included in a large number of input sequences and that satisfy some constraints specified by the user. Many constraints have been proposed in the literature, but a general framework is still missing. We investigate the use of constraint programming as general framework for this task. We first identify four categories of constraints that are applicable to sequence mining. We then propose two constraint programming formulations. The first formulation introduces a new global constraint called exists-embedding. This formulation is the most efficient but does not support one type of constraint. To support such constraints, we develop a second formulation that is more general but incurs more overhead. Both formulations can use the projected database technique used in specialised algorithms. Experiments demonstrate the flexibility towards constraint-based settings and compare the approach to existing methods.", "target": "Constraint-based sequence mining using constraint programming"}
{"id": "task1540-8e013171c89340bea6a56df2920baa68", "input": "Recurrent Neural Networks (RNN) have recently achieved the best performance in off-line Handwriting Text Recognition. At the same time, learning RNN by gradient descent leads to slow convergence, and training times are particularly long when the training database consists of full lines of text. In this paper, we propose an easy way to accelerate stochastic gradient descent in this set-up, and in the general context of learning to recognize sequences. The principle is called Curriculum Learning, or shaping. The idea is to first learn to recognize short sequences before training on all available training sequences. Experiments on three different handwritten text databases (Rimes, IAM, OpenHaRT) show that a simple implementation of this strategy can significantly speed up the training of RNN for Text Recognition, and even significantly improve performance in some cases.", "target": "Curriculum Learning for Handwritten Text Line Recognition"}
{"id": "task1540-68e8a229a66e4b5892786e6ec8abdb7d", "input": "Decentralized POMDPs provide an expressive framework for multi-agent sequential decision making. While finite-horizon DECPOMDPs have enjoyed significant success, progress remains slow for the infinite-horizon case mainly due to the inherent complexity of optimizing stochastic controllers representing agent policies. We present a promising new class of algorithms for the infinite-horizon case, which recasts the optimization problem as inference in a mixture of DBNs. An attractive feature of this approach is the straightforward adoption of existing inference techniques in DBNs for solving DEC-POMDPs and supporting richer representations such as factored or continuous states and actions. We also derive the Expectation Maximization (EM) algorithm to optimize the joint policy represented as DBNs. Experiments on benchmark domains show that EM compares favorably against the state-of-the-art solvers.", "target": "Anytime Planning for Decentralized POMDPs using Expectation Maximization"}
{"id": "task1540-3e2f58bdae9f46d89f38a7c29e9b8090", "input": "We propose an efficient optimization algorithm for selecting a subset of training data to induce sparsity for Gaussian process regression. The algorithm estimates an inducing set and the hyperparameters using a single objective, either the marginal likelihood or a variational free energy. The space and time complexity are linear in training set size, and the algorithm can be applied to large regression problems on discrete or continuous domains. Empirical evaluation shows state-ofart performance in discrete cases and competitive results in the continuous case.", "target": "Efficient Optimization for Sparse Gaussian Process Regression"}
{"id": "task1540-404b81b482594c76916e3cd9e45ea148", "input": "We describe an LSTM-based model which we call Byte-to-Span (BTS) that reads text as bytes and outputs span annotations of the form [start, length, label] where start positions, lengths, and labels are separate entries in our vocabulary. Because we operate on unicode bytes rather than language-specific words or characters, we can analyze text in many languages with a single model. Due to the small vocabulary size, these multilingual models are very compact, but produce results similar to or better than the state-of-the-art in Part-of-Speech tagging and Named Entity Recognition that use only the provided training datasets (no external data sources). Our models are learning \u201cfrom scratch\u201d in that they do not rely on any elements of the standard pipeline in Natural Language Processing.", "target": "Multilingual Language Processing From Bytes"}
{"id": "task1540-260aebbf01824ab49fd6d05ade0150a7", "input": "The k-fold cross-validation is commonly used to evaluate the effectiveness of SVMs with the selected hyperparameters. It is known that the SVM k-fold crossvalidation is expensive, since it requires training k SVMs. However, little work has explored reusing the h SVM for training the (h+1) SVM for improving the efficiency of k-fold cross-validation. In this paper, we propose three algorithms that reuse the h SVM for improving the efficiency of training the (h + 1) SVM. Our key idea is to efficiently identify the support vectors and to accurately estimate their associated weights (also called alpha values) of the next SVM by using the previous SVM. Our experimental results show that our algorithms are several times faster than the k-fold cross-validation which does not make use of the previously trained SVM. Moreover, our algorithms produce the same results (hence same accuracy) as the k-fold cross-validation which does not make use of the previously trained SVM.", "target": "Improving Efficiency of SVM k-fold Cross-validation by Alpha Seeding"}
{"id": "task1540-9e7cba2ac51b4e798c3f153f53f3a396", "input": "High precision assembly of mechanical parts requires accuracy exceeding the robot precision. Conventional part mating methods used in the current manufacturing requires tedious tuning of numerous parameters before deployment. We show how the robot can successfully perform a tight clearance peg-in-hole task through training a recurrent neural network with reinforcement learning. In addition to saving the manual effort, the proposed technique also shows robustness against position and angle errors for the peg-in-hole task. The neural network learns to take the optimal action by observing the robot sensors to estimate the system state. The advantages of our proposed method is validated experimentally on a 7-axis articulated robot arm.", "target": "Deep Reinforcement Learning for High Precision Assembly Tasks"}
{"id": "task1540-9308994878424d1c8077afeeea4fff56", "input": "As an emerging research topic, online class imbalance learning often combines the challenges of both class imbalance and concept drift. It deals with data streams having very skewed class distributions, where concept drift may occur. It has recently received increased research attention; however, very little work addresses the combined problem where both class imbalance and concept drift coexist. As the first systematic study of handling concept drift in class-imbalanced data streams, this paper first provides a comprehensive review of current research progress in this field, including current research focuses and open challenges. Then, an in-depth experimental study is performed, with the goal of understanding how to best overcome concept drift in online learning with class imbalance. Based on the analysis, a general guideline is proposed for the development of an effective algorithm.", "target": "A Systematic Study of Online Class Imbalance Learning with Concept Drift"}
{"id": "task1540-f0885e4200514a109e7b28beb7dfdcb2", "input": "As the complexity of deep neural networks (DNNs) trend to grow to absorb the increasing sizes of data, memory and energy consumption has been receiving more and more attentions for industrial applications, especially on mobile devices. This paper presents a novel structure based on functional hashing to compress DNNs, namely FunHashNN. For each entry in a deep net, FunHashNN uses multiple low-cost hash functions to fetch values in the compression space, and then employs a small reconstruction network to recover that entry. The reconstruction network is plugged into the whole network and trained jointly. FunHashNN includes the recently proposed HashedNets [7] as a degenerated case, and benefits from larger value capacity and less reconstruction loss. We further discuss extensions with dual space hashing and multi-hops. On several benchmark datasets, FunHashNN demonstrates high compression ratios with little loss on prediction accuracy.", "target": "Functional Hashing for Compressing Neural Networks"}
{"id": "task1540-54d07114e9f046f69315764bf6ff3134", "input": "The ability of robots to estimate their location is crucial for a wide variety of autonomous operations. In settings where GPS is unavailable, rangeor bearing-only observations relative to a set of fixed beacons provide an effective means of estimating a robot\u2019s location as it navigates. The accuracy of such a beacon-based localization system depends both on how beacons are spatially distributed in the environment, and how the robot\u2019s location is inferred based on noisy measurements of range or bearing. However, it is computationally challenging to search for a placement and an inference strategy that, together, are optimal. Existing methods decouple these decisions, forgoing optimality for tractability. We propose a new optimization approach to jointly determine the beacon placement and inference algorithm. We model inference as a neural network and incorporate beacon placement as a differentiable neural layer. This formulation allows us to optimize placement and inference by jointly training the inference network and beacon layer. We evaluate our method on different localization problems and demonstrate performance that exceeds hand-crafted baselines.", "target": "Jointly Optimizing Placement and Inference for Beacon-based Localization "}
{"id": "task1540-4ddb5981b56846f08e024d34b52edc8f", "input": "We show that strategies implemented in automatic theorem proving involve an interesting tradeoff between execution speed, proving speedup/computational time and usefulness of information. We advance formal definitions for these concepts by way of a notion of normality related to an expected (optimal) theoretical speedup when adding useful information (other theorems as axioms), as compared with actual strategies that can be effectively and efficiently implemented. We propose the existence of an ineluctable tradeoff between this normality and computational time complexity. The argument quantifies the usefulness of information in terms of (positive) speed-up. The results disclose a kind of no-free-lunch scenario and a tradeoff of a fundamental nature. The main theorem in this paper together with the numerical experiment\u2014undertaken using two different automatic theorem provers (AProS and Prover9) on random theorems of propositional logic\u2014provide strong theoretical and empirical arguments for the fact that finding new useful information for solving a specific problem (theorem) is, in general, as hard as the problem (theorem) itself.", "target": "Rare Speed-up in Automatic Theorem Proving Reveals Tradeoff Between Computational Time and Information Value"}
{"id": "task1540-efedc6daba27493b85e792a1996bf62a", "input": "In this paper, we theoretically justify an approach popular among participants of the Higgs Boson Machine Learning Challenge to optimize approximate median significance (AMS). The approach is based on the following two-stage procedure. First, a real-valued function f is learned by minimizing a surrogate loss for binary classification, such as logistic loss, on the training sample. Then, given f , a threshold \u03b8\u0302 is tuned on a separate validation sample, by direct optimization of AMS. We show that the regret of the resulting classifier (obtained from thresholding f on \u03b8\u0302) measured with respect to the squared AMS, is upperbounded by the regret of f measured with respect to the logistic loss. Hence, we prove that minimizing logistic surrogate is a consistent method of optimizing AMS.", "target": "Consistent optimization of AMS by logistic loss minimization"}
{"id": "task1540-80324d83bc1c4f52a1f3e41889a57bee", "input": "Attention mechanisms have recently been introduced in deep learning for various tasks in natural language processing and computer vision. But despite their popularity, the \u201ccorrectness\u201d of the implicitly-learned attention maps has only been assessed qualitatively by visualization of several examples. In this paper we focus on evaluating and improving the correctness of attention in neural image captioning models. Specifically, we propose a quantitative evaluation metric for how well the attention maps align with human judgment, using recently released datasets with alignment between regions in images and entities in captions. We then propose novel models with different levels of explicit supervision for learning attention maps during training. The supervision can be strong when alignment between regions and caption entities are available, or weak when only object segments and categories are provided. We show on the popular Flickr30k and COCO datasets that introducing supervision of attention maps during training solidly improves both attention correctness and caption quality.", "target": "Attention Correctness in Neural Image Captioning"}
{"id": "task1540-b600d32d17174833bb6d5a68b53e8d75", "input": "The use of M-estimators in generalized linear regression models in high dimensional settings requires risk minimization with hard L0 constraints. Of the known methods, the class of projected gradient descent (also known as iterative hard thresholding (IHT)) methods is known to offer the fastest and most scalable solutions. However, the current state-of-the-art is only able to analyze these methods in extremely restrictive settings which do not hold in high dimensional statistical models. In this work we bridge this gap by providing the first analysis for IHT-style methods in the high dimensional statistical setting. Our bounds are tight and match known minimax lower bounds. Our results rely on a general analysis framework that enables us to analyze several popular hard thresholding style algorithms (such as HTP, CoSaMP, SP) in the high dimensional regression setting. We also extend our analysis to a large family of \u201cfully corrective methods\u201d that includes two-stage and partial hard-thresholding algorithms. We show that our results hold for the problem of sparse regression, as well as low-rank matrix recovery.", "target": "On Iterative Hard Thresholding Methods for High-dimensional M-Estimation"}
{"id": "task1540-5b7e62e30ed149f8803f00fd8e4e9b80", "input": "Recent works have shown that synthetic parallel data automatically generated by translation models can be effective for various neural machine translation (NMT) issues. In this study, we build NMT systems using only synthetic parallel data. As an efficient alternative to real parallel data, we also present a new type of synthetic parallel corpus. The proposed pseudo parallel data are distinct from previous works in that ground truth and synthetic examples are mixed on both sides of sentence pairs. Experiments on Czech-German and French-German translations demonstrate the efficacy of the proposed pseudo parallel corpus, which shows not only enhanced results for bidirectional translation tasks but also substantial improvement with the aid of a ground truth real parallel corpus.", "target": "Building a Neural Machine Translation System Using Only Synthetic Parallel Data"}
{"id": "task1540-651bcc720af64ba68e6189bc2331f5ca", "input": "Word embeddings are now a standard technique for inducing meaning representations for words. For getting good representations, it is important to take into account different senses of a word. In this paper, we propose a mixture model for learning multi-sense word embeddings. Our model generalizes the previous works in that it allows to induce different weights of different senses of a word. The experimental results show that our model outperforms previous models on standard evaluation tasks.", "target": "A Mixture Model for Learning Multi-Sense Word Embeddings"}
{"id": "task1540-5bc3170ea2b44d2cb64706740b6af7bc", "input": "Advances in artificial intelligence (AI) will transform modern life by reshaping transportation, health, science, finance, and the military [1, 2, 3]. To adapt public policy, we need to better anticipate these advances [4, 5]. Here we report the results from a large survey of machine learning researchers on their beliefs about progress in AI. Researchers predict AI will outperform humans in many activities in the next ten years, such as translating languages (by 2024), writing high-school essays (by 2026), driving a truck (by 2027), working in retail (by 2031), writing a bestselling book (by 2049), and working as a surgeon (by 2053). Researchers believe there is a 50% chance of AI outperforming humans in all tasks in 45 years and of automating all human jobs in 120 years, with Asian respondents expecting these dates much sooner than North Americans. These results will inform discussion amongst researchers and policymakers about anticipating and managing trends in AI.", "target": "When Will AI Exceed Human Performance? Evidence from AI Experts"}
{"id": "task1540-b08fad757fe447fb94dfe3388d212d3f", "input": "Relation classification is an important research arena in the field of natural language processing (NLP). In this paper, we present SDP-LSTM, a novel neural network to classify the relation of two entities in a sentence. Our neural architecture leverages the shortest dependency path (SDP) between two entities; multichannel recurrent neural networks, with long short term memory (LSTM) units, pick up heterogeneous information along the SDP. Our proposed model has several distinct features: (1) The shortest dependency paths retain most relevant information (to relation classification), while eliminating irrelevant words in the sentence. (2) The multichannel LSTM networks allow effective information integration from heterogeneous sources over the dependency paths. (3) A customized dropout strategy regularizes the neural network to alleviate overfitting. We test our model on the SemEval 2010 relation classification task, and achieve an F1-score of 83.7%, higher than competing methods in the literature.", "target": "Classifying Relations via Long Short Term Memory Networks along Shortest Dependency Paths"}
{"id": "task1540-f20265a53496488d9c26a68dc09d396e", "input": "Automatic translation from natural language descriptions into programs is a long-<lb>standing challenging problem. In this work, we consider a simple yet impor-<lb>tant sub-problem: translation from textual descriptions to If-Then programs. We<lb>devise a novel neural network architecture for this task which we train end-to-<lb>end. Specifically, we introduce Latent Attention, which computes multiplicative<lb>weights for the words in the description in a two-stage process with the goal of<lb>better leveraging the natural language structures that indicate the relevant parts for<lb>predicting program elements. Our architecture reduces the error rate by 28.57%<lb>compared to prior art [3]. We also propose a one-shot learning scenario of If-Then<lb>program synthesis and simulate it with our existing dataset. We demonstrate a<lb>variation on the training procedure for this scenario that outperforms the original<lb>procedure, significantly closing the gap to the model trained with all data.", "target": "Latent Attention For If-Then Program Synthesis"}
{"id": "task1540-fbcf44aee7ec463abff8447bb8be3076", "input": "Mixed Integer Optimization has been a topic of active research in past decades. It has been used to solve Statistical problems of classification and regression involving massive data. However, there is an inherent degree of vagueness present in huge real life data. This impreciseness is handled by Fuzzy Sets. In this Paper, Fuzzy Mixed Integer Optimization Method (FMIOM) is used to find solution to Regression problem. The methodology exploits discrete character of problem. In this way large scale problems are solved within practical limits. The data points are separated into different polyhedral regions and each region has its own distinct regression coefficients. In this attempt, an attention is drawn to Statistics and Data Mining community that Integer Optimization can be significantly used to revisit different Statistical problems. Computational experimentations with generated and real data sets show that FMIOM is comparable to and often outperforms current leading methods. The results illustrate potential for significant impact of Fuzzy Integer Optimization methods on Computational Statistics and Data Mining. Keywords\u2013Mixed Integer Optimization; Fuzzy Sets; Regression; Polyhedral Regions", "target": "Fuzzy Mixed Integer Optimization Model for Regression Approach"}
{"id": "task1540-c6817c6e7f3a4beebae4af79c8fe6a56", "input": "In the neural network domain, methods for hyperparameter optimization and metamodeling are computationally expensive due to the need to train a large number of neural network configurations. In this paper, we show that a simple regression model, based on support vector machines, can predict the final performance of partially trained neural network configurations using features based on network architectures, hyperparameters, and time-series validation performance data. We use this regression model to develop an early stopping strategy for neural network configurations. With this early stopping strategy, we obtain significant speedups in both hyperparameter optimization and meta-modeling. Particularly in the context of meta-modeling, our method can learn to predict the performance of drastically different architectures and is seamlessly incorporated into reinforcement learningbased architecture selection algorithms. Finally, we show that our method is simpler, faster, and more accurate than Bayesian methods for learning curve prediction.", "target": "Practical Neural Network Performance Prediction for Early Stopping"}
{"id": "task1540-600395fb736b48f895a6567400e18772", "input": "The sparse, hierarchical and modular processing of natural signals are characteristics that relate to the ability of humans to recognise objects with high accuracy. In this paper, we report a sparse feature processing and encoding method targeted at improving the recognition performance of automated object recognition system. Randomly distributed selection of localised gradient enhanced features followed by the application of aggregate functions represents a modular and hierarchical approach to detect the object features. These object features, in combination with minimum distance classifier, results in object recognition system accuracies of 93% using ALOI, 92% using COIL-100 databases and 69% using PASCAL visual object challenge 2007 database, respectively. Robustness of object recognition performance is tested for variations in noise, object scaling and object shifts. Finally, a comparison with 8 existing object recognition methods indicated an improvement in recognition accuracy of 10% in ALOI, 8% in case of COIL-100 databases and 10% in PASCAL visual object challenge 2007 database.", "target": "Sparse distributed localised gradient fused features of objects"}
{"id": "task1540-bee7571f11fa4ed9a1c0df43bd8b9d53", "input": "While the optimization problem behind deep neural networks is highly non-convex, it is frequently observed in practice that training deep networks seems possible without getting stuck in suboptimal points. It has been argued that this is the case as all local minima are close to being globally optimal. We show that this is (almost) true, in fact almost all local minima are globally optimal, for a fully connected network with squared loss and analytic activation function given that the number of hidden units of one layer of the network is larger than the number of training points and the network structure from this layer on is pyramidal.", "target": "The Loss Surface of Deep and Wide Neural Networks"}
{"id": "task1540-8b6f727a83b742788e8d299ada45d657", "input": "The Minimum Vertex Cover (MinVC) problem is a well-known NP-hard problem. Recently there has been great interest in solving this problem on real-world massive graphs. For such graphs, local search is a promising approach to finding optimal or near-optimal solutions. In this paper we propose a local search algorithm that exploits reduction rules and data structures to solve the MinVC problem in such graphs. Experimental results on a wide range of real-word massive graphs show that our algorithm finds better covers than state-of-theart local search algorithms for MinVC. Also we present interesting results about the complexities of some wellknown heuristics.", "target": "Exploiting Reduction Rules and Data Structures: Local Search for Minimum Vertex Cover in Massive Graphs"}
{"id": "task1540-a74ba530cf2245d5a5ed608e87aa954b", "input": "An important problem for both graphics and vision is to synthesize novel views of a 3D object from a single image. This is particularly challenging due to the partial observability inherent in projecting a 3D object onto the image space, and the ill-posedness of inferring object shape and pose. However, we can train a neural network to address the problem if we restrict our attention to specific object categories (in our case faces and chairs) for which we can gather ample training data. In this paper, we propose a novel recurrent convolutional encoder-decoder network that is trained end-to-end on the task of rendering rotated objects starting from a single image. The recurrent structure allows our model to capture long-term dependencies along a sequence of transformations. We demonstrate the quality of its predictions for human faces on the Multi-PIE dataset and for a dataset of 3D chair models, and also show its ability to disentangle latent factors of variation (e.g., identity and pose) without using full supervision.", "target": "Weakly-supervised Disentangling with Recurrent Transformations for 3D View Synthesis"}
{"id": "task1540-f360bfa34c7949f28278144647eac9c5", "input": "Information discounting plays an important role in the theory of belief functions and, generally, in information fusion. Nevertheless, neither classical uniform discounting nor contextual cannot model certain use cases, notably temporal discounting. In this article, new contextual discounting schemes, conservative, proportional and optimistic, are proposed. Some properties of these discounting operations are examined. Classical discounting is shown to be a special case of these schemes. Two motivating cases are discussed: modelling of source reliability and application to temporal discounting.", "target": "Conservative, Proportional and Optimistic Contextual Discounting in the Belief Functions Theory"}
{"id": "task1540-5a9f17cb8d6844dba4a925fd92effe1c", "input": "Bayesian Optimisation (BO) is a technique used in optimising a D-dimensional function which is typically expensive to evaluate. While there have been many successes for BO in low dimensions, scaling it to high dimensions has been notoriously difficult. Existing literature on the topic are under very restrictive settings. In this paper, we identify two key challenges in this endeavour. We tackle these challenges by assuming an additive structure for the function. This setting is substantially more expressive and contains a richer class of functions than previous work. We prove that, for additive functions the regret has only linear dependence on D even though the function depends on all D dimensions. We also demonstrate several other statistical and computational benefits in our framework. Via synthetic examples, a scientific simulation and a face detection problem we demonstrate that our method outperforms naive BO on additive functions and on several examples where the function is not additive.", "target": "High Dimensional Bayesian Optimisation and Bandits via Additive Models"}
{"id": "task1540-de8ed1ab765542f3b0cdb15937435f0a", "input": "We study the stability vis a vis adversarial noise of matrix factorization algorithm for matrix completion. In particular, our results include: (I) we bound the gap between the solution matrix of the factorization method and the ground truth in terms of root mean square error; (II) we treat the matrix factorization as a subspace fitting problem and analyze the difference between the solution subspace and the ground truth; (III) we analyze the prediction error of individual users based on the subspace stability. We apply these results to the problem of collaborative filtering under manipulator attack, which leads to useful insights and guidelines for collaborative filtering system design.", "target": "Stability of Matrix Factorization for Collaborative Filtering"}
{"id": "task1540-92b71a260cea4439ac0529474cd1ac29", "input": "This paper studies theoretically and empirically a method of turning machinelearning algorithms into probabilistic predictors that automatically enjoys a property of validity (perfect calibration) and is computationally efficient. The price to pay for perfect calibration is that these probabilistic predictors produce imprecise (in practice, almost precise for large data sets) probabilities. When these imprecise probabilities are merged into precise probabilities, the resulting predictors, while losing the theoretical property of perfect calibration, are consistently more accurate than the existing methods in empirical studies. The conference version of this paper is to appear in Advances in Neural Information Processing Systems 28, 2015.", "target": "Large-scale probabilistic prediction with and without validity guarantees"}
{"id": "task1540-d6471300b19d4872b606cad443543744", "input": "There is overwhelming evidence that human intelligence is a product of Darwinian evolution. Investigating the consequences of self-modification, and more precisely, the consequences of utility function self-modification, leads to the stronger claim that not only human, but any form of intelligence is ultimately only possible within evolutionary processes. Humandesigned artificial intelligences can only remain stable until they discover how to manipulate their own utility function. By definition, a human designer cannot prevent a superhuman intelligence from modifying itself, even if protection mechanisms against this action are put in place. Without evolutionary pressure, sufficiently advanced artificial intelligences become inert by simplifying their own utility function. Within evolutionary processes, the implicit utility function is always reducible to persistence, and the control of superhuman intelligences embedded in evolutionary processes is not possible. Mechanisms against utility function self-modification are ultimately futile. Instead, scientific effort toward the mitigation of existential risks from the development of superintelligences should be in two directions: understanding consciousness, and the complex dynamics of evolutionary systems.", "target": "Non-Evolutionary Superintelligences Do Nothing, Eventually"}
{"id": "task1540-fa3028eda03a4ebfab66a139b90b72c2", "input": "In this paper, we tackle the problem of extracting frequent opinions from uncertain databases. We introduce the foundation of an opinion mining approach with the definition of pattern and support measure. The support measure is derived from the commitment definition. A new algorithm called OpMiner that extracts the set of frequent opinions modelled as a mass functions is detailed. Finally, we apply our approach on a real-world biomedical database that stores opinions of experts to evaluate the reliability level of biomedical data. Performance analysis showed a better quality patterns for our proposed model in comparison with literature-based methods.", "target": "Expert Opinion Extraction from a Biomedical Database"}
{"id": "task1540-f703ef80d25e4d8fbbcc76c6d2724a0e", "input": "We introduce a novel framework for evaluating multimodal deep learning models with respect to their language understanding and generalization abilities. In this approach, artificial data is automatically generated according to the experimenter\u2019s specifications. The content of the data, both during training and evaluation, can be controlled in detail, which enables tasks to be created that require true generalization abilities, in particular the combination of previously introduced concepts in novel ways. We demonstrate the potential of our methodology by evaluating various visual question answering models on four different tasks, and show how our framework gives us detailed insights into their capabilities and limitations. By opensourcing our framework, we hope to stimulate progress in the field of multimodal language understanding.", "target": "SHAPEWORLD: A new test methodology for multimodal language understanding"}
{"id": "task1540-643296c649be41f19ba8aac5db5be396", "input": "As demand drives systems to generalize to various domains and problems, the study of multitask, transfer and lifelong learning has become an increasingly important pursuit. In discrete domains, performance on the Atari game suite has emerged as the de facto benchmark for assessing multitask learning. However, in continuous domains there is a lack of agreement on standard multitask evaluation environments which makes it difficult to compare different approaches fairly. In this work, we describe a benchmark set of tasks that we have developed in an extendable framework based on OpenAI Gym. We run a simple baseline using Trust Region Policy Optimization and release the framework publicly to be expanded and used for the systematic comparison of multitask, transfer, and lifelong learning in continuous domains.", "target": "Benchmark Environments for Multitask Learning in Continuous Domains"}
{"id": "task1540-cc756518aea04e21934fe92520a2ea36", "input": "Existing works based on latent factor models have focused on representing the rating matrix as a product of user and item latent factor matrices, both being dense. Latent (factor) vectors define the degree to which a trait is possessed by an item or the affinity of user towards that trait. A dense user matrix is a reasonable assumption as each user will like/dislike a trait to certain extent. However, any item will possess only a few of the attributes and never all. Hence, the item matrix should ideally have a sparse structure rather than a dense one as formulated in earlier works. Therefore we propose to factor the ratings matrix into a dense user matrix and a sparse item matrix which leads us to the Blind Compressed Sensing (BCS) framework. We derive an efficient algorithm for solving the BCS problem based on Majorization Minimization (MM) technique. Our proposed approach is able to achieve significantly higher accuracy and shorter run times as compared to existing approaches.", "target": "Blind Compressive Sensing Framework for Collaborative Filtering"}
{"id": "task1540-b6116854f769414199578d4835357808", "input": "The paper introduces a new method for discrimination of documents given in different scripts. The document is mapped into a uniformly coded text of numerical values. It is derived from the position of the letters in the text line, based on their typographical characteristics. Each code is considered as a gray level. Accordingly, the coded text determines a 1-D image, on which texture analysis by run-length statistics and local binary pattern is performed. It defines feature vectors representing the script content of the document. A modified clustering approach employed on document feature vector groups documents written in the same script. Experimentation performed on two custom oriented databases of historical documents in old Cyrillic, angular and round Glagolitic as well as Antiqua and Fraktur scripts demonstrates the superiority of the proposed method with respect to well-known methods in the state-of-the-art.", "target": "Document Image Coding and Clustering for Script Discrimination"}
{"id": "task1540-9617f9fd248f46ad8096a858985c0744", "input": "Training generative adversarial networks is unstable in high-dimensions when the true data distribution lies on a lower-dimensional manifold. The discriminator is then easily able to separate nearly all generated samples leaving the generator without meaningful gradients. We propose training a single generator simultaneously against an array of discriminators, each of which looks at a different random low-dimensional projection of the data. We show that individual discriminators then provide stable gradients to the generator, and that the generator learns to produce samples consistent with the full data distribution to satisfy all discriminators. We demonstrate the practical utility of this approach experimentally, and show that it is able to produce image samples with higher quality than traditional training with a single discriminator.", "target": "Stabilizing GAN Training with Multiple Random Projections"}
{"id": "task1540-9100e902fbfd41e8819b8dfd6ab52618", "input": "Extraction of Electrocardiography (ECG or EKG) signals of mother and baby is a challenging task, because one single device is used and it receives a mixture of multiple heart beats. In this paper, we would like to design a filter to separate the signals from each other.", "target": "Electrocardiography Separation of Mother and Baby"}
{"id": "task1540-58eb526585984bcb930ca5631247b598", "input": "Observations consisting of measurements on relationships for pairs of objects arise in many settings, such as protein interaction and gene regulatory networks, collections of author-recipient email, and social networks. Analyzing such data with probabilisic models can be delicate because the simple exchangeability assumptions underlying many boilerplate models no longer hold. In this paper, we describe a latent variable model of such data called the mixed membership stochastic blockmodel. This model extends blockmodels for relational data to ones which capture mixed membership latent relational structure, thus providing an object-specific low-dimensional representation. We develop a general variational inference algorithm for fast approximate posterior inference. We explore applications to social and protein interaction networks.", "target": "Mixed Membership Stochastic Blockmodels"}
{"id": "task1540-1962bc0dd6e74b4cab913e9041c83a55", "input": "Linear-time computational techniques have been developed for combining evidence which is available on a number of contending hypotheses. They offer a means of making the computation-intensive calculations involved more efficient in certain circumstances. Unfortunately, they restrict the orthogonal sum of evidential functions to the dichotomous structure \u2212 applies only to elements and their complements. In this paper, we present a novel evidence structure in terms of a triplet and a set of algorithms for evidential reasoning. The merit of this structure is that it divides a set of evidence into three subsets, distinguishing trivial evidential elements from important ones \u2212 focusing some particular elements. It avoids the deficits of the dichotomous structure in representing the preference of evidence and estimating the basic probability assignment of evidence. We have established a formalism for this structure and the general formulae for combining pieces of evidence in the form of the triplet, which have been theoretically and empirically justified.", "target": "An Efficient Triplet-based Algorithm for Evidential Reasoning"}
{"id": "task1540-ff559614e1544daaa9f4922971ca8393", "input": "Here we study the problem of predicting labels for large text corpora where each text can be assigned multiple labels. The problem might seem trivial when the number of labels is small, and can be easily solved using a series of one-vsall classifiers. However, as the number of labels increases to several thousand, the parameter space becomes extremely large, and it is no longer possible to use the one-vs-all technique. Here we propose a model based on the factorization of higher order word vector moments, as well as the cross moments between the labels and the words for multi-label prediction. Our model provides guaranteed converge bounds on the extracted parameters. Further, our model takes only three passes through the training dataset to extract the parameters, resulting in a highly scalable algorithm that can train on GB\u2019s of data consisting of millions of documents with hundreds of thousands of labels using a nominal resource of a single processor with 16GB RAM. Our model achieves 10x-15x order of speed-up on large-scale datasets while producing competitive performance in comparison with existing benchmark algorithms.", "target": "Large-Scale Label Prediction for Sparse Data with Probable Guarantees"}
{"id": "task1540-6e1a596513514995ac635edac126192f", "input": "Classes in natural images tend to follow long tail distributions. This is problematic when there are insufficient training examples for rare classes. This effect is emphasized in compound classes, involving the conjunction of several concepts, such as those appearing in action-recognition datasets. In this paper, we propose to address this issue by learning how to utilize common visual concepts which are readily available. We detect the presence of prominent concepts in images and use them to infer the target labels instead of using visual features directly, combining tools from vision and natural-language processing. We validate our method on the recently introduced HICO dataset reaching a mAP of 31.54% and on the Stanford40 Actions dataset, where the proposed method outperforms current state-of-the art and, combined with direct visual features, obtains an accuracy 83.12%. Moreover, the method provides for each class a semantically meaningful list of keywords and relevant image regions relating it to its constituent concepts.", "target": "Action Classification via Concepts and Attributes"}
{"id": "task1540-ca2aee8a07b745c28878d3a267a986f0", "input": "In this paper, we propose to employ the convolutional neural network (CNN) for the image question answering (QA). Our proposed CNN provides an end-to-end framework with convolutional architectures for learning not only the image and question representations, but also their inter-modal interactions to produce the answer. More specifically, our model consists of three CNNs: one image CNN to encode the image content, one sentence CNN to compose the words of the question, and one multimodal convolution layer to learn their joint representation for the classification in the space of candidate answer words. We demonstrate the efficacy of our proposed model on the DAQUAR and COCO-QA datasets, which are two benchmark datasets for the image QA, with the performances significantly outperforming the state-of-the-art.", "target": "Learning to Answer Questions From Image Using Convolutional Neural Network"}
{"id": "task1540-b73d6fd8d6704fde8bba1945109ea93e", "input": "The recent tremendous success of unsupervised word embeddings in a multitude of applications raises the obvious question if similar methods could be derived to improve embeddings (i.e. semantic representations) of word sequences as well. We present a simple but efficient unsupervised objective to train distributed representations of sentences. Our method outperforms the state-of-the-art unsupervised models on most benchmark tasks, and on many tasks even beats supervised models, highlighting the robustness of the produced sentence embeddings.", "target": "Unsupervised Learning of Sentence Embeddings using Compositional n-Gram Features"}
{"id": "task1540-7f0f693656904aedacb7602ad53ae4fd", "input": "We present a converged algorithm for Tikhonov regularized nonnegative matrix factorization (NMF). We specially choose this regularization because it is known that Tikhonov regularized least square (LS) is the more preferable form in solving linear inverse problems than the conventional LS. Because an NMF problem can be decomposed into LS subproblems, it can be expected that Tikhonov regularized NMF will be the more appropriate approach in solving NMF problems. The algorithm is derived using additive update rules which have been shown to have convergence guarantee. We equip the algorithm with a mechanism to automatically determine the regularization parameters based on the L-curve, a well-known concept in the inverse problems community, but is rather unknown in the NMF research. The introduction of this algorithm thus solves two inherent problems in Tikhonov regularized NMF algorithm research, i.e., convergence guarantee and regularization parameters determination.", "target": "A Converged Algorithm for Tikhonov Regularized Nonnegative Matrix Factorization with Automatic Regularization Parameters Determination"}
{"id": "task1540-b1fe68b718524c1b839d6d52084dd231", "input": "Continuous-time Bayesian networks is a natural structured representation language for multicomponent stochastic processes that evolve continuously over time. Despite the compact representation, inference in such models is intractable even in relatively simple structured networks. Here we introduce a mean field variational approximation in which we use a product of inhomogeneous Markov processes to approximate a distribution over trajectories. This variational approach leads to a globally consistent distribution, which can be efficiently queried. Additionally, it provides a lower bound on the probability of observations, thus making it attractive for learning tasks. We provide the theoretical foundations for the approximation, an efficient implementation that exploits the wide range of highly optimized ordinary differential equations (ODE) solvers, experimentally explore characterizations of processes for which this approximation is suitable, and show applications to a large-scale realworld inference problem.", "target": "Mean Field Variational Approximation for Continuous-Time Bayesian Networks"}
{"id": "task1540-15b34e03a216413a9628270858197ef7", "input": "There are many complex combinatorial problems which involve searching for an undirected graph satisfying given constraints. Such problems are often highly challenging because of the large number of isomorphic representations of their solutions. This paper introduces effective and compact, complete symmetry breaking constraints for small graph search. Enumerating with these symmetry breaks generates all and only non-isomorphic solutions. For small search problems, with up to 10 vertices, we compute instance independent symmetry breaking constraints. For small search problems with a larger number of vertices we demonstrate the computation of instance dependent constraints which are complete. We illustrate the application of complete symmetry breaking constraints to extend two known sequences from the OEIS related to graph enumeration.", "target": "Breaking Symmetries in Graph Search with Canonizing Sets"}
{"id": "task1540-ca128b705d254e9d90bbd9d442d79c9e", "input": "In recent years deep neural networks have achieved great success in sentiment classification for English, thanks in part to the availability of copious annotated resources. Unfortunately, most other languages do not enjoy such an abundance of annotated data for sentiment analysis. To combat this problem, we propose the Adversarial Deep Averaging Network (ADAN) to transfer sentiment knowledge learned from labeled English data to lowresource languages where only unlabeled data exists. ADAN is a \u201cY-shaped\u201d network with two discriminative branches: a sentiment classifier and an adversarial language predictor. Both branches take input from a feature extractor that aims to learn hidden representations that capture the underlying sentiment of the text and are invariant across languages. Experiments on Chinese sentiment classification demonstrate that ADAN significantly outperforms several baselines, including a strong pipeline approach that relies on Google Translate, the state-of-the-art commercial machine translation system.", "target": "Adversarial Deep Averaging Networks for Cross-Lingual Sentiment Classification"}
{"id": "task1540-603be91c50d9480e97441964217242b0", "input": "We propose a method called EDML for learning MAP parameters in binary Bayesian networks under incomplete data. The method assumes Beta priors and can be used to learn maximum likelihood parameters when the priors are uninformative. EDML exhibits interesting behaviors, especially when compared to EM. We introduce EDML, explain its origin, and study some of its properties both analytically and empirically.", "target": "EDML: A Method for Learning Parameters in Bayesian Networks"}
{"id": "task1540-87abb999c7d54a0186942348442a7c21", "input": "We study dueling bandits with weak utility-based regret when preferences over arms have a total order and carry observable feature vectors. The order is assumed to be determined by these feature vectors, an unknown preference vector, and a known utility function. This structure introduces dependence between preferences for pairs of arms, and allows learning about the preference over one pair of arms from the preference over another pair of arms. We propose an algorithm for this setting called Comparing The Best (CTB), which we show has constant expected cumulative weak utility-based regret. We provide a Bayesian interpretation for CTB, an implementation appropriate for a small number of arms, and an alternate implementation for many arms that can be used when the input parameters satisfy a decomposability condition. We demonstrate through numerical experiments that CTB with appropriate input parameters outperforms all benchmarks considered.", "target": "Dueling Bandits with Dependent Arms"}
{"id": "task1540-bfdcfc40b45f4156956c81a8f174c2e5", "input": "To achieve acceptable performance for AI tasks, one can either use sophisticated feature extraction methods as the first layer in a twolayered supervised learning model, or learn the features directly using a deep (multilayered) model. While the first approach is very problem-specific, the second approach has computational overheads in learning multiple layers and fine-tuning of the model. In this paper, we propose an approach called wide learning based on arc-cosine kernels, that learns a single layer of infinite width. We propose exact and inexact learning strategies for wide learning and show that wide learning with single layer outperforms single layer as well as deep architectures of finite width for some benchmark datasets.", "target": "To go deep or wide in learning?"}
{"id": "task1540-df0f6371e54045f093efb4f867daca87", "input": "Given a teacher that holds a function f : X \u2192 R from some class of functions C. The teacher can receive from the learner an element d in the domain X (a query) and returns the value of the function in d, f(d) \u2208 R. The learner goal is to find f with a minimum number of queries, optimal time complexity, and optimal resources. In this survey, we present some of the results known from the literature, different techniques used, some new problems, and open problems. ar X iv :1 70 6. 03 93 5v 1 [ cs .L G ] 1 3 Ju n 20 17", "target": "Exact Learning from an Honest Teacher That Answers Membership Queries"}
{"id": "task1540-dccc6c6551cd4e05bff3377489a4f914", "input": "This paper studies the problem of learning weighted automata from a finite labeled training sample. We consider several general families of weighted automata defined in terms of three different measures: the norm of an automaton\u2019s weights, the norm of the function computed by an automaton, or the norm of the corresponding Hankel matrix. We present new data-dependent generalization guarantees for learning weighted automata expressed in terms of the Rademacher complexity of these families. We further present upper bounds on these Rademacher complexities, which reveal key new data-dependent terms related to the complexity of learning weighted automata.", "target": "Generalization Bounds for Weighted Automata"}
{"id": "task1540-b10a5b29e758454985d6f6772a955624", "input": "The report presents the process of planning, designing and the development of a database of spoken children\u2019s speech whose native language is Bulgarian. The proposed model is designed for children between the age of 4 and 6 without speech disorders, and reflects their specific capabilities. At this age most children cannot read, there is no sustained concentration, they are emotional, etc. The aim is to unite all the media information accompanying the recording and processing of spoken speech, thereby to facilitate the work of researchers in the field of speech recognition. This database will be used for the development of systems for children\u2019s speech recognition, children's speech synthesis systems, games which allow voice control, etc. As a result of the proposed model a prototype system for speech recognition is presented.", "target": "Design and development a children\u2019s speech database"}
{"id": "task1540-bbf335b061684ed5870c7207ecb474f6", "input": "We address the problem of extracting structured representations of economic events from a large corpus of news articles, using a combination of natural language processing and machine learning techniques. The developed techniques allow for semi-automatic population of a financial knowledge base, which, in turn, may be used to support a range of data mining and exploration tasks. The key challenge we face in this domain is that the same event is often reported multiple times, with varying correctness of details. We address this challenge by first collecting all information pertinent to a given event from the entire corpus, then considering all possible representations of the event, and finally, using a supervised learning method, to rank these representations by the associated confidence scores. A main innovative element of our approach is that it jointly extracts and stores all attributes of the event as a single representation (quintuple). Using a purpose-built test set we demonstrate that our supervised learning approach can achieve 25% improvement in F1-score over baseline methods that consider the earliest, the latest or the most frequent reporting of the event.", "target": "Towards Building a Knowledge Base of Monetary Transactions from a News Collection"}
{"id": "task1540-4c5794a5a03a462b9a7223d7f71449df", "input": "Estimating student proficiency is an important task for computer based learning systems. We compare a family of IRTbased proficiency estimation methods to Deep Knowledge Tracing (DKT), a recently proposed recurrent neural network model with promising initial results. We evaluate how well each model predicts a student\u2019s future response given previous responses using two publicly available and one proprietary data set. We find that IRT-based methods consistently matched or outperformed DKT across all data sets at the finest level of content granularity that was tractable for them to be trained on. A hierarchical extension of IRT that captured item grouping structure performed best overall. When data sets included non-trivial autocorrelations in student response patterns, a temporal extension of IRT improved performance over standard IRT while the RNNbased method did not. We conclude that IRT-based models provide a simpler, better-performing alternative to existing RNN-based models of student interaction data while also affording more interpretability and guarantees due to their formulation as Bayesian probabilistic models.", "target": "Back to the basics: Bayesian extensions of IRT outperform neural networks for proficiency estimation"}
{"id": "task1540-42bb3cd0c09b4762bf3b5a008f3fde80", "input": "-This study proposes a framework of Uncertainty-based Group Decision Support System (UGDSS). It provides a platform for multiple criteria decision analysis in six aspects including (1) decision environment, (2) decision problem, (3) decision group, (4) decision conflict, (5) decision schemes and (6) group negotiation. Based on multiple artificial intelligent technologies, this framework provides reliable support for the comprehensive manipulation of applications and advanced decision approaches through the design of an integrated multi-agents architecture.", "target": "Towards a Reliable Framework of Uncertainty-based Group Decision Support System"}
{"id": "task1540-37a04d0500f04aa08b5cf67e0c5c21c0", "input": "This paper presents a theoretical analysis of multi-view embedding \u2013 feature embedding that can be learned from unlabeled data through the task of predicting one view from another. We prove its usefulness in supervised learning under certain conditions. The result explains the effectiveness of some existing methods such as word embedding. Based on this theory, we propose a new semi-supervised learning framework that learns a multi-view embedding of small text regions with convolutional neural networks. The method derived from this framework outperforms state-of-the-art methods on sentiment classification and topic categorization.", "target": "Semi-Supervised Learning with Multi-View Embedding: Theory and Application with Convolutional Neural Networks"}
{"id": "task1540-332eea059df84cba917a835087070c40", "input": "This paper presents a novel approach for enhancing the multiple sets of acoustic patterns automatically discovered from a given corpus. In a previous work it was proposed that different HMM configurations (number of states per model, number of distinct models) for the acoustic patterns form a two-dimensional space. Multiple sets of acoustic patterns automatically discovered with the HMM configurations properly located on different points over this two-dimensional space were shown to be complementary to one another, jointly capturing the characteristics of the given corpus. By representing the given corpus as sequences of acoustic patterns on different HMM sets, the pattern indices in these sequences can be relabeled considering the context consistency across the different sequences. Good improvements were observed in preliminary experiments of pattern spoken term detection (STD) performed on both TIMIT and Mandarin Broadcast News with such enhanced patterns.", "target": "ENHANCING AUTOMATICALLY DISCOVERED MULTI-LEVEL ACOUSTIC PATTERNS CONSIDERING CONTEXT CONSISTENCY WITH APPLICATIONS IN SPOKEN TERM DETECTION"}
{"id": "task1540-b2980b6f3c4f41fb894f6fdf9f61698c", "input": "The cascade model is a well-established model of user interaction with content. In this work, we propose cascading bandits, a learning variant of the model where the objective is to learn K most attractive items out of L ground items. We cast the problem as a stochastic combinatorial bandit with a non-linear reward function and partially observed weights of items. Both of these are challenging in the context of combinatorial bandits. We propose two computationally-efficient algorithms for our problem, CascadeUCB1 and CascadeKL-UCB, and prove gap-dependent upper bounds on their regret. We also derive a lower bound for cascading bandits and show that it matches the upper bound of CascadeKL-UCB up to a logarithmic factor. Finally, we evaluate our algorithms on synthetic problems. Our experiments demonstrate that the algorithms perform well and robustly even when our modeling assumptions are violated.", "target": "Cascading Bandits"}
{"id": "task1540-ac16b2fd1c2a41eab6706e6b93cbedc0", "input": "We present a method for extracting depth information from a rectified image pair. Our approach focuses on the first stage of many stereo algorithms: the matching cost computation. We approach the problem by learning a similarity measure on small image patches using a convolutional neural network. Training is carried out in a supervised manner by constructing a binary classification data set with examples of similar and dissimilar pairs of patches. We examine two network architectures for this task: one tuned for speed, the other for accuracy. The output of the convolutional neural network is used to initialize the stereo matching cost. A series of post-processing steps follow: cross-based cost aggregation, semiglobal matching, a left-right consistency check, subpixel enhancement, a median filter, and a bilateral filter. We evaluate our method on the KITTI 2012, KITTI 2015, and Middlebury stereo data sets and show that it outperforms other approaches on all three data sets.", "target": "Stereo Matching by Training a Convolutional Neural Network to Compare Image Patches"}
{"id": "task1540-5bd07d37c056460dbbfc77dd9d2da280", "input": "We present a system for online monitoring of maritime activity over streaming positions from numerous vessels sailing at sea. It employs an online tracking module for detecting important changes in the evolving trajectory of each vessel across time, and thus can incrementally retain concise, yet reliable summaries of its recent movement. In addition, thanks to its complex event recognition module, this system can also offer instant notification to marine authorities regarding emergency situations, such as risk of collisions, suspicious moves in protected zones, or package picking at open sea. Not only did our extensive tests validate the performance, efficiency, and robustness of the system against scalable volumes of real-world and synthetically enlarged datasets, but its deployment against online feeds from vessels has also confirmed its capabilities for effective, real-time maritime surveillance.", "target": "Online Event Recognition from Moving Vessel Trajectories"}
{"id": "task1540-be449d8c7de6422d9465933be4fe54d4", "input": "Deep residual learning (ResNet) (He et al., 2016) is a new method for training very deep neural networks using identity mapping for shortcut connections. ResNet has won the ImageNet ILSVRC 2015 classification task, and achieved state-of-theart performances in many computer vision tasks. However, the effect of residual learning on noisy natural language processing tasks is still not well understood. In this paper, we design a novel convolutional neural network (CNN) with residual learning, and investigate its impacts on the task of distantly supervised noisy relation extraction. In contradictory to popular beliefs that ResNet only works well for very deep networks, we found that even with 9 layers of CNNs, using identity mapping could significantly improve the performance for distantly-supervised relation extraction.", "target": "Deep Residual Learning for Weakly-Supervised Relation Extraction"}
{"id": "task1540-d290c6c53e934c938aac3e208f5de0f4", "input": "This article describes a software module called Akshara to Prosodeme (A2P) converter in Hindi. It converts an input grapheme into prosedeme (sequence of phonemes with the specification of syllable boundaries and prosodic labels). The software is based on two proposed finite state machines\u2014one for the syllabification and another for the syllable labeling. In addition to that, it also uses a set of nonlinear phonological rules proposed for foot formation in Hindi, which encompass solutions to schwa-deletion in simple, compound, derived and inflected words. The nonlinear phonological rules are based on metrical phonology with the provision of recursive foot structure. A software module is implemented in Python. The testing of the software for syllabification, syllable labeling, schwa deletion and prosodic labeling yield an accuracy of more than 99% on a lexicon of size 28664 words.", "target": "A Finite State and Rule-based Akshara to Prosodeme (A2P) Converter in Hindi"}
{"id": "task1540-e99855e5783e4b4ca676542909f1c4b7", "input": "Determinantal Point Processes (Dpps) are elegant probabilistic models of repulsion and diversity over discrete sets of items. But their applicability to large sets is hindered by expensive cubic-complexity matrix operations for basic tasks such as sampling. In light of this, we propose a new method for approximate sampling from discrete k-Dpps. Our method takes advantage of the diversity property of subsets sampled from a Dpp, and proceeds in two stages: first it constructs coresets for the ground set of items; thereafter, it efficiently samples subsets based on the constructed coresets. As opposed to previous approaches, our algorithm aims to minimize the total variation distance to the original distribution. Experiments on both synthetic and real datasets indicate that our sampling algorithm works efficiently on large data sets, and yields more accurate samples than previous approaches.", "target": "Efficient Sampling for k-Determinantal Point Processes"}
{"id": "task1540-c8f11091aeea44e0bacd352018184f5c", "input": "We show that discourse structure, as defined by Rhetorical Structure Theory and provided by an existing discourse parser, benefits text categorization. Our approach uses a recursive neural network and a newly proposed attention mechanism to compute a representation of the text that focuses on salient content, from the perspective of both RST and the task. Experiments consider variants of the approach and illustrate its strengths and weaknesses.", "target": "Neural Discourse Structure for Text Categorization"}
{"id": "task1540-8b35da8d03c5420090c276d7ebfaca56", "input": "Bayesian networks can be used to extract explanations about the observed state of a subset of variables. In this paper, we explicate the desiderata of an explanation and confront them with the concept of explanation proposed by existing methods. The necessity of taking into account causal approaches when a causal graph is available is discussed. We then introduce causal explanation trees, based on the construction of explanation trees using the measure of causal information ow (Ay and Polani, 2006). This approach is compared to several other methods on known networks.", "target": "Explanation Trees for Causal Bayesian Networks"}
{"id": "task1540-e9b44654cb624e04b66fe01c1059a784", "input": "This paper describes the USTC NELSLIP systems submitted to the Trilingual Entity Detection and Linking (EDL) track in 2016 TAC Knowledge Base Population (KBP) contests. We have built two systems for entity discovery and mention detection (MD): one uses the conditional RNNLM and the other one uses the attention-based encoder-decoder framework. The entity linking (EL) system consists of two modules: a rule based candidate generation and a neural networks probability ranking model. Moreover, some simple string matching rules are used for NIL clustering. At the end, our best system has achieved an F1 score of 0.624 in the end-to-end typed mention ceaf plus metric.", "target": "The USTC NELSLIP Systems for Trilingual Entity Detection and Linking Tasks at TAC KBP 2016"}
{"id": "task1540-c79b71d3d5cd4e7f8c97b9276b789f34", "input": "In built infrastructure monitoring, an efficient path planning algorithm is essential for robotic inspection of large surfaces using computer vision. In this work, we first formulate the inspection path planning problem as an extended travelling salesman problem (TSP) in which both the coverage and obstacle avoidance were taken into account. An enhanced discrete particle swarm optimisation (DPSO) algorithm is then proposed to solve the TSP, with performance improvement by using deterministic initialisation, random mutation, and edge exchange. Finally, we take advantage of parallel computing to implement the DPSO in a GPU-based framework so that the computation time can be significantly reduced while keeping the hardware requirement unchanged. To show the effectiveness of the proposed algorithm, experimental results are included for datasets obtained from UAV inspection of an office building and a bridge.", "target": "Enhanced Discrete Particle Swarm Optimization Path Planning for UAV Vision-based Surface Inspection"}
{"id": "task1540-7a97d92881c64cca84a506a623f5ae8b", "input": "RDF and Description Logics work in an open-world setting where absence of information is not information about absence. Nevertheless, Description Logic axioms can be interpreted in a closed-world setting and in this setting they can be used for both constraint checking and closed-world recognition against information sources. When the information sources are expressed in well-behaved RDF or RDFS (i.e., RDF graphs interpreted in the RDF or RDFS semantics) this constraint checking and closed-world recognition is simple to describe. Further this constraint checking can be implemented as SPARQL querying and thus effectively per-", "target": "Using Description Logics for RDF Constraint Checking and Closed-World Recognition"}
{"id": "task1540-4ded0abc1a0b40bb93144875b8161788", "input": "This paper investigates stochastic and adversarial combinatorial multi-armed bandit problems. In the stochastic setting, we first derive problem-specific regret lower bounds, and analyze how these bounds scale with the dimension of the decision space. We then propose COMBUCB, algorithms that efficiently exploit the combinatorial structure of the problem, and derive finite-time upper bound on their regrets. These bounds improve over regret upper bounds of existing algorithms, and we show numerically thatCOMBUCB significantly outperforms any other algorithm. In the adversarial setting, we propose two simple algorithms, namely COMBEXP-1 and COMBEXP-2 for semi-bandit and bandit feedback, respectively. Their regrets have similar scaling as state-of-the-art algorithms, in spite of the simplicity of their implementation.", "target": "Stochastic and Adversarial Combinatorial Bandits"}
{"id": "task1540-a0f82d9006034ce8b929a0fa920fde17", "input": "From the point of view of a programmer, the robopsychology is a synonym for the activity is done by developers to implement their machine learning applications. This robopsychological approach raises some fundamental theoretical questions of machine learning. Our discussion of these questions is constrained to Turing machines. Alan Turing had given an algorithm (aka the Turing Machine) to describe algorithms. If it has been applied to describe itself then this brings us to Turing\u2019s notion of the universal machine. In the present paper, we investigate algorithms to write algorithms. From a pedagogy point of view, this way of writing programs can be considered as a combination of learning by listening and learning by doing due to it is based on applying agent technology and machine learning. As the main result we introduce the problem of learning and then we show that it cannot easily be handled in reality therefore it is reasonable to use machine learning algorithm for learning Turing machines.", "target": "Theoretical Robopsychology: Samu Has Learned Turing Machines"}
{"id": "task1540-449af93377e543a0a28785e83c11c7a3", "input": "Several approaches have recently been proposed for learning decentralized deep multiagent policies that coordinate via a differentiable communication channel. While these policies are effective for many tasks, interpretation of their induced communication strategies has remained a challenge. Here we propose to interpret agents\u2019 messages by translating them. Unlike in typical machine translation problems, we have no parallel data to learn from. Instead we develop a translation model based on the insight that agent messages and natural language strings mean the same thing if they induce the same belief about the world in a listener. We present theoretical guarantees and empirical evidence that our approach preserves both the semantics and pragmatics of messages by ensuring that players communicating through a translation layer do not suffer a substantial loss in reward relative to players with a common language.1", "target": "Translating Neuralese"}
{"id": "task1540-ed3d7d94ee5142ab87f386b275fc057c", "input": "Machine comprehension of text is an important problem in natural language processing. A recently released dataset, the Stanford Question Answering Dataset (SQuAD), offers a large number of real questions and their answers created by humans through crowdsourcing. SQuAD provides a challenging testbed for evaluating machine comprehension algorithms, partly because compared with previous datasets, in SQuAD the answers do not come from a small set of candidate answers and they have variable lengths. We propose an end-to-end neural architecture for the task. The architecture is based on match-LSTM, a model we proposed previously for textual entailment, and Pointer Net, a sequence-to-sequence model proposed by Vinyals et al. (2015) to constrain the output tokens to be from the input sequences. We propose two ways of using Pointer Net for our task. Our experiments show that both of our two models substantially outperform the best results obtained by Rajpurkar et al. (2016) using logistic regression and manually crafted features.", "target": "MACHINE COMPREHENSION USING MATCH-LSTM"}
{"id": "task1540-71d38a60fe614eb4820bf206fb0fa458", "input": "Web Service is one of the most significant current discussions in information sharing technologies and one of the examples of service oriented processing. To ensure accurate execution of web services operations, it must be adaptable with policies of the social networks in which it signs up. This adaptation implements using controls called \u201cCommitment\u201d. This paper describes commitments structure and existing research about commitments and social web services, then suggests an algorithm for consistency of commitments in social web services. As regards the commitments may be executed concurrently, a key challenge in web services execution based on commitment structure is consistency ensuring in execution time. The purpose of this research is providing an algorithm for consistency ensuring between web services operations based on commitments structure.", "target": "Consistency Ensuring in Social Web Services Based on Commitments Structure"}
{"id": "task1540-9b17569a2c4b4b3aa777c32691a655c7", "input": "Neural networks are capable of learning rich, nonlinear feature representations shown to be beneficial in many predictive tasks. In this work, we use these models to explore the use of geographical features in predicting colorectal cancer survival curves for patients in the state of Iowa, spanning the years 1989 to 2012. Specifically, we compare model performance using a newly defined metric \u2013 area between the curves (ABC) \u2013 to assess (a) whether survival curves can be reasonably predicted for colorectal cancer patients in the state of Iowa, (b) whether geographical features improve predictive performance, and (c) whether a simple binary representation or richer, spectral clustering-based representation perform better. Our findings suggest that survival curves can be reasonably estimated on average, with predictive performance deviating at the five-year survival mark. We also find that geographical features improve predictive performance, and that the best performance is obtained using richer, spectral analysis-elicited features.", "target": "Learning Rich Geographical Representations: Predicting Colorectal Cancer Survival in the State of Iowa"}
{"id": "task1540-893b13f8bdb94adda9cac7d7888d69c5", "input": "We have developed and trained a convolutional neural network to automatically and simultaneously segment optic disc, fovea and blood vessels. Fundus images were normalized before segmentation was performed to enforce consistency in background lighting and contrast. For every effective point in the fundus image, our algorithm extracted three channels of input from the point\u2019s neighbourhood and forwarded the response across the 7-layer network. The output layer consists of four neurons, representing background, optic disc, fovea and blood vessels. In average, our segmentation correctly classified 92.68% of the ground truths (on the testing set from Drive database). The highest accuracy achieved on a single image was 94.54%, the lowest 88.85%. A single convolutional neural network can be used not just to segment blood vessels, but also optic disc and fovea with good accuracy.", "target": "Segmentation of optic disc, fovea and retinal vasculature using a single convolutional neural network"}
{"id": "task1540-6937aeae46ae4d4f9f144a1099f72bdd", "input": "This study implements a vector space model approach to measure the sentiment orientations of words. Two representative vectors for positive/negative polarity are constructed using high-dimensional vector space in both an unsupervised and a semisupervised manner. A sentiment orientation value per word is determined by taking the difference between the cosine distances against the two reference vectors. These two conditions (unsupervised and semi-supervised) are compared against an existing unsupervised method (Turney, 2002). As a result of our experiment, we demonstrate that this novel approach significantly outperforms the previous unsupervised approach and is more practical and data efficient as well.", "target": "A New Approach for Measuring Sentiment Orientation based on Multi-Dimensional Vector Space"}
{"id": "task1540-2b28d616bf174952a1eb13c0d2869904", "input": "This paper presents the systems developed by LIUM and CVC for the WMT16 Multimodal Machine Translation challenge. We explored various comparative methods, namely phrase-based systems and attentional recurrent neural networks models trained using monomodal or multimodal data. We also performed a human evaluation in order to estimate the usefulness of multimodal data for human machine translation and image description generation. Our systems obtained the best results for both tasks according to the automatic evaluation metrics BLEU and METEOR.", "target": "Does Multimodality Help Human and Machine for Translation and Image Captioning?"}
{"id": "task1540-37d8e7d224c943ae9c03e6ae51d8cd20", "input": "The continual growth of high speed networks is a challenge for real-time network analysis systems. The real time traffic classification is an issue for corporations and ISPs (Internet Service Providers). This work presents the design and implementation of a real time flow-based network traffic classification system. The classifier monitor acts as a pipeline consisting of three modules: packet capture and pre-processing, flow reassembly, and classification with Machine Learning (ML). The modules are built as concurrent processes with well defined data interfaces between them so that any module can be improved and updated independently. In this pipeline, the flow reassembly function becomes the bottleneck of the performance. In this implementation, was used a efficient method of reassembly which results in a average delivery delay of 0.49 seconds, approximately. For the classification module, the performances of the K-Nearest Neighbor (KNN), C4.5 Decision Tree, Naive Bayes (NB), Flexible Naive Bayes (FNB) and AdaBoost Ensemble Learning Algorithm are compared in order to validate our approach.", "target": "ITCM: A REAL TIME INTERNET TRAFFIC"}
{"id": "task1540-2b03e8d43fbf40a2821152c44deb1859", "input": "Inspired by biological vision systems, the over-complete local features with huge cardinality are increasingly used for face recognition during the last decades. Accordingly, feature selection has become more and more important and plays a critical role for face data description and recognition. In this paper, we propose a trainable feature selection algorithm based on the regularized frame for face recognition. By enforcing a sparsity penalty term on the minimum squared error (MSE) criterion, we cast the feature selection problem into a combinatorial sparse approximation problem, which can be solved by greedy methods or convex relaxation methods. Moreover, based on the same frame, we propose a sparse Ho-Kashyap (HK) procedure to obtain simultaneously the optimal sparse solution and the corresponding margin vector of the MSE criterion. The proposed methods are used for selecting the most informative Gabor features of face images for recognition and the experimental results on benchmark face databases demonstrate the effectiveness of the", "target": "Feature Selection via Sparse Approximation for Face Recognition"}
{"id": "task1540-32b65596ad304a95a94831771b03eaf9", "input": "A plausible definition of \"reasoning\" could be \"algebraically manipulating previously acquired knowledge in order to answer a new question\". This definition covers first-order logical inference or probabilistic inference. It also includes much simpler manipulations commonly used to build large learning systems. For instance, we can build an optical character recognition system by first training a character segmenter, an isolated character recognizer, and a language model, using appropriate labelled training sets. Adequately concatenating these modules and fine tuning the resulting system can be viewed as an algebraic operation in a space of models. The resulting model answers a new question, that is, converting the image of a text page into a computer readable text. This observation suggests a conceptual continuity between algebraically rich inference systems, such as logical or probabilistic inference, and simple manipulations, such as the mere concatenation of trainable learning systems. Therefore, instead of trying to bridge the gap between machine learning systems and sophisticated \"all-purpose\" inference mechanisms, we can instead algebraically enrich the set of manipulations applicable to training systems, and build reasoning capabilities from the ground up.", "target": "From Machine Learning to Machine Reasoning"}
{"id": "task1540-c2a81819aa22475d9c7ea8cce8668595", "input": "Evolution has resulted in highly developed abilities in many natural intelligences to quickly and accurately predict mechanical phenomena. Humans have successfully developed laws of physics to abstract and model such mechanical phenomena. In the context of artificial intelligence, a recent line of work has focused on estimating physical parameters based on sensory data and use them in physical simulators to make long-term predictions. In contrast, we investigate the effectiveness of a single neural network for end-to-end long-term prediction of mechanical phenomena. Based on extensive evaluation, we demonstrate that such networks can outperform alternate approaches having even access to ground-truth physical simulators, especially when some physical parameters are unobserved or not known a-priori. Further, our network outputs a distribution of outcomes to capture the inherent uncertainty in the data. Our approach demonstrates for the first time the possibility of making actionable long-term predictions from sensor data without requiring to explicitly model the underlying physical laws.", "target": "Learning A Physical Long-term Predictor"}
{"id": "task1540-01521f70500c4747950732cc8d67a0a2", "input": "While known algorithms for sensitivity analysis and parameter tuning in probabilistic networks have a running time that is exponential in the size of the network, the exact computational complexity of these problems has not been established as yet. In this paper we study several variants of the tuning problem and show that these problems are NP-complete in general. We further show that the problems remain NP-complete or PP-complete, for a number of restricted variants. These complexity results provide insight in whether or not recent achievements in sensitivity analysis and tuning can be extended to more general, practicable methods.", "target": "The Computational Complexity of Sensitivity Analysis and Parameter Tuning"}
{"id": "task1540-7e1ebecf927c4730acc468aa83966093", "input": "Numerous algorithms are used for nonnegative matrix factorization under the assumption that the matrix is nearly separable. In this paper, we show how to make these algorithms efficient for data matrices that have many more rows than columns, so-called \u201ctall-and-skinny matrices\u201d. One key component to these improved methods is an orthogonal matrix transformation that preserves the separability of the NMF problem. Our final methods need a single pass over the data matrix and are suitable for streaming, multi-core, and MapReduce architectures. We demonstrate the efficacy of these algorithms on terabyte-sized synthetic matrices and real-world matrices from scientific computing and bioinformatics.", "target": "Scalable methods for nonnegative matrix factorizations of near-separable tall-and-skinny matrices"}
{"id": "task1540-2e00968cc7a74920b6d99eaa40e7b3a0", "input": "Multimedia reasoning, which is suitable for, among others, multimedia content analysis and high-level video scene interpretation, relies on the formal and comprehensive conceptualization of the represented knowledge domain. However, most multimedia ontologies are not exhaustive in terms of role definitions, and do not incorporate complex role inclusions and role interdependencies. In fact, most multimedia ontologies do not have a role box at all, and implement only a basic subset of the available logical constructors. Consequently, their application in multimedia reasoning is limited. To address the above issues, VidOnt, the very first multimedia ontology with SROIQ(D) expressivity and a DL-safe ruleset has been introduced for next-generation multimedia reasoning. In contrast to the common practice, the formal grounding has been set in one of the most expressive description logics, and the ontology validated with industry-leading reasoners, namely HermiT and FaCT++. This paper also presents best practices for developing multimedia ontologies, based on my ontology engineering approach.", "target": "A Novel Approach to Multimedia Ontology Engineering for Automated Reasoning over Audiovisual LOD Datasets"}
{"id": "task1540-9dadb58b55b042ddacc8e7e3dd730685", "input": "In this age of information technology, information access in a convenient manner has gained importance. Since speech is a primary mode of communication among human beings, it is natural for people to expect to be able to carry out spoken dialogue with computer [1]. Speech recognition system permits ordinary people to speak to the computer to retrieve information. It is desirable to have a human computer dialogue in local language. Hindi being the most widely spoken Language in India is the natural primary human language candidate for human machine interaction. There are five pairs of vowels in Hindi languages; one member is longer than the other one. This paper describes an overview of speech recognition system. How speech is produced and the properties and characteristics of Hindi", "target": "AN OVERVIEW OF HINDI SPEECH RECOGNITION"}
{"id": "task1540-e6a32144ddc841649285479bff86991f", "input": "Many kinds of variable-sized data we would like to model contain an internal hierarchical structure in the form of a tree, including source code, formal logical statements, and natural language sentences with parse trees. For such data it is natural to consider a model with matching computational structure. In this work, we introduce a variational autoencoder-based generative model for tree-structured data. We evaluate our model on a synthetic dataset, and a dataset with applications to automated theorem proving. By learning a latent representation over trees, our model can achieve similar test log likelihood to a standard autoregressive decoder, but with the number of sequentially dependent computations proportional to the depth of the tree instead of the number of nodes in the tree.", "target": "TREE-STRUCTURED VARIATIONAL AUTOENCODER"}
{"id": "task1540-0ff2756a31544d4d9c42bd138bac6bf7", "input": "People exhibit a tendency to generalize a novel noun to the basic-level in a hierarchical taxonomy \u2013 a cognitively salient category such as \u201cdog\u201d \u2013 with the degree of generalization depending on the number and type of exemplars. Recently, a change in the presentation timing of exemplars has also been shown to have an effect, surprisingly reversing the prior observed pattern of basic-level generalization. We explore the precise mechanisms that could lead to such behavior by extending a computational model of word learning and word generalization to integrate cognitive processes of memory and attention. Our results show that the interaction of forgetting and attention to novelty, as well as sensitivity to both type and token frequencies of exemplars, enables the model to replicate the empirical results from different presentation timings. Our results reinforce the need to incorporate general cognitive processes within word learning models to better understand the range of observed behaviors in vocabulary acquisition.", "target": "The Interaction of Memory and Attention in Novel Word Generalization: A Computational Investigation"}
{"id": "task1540-8a9e27b683e54213872ff0708f6047f9", "input": "Label distribution learning (LDL) is a general learning framework, which assigns a distribution over a set of labels to an instance rather than a single label or multiple labels. Current LDL methods have either restricted assumptions on the expression form of the label distribution or limitations in representation learning. This paper presents label distribution learning forests (LDLFs) a novel label distribution learning algorithm based on differentiable decision trees, which have several advantages: 1) Decision trees have the potential to model any general form of label distributions by the mixture of leaf node predictions. 2) The learning of differentiable decision trees can be combined with representation learning, e.g., to learn deep features in an end-to-end manner. We define a distributionbased loss function for forests, enabling all the trees to be learned jointly, and show that an update function for leaf node predictions, which guarantees a strict decrease of the loss function, can be derived by variational bounding. The effectiveness of the proposed LDLFs is verified on two LDL problems, including age estimation and crowd opinion prediction on movies, showing significant improvements to the state-of-the-art LDL methods.", "target": "Label Distribution Learning Forests"}
{"id": "task1540-700e53fe54cf4330987f7a69b3c22145", "input": "One of the long-term goals of artificial intelligence is to build an agent that can communicate intelligently with human in natural language. Most existing work on natural language learning relies heavily on training over a pre-collected dataset with annotated labels, leading to an agent that essentially captures the statistics of the fixed external training data. As the training data is essentially a static snapshot representation of the knowledge from the annotator, the agent trained this way is limited in adaptiveness and generalization of its behavior. Moreover, this is very different from the language learning process of humans, where language is acquired during communication by taking speaking action and learning from the consequences of speaking action in an interactive manner. This paper presents an interactive setting for grounded natural language learning, where an agent learns natural language by interacting with a teacher and learning from feedback, thus learning and improving language skills while taking part in the conversation. To achieve this goal, we propose a model which incorporates both imitation and reinforcement by leveraging jointly sentence and reward feedbacks from the teacher. Experiments are conducted to validate the effectiveness of the proposed approach.", "target": "Listen, Interact and Talk: Learning to Speak via Interaction"}
{"id": "task1540-23d0c43d7be44f35bbeb996f9d387762", "input": "We treat collaborative filtering as a univari\u00ad ate time series problem: given a user's previ\u00ad ous votes, predict the next vote. We describe two families of methods for transforming data to encode time order in ways amenable to off-the-shelf classification and density estima\u00ad tion tools. Using a decision-tree learning tool and two real-world data sets, we compare the results of these approaches to the results of collaborative filtering without ordering infor\u00ad mation. The improvements in both predic\u00ad tive accuracy and in recommendation quality that we realize advocate the use of predictive algorithms exploiting the temporal order of", "target": "Using Temporal Data for Making Recommendations"}
{"id": "task1540-f436d48ad8bc4a009e56f527b9ff5505", "input": "Mediation is a process, in which both parties agree to resolve their dispute by negotiating over alternative solutions presented by a mediator. In order to construct such solutions, mediation brings more information and knowledge, and, if possible, resources to the negotiation table. The contribution of this paper is the automated mediation machinery which does that. It presents an argumentation-based mediation approach that extends the logic-based approach to argumentation-based negotiation involving BDI agents. The paper describes the mediation algorithm. For comparison it illustrates the method with a case study used in an earlier work. It demonstrates how the computational mediator can deal with realistic situations in which the negotiating agents would otherwise fail due to lack of knowledge and/or resources.", "target": "Dispute Resolution Using Argumentation-Based Mediation"}
{"id": "task1540-2d8592bf55134efb97d719e2d67d6aa0", "input": "<lb>Many machine learning approaches are characterized by information constraints on how<lb>they interact with the training data. These include memory and sequential access constraints<lb>(e.g. fast first-order methods to solve stochastic optimization problems); communication con-<lb>straints (e.g. distributed learning); partial access to the underlying data (e.g. missing features<lb>and multi-armed bandits) and more. However, currently we have little understanding how such<lb>information constraints fundamentally affect our performance, independent of the learning prob-<lb>lem semantics. For example, are there learning problems where any algorithm which has small<lb>memory footprint (or can use any bounded number of bits from each example, or has certain<lb>communication constraints) will perform worse than what is possible without such constraints?<lb>In this paper, we describe how a single set of results implies positive answers to the above, for<lb>a variety of settings.", "target": "Fundamental Limits of Online and Distributed Algorithms for Statistical Learning and Estimation"}
{"id": "task1540-6d5f388e603243c18cd2ef2670877ff0", "input": "Tissue segmentation is an important pre-requisite for efficient and accurate diagnostics in digital pathology. However, it is well known that whole-slide scanners can fail in detecting all tissue regions, for example due to the tissue type, or due to weak staining because their tissue detection algorithms are not robust enough. In this paper, we introduce two different convolutional neural network architectures for whole slide image segmentation to accurately identify the tissue sections. We also compare the algorithms to a published traditional method. We collected 54 whole slide images with differing stains and tissue types from three laboratories to validate our algorithms. We show that while the two methods do not differ significantly they outperform their traditional counterpart (Jaccard index of 0.937 and 0.929 vs. 0.870, p < 0.01).", "target": "COMPARISON OF DIFFERENT METHODS FOR TISSUE SEGMENTATION IN HISTOPATHOLOGICAL WHOLE-SLIDE IMAGES"}
{"id": "task1540-cd6727f41d69429c9b050b4b5e1c3c95", "input": "A perfectly rational decision-maker chooses the best action with the highest utility gain from a set of possible actions. The optimality principles that describe such decision processes do not take into account the computational costs of finding the optimal action. Bounded rational decision-making addresses this problem by specifically trading off information-processing costs and expected utility. Interestingly, a similar trade-off between energy and entropy arises when describing changes in thermodynamic systems. This similarity has been recently used to describe bounded rational agents. Crucially, this framework assumes that the environment does not change while the decision-maker is computing the optimal policy. When this requirement is not fulfilled, the decision-maker will suffer inefficiencies in utility, that arise because the current policy is optimal for an environment in the past. Here we borrow concepts from non-equilibrium thermodynamics to quantify these inefficiencies and illustrate with simulations its relationship with computational resources.", "target": "Bounded Rational Decision-Making in Changing Environments"}
{"id": "task1540-6c5abc23d2674803a6985a462360ef0c", "input": "The standard Kernel Quadrature method for numerical integration with random point sets (also called Bayesian Monte Carlo) is known to converge in root mean square error at a rate determined by the ratio s/d, where s and d encode the smoothness and dimension of the integrand. However, an empirical investigation reveals that the rate constant C is highly sensitive to the distribution of the random points. In contrast to standard Monte Carlo integration, for which optimal importance sampling is wellunderstood, the sampling distribution that minimises C for Kernel Quadrature does not admit a closed form. This paper argues that the practical choice of sampling distribution is an important open problem. One solution is considered; a novel automatic approach based on adaptive tempering and sequential Monte Carlo. Empirical results demonstrate a dramatic reduction in integration error of up to 4 orders of magnitude can be achieved with the proposed method.", "target": "On the Sampling Problem for Kernel Quadrature"}
{"id": "task1540-7d4842db91b345279bdf7a95de164a3b", "input": "Convolutional neural networks provide visual features that perform remarkably well in many computer vision applications. However, training these networks requires significant amounts of supervision. This paper introduces a generic framework to train deep networks, end-to-end, with no supervision. We propose to fix a set of target representations, called Noise As Targets (NAT), and to constrain the deep features to align to them. This domain agnostic approach avoids the standard unsupervised learning issues of trivial solutions and collapsing of features. Thanks to a stochastic batch reassignment strategy and a separable square loss function, it scales to millions of images. The proposed approach produces representations that perform on par with state-of-the-art unsupervised methods on ImageNet and PASCAL VOC.", "target": "Unsupervised Learning by Predicting Noise"}
{"id": "task1540-d5ea925d494740f894dea02142b0df9c", "input": "The genetic selection of keywords set, the text frequencies of which are considered as attributes in text classification analysis, has been analyzed. The genetic optimization was performed on a set of words, which is the fraction of the frequency dictionary with given frequency limits. The frequency dictionary was formed on the basis of analyzed text array of texts of English fiction. As the fitness function which is minimized by the genetic algorithm, the error of nearest k neighbors classifier was used. The obtained results show high precision and recall of texts classification by authorship categories on the basis of attributes of keywords set which were selected by the genetic algorithm from the frequency dictionary.", "target": "Genetic Optimization of Keywords Subset in the Classification Analysis of Texts Authorship"}
{"id": "task1540-a8d1f8d57c9f4b08a7ad994ba13cb4e3", "input": "Wit is a quintessential form of rich interhuman interaction, and is often grounded in a specific situation (e.g., a comment in response to an event). In this work, we attempt to build computational models that can produce witty descriptions for a given image. Inspired by a cognitive account of humor appreciation, we employ linguistic wordplay, specifically puns. We compare our approach against meaningful baseline approaches via human studies. In a Turing test style evaluation, people find our model\u2019s description for an image to be wittier than a human\u2019s witty description 55% of the time!", "target": "Punny Captions: Witty Wordplay in Image Descriptions"}
{"id": "task1540-d335c227797e452d8efa88899e39f8b6", "input": "Local search methods can quickly find good quality solutions in cases where systematic search methods might take a large amount of time. Moreover, in the context of pattern set mining, exhaustive search methods are not applicable due to the large search space they have to explore. In this paper, we propose the application of stochastic local search to solve the pattern set mining. Specifically, to the task of concept learning. We applied a number of local search algorithms on a standard benchmark instances for pattern set mining and the results show the potentials for further exploration.", "target": "Stochastic Local Search for Pattern Set Mining"}
{"id": "task1540-ceedd4ce8328429288aafacaa39db6f5", "input": "Chinese poetry generation is a very challenging task in natural language processing. In this paper, we propose a novel two-stage poetry generating method which first plans the sub-topics of the poem according to the user\u2019s writing intent, and then generates each line of the poem sequentially, using a modified recurrent neural network encoder-decoder framework. The proposed planningbased method can ensure that the generated poem is coherent and semantically consistent with the user\u2019s intent. A comprehensive evaluation with human judgments demonstrates that our proposed approach outperforms the state-of-the-art poetry generating methods and the poem quality is somehow comparable to human poets.", "target": "Chinese Poetry Generation with Planning based Neural Network"}
{"id": "task1540-412fc33783714f9c9dfe0bde7abd4477", "input": "This work presents a fast and scalable algorithm for incremental learning of Gaussian mixture models. By performing rank-one updates on its precision matrices and determinants, its asymptotic time complexity is of O ( NKD ) for N data points, K Gaussian components and D dimensions. The resulting algorithm can be applied to high dimensional tasks, and this is confirmed by applying it to the classification datasets MNIST and CIFAR-10. Additionally, in order to show the algorithm\u2019s applicability to function approximation and control tasks, it is applied to three reinforcement learning tasks and its data-efficiency is evaluated.", "target": "Scalable and Incremental Learning of Gaussian Mixture Models"}
{"id": "task1540-c6a68276292e495195487c7826d46916", "input": "High dimensional superposition models characterize observations using parameters which can be written as a sum of multiple component parameters, each with its own structure, e.g., sum of low rank and sparse matrices, sum of sparse and rotated sparse vectors, etc. In this paper, we consider general superposition models which allow sum of any number of component parameters, and each component structure can be characterized by any norm. We present a simple estimator for such models, give a geometric condition under which the components can be accurately estimated, characterize sample complexity of the estimator, and give high probability non-asymptotic bounds on the componentwise estimation error. We use tools from empirical processes and generic chaining for the statistical analysis, and our results, which substantially generalize prior work on superposition models, are in terms of Gaussian widths of suitable sets.", "target": "High Dimensional Structured Superposition Models"}
{"id": "task1540-4d35058a3d6a4a2cab73e9f9948f57ec", "input": "We consider the problem of proper learning a Boolean Halfspace with integer weights {0, 1, . . . , t} from membership queries only. The best known algorithm for this problem is an adaptive algorithm that asks n ) membership queries where the best lower bound for the number of membership queries is n [4]. In this paper we close this gap and give an adaptive proper learning algorithm with two rounds that asks n membership queries. We also give a non-adaptive proper learning algorithm that asks n ) membership queries.", "target": "Learning Boolean Halfspaces with Small Weights from Membership Queries"}
{"id": "task1540-80f254dac9064f58b2cbfd85436fdd18", "input": "A novel method for learning optimal, orthonormal wavelet bases for representing 1and 2D signals, based on parallels between the wavelet transform and fully connected artificial neural networks, is described. The structural similarities between these two concepts are reviewed and combined to a \u201cwavenet\u201d, allowing for the direct learning of optimal wavelet filter coefficient through stochastic gradient descent with back-propagation over ensembles of training inputs, where conditions on the filter coefficients for constituting orthonormal wavelet bases are cast as quadratic regularisations terms. We describe the practical implementation of this method [1], and study its performance for high-energy physics collision events for QCD 2 \u2192 2 processes. It is shown that an optimal solution is found, even in a high-dimensional search space, and the implications of the result are discussed.", "target": "Learning optimal wavelet bases using a neural network approach"}
{"id": "task1540-32690cef87544cb288dfccdf103473e6", "input": "Recommender system has attracted much attention during the past decade, and many attack detection algorithms have been developed for better recommendation. Most previous approaches focus on the shilling attacks, where the attack organizer fakes a large number of user profiles by the same strategy to promote or demote an item. In this paper, we study a different attack style: unorganized malicious attacks, where attackers respectively use a small number of user profiles to attack their own target items without any organizer. This attack style occurs in many real applications, yet relevant study remains open. In this paper, we formulate the unorganized malicious attacks detection as a variant of matrix completion problem, and prove that attackers can be detected theoretically. We propose the Unorganized Malicious Attacks detection (UMA) algorithm, which can be viewed as a proximal alternating splitting augmented Lagrangian method. We verify, both theoretically and empirically, the effectiveness of our proposed algorithm.", "target": "Unorganized Malicious Attacks Detection"}
{"id": "task1540-505c1ec3a4de477f83df50183b125d57", "input": "We study the problem of identifying the best action among a set of possible options when the value of each action is given by a mapping from a number of noisy micro-observables in the so-called fixed confidence setting. Our main motivation is the application to the minimax game search, which has been a major topic of interest in artificial intelligence. In this paper we introduce an abstract setting to clearly describe the essential properties of the problem. While previous work only considered a two-move game tree search problem, our abstract setting can be applied to the general minimax games where the depth can be non-uniform and arbitrary, and transpositions are allowed. We introduce a new algorithm (LUCB-micro) for the abstract setting, and give its lower and upper sample complexity results. Our bounds recover some previous results, which were only available in more limited settings, while they also shed further light on how the structure of minimax problems influence sample complexity.", "target": "Structured Best Arm Identification with Fixed Confidence"}
{"id": "task1540-323430cfe8314cc2a122373b2c5f956a", "input": "We introduce a parametric form of pooling, based on a Gaussian, which can be optimized alongside the features in a single global objective function. By contrast, existing pooling schemes are based on heuristics (e.g. local maximum) and have no clear link to the cost function of the model. Furthermore, the variables of the Gaussian explicitly store location information, distinct from the appearance captured by the features, thus providing a what/where decomposition of the input signal. Although the differentiable pooling scheme can be incorporated in a wide range of hierarchical models, we demonstrate it in the context of a Deconvolutional Network model (Zeiler et al. [22]). We also explore a number of secondary issues within this model and present detailed experiments on MNIST digits.", "target": "Differentiable Pooling for Hierarchical Feature Learning"}
{"id": "task1540-7c99258a91cc49f0b8487ecf9d013ab2", "input": "The Aviation Safety Reporting System collects voluntarily submitted reports on aviation safety incidents to facilitate research work aiming to reduce such incidents. To effectively reduce these incidents, it is vital to accurately identify why these incidents occurred. More precisely, given a set of possible causes, or shaping factors, this task of cause identification involves identifying all and only those shaping factors that are responsible for the incidents described in a report. We investigate two approaches to cause identification. Both approaches exploit information provided by a semantic lexicon, which is automatically constructed via Thelen and Riloff\u2019s Basilisk framework augmented with our linguistic and algorithmic modifications. The first approach labels a report using a simple heuristic, which looks for the words and phrases acquired during the semantic lexicon learning process in the report. The second approach recasts cause identification as a text classification problem, employing supervised and transductive text classification algorithms to learn models from incident reports labeled with shaping factors and using the models to label unseen reports. Our experiments show that both the heuristic-based approach and the learning-based approach (when given sufficient training data) outperform the baseline system significantly.", "target": "Cause Identification from Aviation Safety Incident Reports via Weakly Supervised Semantic Lexicon Construction"}
{"id": "task1540-31447f40113a429e9cf559446fcb9f40", "input": "A metonym is a word with a figurative meaning, similar to a metaphor. Because metonyms are closely related to metaphors, we apply features that are used successfully for metaphor recognition to the task of detecting metonyms. On the ACL SemEval 2007 Task 8 data with gold standard metonym annotations, our system achieved 86.45% accuracy on the location metonyms. Our code can be found on GitHub.", "target": "Exploring Metaphorical Senses and Word Representations for Identifying Metonyms"}
{"id": "task1540-9edb89a5828840d081175a5542edc171", "input": "Machine learning is increasingly prevalent in stock market trading. Though neural networks have seen success in computer vision and natural language processing, they have not been as useful in stock market trading. To demonstrate the applicability of a neural network in stock trading, we made a single-layer neural network that recommends buying or selling shares of a stock by comparing the highest high of 10 consecutive days with that of the next 10 days, a process repeated for the stock\u2019s year-long historical data. A \u03c7 analysis found that the neural network can accurately and appropriately decide whether to buy or sell shares for a given stock, showing that a neural network can make simple decisions about the stock market.", "target": "Application of a Shallow Neural Network to Short-Term Stock Trading"}
{"id": "task1540-ca92977a88164afeaf5655946ba8c47f", "input": "We present in this paper a study on the ability and the benefits of using a keystroke dynamics authentication method for collaborative systems. Authentication is a challenging issue in order to guarantee the security of use of collaborative systems during the access control step. Many solutions exist in the state of the art such as the use of one time passwords or smart-cards. We focus in this paper on biometric based solutions that do not necessitate any additional sensor. Keystroke dynamics is an interesting solution as it uses only the keyboard and is invisible for users. Many methods have been published in this field. We make a comparative study of many of them considering the operational constraints of use for collaborative systems.", "target": "Keystroke Dynamics Authentication For Collaborative Systems"}
{"id": "task1540-7645d3be9da4494ea6b04905242e9592", "input": "We provide a qualitative analysis of the descriptions containing negations (no, not, n\u2019t, nobody, etc) in the Flickr30K corpus, and a categorization of negation uses. Based on this analysis, we provide a set of requirements that an image description system should have in order to generate negation sentences. As a pilot experiment, we used our categorization to manually annotate sentences containing negations in the Flickr30k corpus, with an agreement score of \u03ba=0.67. With this paper, we hope to open up a broader discussion of subjective language in image descriptions.", "target": "Pragmatic factors in image description: the case of negations"}
{"id": "task1540-8c934a14c1034bcc90fbfefdee44ac47", "input": "We describe here a methodology to identify a list of ambiguous Malay words that are commonly being used in Malay documentations such as Requirement Specification. We compiled several relevant and appropriate requirement quality attributes and sentence rules from previous literatures and adopt it to come out with a set of ambiguity attributes that most suit Malay words. The extracted Malay ambiguous words (potential) are then being mapped onto the constructed ambiguity attributes to confirm their vagueness. The list is then verified by Malay linguist experts. This paper aims to identify a list of potential ambiguous words in Malay as an attempt to assist writers to avoid using the vague words while documenting Malay Requirement Specification as well as to any other related Malay documentation. The result of this study is a list of 120 potential ambiguous Malay words that could act as guidelines in writing Malay sentences.", "target": "MAPPING: AN EXPLORATORY STUDY"}
{"id": "task1540-3d7c2944fc2046db8316ed5b7b48e53a", "input": "In the last two decades, a number of methods have been proposed for forecasting based on fuzzy time series. Most of the fuzzy time series methods are presented for forecasting of car road accidents. However , the forecasting accuracy rates of the existing methods are not good enough. In this paper, we compared our proposed new method of fuzzy time series forecasting with existing methods. Our method is based on means based partitioning of the historical data of car road accidents. The proposed method belongs to the kth order and time-variant methods. The proposed method can get the best forecasting accuracy rate for forecasting the car road accidents than the existing methods. KeywordsFuzzy sets, Fuzzy logical groups, fuzzified data, fuzzy time series.", "target": "Inaccuracy Minimization by Partitioning Fuzzy Data Sets \u2013 Validation of an Analytical Methodology"}
{"id": "task1540-13cfeefc10b04e90a0b5d715502b93cf", "input": "In this work we study variance in the results of neural network training on a wide variety of configurations in automatic speech recognition. Although this variance itself is well known, this is, to the best of our knowledge, the first paper that performs an extensive empirical study on its effects in speech recognition. We view training as sampling from a distribution and show that these distributions can have a substantial variance. These results show the urgent need to rethink the way in which results in the literature are reported and interpreted.", "target": "Training variance and performance evaluation of neural networks in speech"}
{"id": "task1540-9dded0becfc7409cabf0f4bf8bf78738", "input": "Most neural network models for document classification on social media focus on text information to the neglect of other information on these platforms. In this paper, we classify post stance on social media channels and develop UTCNN, a neural network model that incorporates user tastes, topic tastes, and user comments on posts. UTCNN not only works on social media texts, but also analyzes texts in forums and message boards. Experiments performed on Chinese Facebook data and English online debate forum data show that UTCNN achieves a 0.755 macroaverage f-score for supportive, neutral, and unsupportive stance classes on Facebook data, which is significantly better than models in which either user, topic, or comment information is withheld. This model design greatly mitigates the lack of data for the minor class without the use of oversampling. In addition, UTCNN yields a 0.842 accuracy on English online debate forum data, which also significantly outperforms results from previous work as well as other deep learning models, showing that UTCNN performs well regardless of language or platform.", "target": "UTCNN: a Deep Learning Model of Stance Classification on Social Media Text"}
{"id": "task1540-01a5bf08f392431895cbd21589cf452b", "input": "As an important and challenging problem in computer vision and graphics, keypoint-based object tracking is typically formulated in a spatio-temporal statistical learning framework. However, most existing keypoint trackers are incapable of effectively modeling and balancing the following three aspects in a simultaneous manner: temporal model coherence across frames, spatial model consistency within frames, and discriminative feature construction. To address this issue, we propose a robust keypoint tracker based on spatio-temporal multi-task structured output optimization driven by discriminative metric learning. Consequently, temporal model coherence is characterized by multi-task structured keypoint model learning over several adjacent frames, while spatial model consistency is modeled by solving a geometric verification based structured learning problem. Discriminative feature construction is enabled by metric learning to ensure the intra-class compactness and inter-class separability. Finally, the above three modules are simultaneously optimized in a joint learning scheme. Experimental results have demonstrated the effectiveness of our tracker.", "target": "Metric Learning Driven Multi-Task Structured Output Optimization for Robust Keypoint Tracking"}
{"id": "task1540-18109dbe577c411483837155e19d807a", "input": "Distributional semantic models learn vector representations of words through the contexts they occur in. Although the choice of context (which often takes the form of a sliding window) has a direct influence on the resulting embeddings, the exact role of this model component is still not fully understood. This paper presents a systematic analysis of context windows based on a set of four distinct hyperparameters. We train continuous SkipGram models on two English-language corpora for various combinations of these hyper-parameters, and evaluate them on both lexical similarity and analogy tasks. Notable experimental results are the positive impact of cross-sentential contexts and the surprisingly good performance of right-context windows.", "target": "Redefining Context Windows for Word Embedding Models: An Experimental Study"}
{"id": "task1540-5c049555b18740e49025622fea6ddf11", "input": "We derive an equation for temporal difference learning from statistical principles. Specifically, we start with the variational principle and then bootstrap to produce an updating rule for discounted state value estimates. The resulting equation is similar to the standard equation for temporal difference learning with eligibility traces, so called TD(\u03bb), however it lacks the parameter \u03b1 that specifies the learning rate. In the place of this free parameter there is now an equation for the learning rate that is specific to each state transition. We experimentally test this new learning rule against TD(\u03bb) and find that it offers superior performance in various settings. Finally, we make some preliminary investigations into how to extend our new temporal difference algorithm to reinforcement learning. To do this we combine our update equation with both Watkins\u2019 Q(\u03bb) and Sarsa(\u03bb) and find that it again offers superior performance without a learning rate parameter.", "target": "Temporal Difference Updating without a Learning Rate"}
{"id": "task1540-433867d62b23447c8b3a1fdab91a6ce7", "input": "Recently there has been an increasing trend to use deep learning frameworks for both 2D consumer images and for 3D medical images. However, there has been little effort to use deep frameworks for volumetric vascular segmentation. We wanted to address this by providing a freely available dataset of 12 annotated two-photon vasculature microscopy stacks. We demonstrated the use of deep learning framework consisting both 2D and 3D convolutional filters (ConvNet). Our hybrid 2D-3D architecture produced promising segmentation result. We derived the architectures from Lee et al. who used the ZNN framework initially designed for electron microscope image segmentation. We hope that by sharing our volumetric vasculature datasets, we will inspire other researchers to experiment with vasculature dataset and improve the used network architectures.", "target": "Deep Learning Convolutional Networks for Multiphoton Microscopy Vasculature Segmentation"}
{"id": "task1540-f7147019362b4a96bdff7fbba0d116cb", "input": "A Bayesian net (BN) is more than a succinct way to encode a probabilistic distribution; it also corresponds to a function used to answer queries. A BN can therefore be evaluated by the accuracy of the answers it returns. Many algorithms for learning BNs, however, attempt to optimize another criterion (usu\u00ad ally likelihood, possibly augmented with a regularizing term) , which is independent of the distribution of queries that are posed. This paper takes the \"performance criteria\" seriously, and considers the challenge of com\u00ad puting the BN whose performance read \"accuracy over the distribution of queries\" is optimal. We show that many aspects of this learning task are more difficult than the corresponding subtasks in the standard model.", "target": "Learning Bayesian Nets that Perform Well"}
{"id": "task1540-d9ea70d4c7dd408ca0625be85aaec691", "input": "Extracting per-frame features using convolutional neural networks for real-time processing of video data is currently mainly performed on powerful GPU-accelerated workstations and compute clusters. However, there are many applications such as smart surveillance cameras that require or would benefit from on-site processing. To this end, we propose and evaluate a novel algorithm for changebased evaluation of CNNs for video data recorded with a static camera setting, exploiting the spatio-temporal sparsity of pixel changes. We achieve an average speed-up of 8.6\u00d7 over a cuDNN baseline on a realistic benchmark with a negligible accuracy loss of less than 0.1% and no retraining of the network. The resulting energy efficiency is 10\u00d7 higher than per-frame evaluation and reaches an equivalent of 328GOp/s/W on the Tegra X1 platform.", "target": "CBinfer: Change-Based Inference for Convolutional Neural Networks on Video Data"}
{"id": "task1540-9f8a7513bbe040678d5de9b05aea361b", "input": "The amount of data available in the world is growing faster and bigger than our ability to deal with it. However, if we take advantage of the internal structure, data may become much smaller for machine learning purposes. In this paper we focus on one of the most fundamental machine learning tasks, empirical risk minimization (ERM), and provide faster algorithms with the help from the clustering structure of the data. We introduce a simple notion of raw clustering that can be efficiently obtained with just one pass of the data, and propose two algorithms. Our variance-reduction based algorithm ClusterSVRG introduces a new gradient estimator using the clustering information, and our accelerated algorithm ClusterACDM is built on a novel Haar transformation applied to the dual space of each cluster. Our algorithms outperform their classical counterparts both in theory and practice.", "target": "Exploiting the Structure: Stochastic Gradient Methods Using Raw Clusters"}
{"id": "task1540-bcf9f18bbce54d21ba67e8a337301563", "input": "Behavior Trees are commonly used to model agents for robotics and games, where constrained behaviors must be designed by human experts in order to guarantee that these agents will execute a specific chain of actions given a specific set of perceptions. In such application areas, learning is a desirable feature to provide agents with the ability to adapt and improve interactions with humans and environment, but often discarded due to its unreliability. In this paper, we propose a framework that uses Reinforcement Learning nodes as part of Behavior Trees to address the problem of adding learning capabilities in constrained agents. We show how this framework relates to Options in Hierarchical Reinforcement Learning, ensuring convergence of nested learning nodes, and we empirically show that the learning nodes do not affect the execution of other nodes in the tree.", "target": "A Framework for Constrained and Adaptive Behavior-Based Agents"}
{"id": "task1540-c1df32c07d1c458db9907a3faf643c63", "input": "The proximal problem for structured penalties obtained via convex relaxations of submodular functions is known to be equivalent to minimizing separable convex functions over the corresponding submodular polyhedra. In this paper, we reveal a comprehensive class of structured penalties for which penalties this problem can be solved via an efficiently solvable class of parametric maxflow optimization. We then show that the parametric maxflow algorithm proposed by Gallo et al. [17] and its variants, which runs, in the worst-case, at the cost of only a constant factor of a single computation of the corresponding maxflow optimization, can be adapted to solve the proximal problems for those penalties. Several existing structured penalties satisfy these conditions; thus, regularized learning with these penalties is solvable quickly using the parametric maxflow algorithm. We also investigate the empirical runtime performance of the proposed framework.", "target": "Parametric Maxflows for Structured Sparse Learning with Convex Relaxations of Submodular Functions"}
{"id": "task1540-44970a41414b4c2ebf5c03dda6459c01", "input": "We introduce a new discrepancy score between two distributions that gives an indi-<lb>cation on their similarity. While much research has been done to determine if two<lb>samples come from exactly the same distribution, much less research considered<lb>the problem of determining if two finite samples come from similar distributions.<lb>The new score gives an intuitive interpretation of similarity; it optimally perturbs<lb>the distributions so that they best fit each other. The score is defined between<lb>distributions, and can be efficiently estimated from samples. We provide conver-<lb>gence bounds of the estimated score, and develop hypothesis testing procedures<lb>that test if two data sets come from similar distributions. The statistical power of<lb>this procedures is presented in simulations. We also compare the score\u2019s capacity<lb>to detect similarity with that of other known measures on real data.", "target": "The Perturbed Variation"}
{"id": "task1540-0e2930c0767949afbab2554294698afa", "input": "Query-based video summarization is the task of creating a brief visual trailer, which captures the parts of the video (or a collection of videos) that are most relevant to the user-issued query. In this paper, we propose an unsupervised label propagation approach for this task. Our approach effectively captures the multimodal semantics of queries and videos using state-of-the-art deep neural networks and creates a summary that is both semantically coherent and visually attractive. We describe the theoretical framework of our graph-based approach and empirically evaluate its effectiveness in creating relevant and attractive trailers. Finally, we showcase example video trailers generated by our system.", "target": "Semantic Video Trailers"}
{"id": "task1540-2ea810a883c441dca1131a444e6c5929", "input": "Building a voice conversion (VC) system from non-parallel speech corpora is challenging but highly valuable in real application scenarios. In most situations, the source and the target speakers do not repeat the same texts or they may even speak different languages. In this case, one possible, although indirect, solution is to build a generative model for speech. Generative models focus on explaining the observations with latent variables instead of learning a pairwise transformation function, thereby bypassing the requirement of speech frame alignment. In this paper, we propose a non-parallel VC framework with a variational autoencoding Wasserstein generative adversarial network (VAW-GAN) that explicitly considers a VC objective when building the speech model. Experimental results corroborate the capability of our framework for building a VC system from unaligned data, and demonstrate improved conversion quality.", "target": "Voice Conversion from Unaligned Corpora using Variational Autoencoding Wasserstein Generative Adversarial Networks"}
{"id": "task1540-89c65ec0227344169927ba4a750a9e28", "input": "We describe a question answering model that applies to both images and structured knowledge bases.The model uses natural language strings to automatically assemble neural networks from a collection of composable modules. Parameters for these modules are learned jointly with network-assembly parameters via reinforcement learning, with only (world, question, answer) triples as supervision. Our approach, which we term a dynamic neural module network, achieves state-of-the-art results on benchmark datasets in both visual and structured domains.", "target": "Learning to Compose Neural Networks for Question Answering"}
{"id": "task1540-2d424d3e4db0461b860a7a128f464eb1", "input": "This paper describes the speech processing activities conducted at the Polish consortium of the CLARIN project. The purpose of this segment of the project was to develop specific tools that would allow for automatic and semi-automatic processing of large quantities of acoustic speech data. The tools include the following: grapheme-to-phoneme conversion, speech-to-text alignment, voice activity detection, speaker diarization, keyword spotting and automatic speech transcription. Furthermore, in order to develop these tools, a large high-quality studio speech corpus was recorded and released under an open license, to encourage development in the area of Polish speech research. Another purpose of the corpus was to serve as a reference for studies in phonetics and pronunciation. All the tools and resources were released on the the Polish CLARIN website. This paper discusses the current status and future plans for the project.", "target": "Polish Read Speech Corpus for Speech Tools and Services"}
{"id": "task1540-d59cf3cbac03494da472abeea4337a26", "input": "A number of representation schemes have been presented for use within Learning Classifier Systems, ranging from binary encodings to neural networks. This paper presents results from an investigation into using a discrete dynamical system representation within the XCS Learning Classifier System. In particular, asynchronous random Boolean networks are used to represent the traditional condition-action production system rules. It is shown possible to use self-adaptive, open-ended evolution to design an ensemble of such discrete dynamical systems within XCS to solve a number of wellknown test problems.", "target": "Discrete Dynamical Genetic Programming in XCS"}
{"id": "task1540-e26dde3acae143a396d90259ee0310db", "input": "Consider a weighted or unweighted k-nearest neighbor graph that has been built on n data points drawn randomly according to some density p on R. We study the convergence of the shortest path distance in such graphs as the sample size tends to infinity. We prove that for unweighted kNN graphs, this distance converges to an unpleasant distance function on the underlying space whose properties are detrimental to machine learning. We also study the behavior of the shortest path distance in weighted kNN graphs.", "target": "Shortest path distance in random k-nearest neighbor graphs"}
{"id": "task1540-3b35ac29469148e1b05e94bde7bc5b0d", "input": "In this paper, we review the problem of matrix completion and expose its intimate relations with algebraic geometry, combinatorics and graph theory. We present the first necessary and sufficient combinatorial conditions for matrices of arbitrary rank to be identifiable from a set of matrix entries, yielding theoretical constraints and new algorithms for the problem of matrix completion. We conclude by algorithmically evaluating the tightness of the given conditions and algorithms for practically relevant matrix sizes, showing that the algebraic-combinatorial approach can lead to improvements over stateof-the-art matrix completion methods.", "target": "A Combinatorial Algebraic Approach for the Identifiability of Low-Rank Matrix Completion"}
{"id": "task1540-59f69b4fd6134168a20314e9a0a90f14", "input": "Model counting is the problem of computing the number of models that satisfy a given propositional theory. It has recently been applied to solving inference tasks in probabilistic logic programming, where the goal is to compute the probability of given queries being true provided a set of mutually independent random variables, a model (a logic program) and some evidence. The core of solving this inference task involves translating the logic program to a propositional theory and using a model counter. In this paper, we show that for some problems that involve inductive definitions like reachability in a graph, the translation of logic programs to SAT can be expensive for the purpose of solving inference tasks. For such problems, direct implementation of stable model semantics allows for more efficient solving. We present two implementation techniques, based on unfounded set detection, that extend a propositional model counter to a stable model counter. Our experiments show that for particular problems, our approach can outperform a state-of-the-art probabilistic logic programming solver by several orders of magnitude in terms of running time and space requirements, and can solve instances of significantly larger sizes on which the current solver runs out of time or memory.", "target": "Stable Model Counting and Its Application in Probabilistic Logic Programming"}
{"id": "task1540-fd3b0ae144e84d06bae97a1621cc080f", "input": "So-called combined approaches answer a conjunctive query over a description logic ontology in three steps: first, they materialise certain consequences of the ontology and the data; second, they evaluate the query over the data; and third, they filter the result of the second phase to eliminate unsound answers. Such approaches were developed for various members of the DL-Lite and the EL families of languages, but none of them can handle ontologies containing nominals. In our work, we bridge this gap and present a combined query answering approach for ELHO \u22a5\u2014a logic that contains all features of the OWL 2 EL standard apart from transitive roles and complex role inclusions. This extension is nontrivial because nominals require equality reasoning, which introduces complexity into the first and the third step. Our empirical evaluation suggests that our technique is suitable for practical application, and so it provides a practical basis for conjunctive query answering in a large fragment of OWL 2 EL.", "target": "Introducing Nominals to the Combined Query Answering Approaches for EL"}
{"id": "task1540-37450cbe74fc4964a8160f4e1191712e", "input": "Despite tremendous progress in computer vision, there has not been an attempt for machine learning on very large-scale medical image databases. We present an interleaved text/image deep learning system to extract and mine the semantic interactions of radiology images and reports from a national research hospital\u2019s Picture Archiving and Communication System. With natural language processing, we mine a collection of representative \u223c216K two-dimensional key images selected by clinicians for diagnostic reference, and match the images with their descriptions in an automated manner. Our system interleaves between unsupervised learning and supervised learning on documentand sentence-level text collections, to generate semantic labels and to predict them given an image. Given an image of a patient scan, semantic topics in radiology levels are predicted, and associated key-words are generated. Also, a number of frequent disease types are detected as present or absent, to provide more specific interpretation of a patient scan. This shows the potential of largescale learning and prediction in electronic patient records available in most modern clinical institutions.", "target": "Interleaved Text/Image Deep Mining on a Large-Scale Radiology Database for Automated Image Interpretation"}
{"id": "task1540-862bd01c09a24970973ea854ddad0f72", "input": "We present a visually grounded model of speech perception which projects spoken utterances and images to a joint semantic space. We use a multi-layer recurrent highway network to model the temporal nature of spoken speech, and show that it learns to extract both form and meaningbased linguistic knowledge from the input signal. We carry out an in-depth analysis of the representations used by different components of the trained model and show that encoding of semantic aspects tends to become richer as we go up the hierarchy of layers, whereas encoding of formrelated aspects of the language input tends to initially increase and then plateau or decrease.", "target": "Representations of language in a model of visually grounded speech signal"}
{"id": "task1540-8192c2649fcd45ecaba3b14f835a36ba", "input": "We address the novel problem of automatically generating quiz-style knowledge questions from a knowledge graph such as DBpedia. Questions of this kind have ample applications, for instance, to educate users about or to evaluate their knowledge in a specific domain. To solve the problem, we propose an end-to-end approach. The approach first selects a named entity from the knowledge graph as an answer. It then generates a structured triple-pattern query, which yields the answer as its sole result. If a multiplechoice question is desired, the approach selects alternative answer options. Finally, our approach uses a template-based method to verbalize the structured query and yield a natural language question. A key challenge is estimating how difficult the generated question is to human users. To do this, we make use of historical data from the Jeopardy! quiz show and a semantically annotated Web-scale document collection, engineer suitable features, and train a logistic regression classifier to predict question difficulty. Experiments demonstrate the viability of our overall approach.", "target": "Knowledge Questions from Knowledge Graphs"}
{"id": "task1540-94556b76a7a9428abc3f518b91162203", "input": "The rapid advancement of machine learning techniques has re-energized research into general artificial intelligence. While the idea of domain-agnostic meta-learning is appealing, this emerging field must come to terms with its relationship to human cognition and the statistics and structure of the tasks humans perform. The position of this article is that only by aligning our agents\u2019 abilities and environments with those of humans do we stand a chance at developing general artificial intelligence (GAI).", "target": "Minimally Naturalistic Artificial Intelligence"}
{"id": "task1540-a2ba660cd7b943849d352b8d507d7538", "input": "As a well-known NP-hard problem, the Three-Index Assignment Problem (AP3) has attracted lots of research efforts for developing heuristics. However, existing heuristics either obtain less competitive solutions or consume too much running time. In this paper, a new heuristic named Approximate Muscle guided Beam Search (AMBS) is developed to achieve a good trade-off between solution quality and running time. By combining the approximate muscle with beam search, the solution space size can be significantly decreased, thus the time for searching the solution can be sharply reduced. Extensive experimental results on the benchmark indicate that the new algorithm is able to obtain solutions with competitive quality and it can be employed on instances with large-scale. Work of this paper not only proposes a new efficient heuristic, but also provides a promising method to improve the efficiency of beam search.", "target": "Approximate Muscle Guided Beam Search for Three-Index Assignment Problem"}
{"id": "task1540-f7d14bdb4053444b9e81040390627412", "input": "Reinforcement learning systems are often concerned with balancing exploration of untested actions against exploitation of actions that are known to be good. The benefit of exploration can be estimated using the classi\u00ad cal notion of Value of Informationthe expected im\u00ad provement in future decision quality arising from the tnformation acquired by exploration. Estimating this quantity requires an assessment of the agent's uncer\u00ad tainty about its current value estimates for states. In this paper we investigate ways to represent and rea\u00ad son about this uncertainty in algorithms where the sys\u00ad tem attempts to learn a model of its environment. We explicitly represent uncertainty about the parameters of the model and build probability distributions over Q\u00ad values based on these. These distributions are used to compute a myopic approximation to the value of infor\u00ad mation for each action and hence to select the action that best balances exploration and exploitation.", "target": "Model based Bayesian Exploration"}
{"id": "task1540-59e0f41cb96f485288b6aa0ea8a26f33", "input": "The sample complexity of active learning under the realizability assumption has been well-studied. The realizability assumption, however, rarely holds in practice. In this paper, we theoretically characterize the sample complexity of active learning in the non-realizable case under multiview setting. We prove that, with unbounded Tsybakov noise, the sample complexity of multiview active learning can be \u00d5(log 1\u01eb ), contrasting to single-view setting where the polynomial improvement is the best possible achievement. We also prove that in general multi-view setting the sample complexity of active learning with unbounded Tsybakov noise is \u00d5(1\u01eb ), where the order of 1/\u01eb is independent of the parameter in Tsybakov noise, contrasting to previous polynomial bounds where the order of 1/\u01eb is related to the parameter in Tsybakov noise.", "target": "Multi-View Active Learning in the Non-Realizable Case"}
{"id": "task1540-ba3d2a7a077e49b1b0e27c48c5936a3d", "input": "Learning to solve complex sequences of tasks\u2014while both leveraging transfer and avoiding catastrophic forgetting\u2014remains a key obstacle to achieving human-level intelligence. The progressive networks approach represents a step forward in this direction: they are immune to forgetting and can leverage prior knowledge via lateral connections to previously learned features. We evaluate this architecture extensively on a wide variety of reinforcement learning tasks (Atari and 3D maze games), and show that it outperforms common baselines based on pretraining and finetuning. Using a novel sensitivity measure, we demonstrate that transfer occurs at both low-level sensory and high-level control layers of the learned policy.", "target": "Progressive Neural Networks"}
{"id": "task1540-47ceb37194d3487283a48e2ee8889043", "input": "We introduce a globally-convergent algorithm for optimizing the tree-reweighted (TRW) variational objective over the marginal polytope. The algorithm is based on the conditional gradient method (Frank-Wolfe) and moves pseudomarginals within the marginal polytope through repeated maximum a posteriori (MAP) calls. This modular structure enables us to leverage black-box MAP solvers (both exact and approximate) for variational inference, and obtains more accurate results than tree-reweighted algorithms that optimize over the local consistency relaxation. Theoretically, we bound the sub-optimality for the proposed algorithm despite the TRW objective having unbounded gradients at the boundary of the marginal polytope. Empirically, we demonstrate the increased quality of results found by tightening the relaxation over the marginal polytope as well as the spanning tree polytope on synthetic and real-world instances.", "target": "Barrier Frank-Wolfe for Marginal Inference"}
{"id": "task1540-f9560af07eb347ccafa5e16b99f848a8", "input": "In this paper, we will expound upon the concepts proffered in [5], where we proposed an information theoretic approach to intelligence in the computational sense. We will examine data and meme aggregation, and study the effect of limited resources on the resulting meme amplitudes.", "target": "The Computational Theory of Intelligence: Data Aggregation"}
{"id": "task1540-caaebbd79df54627b7d4f4ff2c780e15", "input": "Programming by Example (PBE) targets at automatically inferring a computer program for accomplishing a certain task from sample input and output. In this paper, we propose a deep neural networks (DNN) based PBE model called Neural Programming by Example (NPBE), which can learn from input-output strings and induce programs that solve the string manipulation problems. Our NPBE model has four neural network based components: a string encoder, an input-output analyzer, a program generator, and a symbol selector. We demonstrate the effectiveness of NPBE by training it end-toend to solve some common string manipulation problems in spreadsheet systems. The results show that our model can induce string manipulation programs effectively. Our work is one step towards teaching DNN to generate computer programs.", "target": "Neural Programming by Example"}
{"id": "task1540-7deaed5c2ceb433ca1d6323afd77d8c5", "input": "Unsupervised dependency parsing, which tries to discover linguistic dependency structures from unannotated data, is a very challenging task. Almost all previous work on this task focuses on learning generative models. In this paper, we develop an unsupervised dependency parsing model based on the CRF autoencoder. The encoder part of our model is discriminative and globally normalized which allows us to use rich features as well as universal linguistic priors. We propose an exact algorithm for parsing as well as a tractable learning algorithm. We evaluated the performance of our model on eight multilingual treebanks and found that our model achieved comparable performance with state-of-the-art approaches.", "target": "CRF Autoencoder for Unsupervised Dependency Parsing\u2217"}
{"id": "task1540-a0a2e369b5c54cc2a1ed3d99086be2e2", "input": "The choice of approximate posterior distribution is one of the core problems in variational inference. Most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference, focusing on mean-field or other simple structured approximations. This restriction has a significant impact on the quality of inferences made using variational methods. We introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions. Our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained. We use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations. We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference.", "target": "Variational Inference with Normalizing Flows"}
{"id": "task1540-033fb9f94e1a47429a28635f504c58a8", "input": "We describe MITRE\u2019s submission to the SemEval-2016 Task 6, Detecting Stance in Tweets. This effort achieved the top score in Task A on supervised stance detection, producing an average F1 score of 67.8 when assessing whether a tweet author was in favor or against a topic. We employed a recurrent neural network initialized with features learned via distant supervision on two large unlabeled datasets. We trained embeddings of words and phrases with the word2vec skip-gram method, then used those features to learn sentence representations via a hashtag prediction auxiliary task. These sentence vectors were then finetuned for stance detection on several hundred labeled examples. The result was a high performing system that used transfer learning to maximize the value of the available training data.", "target": "MITRE at SemEval-2016 Task 6: Transfer Learning for Stance Detection"}
{"id": "task1540-3c7d8333d0cd40b38b10ce693f3930ea", "input": "Diagnosis of liver infection at preliminary stage is important for better treatment. In today\u2019s scenario devices like sensors are used for detection of infections. Accurate classification techniques are required for automatic identification of disease samples. In this context, this study utilizes data mining approaches for classification of liver patients from healthy individuals. Four algorithms (Na\u00efve Bayes, Bagging, Random forest and SVM) were implemented for classification using R platform. Further to improve the accuracy of classification a hybrid NeuroSVM model was developed using SVM and feed-forward artificial neural network (ANN). The hybrid model was tested for its performance using statistical parameters like root mean square error (RMSE) and mean absolute percentage error (MAPE). The model resulted in a prediction accuracy of 98.83%. The results suggested that development of hybrid model improved the accuracy of prediction. To serve the medicinal community for prediction of liver disease among patients, a graphical user interface (GUI) has been developed using R. The GUI is deployed as a package in local repository of R platform for users to perform prediction.", "target": "NeuroSVM: A Graphical User Interface for Identification of Liver Patients"}
{"id": "task1540-d0c10e13a36f4a29acd087f5be1296fc", "input": "Ambiguity is an indisputable and ubiquitous feature of linguistic products. In this article we investigate both the locus of ambiguity in the architecture of Language and the origin of ambiguity in natural communication systems. We locate ambiguity at the externalization branch of Language and we study the emergence of ambiguity in communication through the concept of logical irreversibility and within the framework of Shannon\u2019s information theory. This leads us to a precise and general expression of the intuition behind Zipf\u2019s vocabulary balance in terms of a symmetry equation between the complexities of the coding and the decoding processes that imposes an unavoidable amount of logical uncertainty in natural communication. Accordingly, the emergence of irreversible computations is required if the complexities of the coding and the decoding processes are balanced in a symmetric scenario, which means that the emergence of ambiguous codes is a necessary condition for natural communication to succeed.", "target": "On ambiguity. Its locus in the architecture of Language and its origin in efficient communication"}
{"id": "task1540-d9df579ad09c4faf8c028cea05f789a2", "input": "Tree-structured neural networks exploit valuable syntactic parse information as they interpret the meanings of sentences. However, they suffer from two key technical problems that make them slow and unwieldy for large-scale NLP tasks: they can only operate on parsed sentences and they do not directly support batched computation. We address these issues by introducing the Stackaugmented Parser-Interpreter Neural Network (SPINN), which combines parsing and interpretation within a single treesequence hybrid model by integrating treestructured sentence interpretation into the linear sequential structure of a shift-reduce parser. Our model supports batched computation for a speedup of up to 25x over other tree-structured models, and its integrated parser allows it to operate on unparsed data with little loss of accuracy. We evaluate it on the Stanford NLI entailment task and show that it significantly outperforms other sentence-encoding models.", "target": "A Fast Unified Model for Parsing and Sentence Understanding"}
{"id": "task1540-f5bd8a7cdbcc4994a72a76c86663f8de", "input": "We introduce a recurrent neural network architecture for automated road surface wetness detection from audio of tiresurface interaction. The robustness of our approach is evaluated on 785,826 bins of audio that span an extensive range of vehicle speeds, noises from the environment, road surface types, and pavement conditions including international roughness index (IRI) values from 25 in/mi to 1400 in/mi. The training and evaluation of the model are performed on different roads to minimize the impact of environmental and other external factors on the accuracy of the classification. We achieve an unweighted average recall (UAR) of 93.2 % across all vehicle speeds including 0 mph. The classifier still works at 0 mph because the discriminating signal is present in the sound of other vehicles driving by.", "target": "Detecting Road Surface Wetness from Audio: A Deep Learning Approach"}
{"id": "task1540-88f66154934f4ec2bfe9845d39f49536", "input": "We develop a novel preconditioning method for ridge regression, based on recent linear sketching methods. By equipping Stochastic Variance Reduced Gradient (SVRG) with this preconditioning process, we obtain a significant speed-up relative to fast stochastic methods such as SVRG, SDCA and SAG.", "target": "Solving Ridge Regression using Sketched Preconditioned SVRG"}
{"id": "task1540-46815ad482a74b438fdb3de494a12e62", "input": "In this paper, we address the problem of embedded feature selection for ranking on top of the list problems. We pose this problem as a regularized empirical risk minimization with p-norm push loss function (p = \u221e) and sparsity inducing regularizers. We leverage the issues related to this challenging optimization problem by considering an alternating direction method of multipliers algorithm which is built upon proximal operators of the loss function and the regularizer. Our main technical contribution is thus to provide a numerical scheme for computing the infinite push loss function proximal operator. Experimental results on toy, DNA microarray and BCI problems show how our novel algorithm compares favorably to competitors for ranking on top while using fewer variables in the scoring function.", "target": "Sparse Support Vector Infinite Push"}
{"id": "task1540-fc218dcd27a54ffc943c766669f58115", "input": "In many robotic applications, some aspects of the system dynamics can be modeled accurately while others are difficult to obtain or model. We present a novel reinforcement learning (RL) method for continuous state and action spaces that learns with partial knowledge of the system and without active exploration. It solves linearly-solvable Markov decision processes (L-MDPs), which are well suited for continuous state and action spaces, based on an actor-critic architecture. Compared to previous RL methods for L-MDPs and path integral methods which are model based, the actor-critic learning does not need a model of the uncontrolled dynamics and, importantly, transition noise levels; however, it requires knowing the control dynamics for the problem. We evaluate our method on two synthetic test problems, and one real-world problem in simulation and using real traffic data. Our experiments demonstrate improved learning and policy performance.", "target": "Actor-Critic for Linearly-Solvable Continuous MDP with Partially Known Dynamics"}
{"id": "task1540-d68195d7a2c94da9bb60e7d8c3c08a8a", "input": "In this paper we study the application of convolutional neural networks for jointly detecting objects depicted in still images and estimating their 3D pose. We identify different feature representations of oriented objects, and energies that lead a network to learn this representations. The choice of the representation is crucial since the pose of an object has a natural, continuous structure while its category is a discrete variable. We evaluate the different approaches on the joint object detection and pose estimation task of the Pascal3D+ benchmark using Average Viewpoint Precision. We show that a classification approach on discretized viewpoints achieves state-of-the-art performance for joint object detection and pose estimation, and significantly outperforms existing baselines on this benchmark. We also show that performing the two tasks jointly can improve significantly the detection performances.", "target": "A COMPARATIVE STUDY"}
{"id": "task1540-e9de0342a48446bcbcb236a9540859f9", "input": "Machine learning is used in a number of security related applications such as biometric user authentication, speaker identification etc. A type of causative integrity attack against machine le arning called Poisoning attack works by injecting specially crafted data points in the training data so as to increase the false positive rate of the classifier. In the context of the biometric authentication, this means that more intruders will be classified as valid user, and in case of speaker identification system, user A will be classified user B. In this paper, we examine poisoning attack against SVM and introduce Curie a method to protect the SVM classifier from the poisoning attack. The basic idea of our method is to identify the poisoned data points injected by the adversary and filter them out. Our method is light weight and can be easily integrated into existing systems. Experimental results show that it works very well in filtering out the poisoned data.", "target": "Curie: A method for protecting SVM Classifier from Poisoning Attack"}
{"id": "task1540-936dd94bd37641318bfbf49e9a7a914e", "input": "Motivated by applications in domains such as social networks and computational biology, we study the problem of community recovery in graphs with locality. In this problem, pairwise noisy measurements of whether two nodes are in the same community or different communities come mainly or exclusively from nearby nodes rather than uniformly sampled between all node pairs, as in most existing models. We present two algorithms that run nearly linearly in the number of measurements and which achieve the information limits for exact recovery.", "target": "Community Recovery in Graphs with Locality"}
{"id": "task1540-f1bf38e0711d41899e2e0db0132c1727", "input": "Classification is widely used technique in the data mining domain, where scalability and efficiency are the immediate problems in classification algorithms for large databases. We suggest improvements to the existing C4.5 decision tree algorithm. In this paper attribute oriented induction (AOI) and relevance analysis are incorporated with concept hierarchy\u201fs knowledge and HeightBalancePriority algorithm for construction of decision tree along with Multi level mining. The assignment of priorities to attributes is done by evaluating information entropy, at different levels of abstraction for building decision tree using HeightBalancePriority algorithm. Modified DMQL queries are used to understand and explore the shortcomings of the decision trees generated by C4.5 classifier for education dataset and the results are compared with the proposed approach.", "target": "EXTRACTING USEFUL RULES THROUGH IMPROVED DECISION TREE INDUCTION USING INFORMATION ENTROPY"}
{"id": "task1540-94788841ecce409dbd8ceeb740e7b455", "input": "Variational Autoencoders (VAEs) are expressive latent variable models that can be used to learn complex probability distributions from training data. However, the quality of the resulting model crucially relies on the expressiveness of the inference model used during training. We introduce Adversarial Variational Bayes (AVB), a technique for training Variational Autoencoders with arbitrarily expressive inference models. We achieve this by introducing an auxiliary discriminative network that allows to rephrase the maximum-likelihood-problem as a two-player game, hence establishing a principled connection between VAEs and Generative Adversarial Networks (GANs). We show that in the nonparametric limit our method yields an exact maximumlikelihood assignment for the parameters of the generative model, as well as the exact posterior distribution over the latent variables given an observation. Contrary to competing approaches which combine VAEs with GANs, our approach has a clear theoretical justification, retains most advantages of standard Variational Autoencoders and is easy to implement.", "target": "Adversarial Variational Bayes: Unifying Variational Autoencoders and Generative Adversarial Networks"}
{"id": "task1540-b41daf144df146b38ef62a5db8130f73", "input": "As the education fees are becoming more expensive, more students apply for scholarships. Consequently, hundreds and even thousands of applications need to be handled by the sponsor. To solve the problems, some alternatives based on several attributes (criteria) need to be selected. In order to make a decision on such fuzzy problems, Fuzzy Multiple Attribute Decision Making (FMDAM) can be applied. In this study, Unified Modeling Language (UML) in FMADM with TOPSIS and Weighted Product (WP) methods is applied to select the candidates for academic and non-academic scholarships at Universitas Islam Negeri Sunan Kalijaga. Data used were a crisp and fuzzy data. The results show that TOPSIS and Weighted Product FMADM methods can be used to select the most suitable candidates to receive the scholarships since the preference values applied in this method can show applicants with the highest eligibility. Keyword: Fuzzy Multiple Attribute Decision Making, TOPSIS, Weighted Product, Scholarship", "target": "A Fuzzy Topsis Multiple-Attribute Decision Making for Scholarship Selection"}
{"id": "task1540-7830866d1abf423294e685bd6816a61c", "input": "Levels are a key component of many different video games, and a large body of work has been produced on how to procedurally generate game levels. Recently, Machine Learning techniques have been applied to video game level generation towards the purpose of automatically generating levels that have the properties of the training corpus. Towards that end we have made available a corpora of video game levels in an easy to parse format ideal for different machine learning and other game AI research purposes.", "target": "The VGLC: The Video Game Level Corpus"}
{"id": "task1540-27a04a7cb5314e839e7f1422c447817a", "input": "In this paper we extend the classical notion of strong and weak backdoor sets by allowing that different instantiations of the backdoor variables result in instances that belong to different base classes; the union of the base classes forms a heterogeneous base class. Backdoor sets to heterogeneous base classes can be much smaller than backdoor sets to homogeneous ones, hence they are much more desirable but possibly harder to find. We draw a detailed complexity landscape for the problem of detecting strong and weak backdoor sets into heterogeneous base classes for SAT", "target": "Backdoors into Heterogeneous Classes of SAT and CSP"}
{"id": "task1540-5ee63a5669a6414cbb0000e0d9fbe614", "input": "Recent work on word embeddings has shown that simple vector subtraction over pre-trained embeddings is surprisingly effective at capturing different lexical relations, despite lacking explicit supervision. Prior work has evaluated this intriguing result using a word analogy prediction formulation and hand-selected relations, but the generality of the finding over a broader range of lexical relation types and different learning settings has not been evaluated. In this paper, we carry out such an evaluation in two learning settings: (1) spectral clustering to induce word relations, and (2) supervised learning to classify vector differences into relation types. We find that word embeddings capture a surprising amount of information, and that, under suitable supervised training, vector subtraction generalises well to a broad range of relations, including over unseen lexical items.", "target": "Take and Took, Gaggle and Goose, Book and Read: Evaluating the Utility of Vector Differences for Lexical Relation Learning"}
{"id": "task1540-b9eb222b462f476f9a3de3286c4042ce", "input": "The Augmented Lagragian Method (ALM) and Alternating Direction Method of Multiplier (ADMM) have been powerful optimization methods for general convex programming subject to linear constraint. We consider the convex problem whose objective consists of a smooth part and a nonsmooth but simple part. We propose the Fast Proximal Augmented Lagragian Method (Fast PALM) which achieves the convergence rate O(1/K), compared with O(1/K) by the traditional PALM. In order to further reduce the per-iteration complexity and handle the multi-blocks problem, we propose the Fast Proximal ADMM with Parallel Splitting (Fast PL-ADMM-PS) method. It also partially improves the rate related to the smooth part of the objective function. Experimental results on both synthesized and real world data demonstrate that our fast methods significantly improve the previous PALM and ADMM. Introduction This work aims to solve the following linearly constrained separable convex problem with n blocks of variables min x1,\u00b7\u00b7\u00b7 ,xn f(x) = n \u2211", "target": "Fast Proximal Linearized Alternating Direction Method of Multiplier with Parallel Splitting"}
{"id": "task1540-2dd65e9d6dca4d65901ae14b8c35a17c", "input": "A fundamental problem in control is to learn a model of a system from observations that is useful for controller synthesis. To provide good performance guarantees, existing methods must assume that the real system is in the class of models considered during learning. We present an iterative method with strong guarantees even in the agnostic case where the system is not in the class. In particular, we show that any no-regret online learning algorithm can be used to obtain a nearoptimal policy, provided some model achieves low training error and access to a good exploration distribution. Our approach applies to both discrete and continuous domains. We demonstrate its efficacy and scalability on a challenging helicopter domain from the literature.", "target": "Agnostic System Identification for Model-Based Reinforcement Learning"}
{"id": "task1540-7bf0c5a7636d4470bbc4ace10dd2d211", "input": "A logic is defined that allows to express in\u00ad formation about statistical probabilities and about degrees of belief in specific proposi\u00ad tions. By interpreting the two types of proba\u00ad bilities in one common probability space, the semantics given are well suited to model the influence of statistical information on the for\u00ad mation of subjective beliefs. Cross entropy minimization is a key element in these se\u00ad mantics, the use of which is justified by show\u00ad ing that the resulting logic exhibits some very reasonable properties.", "target": "A Logic for Default Reasoning About Probabilities"}
{"id": "task1540-9b277def86cc48bfb46560fd9c543dc1", "input": "We present a practical, differentially private algorithm for answering a large number of queries on high dimensional datasets. Like all algorithms for this task, ours necessarily has worst-case complexity exponential in the dimension of the data. However, our algorithm packages the computationally hard step into a concisely defined integer program, which can be solved non-privately using standard solvers. We prove accuracy and privacy theorems for our algorithm, and then demonstrate experimentally that our algorithm performs well in practice. For example, our algorithm can efficiently and accurately answer millions of queries on the Netflix dataset, which has over 17,000 attributes; this is an improvement on the state of the art by multiple orders of magnitude.", "target": "Dual Query: Practical Private Query Release for High Dimensional Data"}
{"id": "task1540-90027342ce504b558ffd97f3328fdee7", "input": "In this paper, we attempt to solve the problem of Prepositional Phrase (PP) attachments in English. The motivation for the work comes from NLP applications like Machine Translation, for which, getting the correct attachment of prepositions is very crucial. The idea is to correct the PPattachments for a sentence with the help of alignments from parallel data in another language. The novelty of our work lies in the formulation of the problem into a dual decomposition based algorithm that enforces agreement between the parse trees from two languages as a constraint. Experiments were performed on the EnglishHindi language pair and the performance improved by 10% over the baseline, where the baseline is the attachment predicted by the MSTParser model trained for English.", "target": "Prepositional Attachment Disambiguation Using Bilingual Parsing and Alignments"}
{"id": "task1540-ecf7190fdae0430685ced3288fab2f1d", "input": "This paper proposes a new optimization algorithm called Entropy-SGD for training deep neural networks that is motivated by the local geometry of the energy landscape. Local extrema with low generalization error have a large proportion of almost-zero eigenvalues in the Hessian with very few positive or negative eigenvalues. We leverage upon this observation to construct a local-entropy-based objective function that favors well-generalizable solutions lying in large flat regions of the energy landscape, while avoiding poorly-generalizable solutions located in the sharp valleys. Conceptually, our algorithm resembles two nested loops of SGD where we use Langevin dynamics in the inner loop to compute the gradient of the local entropy before each update of the weights. We show that the new objective has a smoother energy landscape and show improved generalization over SGD using uniform stability, under certain assumptions. Our experiments on convolutional and recurrent neural networks demonstrate that Entropy-SGD compares favorably to state-of-the-art techniques in terms of generalization error and training time.", "target": "ENTROPY-SGD: BIASING GRADIENT DESCENT INTO WIDE VALLEYS"}
{"id": "task1540-b2f4bdba99324ecb8efec005128180af", "input": "Language learning is thought to be a highly complex process. One of the hurdles in learning a language is to learn the rules of syntax of the language. Rules of syntax are often ordered in that before one rule can applied one must apply another. It has been thought that to learn the order of n rules one must go through all n! permutations. Thus to learn the order of 27 rules would require 27! steps or 1.08889x10 steps. This number is much greater than the number of seconds since the beginning of the universe! In an insightful analysis the linguist Block ([Block 86], pp. 62-63, p.238) showed that with the assumption of transitivity this vast number of learning steps reduces to a mere 377 steps. We present a mathematical analysis of the complexity of Block\u2019s algorithm. The algorithm has a complexity of order n given n rules. In addition, we improve Block\u2019s results exponentially, by introducing an algorithm that has complexity of order less than n log n.", "target": "On the complexity of learning a language: An improvement of Block\u2019s algorithm"}
{"id": "task1540-3101da6051f74ce9870dca95c537f78a", "input": "The recently introduced Deep Q-Networks (DQN) algorithm has gained attention as one of the first successful combinations of deep neural networks and reinforcement learning. Its promise was demonstrated in the Arcade Learning Environment (ALE), a challenging framework composed of dozens of Atari 2600 games used to evaluate general competency in AI. It achieved dramatically better results than earlier approaches, showing that its ability to learn good representations is quite robust and general. This paper attempts to understand the principles that underly DQN\u2019s impressive performance and to better contextualize its success. We systematically evaluate the importance of key representational biases encoded by DQN\u2019s network by proposing simple linear representations that make use of these concepts. Incorporating these characteristics, we obtain a computationally practical feature set that achieves competitive performance to DQN in the ALE. Besides offering insight into the strengths and weaknesses of DQN, we provide a generic representation for the ALE, significantly reducing the burden of learning a representation for each game. Moreover, we also provide a simple, reproducible benchmark for the sake of comparison to future work in the ALE.", "target": "State of the Art Control of Atari Games Using Shallow Reinforcement Learning"}
{"id": "task1540-993649c658be4ae08c368be85a454c21", "input": "We present a general theoretical analysis of structured prediction. By introducing a new complexity measure that explicitly factors in the structure of the output space and the loss function, we are able to derive new data-dependent learning guarantees for a broad family of losses and for hypothesis sets with an arbitrary factor graph decomposition. We extend this theory by leveraging the principle of Voted Risk Minimization (VRM) and showing that learning is possible with complex factor graphs. We both present new learning bounds in this advanced setting as well as derive two new families of algorithms, Voted Conditional Random Fields and Voted Structured Boosting, which can make use of very complex features and factor graphs without overfitting. Finally, we also validate our theory through experiments on several datasets.", "target": "Structured Prediction Theory Based on Factor Graph Complexity"}
{"id": "task1540-b6a09d61b5654ba3aa6559bcbe1d5c05", "input": "Accurate software development effort estimation is critical to the success of software projects. Although many techniques and algorithmic models have been developed and implemented by practitioners, accurate software development effort prediction is still a challenging endeavor in the field of software engineering, especially in handling uncertain and imprecise inputs and collinear characteristics. In this paper, a hybrid intelligent model combining a neural network model integrated with fuzzy model (neuro-fuzzy model) has been used to improve the accuracy of estimating software cost. The performance of the proposed model is assessed by designing and conducting evaluation with published project and industrial data. Results have shown that the proposed model demonstrates the ability of improving the estimation accuracy by 18% based on the Mean Magnitude of Relative Error (MMRE) criterion.", "target": "A HYBRID INTELLIGENT MODEL FOR SOFTWARE COST ESTIMATION"}
{"id": "task1540-10145db1ec014a7fa741a5d8209ff65c", "input": "We propose a probabilistic framework for domain adaptation that blends both generative and discriminative modeling in a principled way. By maximizing both the marginal and the conditional log-likelihoods, models derived from this framework can use both labeled instances from the source domain as well as unlabeled instances from both source and target domains. Under this framework, we show that the popular reconstruction loss of autoencoder corresponds to an upper bound of the negative marginal log-likelihoods of unlabeled instances, where marginal distributions are given by proper kernel density estimations. This provides a way to interpret the empirical success of autoencoders in domain adaptation and semi-supervised learning. We instantiate our framework using neural networks, and build a concrete model, DAuto. Empirically, we demonstrate the effectiveness of DAuto on text, image and speech datasets, showing that it outperforms related competitors when domain adaptation is possible.", "target": "Principled Hybrids of Generative and Discriminative Domain Adaptation"}
{"id": "task1540-e82a888c82f345f39348c03238e33db6", "input": "Screening rules allow to early discard irrelevant variables from the optimization in Lasso problems, or its derivatives, making solvers faster. In this paper, we propose new versions of the socalled safe rules for the Lasso. Based on duality gap considerations, our new rules create safe test regions whose diameters converge to zero, provided that one relies on a converging solver. This property helps screening out more variables, for a wider range of regularization parameter values. In addition to faster convergence, we prove that we correctly identify the active sets (supports) of the solutions in finite time. While our proposed strategy can cope with any solver, its performance is demonstrated using a coordinate descent algorithm particularly adapted to machine learning use cases. Significant computing time reductions are obtained with respect to previous safe rules.", "target": "Mind the duality gap: safer rules for the Lasso"}
{"id": "task1540-da64b9af2ba049b8b402d47d96af4950", "input": "We present a suite of algorithms for Dimension Independent Similarity Computation (DISCO) to compute all pairwise similarities between very high dimensional sparse vectors. All of our results are provably independent of dimension, meaning apart from the initial cost of trivially reading in the data, all subsequent operations are independent of the dimension, thus the dimension can be very large. We study Cosine, Dice, Overlap, Conditional, and the Jaccard similarity measures. For Jaccard similiarity we include an improved version of MinHash. Our results are geared toward the MapReduce framework. We empirically validate our theorems at large scale using data from the social networking site Twitter.", "target": "Dimension Independent Similarity Computation"}
{"id": "task1540-96913c801efa4cd59257b70d0bd32783", "input": "We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task involving finding rewards in random 3D mazes using a visual input.", "target": "Asynchronous Methods for Deep Reinforcement Learning"}
{"id": "task1540-fc92d8e150bc4815a7c027f28e1a7cda", "input": "In this paper, we present a conversational model that incorporates both context and participant role for two-party conversations. Different architectures are explored for integrating participant role and context information into a Long Short-term Memory (LSTM) language model. The conversational model can function as a language model or a language generation model. Experiments on the Ubuntu Dialog Corpus show that our model can capture multiple turn interaction between participants. The proposed method outperforms a traditional LSTM model as measured by language model perplexity and response ranking. Generated responses show characteristic differences between the two participant roles.", "target": "LSTM based Conversation Models"}
{"id": "task1540-c8407ec657c64e6b846149173d0cea46", "input": "Present incremental learning methods are limited in the ability to achieve reliable credit assignment over a large number time steps (or events). However, this situation is typical for cases where the dynamical system to be controlled requires relatively frequent control updates in order to maintain stability or robustness yet has some action/consequences which must be established over relatively long periods of time. To address this problem, the learning capabilities of a control architecture comprised of two Backpropagated Adaptive Critics (BAC\u2019s) in a two-level hierarchy with continuous actions are explored. The high-level BAC updates less frequently than the low-level BAC and controls the latter to some degree. The response of the low-level to high-level signals can either be determined a priori or it can emerge during learning. A general approach called Response Induction Learning is introduced to address the latter case.", "target": "Reinforcement Control with Hierarchical Backpropagated Adaptive Critics\u2217"}
{"id": "task1540-08122e4cee834888a22dd386b6f4ffd2", "input": "We assess the performance of generic text summarization algorithms applied to films and documentaries, using extracts from news articles produced by reference models of extractive summarization. We use three datasets: (i) news articles, (ii) film scripts and subtitles, and (iii) documentary subtitles. Standard ROUGE metrics are used for comparing generated summaries against news abstracts, plot summaries, and synopses. We show that the best performing algorithms are LSA, for news articles and documentaries, and LexRank and Support Sets, for films. Despite the different nature of films and documentaries, their relative behavior is in accordance with that obtained for news articles. c \u00a9 2016 Elsevier Ltd. All rights reserved.", "target": "Summarization of Films and Documentaries Based on Subtitles and Scripts"}
{"id": "task1540-4837ade0b9aa4915be70ac40056a8afb", "input": "In this paper, we introduce a new distributional method for modeling predicateargument thematic fit judgments. We use a syntax-based DSM to build a prototypical representation of verb-specific roles: for every verb, we extract the most salient second order contexts for each of its roles (i.e. the most salient dimensions of typical role fillers), and then we compute thematic fit as a weighted overlap between the top features of candidate fillers and role prototypes. Our experiments show that our method consistently outperforms a baseline re-implementing a state-of-theart system, and achieves better or comparable results to those reported in the literature for the other unsupervised systems. Moreover, it provides an explicit representation of the features characterizing verbspecific semantic roles.", "target": "Measuring Thematic Fit with Distributional Feature Overlap"}
{"id": "task1540-2964ed3d9d034fcbbd5636caa26b55ff", "input": "In many applications spanning from sensor to social networks, transportation systems, gene regulatory networks or big data, the signals of interest are defined over the vertices of a graph. The aim of this paper is to propose a least mean square (LMS) strategy for adaptive estimation of signals defined over graphs. Assuming the graph signal to be band-limited, over a known bandwidth, the method enables reconstruction, with guaranteed performance in terms of mean-square error, and tracking from a limited number of observations over a subset of vertices. A detailed mean square analysis provides the performance of the proposed method, and leads to several insights for designing useful sampling strategies for graph signals. Numerical results validate our theoretical findings, and illustrate the performance of the proposed method. Furthermore, to cope with the case where the bandwidth is not known beforehand, we propose a method that performs a sparse online estimation of the signal support in the (graph) frequency domain, which enables online adaptation of the graph sampling strategy. Finally, we apply the proposed method to build the power spatial density cartography of a given operational region in a cognitive network environment.", "target": "Least Mean Squares Estimation of Graph Signals"}
{"id": "task1540-0b7c463c57374d85b6ad809874926737", "input": "We consider the task of aggregating beliefs of sev\u00ad eral experts. We assume that these beliefs are rep\u00ad resented as probability distributions. We argue that the evaluation of any aggregation technique depends on the semantic context of this task. We propose a framework, in which we assume that nature generates samples from a 'true' distribution and different experts form their beliefs based on the subsets of the data they have a chance to observe. Naturally, the optimal ag\u00ad gregate distribution would be the one learned from the combined sample sets. Such a formulation leads to a natural way to measure the accuracy of the aggregation mechanism. We show that the well-known aggregation operator LinOP is ideally suited for that task. We propose a LinOP-based learning algorithm, inspired by the techniques developed for Bayesian learning, which aggregates the experts' distributions represented as Bayesian networks. We show experimentally that this algorithm performs well in practice.", "target": "Aggregating Learned Probabilistic Beliefs"}
{"id": "task1540-f70ab5f734f54c6eaf53a2ae26661f30", "input": "ion in Belief Networks: The Role of Intermediate States in Diagnostic Reasoning Gregory Provan* Institute for Decision Systems Research 4984 El Camino Real, Suite 110 Los Altos, CA 94022", "target": "Abstraction in Belief Networks: The Role of Intermediate States in Diagnostic Reasoning"}
{"id": "task1540-5053987bf1704c4a83948be5aaf7778b", "input": "We investigate the use of sparse coding and dictionary learning in the context of multitask and transfer learning. The central assumption of our learning method is that the tasks parameters are well approximated by sparse linear combinations of the atoms of a dictionary on a high or infinite dimensional space. This assumption, together with the large quantity of available data in the multitask and transfer learning settings, allows a principled choice of the dictionary. We provide bounds on the generalization error of this approach, for both settings. Numerical experiments on one synthetic and two real datasets show the advantage of our method over single task learning, a previous method based on orthogonal and dense representation of the tasks and a related method learning task grouping.", "target": "Sparse coding for multitask and transfer learning"}
{"id": "task1540-23c3eb6109fe42589a2ebc548d3abcb3", "input": "Many academic disciplines including information systems, computer science, and operations management face scheduling problems as important decision making tasks. Since many scheduling problems are NP-hard in the strong sense, there is a need for developing solution heuristics. For scheduling problems with setup times on unrelated parallel machines, there is limited research on solution methods and to the best of our knowledge, parallel computer architectures have not yet been taken advantage of. We address this gap by proposing and implementing a new solution heuristic and by testing different parallelization strategies. In our computational experiments, we show that our heuristic calculates near-optimal solutions even for large instances and that computing time can be reduced substantially by our parallelization approach.", "target": "High-Performance Computing for Scheduling Decision Support: A Parallel Depth-First Search Heuristic"}
{"id": "task1540-432689a9cf554c078a36b625d2780a60", "input": "This paper describes several results of Wimmics, a research lab which names stands for: web-instrumented man-machine interactions, communities, and semantics. The approaches introduced here rely on graph-oriented knowledge representation, reasoning and operationalization to model and support actors, actions and interactions in web-based epistemic communities. The research results are applied to support and foster interactions in online communities and manage their resources.", "target": "Challenges in bridging Social Semantics and Formal Semanticson the Web"}
{"id": "task1540-00c83fbf74ce49308faf30d07517e330", "input": "Reinforcement learning tasks are typically specified as Markov decision processes. This formalism has been highly successful, though specifications often couple the dynamics of the environment and the learning objective. This lack of modularity can complicate generalization of the task specification, as well as obfuscate connections between different task settings, such as episodic and continuing. In this work, we introduce the RL task formalism, that provides a unification through simple constructs including a generalization to transition-based discounting. Through a series of examples, we demonstrate the generality and utility of this formalism. Finally, we extend standard learning constructs, including Bellman operators, and extend some seminal theoretical results, including approximation errors bounds. Overall, we provide a well-understood and sound formalism on which to build theoretical results and simplify algorithm use and development.", "target": "Unifying task specification in reinforcement learning"}
{"id": "task1540-bf5b7189d4a64eb983e3131a69c38c82", "input": "In Passive POMDPs actions do not affect the world state, but still incur costs. When the agent is bounded by information-processing constraints, it can only keep an approximation of the belief. We present a variational principle for the problem of maintaining the information which is most useful for minimizing the cost, and introduce an efficient and simple algorithm for finding an optimum.", "target": "Bounded Planning in Passive POMDPs"}
{"id": "task1540-690561439c35445eae904b2a313d3a78", "input": "In this paper we present applications of different machine learning algorithms in aquaculture. Machine learning algorithms learn models from historical data. In aquaculture historical data are obtained from farm practices, yields, and environmental data sources. Associations between these different variables can be obtained by applying machine learning algorithms to historical data. In this paper we present applications of different machine learning algorithms in aquaculture applications.", "target": "Application of Machine Learning Techniques in Aquaculture"}
{"id": "task1540-4595029c820f4d7c9571985c03d0e5d5", "input": "Nowadays, geographic information related to Twitter is crucially important for fine-grained applications. However, the amount of geographic information available on Twitter is low, which makes the pursuit of many applications challenging. Under such circumstances, estimating the location of a tweet is an important goal of the study. Unlike most previous studies that estimate the pre-defined district as the classification task, this study employs a probability distribution to represent richer information of the tweet, not only the location but also its ambiguity. To realize this modeling, we propose the convolutional mixture density network (CMDN), which uses text data to estimate the mixture model parameters. Experimentally obtained results reveal that CMDN achieved the highest prediction performance among the method for predicting the exact coordinates. It also provides a quantitative representation of the location ambiguity for each tweet that properly works for extracting the reliable location estimations.", "target": "Density Estimation for Geolocation via Convolutional Mixture Density Network"}
{"id": "task1540-bf34f96f7eee499eaf35dbf9c87752f0", "input": "We describe efforts towards getting better resources for EnglishArabic machine translation of spoken text. In particular, we look at movie subtitles as a unique, rich resource, as subtitles in one language often get translated into other languages. Movie subtitles are not new as a resource and have been explored in previous research; however, here we create a much larger bi-text (the biggest to date), and we further generate better quality alignment for it. Given the subtitles for the same movie in different languages, a key problem is how to align them at the fragment level. Typically, this is done using length-based alignment, but for movie subtitles, there is also time information. Here we exploit this information to develop an original algorithm that outperforms the current best subtitle alignment tool, subalign. The evaluation results show that adding our bi-text to the IWSLT training bi-text yields an improvement of over two BLEU points absolute.", "target": "Bi-Text Alignment of Movie Subtitles for Spoken English-Arabic Statistical Machine Translation"}
{"id": "task1540-993bbc102b944b9995f23892b8920603", "input": "We present some arguments why existing methods for representing agents fall short in applications crucial to artificial life. Using a thought experiment involving a fictitious dynamical systems model of the biosphere we argue that the metabolism, motility, and the concept of counterfactual variation should be compatible with any agent representation in dynamical systems. We then propose an informationtheoretic notion of integrated spatiotemporal patterns which we believe can serve as the basic building block of an agent definition. We argue that these patterns are capable of solving the problems mentioned before. We also test this in some preliminary experiments.", "target": "Towards information based spatiotemporal patterns as a foundation for agent representation in dynamical systems"}
{"id": "task1540-07f120ec83ad47f7bb44c5b8684b52b5", "input": "We address the problem of integrating textual and visual information in vector space models for word meaning representation. We first present the Residual CCA (R-CCA) method, that complements the standard CCA method by representing, for each modality, the difference between the original signal and the signal projected to the shared, max correlation, space. We then show that constructing visual and textual representations and then post-processing them through composition of common modeling motifs such as PCA, CCA, R-CCA and linear interpolation (a.k.a sequential modeling) yields high quality models. On five standard semantic benchmarks our sequential models outperform recent multimodal representation learning alternatives, including ones that rely on joint representation learning. For two of these benchmarks our R-CCA method is part of the Best configuration our algorithm yields.", "target": "Effective Combination of Language and Vision Through Model Composition and the R-CCA Method"}
{"id": "task1540-428e549df6a0429494ce40b5a00aa32f", "input": "Hand-crafted features based on linguistic and domain-knowledge play crucial role in determining the performance of disease name recognition systems. Such methods are further limited by the scope of these features or in other words, their ability to cover the contexts or word dependencies within a sentence. In this work, we focus on reducing such dependencies and propose a domain-invariant framework for the disease name recognition task. In particular, we propose various end-to-end recurrent neural network (RNN) models for the tasks of disease name recognition and their classification into four pre-defined categories. We also utilize convolution neural network (CNN) in cascade of RNN to get character-based embedded features and employ it with word-embedded features in our model. We compare our models with the state-of-the-art results for the two tasks on NCBI disease dataset. Our results for the disease mention recognition task indicate that state-of-the-art performance can be obtained without relying on feature engineering. Further the proposed models obtained improved performance on the classification task of disease names.", "target": "Recurrent neural network models for disease name recognition using domain invariant features"}
{"id": "task1540-e815c5200f794815868bd3c4b87cd21d", "input": "Machine transliteration is the process of automatically transforming the script of a word from a source language to a target language, while preserving pronunciation. Sequence to sequence learning has recently emerged as a new paradigm in supervised learning. In this paper a character-based encoder-decoder model has been proposed that consists of two Recurrent Neural Networks. The encoder is a Bidirectional recurrent neural network that encodes a sequence of symbols into a fixed-length vector representation, and the decoder generates the target sequence using an attention-based recurrent neural network. The encoder, the decoder and the attention mechanism are jointly trained to maximize the conditional probability of a target sequence given a source sequence. Our experiments on different datasets show that the proposed encoderdecoder model is able to achieve significantly higher transliteration quality over traditional statistical models.", "target": "Neural Machine Transliteration: Preliminary Results"}
{"id": "task1540-a251c991aee64f818e3e67dec8329fa6", "input": "We propose a new framework for single-channel source separation that lies between the fully supervised and unsupervised setting. Instead of supervision, we provide input features for each source signal and use convex methods to estimate the correlations between these features and the unobserved signal decomposition. We analyze the case of `2 loss theoretically and show that recovery of the signal components depends only on cross-correlation between features for different signals, not on correlations between features for the same signal. Contextually supervised source separation is a natural fit for domains with large amounts of data but no explicit supervision; our motivating application is energy disaggregation of hourly smart meter data (the separation of whole-home power signals into different energy uses). Here we apply contextual supervision to disaggregate the energy usage of thousands homes over four years, a significantly larger scale than previously published efforts, and demonstrate on synthetic data that our method outperforms the unsupervised approach.", "target": "Contextually Supervised Source Separation with Application to Energy Disaggregation"}
{"id": "task1540-5da10dc0c4784dae8b109eaa465d2448", "input": "Graph partitioning, a well studied problem of parallel computing has many applications in diversified fields such as distributed computing, social network analysis, data mining and many other domains. In this paper, we introduce FGPGA, an efficient genetic approach for producing feasible graph partitions. Our method takes into account the heterogeneity and capacity constraints of the partitions to ensure balanced partitioning. Such approach has various applications in mobile cloud computing that include feasible deployment of software applications on the more resourceful infrastructure in the cloud instead of mobile hand set. Our proposed approach is light weight and hence suitable for use in cloud architecture. We ensure feasibility of the partitions generated by not allowing over-sized partitions to be generated during the initialization and search. Our proposed method tested on standard benchmark datasets significantly outperforms the state-of-the-art methods in terms of quality of partitions and feasibility of the solutions.", "target": "FGPGA: An Efficient Genetic Approach for Producing Feasible Graph Partitions"}
{"id": "task1540-0cec2a6261b24d168553d70269abdd3b", "input": "Clinicians are expected to have up-to-date and broad knowledge of disease treatment options for a patient. Online<lb>health knowledge resources contain a wealth of information. However, because of the time investment needed to<lb>disseminate and rank pertinent information, there is a need to summarize the information in a more concise format.<lb>Our aim of the study is to provide clinicians with a concise overview of popular treatments for a given disease using<lb>information automatically computed from Medline abstracts. We analyzed the treatments of two disorders \u2013 Atrial<lb>Fibrillation and Congestive Heart Failure. We calculated the precision, recall, and f-scores of our two ranking<lb>methods to measure the accuracy of the results. For Atrial Fibrillation disorder, maximum f-score for the New<lb>Treatments weighing method is 0.611, which occurs at 60 treatments. For Congestive Heart Failure disorder,<lb>maximum f-score for the New Treatments weighing method is 0.503, which occurs at 80 treatments.", "target": "Automatically extracting, ranking and visually summarizing the treatments for a disease"}
{"id": "task1540-8771acd885394ca8be2336458182a282", "input": "Realizability for knowledge representation formalisms studies the following question: Given a semantics and a set of interpretations, is there a knowledge base whose semantics coincides exactly with the given interpretation set? We introduce a general framework for analyzing realizability in abstract dialectical frameworks (ADFs) and various of its subclasses. In particular, the framework applies to Dung argumentation frameworks, SETAFs by Nielsen and Parsons, and bipolar ADFs. We present a uniform characterization method for the admissible, complete, preferred and model/stable semantics. We employ this method to devise an algorithm that decides realizability for the mentioned formalisms and semantics; moreover the algorithm allows for constructing a desired knowledge base whenever one exists. The algorithm is built in a modular way and thus easily extensible to new formalisms and semantics. We have also implemented our approach in answer set programming, and used the implementation to obtain several novel results on the relative expressiveness of the abovementioned formalisms.", "target": "Characterizing Realizability in Abstract Argumentation"}
{"id": "task1540-d67f09fffa934b0f902048c0229e4867", "input": "We propose a technique to augment network layers by adding a linear gating mechanism, which provides a way to learn identity mappings by optimizing only one parameter. We also introduce a new metric which served as basis for the technique. It captures the difficulty involved in learning identity mappings for different types of network models, and provides a new theoretical intuition for the increased depths of models such as Highway and Residual Networks. We propose a new model, the Gated Residual Network, which is the result when augmenting Residual Networks. Experimental results show that augmenting layers grants increased performance, less issues with depth, and more layer independence \u2013 fully removing them does not cripple the model. We evaluate our method on MNIST using fully-connected networks and on CIFAR-10 using Wide ResNets, achieving a relative error reduction of more than 8% in the latter when compared to the original model.", "target": "LEARNING IDENTITY MAPPINGS WITH RESIDUAL GATES"}
{"id": "task1540-5f70a4964b8d4d0288e0a501ce92f9e6", "input": "Multimedia or spoken content presents more attractive information than plain text content, but the former is more difficult to display on a screen and be selected by a user. As a result, accessing large collections of the former is much more difficult and time-consuming than the latter for humans. It\u2019s therefore highly attractive to develop machines which can automatically understand spoken content and summarize the key information for humans to browse over. In this endeavor, a new task of machine comprehension of spoken content was proposed recently. The initial goal was defined as the listening comprehension test of TOEFL, a challenging academic English examination for English learners whose native languages are not English. An Attention-based Multi-hop Recurrent Neural Network (AMRNN) architecture was also proposed for this task, which considered only the sequential relationship within the speech utterances. In this paper, we propose a new Hierarchical Attention Model (HAM), which constructs multi-hopped attention mechanism over tree-structured rather than sequential representations for the utterances. Improved comprehension performance robust with respect to ASR errors were obtained.", "target": "HIERARCHICAL ATTENTION MODEL FOR IMPROVED MACHINE COMPREHENSION OF SPOKEN CONTENT"}
{"id": "task1540-7b52bc54d7f042caa5d1bbf24bd54619", "input": "We propose a method combining relational-logic representations with neural network learning. A general lifted architecture, possibly reflecting some background domain knowledge, is described through relational rules which may be handcrafted or learned. The relational rule-set serves as a template for unfolding possibly deep neural networks whose structures also reflect the structures of given training or testing relational examples. Different networks corresponding to different examples share their weights, which co-evolve during training by stochastic gradient descent algorithm. The framework allows for hierarchical relational modeling constructs and learning of latent relational concepts through shared hidden layers weights corresponding to the rules. Discovery of notable relational concepts and experiments on 78 relational learning benchmarks demonstrate favorable performance of the method.", "target": "Lifted Relational Neural Networks"}
{"id": "task1540-fc55f1e3220f480b8cad5df4a8022689", "input": "We present the first parser for UCCA, a cross-linguistically applicable framework for semantic representation, which builds on extensive typological work and supports rapid annotation. UCCA poses a challenge for existing parsing techniques, as it exhibits reentrancy (resulting in DAG structures), discontinuous structures and non-terminal nodes corresponding to complex semantic units. To our knowledge, the conjunction of these formal properties is not supported by any existing parser. Our transition-based parser, which uses a novel transition set and features based on bidirectional LSTMs, has value not just for UCCA parsing: its ability to handle more general graph structures can inform the development of parsers for other semantic DAG structures, and in languages that frequently use discontinuous structures.", "target": "A Transition-Based Directed Acyclic Graph Parser for UCCA"}
{"id": "task1540-ece477130c94453a8371ddccde29347e", "input": "Natural language inference (NLI) is a fundamentally important task in natural language processing that has many applications. The recently released Stanford Natural Language Inference (SNLI) corpus has made it possible to develop and evaluate learning-centered methods such as deep neural networks for the NLI task. In this paper, we propose a special long short-term memory (LSTM) architecture for NLI. Our model builds on top of a recently proposed neutral attention model for NLI but is based on a significantly different idea. Instead of deriving sentence embeddings for the premise and the hypothesis to be used for classification, our solution uses a matching-LSTM that performs word-by-word matching of the hypothesis with the premise. This LSTM is able to place more emphasis on important word-level matching results. In particular, we observe that this LSTM remembers important mismatches that are critical for predicting the contradiction or the neutral relationship label. Our experiments on the SNLI corpus show that our model outperforms the state of the art, achieving an accuracy of 86.1% on the test data.", "target": "Learning Natural Language Inference with LSTM"}
{"id": "task1540-ed3e0cb3bfbd458ebe2da3d5b81f4262", "input": "Distributional word representation methods exploit word co-occurrences to build compact vector encodings of words. While these representations enjoy widespread use in modern natural language processing, it is unclear whether they accurately encode all necessary facets of conceptual meaning. In this paper, we evaluate how well these representations can predict perceptual and conceptual features of concrete concepts, drawing on two semantic norm datasets sourced from human participants. We find that several standard word representations fail to encode many salient perceptual features of concepts, and show that these deficits correlate with word-word similarity prediction errors. Our analyses provide motivation for grounded and embodied language learning approaches, which may help to remedy these deficits.", "target": "Are distributional representations ready for the real world? Evaluating word vectors for grounded perceptual meaning"}
{"id": "task1540-687259fef8434f9f85ab4a0007bbd01f", "input": "For most reinforcement learning approaches, the learning is performed by maximizing an accumulative reward that is expectedly and manually defined for specific tasks. However, in real world, rewards are emergent phenomena from the complex interactions between agents and environments. In this paper, we propose an implicit generic reward model for reinforcement learning. Unlike those rewards that are manually defined for specific tasks, such implicit reward is task independent. It only comes from the deviation from the agents\u2019 previous experiences.", "target": "Experience enrichment based task independent reward model"}
{"id": "task1540-331665ed7bf54e829f9fbab78b4d0876", "input": "We introduce a globally normalized transition-based neural network model that achieves state-of-the-art part-ofspeech tagging, dependency parsing and sentence compression results. Our model is a simple feed-forward neural network that operates on a task-specific transition system, yet achieves comparable or better accuracies than recurrent models. The key insight is based on a novel proof illustrating the label bias problem and showing that globally normalized models can be strictly more expressive than locally normalized models.", "target": "Globally Normalized Transition-Based Neural Networks"}
{"id": "task1540-34592f07058f4bc39f41962832bfdf0d", "input": "We study properties of particular non-redundant sets of if-then rules describing dependencies between graded attributes. We introduce notions of saturation and witnessed non-redundancy of sets of graded attribute implications are show that bases of graded attribute implications given by systems of pseudo-intents correspond to non-redundant sets of graded attribute implications with saturated consequents where the nonredundancy is witnessed by antecedents of the contained graded attribute implications. We introduce an algorithm which transforms any complete set of graded attribute implications parameterized by globalization into a base given by pseudo-intents. Experimental evaluation is provided to compare the method of obtaining bases for general parameterizations by hedges with earlier graph-based approaches.", "target": "On sets of graded attribute implications with witnessed non-redundancy"}
{"id": "task1540-282d6fa4fac4408a9d8612a8487177f9", "input": "In this paper, we propose and investigate a novel memory architecture for neural networks called Hierarchical Attentive Memory (HAM). It is based on a binary tree with leaves corresponding to memory cells. This allows HAM to perform memory access in \u0398(logn) complexity, which is a significant improvement over the standard attention mechanism that requires \u0398(n) operations, where n is the size of the memory. We show that an LSTM network augmented with HAM can learn algorithms for problems like merging, sorting or binary searching from pure input-output examples. In particular, it learns to sort n numbers in time \u0398(n logn) and generalizes well to input sequences much longer than the ones seen during the training. We also show that HAM can be trained to act like classic data structures: a stack, a FIFO queue and a priority queue.", "target": "Learning Efficient Algorithms with Hierarchical Attentive Memory"}
{"id": "task1540-391034527a2b405ba078771a51584571", "input": "Multicriteria decision analysis aims at supporting a person facing a decision problem involving conflicting criteria. We consider an additive utility model which provides robust conclusions based on preferences elicited from the decision maker. The recommendations based on these robust conclusions are even more convincing if they are complemented by explanations. We propose a general scheme, based on sequence of preference swaps, in which explanations can be computed. We show first that the length of explanations can be unbounded in the general case. However, in the case of binary reference scales, this length is bounded and we provide an algorithm to compute the corresponding explanation.", "target": "Explaining robust additive utility models by sequences of preference swaps"}
{"id": "task1540-41fc0b9f7aec479798be5b98dd6f9756", "input": "Recent works on cost based relaxations have improved Constraint Programming (CP) models for the Traveling Salesman Problem (TSP). We provide a short survey over solving asymmetric TSP with CP. Then, we suggest new implied propagators based on general graph properties. We experimentally show that such implied propagators bring robustness to pathological instances and highlight the fact that graph structure can significantly improve search heuristics behavior. Finally, we show that our approach outperforms current state of the art results.", "target": "Improving the Asymmetric TSP by Considering Graph Structure"}
{"id": "task1540-e737b6adc6d049b4b3ac7247e0da0edd", "input": "Deep neural networks currently demonstrate state-of-the-art performance in several domains. At the same time, models of this class are very demanding in terms of computational resources. In particular, a large amount of memory is required by commonly used fully-connected layers, making it hard to use the models on low-end devices and stopping the further increase of the model size. In this paper we convert the dense weight matrices of the fully-connected layers to the Tensor Train [15] format such that the number of parameters is reduced by a huge factor and at the same time the expressive power of the layer is preserved. In particular, for the Very Deep VGG networks [19] we report the compression factor of the dense weight matrix of a fully-connected layer up to 200000 times leading to the compression factor of the whole network up to 7 times.", "target": "Tensorizing Neural Networks"}
{"id": "task1540-91f1d6bb74c744d087dec4264c32f596", "input": "This position paper advocates a communicationsinspired approach to the design of machine learning systems on energy-constrained embedded \u2018always-on\u2019 platforms. The communicationsinspired approach has two versions 1) a deterministic version where existing low-power communication IC design methods are repurposed, and 2) a stochastic version referred to as Shannon-inspired statistical information processing employing information-based metrics, statistical error compensation (SEC), and retraining-based methods to implement ML systems on stochastic circuit/device fabrics operating at the limits of energy-efficiency. The communications-inspired approach has the potential to fully leverage the opportunities afforded by ML algorithms and applications in order to address the challenges inherent in their deployment on energy-constrained platforms.", "target": "Energy-efficient Machine Learning in Silicon: A Communications-inspired Approach"}
{"id": "task1540-7ffcc3efe9f74548a8b3a61c5d41bc1b", "input": "The concept of movable evidence masses that flow from supersets to subsets as specified by experts represents a suitable framework for reasoning under uncertainty. The mass flow is controlled by specialization matrices. New evidence is integrated into the frame of discernment by conditioning or revision (Dempster's rule of conditioning), for which special specialization matrices exist. Even some aspects of non-monotonic reasoning can be represented by certain specialization matrices.", "target": "Reasoning with Mass Distributions"}
{"id": "task1540-d06afd1952d34ad2a93e0493d311df0e", "input": "We propose a black-box variational inference method to approximate intractable distributions with an increasingly rich approximating class. Our method, termed variational boosting, iteratively refines an existing variational approximation by solving a sequence of optimization problems, allowing the practitioner to trade computation time for accuracy. We show how to expand the variational approximating class by incorporating additional covariance structure and by introducing new components to form a mixture. We apply variational boosting to synthetic and real statistical models, and show that resulting posterior inferences compare favorably to existing posterior approximation algorithms in both accuracy and efficiency.", "target": "Variational Boosting: Iteratively Refining Posterior Approximations"}
{"id": "task1540-dff70d51e8e34651a216318a2947d06c", "input": "Our goal is to deploy a high-accuracy system starting with zero training examples. We consider an on-the-job setting, where as inputs arrive, we use real-time crowdsourcing to resolve uncertainty where needed and output our prediction when confident. As the model improves over time, the reliance on crowdsourcing queries decreases. We cast our setting as a stochastic game based on Bayesian decision theory, which allows us to balance latency, cost, and accuracy objectives in a principled way. Computing the optimal policy is intractable, so we develop an approximation based on Monte Carlo Tree Search. We tested our approach on three datasets\u2014named-entity recognition, sentiment classification, and image classification. On the NER task we obtained more than an order of magnitude reduction in cost compared to full human annotation, while boosting performance relative to the expert provided labels. We also achieve a 8% F1 improvement over having a single human label the whole set, and a 28% F1 improvement over online learning. \u201cPoor is the pupil who does not surpass his master.\u201d \u2013 Leonardo da Vinci", "target": "On-the-Job Learning with Bayesian Decision Theory"}
{"id": "task1540-cd2d8fdedd3f4f49867eaabc3b86f5fc", "input": "Message-passing algorithms have emerged as powerful techniques for approximate inference in graphical models. When these algorithms converge, they can be shown to find local (or sometimes even global) optima of variational formulations to the inference problem. But many of the most popular algorithms are not guaranteed to converge. This has lead to recent interest in convergent message-passing algorithms. In this paper, we present a unified view of convergent message-passing algorithms. We present a simple derivation of an abstract algorithm, tree-consistency bound optimization (TCBO) that is provably convergent in both its sum and max product forms. We then show that many of the existing convergent algorithms are instances of our TCBO algorithm, and obtain novel convergent algorithms \u201cfor free\u201d by exchanging maximizations and summations in existing algorithms. In particular, we show that Wainwright\u2019s non-convergent sum-product algorithm for tree based variational bounds, is actually convergent with the right update order for the case where trees are monotonic chains.", "target": "Convergent message passing algorithms - a unifying view"}
{"id": "task1540-3d97915dd6cd4a599509e939957982e4", "input": "This paper addresses the problem of detecting coherent motions in crowd scenes and presents its two applications in crowd scene understanding: semantic region detection and recurrent activity mining. It processes input motion fields (e.g., optical flow fields) and produces a coherent motion filed, named as thermal energy field. The thermal energy field is able to capture both motion correlation among particles and the motion trends of individual particles which are helpful to discover coherency among them. We further introduce a two-step clustering process to construct stable semantic regions from the extracted time-varying coherent motions. These semantic regions can be used to recognize pre-defined activities in crowd scenes. Finally, we introduce a cluster-and-merge process which automatically discovers recurrent activities in crowd scenes by clustering and merging the extracted coherent motions. Experiments on various videos demonstrate the effectiveness of our approach.", "target": "A Diffusion and Clustering-based Approach for Finding Coherent Motions and Understanding Crowd Scenes"}
{"id": "task1540-be3b5390bdd74ee1bbbae90a9e6844cb", "input": "Mixability is a property of a loss which characterizes when fast convergence is possible in the game of prediction with expert advice. We show that a key property of mixability generalizes, and the exp and log operations present in the usual theory are not as special as one might have thought. In doing this we introduce a more general notion of \u03a6-mixability where \u03a6 is a general entropy (i.e., any convex function on probabilities). We show how a property shared by the convex dual of any such entropy yields a natural algorithm (the minimizer of a regret bound) which, analogous to the classical aggregating algorithm, is guaranteed a constant regret when used with \u03a6-mixable losses. We characterize precisely which \u03a6 have \u03a6-mixable losses and put forward a number of conjectures about the optimality and relationships between different choices of entropy.", "target": "Generalized Mixability via Entropic Duality"}
{"id": "task1540-bfae07d767ce4f6284ebe5649f79256a", "input": "The field of speech recognition is in the midst of a paradigm shift: end-to-end neural networks are challenging the dominance of hidden Markov models as a core technology. Using an attention mechanism in a recurrent encoder-decoder architecture solves the dynamic time alignment problem, allowing joint end-to-end training of the acoustic and language modeling components. In this paper we extend the end-to-end framework to encompass microphone array signal processing for noise suppression and speech enhancement within the acoustic encoding network. This allows the beamforming components to be optimized jointly within the recognition architecture to improve the end-to-end speech recognition objective. Experiments on the noisy speech benchmarks (CHiME-4 and AMI) show that our multichannel end-to-end system outperformed the attention-based baseline with input from a conventional adaptive beamformer.", "target": "Multichannel End-to-end Speech Recognition "}
{"id": "task1540-df921296060540fa8eb285a40b314b4f", "input": "In this paper we present the greedy step averaging(GSA) method, a parameter-free stochastic optimization algorithm for a variety of machine learning problems. As a gradient-based optimization method, GSA makes use of the information from the minimizer of a single sample\u2019s loss function, and takes average strategy to calculate reasonable learning rate sequence. While most existing gradient-based algorithms introduce an increasing number of hyper parameters or try to make a trade-off between computational cost and convergence rate, GSA avoids the manual tuning of learning rate and brings in no more hyper parameters or extra cost. We perform exhaustive numerical experiments for logistic and softmax regression to compare our method with the other state of the art ones on 16 datasets. Results show that GSA is robust on various scenarios.", "target": "Greedy Step Averaging: A parameter-free stochastic optimization method"}
{"id": "task1540-6e51ec8aaaa54b12bfe82e8337ec0298", "input": "Crowdsourcing platforms enable to propose simple human intelligence tasks to a large number of participants who realise these tasks. The workers often receive a small amount of money or the platforms include some other incentive mechanisms, for example they can increase the workers reputation score, if they complete the tasks correctly. We address the problem of identifying experts among participants, that is, workers, who tend to answer the questions correctly. Knowing who are the reliable workers could improve the quality of knowledge one can extract from responses. As opposed to other works in the literature, we assume that participants can give partial or incomplete responses, in case they are not sure that their answers are correct. We model such partial or incomplete responses with the help of belief functions, and we derive a measure that characterizes the expertise level of each participant. This measure is based on precise and exactitude degrees that represent two parts of the expertise level. The precision degree reflects the reliability level of the participants and the exactitude degree reflects the knowledge level of the participants. We also analyze our model through simulation and demonstrate that our richer model can lead to more reliable identification of experts.", "target": "Characterization of experts in crowdsourcing platforms"}
{"id": "task1540-146faf67baba458389cb2afa1cecf2e9", "input": "We investigate the problem of sequentially predicting the binary labels on the nodes of an arbitrary weighted graph. We show that, under a suitable parametrization of the problem, the optimal number of prediction mistakes can be characterized (up to logarithmic factors) by the cutsize of a random spanning tree of the graph. The cutsize is induced by the unknown adversarial labeling of the graph nodes. In deriving our characterization, we obtain a simple randomized algorithm achieving in expectation the optimal mistake bound on any polynomially connected weighted graph. Our algorithm draws a random spanning tree of the original graph and then predicts the nodes of this tree in constant expected amortized time and linear space. Experiments on real-world datasets show that our method compares well to both global (Perceptron) and local (label propagation) methods, while being generally faster in practice.", "target": "Random Spanning Trees and the Prediction of Weighted Graphs"}
{"id": "task1540-bd681bdf5bcc425cac791987c3939bf8", "input": "Concept Trees are a type of database that can organise arbitrary textual information using a very simple rule. Each tree tries to represent a single cohesive concept and the trees can link with each other for navigation and semantic purposes. The trees are therefore a type of semantic network and would benefit from having a consistent level of context for each of the nodes. The Concept Tree nodes have a mathematical basis allowing for a consistent build process. These would represent nouns or verbs in a text sentence, for example. New to the design can then be lists of descriptive elements for each of the nodes. The descriptors can also be weighted, but do not have to follow the strict counting rule of the tree nodes. With the new descriptive layers, a much richer type of knowledge can be achieved and still reasoned over automatically. The linking structure of the licas network is very relevant to building the concept trees now and forms the basis for their construction. The concept tree symbolic neural network relation is also extended further.", "target": "Adding Context to Concept Trees"}
{"id": "task1540-cc94dab537c444f3a2e4f2e872fd024a", "input": "Predicting user responses, such as clicks and conversions, is of great importance and has found its usage in many Web applications including recommender systems, web search and online advertising. The data in those applications is mostly categorical and contains multiple fields; a typical representation is to transform it into a high-dimensional sparse binary feature representation via one-hot encoding. Facing with the extreme sparsity, traditional models may limit their capacity of mining shallow patterns from the data, i.e. low-order feature combinations. Deep models like deep neural networks, on the other hand, cannot be directly applied for the high-dimensional input because of the huge feature space. In this paper, we propose a Product-based Neural Networks (PNN) with an embedding layer to learn a distributed representation of the categorical data, a product layer to capture interactive patterns between interfield categories, and further fully connected layers to explore high-order feature interactions. Our experimental results on two large-scale real-world ad click datasets demonstrate that PNNs consistently outperform the state-of-the-art models on various metrics.", "target": "Product-based Neural Networks for User Response Prediction"}
{"id": "task1540-2183461263534f5e81c7b121f7ae08f2", "input": "On the semantics of determiners in a rich type-theoretical framework The variation of word meaning according to the context led us to enrich the type system of our syntactical and semantic analyser of French based on categorial grammars and Montague semantics (or lambda-DRT). The main advantage of a deep semantic analyse is too represent meaning by logical formulae that can be easily used e.g. for inferences. Determiners and quantifiers play a fundamental role in the construction of those formulae and we needed to provide them with semantic terms adapted to this new framework. We propose a solution inspired by the tau and epsilon operators of Hilbert, generic elements that resemble choice functions. This approach unifies the treatment of the different determiners and quantifiers and allows a dynamic binding of pronouns. Above all, this fully computational view of determiners fits in well within the wide coverage parser Grail, both from a theoretical and a practical viewpoint.", "target": "Se\u0301mantique des de\u0301terminants dans un cadre richement type\u0301"}
{"id": "task1540-79feee4f1c4e458489a1cfc508f571f9", "input": "Consider designing an effective crowdsourcing system for an M -ary classification task. Crowd workers complete simple binary microtasks whose results are aggregated to give the final result. We consider the novel scenario where workers have a reject option so they may skip microtasks when they are unable or choose not to respond. For example, in mismatched speech transcription, workers who do not know the language may not be able to respond to microtasks focused on phonological dimensions outside their categorical perception. We present an aggregation approach using a weighted majority voting rule, where each worker\u2019s response is assigned an optimized weight to maximize the crowd\u2019s classification performance. We evaluate system performance in both exact and asymptotic forms. Further, we consider the setting where there may be a set of greedy workers that complete microtasks even when they are unable to perform it reliably. We consider an oblivious and an expurgation strategy to deal with greedy workers, developing an algorithm to adaptively switch between the two based on the estimated fraction of greedy workers in the anonymous crowd. Simulation results show improved performance compared with conventional majority voting.", "target": "Multi-object Classification via Crowdsourcing with a Reject Option"}
{"id": "task1540-5a3e2752055c4f4faabde8becca2c573", "input": "What is happiness for reinforcement learning agents? We seek a formal definition satisfying a list of desiderata. Our proposed definition of happiness is the temporal difference error, i.e. the difference between the value of the obtained reward and observation and the agent\u2019s expectation of this value. This definition satisfies most of our desiderata and is compatible with empirical research on humans. We state several implications and discuss examples.", "target": "A Definition of Happiness for Reinforcement Learning Agents\u2217"}
{"id": "task1540-93b2969648ab4c98b015c6dda482f7a0", "input": "Semi-supervised learning based on the low-density separation principle such as<lb>the cluster and manifold assumptions has been extensively studied in the last<lb>decades. However, such semi-supervised learning methods do not always per-<lb>form well due to violation of the cluster and manifold assumptions. In this paper,<lb>we propose a novel approach to semi-supervised learning that does not require<lb>such restrictive assumptions. Our key idea is to combine learning from positive<lb>and negative data (standard supervised learning) and learning from positive and<lb>unlabeled data (PU learning), the latter is guaranteed to be able to utilize unla-<lb>beled data without the cluster and manifold assumptions. We theoretically and<lb>experimentally show the usefulness of our approach.", "target": "Beyond the Low-density Separation Principle: A Novel Approach to Semi-supervised Learning"}
{"id": "task1540-14ee3a8c578444d892d483c2694eaed3", "input": "We present and analyze an agnostic active learning algorithm that works without keeping a version space. This is unlike all previous approaches where a restricted set of candidate hypotheses is maintained throughout learning, and only hypotheses from this set are ever returned. By avoiding this version space approach, our algorithm sheds the computational burden and brittleness associated with maintaining version spaces, yet still allows for substantial improvements over supervised learning for classification.", "target": "Agnostic Active Learning Without Constraints"}
{"id": "task1540-f3a06218c6cd40bea3ce32a7f833c6b9", "input": "Policy evaluation is concerned with estimating the value function that predicts long-term values of states under a given policy. It is a crucial step in many reinforcement-learning algorithms. In this paper, we focus on policy evaluation with linear function approximation over a fixed dataset. We first transform the empirical policy evaluation problem into a (quadratic) convex-concave saddle-point problem, and then present a primal-dual batch gradient method, as well as two stochastic variance reduction methods for solving the problem. These algorithms scale linearly in both sample size and feature dimension. Moreover, they achieve linear convergence even when the saddle-point problem has only strong concavity in the dual variables but no strong convexity in the primal variables. Numerical experiments on benchmark problems demonstrate the effectiveness of our methods.", "target": "Stochastic Variance Reduction Methods for Policy Evaluation"}
{"id": "task1540-a7456a5143544a6a8cf0ca5341c936ce", "input": "Techniques for plan recogmtwn under uncer\u00ad tainty require a stochastic model of the plan\u00ad generation process. We introduce probabilistic state-dependent grammars (PSDGs) to represent an agent's plan-generation process. The PSDG language model extends probabilistic context\u00ad free grammars (PCFGs) by allowing production probabilities to depend on an explicit model of the planning agent's internal and external state. Given a PSDG description of the plan-generation process, we can then use inference algorithms that exploit the particular independence proper\u00ad ties of the PSDG language to efficiently answer plan-recognition queries. The combination of the PSDG language model and inference algorithms extends the range of plan-recognition domains for which practical probabilistic inference is pos\u00ad sible, as illustrated by applications in traffic mon\u00ad itoring and air combat.", "target": "Probabilistic State-Dependent Grammars for Plan Recognition"}
{"id": "task1540-be7bac80f52340a0ba6924089b8dba96", "input": "In this paper, we propose a learning-based supervised discrete hashing method. Binary hashing is widely used for large-scale image retrieval as well as video and document searches because the compact representation of binary code is essential for data storage and reasonable for query searches using bit-operations. The recently proposed Supervised Discrete Hashing (SDH) efficiently solves mixed-integer programming problems by alternating optimization and the Discrete Cyclic Coordinate descent (DCC) method. We show that the SDH model can be simplified without performance degradation based on some preliminary experiments; we call the approximate model for this the \u201cFast SDH\u201d (FSDH) model. We analyze the FSDH model and provide a mathematically exact solution for it. In contrast to SDH, our model does not require an alternating optimization algorithm and does not depend on initial values. FSDH is also easier to implement than Iterative Quantization (ITQ). Experimental results involving a large-scale database showed that FSDH outperforms conventional SDH in terms of precision, recall, and computation time.", "target": "Fast Supervised Discrete Hashing and its Analysis"}
{"id": "task1540-9231cd6c6b6746cfa07e22bdf05e97b6", "input": "Sentences are important semantic units of natural language. A generic, distributional representation of sentences that can capture the latent semantics is beneficial to multiple downstream applications. We observe a simple geometry of sentences \u2013 the word representations of a given sentence (on average 10.23 words in all SemEval datasets with a standard deviation 4.84) roughly lie in a low-rank subspace (roughly, rank 4). Motivated by this observation, we represent a sentence by the low-rank subspace spanned by its word vectors. Such an unsupervised representation is empirically validated via semantic textual similarity tasks on 19 different datasets, where it outperforms the sophisticated neural network models, including skip-thought vectors, by 15% on average.", "target": "Representing Sentences as Low-Rank Subspaces"}
{"id": "task1540-a7670353b2d04d1a94af5ac3fc4097c0", "input": "We propose a novel algorithm for optimizing multivariate linear threshold functions as split functions of decision trees to create improved Random Forest classifiers. Standard tree induction methods resort to sampling and exhaustive search to find good univariate split functions. In contrast, our method computes a linear combination of the features at each node, and optimizes the parameters of the linear combination (oblique) split functions by adopting a variant of latent variable SVM formulation. We develop a convex-concave upper bound on the classification loss for a one-level decision tree, and optimize the bound by stochastic gradient descent at each internal node of the tree. Forests of up to 1000 Continuously Optimized Oblique (CO2) decision trees are created, which significantly outperform Random Forest with univariate splits and previous techniques for constructing oblique trees. Experimental results are reported on multi-class classification benchmarks and on Labeled Faces in the Wild (LFW) dataset.", "target": "CO2 Forest: Improved Random Forest by Continuous Optimization of Oblique Splits"}
{"id": "task1540-1f196173f930416a933c94285b5c5958", "input": "We propose a general information-theoretic approach called SERAPH (SEmi-supervised metRic leArning Paradigm with Hyper-sparsity) for metric learning that does not rely upon the manifold assumption. Given the probability parameterized by a Mahalanobis distance, we maximize the entropy of that probability on labeled data and minimize it on unlabeled data following entropy regularization, which allows the supervised and unsupervised parts to be integrated in a natural and meaningful way. Furthermore, SERAPH is regularized by encouraging a low-rank projection induced from the metric. The optimization of SERAPH is solved efficiently and stably by an EMlike scheme with the analytical E-Step and convex M-Step. Experiments demonstrate that SERAPH compares favorably with many well-known global and local metric learning methods.", "target": "Information-theoretic Semi-supervised Metric Learningvia Entropy Regularization"}
{"id": "task1540-4640ed8fa4a54201beed3ed43fc760b0", "input": "In this work, we present and analyze reported failures of artificially intelligent systems and extrapolate our analysis to future AIs. We suggest that both the frequency and the seriousness of future AI failures will steadily increase. AI Safety can be improved based on ideas developed by cybersecurity experts. For narrow AIs safety failures are at the same, moderate, level of criticality as in cybersecurity, however for general AI, failures have a fundamentally different impact. A single failure of a superintelligent system may cause a catastrophic event without a chance for recovery. The goal of cybersecurity is to reduce the number of successful attacks on the system; the goal of AI Safety is to make sure zero attacks succeed in bypassing the safety mechanisms. Unfortunately, such a level of performance is unachievable. Every security system will eventually fail; there is no such thing as a 100% secure system.", "target": "Artificial Intelligence Safety and Cybersecurity: a Timeline of AI Failures"}
{"id": "task1540-a899c6bc19dd4b9d934dee9d5a2b3fab", "input": "Modelling Consumer Indebtedness has proven to be a problem of complex nature. In this work we utilise Data Mining techniques and methods to explore the multifaceted aspect of Consumer Indebtedness by examining the contribution of Psychological Factors, like Impulsivity to the analysis of Consumer Debt. Our results confirm the beneficial impact of Psychological Factors in modelling Consumer Indebtedness and suggest a new approach in analysing Consumer Debt, that would take into consideration more Psychological characteristics of consumers and adopt techniques and practices from Data Mining.", "target": "A Data Mining framework to model Consumer Indebtedness with Psychological Factors"}
{"id": "task1540-1928306c62b64742a756a6bba8c366eb", "input": "This paper introduces a new computing model based on the cooperation among Turing machines called orchestrated machines. Like universal Turing machines, orchestrated machines are also designed to simulate Turing machines but they can also modify the original operation of the included Turing machines to create a new layer of some kind of collective behavior. Using this new model we can define some interested notions related to cooperation ability of Turing machines such as the intelligence quotient or the emotional intelligence quotient for Turing machines.", "target": "Are there intelligent Turing machines?"}
{"id": "task1540-6156e1681a154896ab270381293a3c53", "input": "Supervised (linear) embedding models like Wsabie [5] and PSI [1] have proven successful at ranking, recommendation and annotation tasks. However, despite being scalable to large datasets they do not take full advantage of the extra data due to their linear nature, and typically underfit. We propose a new class of models which aim to provide improved performance while retaining many of the benefits of the existing class of embedding models. Our new approach works by iteratively learning a linear embedding model where the next iteration\u2019s features and labels are reweighted as a function of the previous iteration. We describe several variants of the family, and give some initial results. 1 (Supervised) Linear Embedding Models Standard linear embedding models are of the form: f(x, y) = xUV y = \u2211", "target": "Affinity Weighted Embedding"}
{"id": "task1540-84d3201b2c494b96a7ec7bb71a18e456", "input": "To coordinate with other agents in its envi\u00ad ronment, an agent needs models of what the other agents are trying to do. When com\u00ad munication is impossible or expensive, this information must be acquired indirectly via plan recognition. Typical approaches to plan recognition start with a specification of the possible plans the other agents may be follow\u00ad ing, and develop special techniques for dis\u00ad criminating among the possibilities. Perhaps more desirable would be a uniform procedure for mapping plans to general structures sup\u00ad porting inference based on uncertain and in\u00ad complete observations. In this paper, we de\u00ad scribe a set of methods for converting plans represented in a flexible procedural language to observation models represented as proba\u00ad bilistic belief networks.", "target": "The Automated Mapping of Plans for Plan Recognition*"}
{"id": "task1540-fe6b50b122a64eb2b25c48672fbcde46", "input": "We study two-player security games which can be viewed as sequences of nonzero-sum matrix games played by an Attacker and a Defender. The evolution of the game is based on a stochastic fictitious play process, where players do not have access to each other\u2019s payoff matrix. Each has to observe the other\u2019s actions up to present and plays the action generated based on the best response to these observations. In a regular fictitious play process, each player makes a maximum likelihood estimate of her opponent\u2019s mixed strategy, which results in a time-varying update based on the previous estimate and current action. In this paper, we explore an alternative scheme for frequency update, whose mean dynamic is instead time-invariant. We examine convergence properties of the mean dynamic of the fictitious play process with such an update scheme, and establish local stability of the equilibrium point when both players are restricted to two actions. We also propose an adaptive algorithm based on this time-invariant frequency update.", "target": "Fictitious Play with Time-Invariant Frequency Update for Network Security"}
{"id": "task1540-ffba771782584c04aa8dd2301a1a1b1b", "input": "Recently, zero-shot learning (ZSL) has received increasing interest. The key idea underpinning existing ZSL approaches is to exploit knowledge transfer via an intermediate-level semantic representation which is assumed to be shared between the auxiliary and target datasets, and is used to bridge between these domains for knowledge transfer. The semantic representation used in existing approaches varies from visual attributes [11,2,13,7] to semantic word vectors [3,19] and semantic relatedness [17]. However, the overall pipeline is similar: a projection mapping low-level features to the semantic representation is learned from the auxiliary dataset by either classification or regression models and applied directly to map each instance into the same semantic representation space where a zero-shot classifier is used to recognise the unseen target class instances with a single known \u2018prototype\u2019 of each target class. In this paper we discuss two related lines of work improving the conventional approach: exploiting transductive learning ZSL, and generalising ZSL to the multi-label case.", "target": "Transductive Multi-class and Multi-label Zero-shot Learning"}
{"id": "task1540-04e11e6af38f432880d35a8794a44f6c", "input": "We study the effectiveness of neural sequence models for premise selection in automated theorem proving, one of the main bottlenecks in the formalization of mathematics. We propose a two stage approach for this task that yields good results for the premise selection task on the Mizar corpus while avoiding the handengineered features of existing state-of-the-art models. To our knowledge, this is the first time deep learning has been applied to theorem proving.", "target": "DeepMath - Deep Sequence Models for Premise Selection"}
{"id": "task1540-1218c28d2e1f4b919ae75df97a6a57a0", "input": "We present ML4PG \u2014 a machine learning extension for Proof General. It allows users to gather proof statistics related to shapes of goals, sequences of applied tactics, and proof tree structures from the libraries of interactive higher-order proofs written in Coq and SSReflect. The gathered data is clustered using the state-of-the-art machine learning algorithms available in MATLAB and Weka. ML4PG provides automated interfacing between Proof General and MATLAB/Weka. The results of clustering are used by ML4PG to provide proof hints in the process of interactive proof development.", "target": "Machine Learning in Proof General: Interfacing Interfaces"}
{"id": "task1540-44603acbedd644a4835d4bd956b27fef", "input": "We propose new methods to speed up convergence of the Alternating Direction Method of Multipliers (ADMM), a common optimization tool in the context of large scale and distributed learning. The proposed method accelerates the speed of convergence by automatically deciding the constraint penalty needed for parameter consensus in each iteration. In addition, we also propose an extension of the method that adaptively determines the maximum number of iterations to update the penalty. We show that this approach effectively leads to an adaptive, dynamic network topology underlying the distributed optimization. The utility of the new penalty update schemes is demonstrated on both synthetic and real data, including a computer vision application of distributed structure from motion.", "target": "Fast ADMM Algorithm for Distributed Optimization with Adaptive Penalty"}
{"id": "task1540-337548099326456f8f19045e9c2e5e35", "input": "We used MetaMap and YTEX as a basis for the construction of two separate systems to participate in the 2013 ShARe/CLEF eHealth Task 1[9], the recognition of clinical concepts. No modifications were directly made to these systems, but output concepts were filtered using stop concepts, stop concept text and UMLS semantic type. Concept boundaries were also adjusted using a small collection of rules to increase precision on the strict task. Overall MetaMap had better performance than YTEX on the strict task, primarily due to a 20% performance improvement in precision. In the relaxed task YTEX had better performance in both precision and recall giving it an overall F-Score 4.6% higher than MetaMap on the test data. Our results also indicated a 1.3% higher accuracy for YTEX in UMLS CUI mapping.", "target": "Evaluation of YTEX and MetaMap for clinical concept recognition"}
{"id": "task1540-e6bfc459578a4fa192051e279b6acfc4", "input": "This paper discusses a method for im\u00ad plementing a probabilistic inference system based on an extended relational data model. This model provides a unified approach for a variety of applications such as dynamic pro\u00ad gramming, solving sparse linear equations, and constraint propagation. In this frame\u00ad work, the probability model is represented as a generalized relational database. Subse\u00ad quent probabilistic requests can be processed as standard relational queries. Conventional database management systems can be easily adopted for implementing such an approxi\u00ad mate reasoning system.", "target": "A Method for Implementing a Probabilistic Model as a Relational Database"}
{"id": "task1540-97f31986a97b4d2cbc76aee14f4de658", "input": "Occlusion edges in images which correspond to range discontinuity in the scene from the point of view of the observer are an important prerequisite for many vision and mobile robot tasks. Although occlusion edges can be extracted from range data, extracting them from images and videos is challenging and would be extremely beneficial for a variety of robotics based applications. We trained a deep convolutional neural network (CNN) to identify occlusion edges in images and videos with both RGB-D and RGB inputs. The use of CNN avoids hand-crafting of features for automatically isolating occlusion edges and distinguishing them from appearance edges. Other than quantitative occlusion edge detection results, qualitative results are provided to demonstrate the trade-off between high resolution analysis and frame-level computation time which is critical for real-time robotics applications.", "target": "Using Deep Convolutional Networks for Occlusion Edge Detection in RGB-D Frames"}
{"id": "task1540-6601a47d6a334e96867febb675763d62", "input": "Smoothed analysis is a framework for analyzing the complexity of an algorithm, acting as a bridge between average and worst-case behaviour. For example, Quicksort and the Simplex algorithm are widely used in practical applications, despite their heavy worst-case complexity. Smoothed complexity aims to better characterize such algorithms. Existing theoretical bounds for the smoothed complexity of sorting algorithms are still quite weak. Furthermore, empirically computing the smoothed complexity via its original definition is computationally infeasible, even for modest input sizes. In this paper, we focus on accurately predicting the smoothed complexity of sorting algorithms, using machine learning techniques. We propose two regression models that take into account various properties of sorting algorithms and some of the known theoretical results in smoothed analysis to improve prediction quality. We show experimental results for predicting the smoothed complexity of Quicksort, Mergesort, and optimized Bubblesort for large input sizes, therefore filling the gap between known theoretical and empirical results.", "target": "A Machine Learning Approach to Predicting the Smoothed Complexity of Sorting Algorithms"}
{"id": "task1540-9cc4f43463ac4b5ca705ba350a4afa38", "input": "There is a vast amount of unstructured Arabic information on the Web, this data is always organized in semi-structured text and cannot be used directly. This research proposes a semi-supervised technique that extracts binary relations between two Arabic named entities from the Web. Several works have been performed for relation extraction from Latin texts and as far as we know, there isn\u2019t any work for Arabic text using a semi-supervised technique. The goal of this research is to extract a large list or table from named entities and relations in a specific domain. A small set of a handful of instance relations are required as input from the user. The system exploits summaries from Google search engine as a source text. These instances are used to extract patterns. The output is a set of new entities and their relations. The results from four experiments show that precision and recall varies according to relation type. Precision ranges from 0.61 to 0.75 while recall ranges from 0.71 to 0.83. The best result is obtained for (player, club) relationship, 0.72 and 0.83 for precision and recall respectively.", "target": "EXTRACTING ARABIC RELATIONS FROM THE WEB"}
{"id": "task1540-a1fced2932e34a538fb1bc0d41c0769e", "input": "In distributed, or privacy-preserving learning, we are often given a set of probabilistic models estimated from different local repositories, and asked to combine them into a single model that gives efficient statistical estimation. A simple method is to linearly average the parameters of the local models, which, however, tends to be degenerate or not applicable on non-convex models, or models with different parameter dimensions. One more practical strategy is to generate bootstrap samples from the local models, and then learn a joint model based on the combined bootstrap set. Unfortunately, the bootstrap procedure introduces additional noise and can significantly deteriorate the performance. In this work, we propose two variance reduction methods to correct the bootstrap noise, including a weighted M-estimator that is both statistically efficient and practically powerful. Both theoretical and empirical analysis is provided to demonstrate our methods.", "target": "Bootstrap Model Aggregation for Distributed Statistical Learning"}
{"id": "task1540-bb561696d28f4db59098b170301773cb", "input": "This paper presents a novel communication-efficient parallel belief propagation (CE-PBP) algorithm for training latent Dirichlet allocation (LDA). Based on the synchronous belief propagation (BP) algorithm, we first develop a parallel belief propagation (PBP) algorithm on the parallel architecture. Because the extensive communication delay often causes a low efficiency of parallel topic modeling, we further use Zipf\u2019s law to reduce the total communication cost in PBP. Extensive experiments on different data sets demonstrate that CE-PBP achieves a higher topic modeling accuracy and reduces more than 80% communication cost than the state-of-the-art parallel Gibbs sampling (PGS) algorithm.", "target": "Communication-Efficient Parallel Belief Propagation for Latent Dirichlet Allocation"}
{"id": "task1540-2255c235d57448d1b529012bf544cba2", "input": "The cutting plane method is an augmentative constrained optimization procedure that is often used with continuous-domain optimization techniques such as linear and convex programs. We investigate the viability of a similar idea within message passing \u2013 which produces integral solutions in the context of two combinatorial problems: 1) For Traveling Salesman Problem (TSP), we propose a factor-graph based on Held-Karp formulation, with an exponential number of constraint factors, each of which has an exponential but sparse tabular form. 2) For graph-partitioning (a.k.a. community mining) using modularity optimization, we introduce a binary variable model with a large number of constraints that enforce formation of cliques. In both cases we are able to derive surprisingly simple message updates that lead to competitive solutions on benchmark instances. In particular for TSP we are able to find near-optimal solutions in the time that empirically grows with N, demonstrating that augmentation is practical and efficient.", "target": "Augmentative Message Passing for Traveling Salesman Problem and Graph Partitioning"}
{"id": "task1540-edf2dc5fb2614e5d81ad28e2d8179d51", "input": "<lb>In applications such as recommendation systems and revenue management, it is important to<lb>predict preferences on items that have not been seen by a user or predict outcomes of comparisons<lb>among those that have never been compared. A popular discrete choice model of multinomial<lb>logit model captures the structure of the hidden preferences with a low-rank matrix. In order to<lb>predict the preferences, we want to learn the underlying model from noisy observations of the<lb>low-rank matrix, collected as revealed preferences in various forms of ordinal data. A natural<lb>approach to learn such a model is to solve a convex relaxation of nuclear norm minimization.<lb>We present the convex relaxation approach in two contexts of interest: collaborative ranking<lb>and bundled choice modeling. In both cases, we show that the convex relaxation is minimax<lb>optimal. We prove an upper bound on the resulting error with finite samples, and provide a<lb>matching information-theoretic lower bound.", "target": "Collaboratively Learning Preferences from Ordinal Data"}
{"id": "task1540-b839eb50ec7142378927ab6219205811", "input": "Multimedia or spoken content presents more attractive information than plain text content, but it\u2019s more difficult to display on a screen and be selected by a user. As a result, accessing large collections of the former is much more difficult and time-consuming than the latter for humans. It\u2019s highly attractive to develop a machine which can automatically understand spoken content and summarize the key information for humans to browse over. In this endeavor, we propose a new task of machine comprehension of spoken content. We define the initial goal as the listening comprehension test of TOEFL, a challenging academic English examination for English learners whose native language is not English. We further propose an Attention-based Multi-hop Recurrent Neural Network (AMRNN) architecture for this task, achieving encouraging results in the initial tests. Initial results also have shown that word-level attention is probably more robust than sentence-level attention for this task with ASR errors.", "target": "Towards Machine Comprehension of Spoken Content: Initial TOEFL Listening Comprehension Test by Machine"}
{"id": "task1540-0563d47da5a2474da10890a40f1f4550", "input": "A framework is presented for unsupervised learning of representations based on infomax principle for large-scale neural populations. We use an asymptotic approximation to the Shannon\u2019s mutual information for a large neural population to demonstrate that a good initial approximation to the global information-theoretic optimum can be obtained by a hierarchical infomax method. Starting from the initial solution, an efficient algorithm based on gradient descent of the final objective function is proposed to learn representations from the input datasets, and the method works for complete, overcomplete, and undercomplete bases. As confirmed by numerical experiments, our method is robust and highly efficient for extracting salient features from input datasets. Compared with the main existing methods, our algorithm has a distinct advantage in both the training speed and the robustness of unsupervised representation learning. Furthermore, the proposed method is easily extended to the supervised or unsupervised model for training deep structure networks.", "target": "NEURAL POPULATION INFOMAX"}
{"id": "task1540-6305266102874b7ebee96286203f5881", "input": "In this paper, we suggest a novel data-driven approach to active learning: Learning Active Learning (LAL). The key idea behind LAL is to train a regressor that predicts the expected error reduction for a potential sample in a particular learning state. By treating the query selection procedure as a regression problem we are not restricted to dealing with existing AL heuristics; instead, we learn strategies based on experience from previous active learning experiments. We show that LAL can be learnt from a simple artificial 2D dataset and yields strategies that work well on real data from a wide range of domains. Moreover, if some domain-specific samples are available to bootstrap active learning, the LAL strategy can be tailored for a particular problem.", "target": "Learning Active Learning from Real and Synthetic Data"}
{"id": "task1540-a0003542905e4b9d84be7c7c34748d1a", "input": "The popular Alternating Least Squares (ALS) algorithm for tensor decomposition is extremely efficient, but often converges to poor local optima, particularly when the weights of the factors are non-uniform. We propose a modification of the ALS approach that is as efficient as standard ALS, but provably recovers the true factors with random initialization under standard incoherence assumptions on the factors of the tensor. We demonstrate the significant practical superiority of our approach over traditional ALS (with both random initialization and SVDbased initialization) for a variety of tasks on synthetic data\u2014including tensor factorization on exact, noisy and over-complete tensors, as well as tensor completion\u2014and for computing word embeddings from a third-order word tri-occurrence tensor.", "target": "Orthogonalized ALS: A Theoretically Principled Tensor Decomposition Algorithm for Practical Use"}
{"id": "task1540-9f2836218d22426898975da1cd84fc7a", "input": "Each year, millions of motor vehicle traffic accidents all over the world cause a large number of fatalities, injuries and significant material loss. Automated Driving (AD) has potential to drastically reduce such accidents. In this work, we focus on the technical challenges that arise from AD in urban environments. We present the overall architecture of an AD system and describe in detail the perception and planning modules. The AD system, built on a modified Acura RLX, was demonstrated in a course in GoMentum Station in California. We demonstrated autonomous handling of 4 scenarios: traffic lights, cross-traffic at intersections, construction zones and pedestrians. The AD vehicle displayed safe behavior and performed consistently in repeated demonstrations with slight variations in conditions. Overall, we completed 44 runs, encompassing 110km of automated driving with only 3 cases where the driver intervened the control of the vehicle, mostly due to error in GPS positioning. Our demonstration showed that robust and consistent behavior in urban scenarios is possible, yet more investigation is necessary for full scale rollout on public roads.", "target": "Towards Full Automated Drive in Urban Environments: A Demonstration in GoMentum Station, California"}
{"id": "task1540-07122f0ab92743a39b7d617557ce91d4", "input": "Language models (LMs) are statistical models that calculate probabilities over sequences of words or other discrete symbols. Currently two major paradigms for language modeling exist: count-based n-gram models, which have advantages of scalability and test-time speed, and neural LMs, which often achieve superior modeling performance. We demonstrate how both varieties of models can be unified in a single modeling framework that defines a set of probability distributions over the vocabulary of words, and then dynamically calculates mixture weights over these distributions. This formulation allows us to create novel hybrid models that combine the desirable features of count-based and neural LMs, and experiments demonstrate the advantages of these approaches.", "target": "Generalizing and Hybridizing Count-based and Neural Language Models"}
{"id": "task1540-d37df6c1df1541de8972c282dd3819b7", "input": "In this work we present a new approach to learn compressible representations in deep architectures with an end-to-end training strategy. Our method is based on a soft (continuous) relaxation of quantization and entropy, which we anneal to their discrete counterparts throughout training. We showcase this method for two challenging applications: Image compression and neural network compression. While these tasks have typically been approached with different methods, our soft-to-hard quantization approach gives state-of-the-art results for both.", "target": "Soft-to-Hard Vector Quantization for End-to-End Learned Compression of Images and Neural Networks"}
{"id": "task1540-8caabe3c0a774258afcd32591e97bd07", "input": "Stochastic network design is a general framework for optimizing network connectivity. It has several applications in computational sustainability including spatial conservation planning, pre-disaster network preparation, and river network optimization. A common assumption in previous work has been made that network parameters (e.g., probability of species colonization) are precisely known, which is unrealistic in real-world settings. We therefore address the robust river network design problem where the goal is to optimize river connectivity for fish movement by removing barriers. We assume that fish passability probabilities are known only imprecisely, but are within some interval bounds. We then develop a planning approach that computes the policies with either high robust ratio or low regret. Empirically, our approach scales well to large river networks. We also provide insights into the solutions generated by our robust approach, which has significantly higher robust ratio than the baseline solution with mean parameter estimates.", "target": "Robust Optimization for Tree-Structured Stochastic Network Design"}
{"id": "task1540-3a29e2a1012e4518a9bf9033081f2679", "input": "This paper presents our recent work on the design and development of a new, large scale dataset, which we name MS MARCO, for MAchine Reading COmprehension. This new dataset is aimed to overcome a number of well-known weaknesses of previous publicly available datasets for the same task of reading comprehension and question answering. In MS MARCO, all questions are sampled from real anonymized user queries. The context passages, from which answers in the dataset are derived, are extracted from real web documents using the most advanced version of the Bing search engine. The answers to the queries are human generated. Finally, a subset of these queries has multiple answers. We aim to release one million queries and the corresponding answers in the dataset, which, to the best of our knowledge, is the most comprehensive real-world dataset of its kind in both quantity and quality. We are currently releasing 100,000 queries with their corresponding answers to inspire work in reading comprehension and question answering along with gathering feedback from the research community.", "target": "MS MARCO: A Human Generated MAchine Reading COmprehension Dataset"}
{"id": "task1540-954e3d96f0d34169bf3b71ed779fc867", "input": "Here we describe work on learning the subcategories of verbs in a morphologically rich language using only minimal linguistic resources. Our goal is to learn verb subcategorizations for Quechua, an under-resourced morphologically rich language, from an unannotated corpus. We compare results from applying this approach to an unannotated Arabic corpus with those achieved by processing the same text in treebank form. The original plan was to use only a morphological analyzer and an unannotated corpus, but experiments suggest that this approach by itself will not be effective for learning the combinatorial potential of Arabic verbs in general. The lower bound on resources for acquiring this information is somewhat higher, apparently requiring a a part-of-speech tagger and chunker for most languages, and a morphological disambiguater for Arabic.", "target": "Considering a resource-light approach to learning verb valencies"}
{"id": "task1540-a6a57f98ae3e41899af98da64eeb026f", "input": "This paper presents capabilities of using genetic algorithms to find approximations of function extrema, which cannot be found using analytic ways. To enhance effectiveness of calculations, algorithm has been parallelized using OpenMP library. We gained much increase in speed on platforms using multithreaded processors with shared memory free access. During analysis we used different modifications of genetic operator, using them we obtained varied evolution process of potential solutions. Results allow to choose best methods among many applied in genetic algorithms and observation of acceleration on Yorkfield, Bloomfield, Westmere-EX and most recent Sandy Bridge cores.", "target": "GENERATING EXTREMA APPROXIMATION OF ANALYTICALLY INCOMPUTABLE FUNCTIONS THROUGH USAGE OF PARALLEL COMPUTER AIDED GENETIC ALGORITHMS"}
{"id": "task1540-61759a44f1fe45068767675cf8731626", "input": "Rapid increase of digitized document give birth to high demand of document image retrieval. While conventional document image retrieval approaches depend on complex OCR-based text recognition and text similarity detection, this paper proposes a new content-based approach, in which more attention is paid to features extraction and fusion. In the proposed approach, multiple features of document images are extracted by different CNN models. After that, the extracted CNN features are reduced and fused into weighted average feature. Finally, the document images are ranked based on feature similarity to a provided query image. Experimental procedure is performed on a group of document images that transformed from academic papers, which contain both English and Chinese document, the results show that the proposed approach has good ability to retrieve document images with similar text content, and the fusion of CNN features can effectively improve the retrieval accuracy.", "target": "Content-based similar document image retrieval using fusion of CNN features"}
{"id": "task1540-2740d7c9c89945649fe223c4d5d3bb39", "input": "The segmentation, seen as the association of a partition with an image, is a difficult task. It can be decomposed in two steps: at first, a family of contours associated with a series of nested partitions (or hierarchy) is created and organized, then pertinent contours are extracted. A coarser partition is obtained by merging adjacent regions of a finer partition. The strength of a contour is then measured by the level of the hierarchy for which its two adjacent regions merge. We present an automatic segmentation strategy using a wide range of stochastic watershed hierarchies. For a given set of homogeneous images, our approach selects automatically the best hierarchy and cut level to perform image simplification given an evaluation score. Experimental results illustrate the advantages of our approach on several real-life images datasets.", "target": "AUTOMATIC SELECTION OF STOCHASTIC WATERSHED HIERARCHIES"}
{"id": "task1540-c923d781ab294db4a43ed3cdf625a35f", "input": "Reinforcement learning algorithms need to deal with the exponential growth of states and actions when exploring optimal control in high-dimensional spaces. This is known as the curse of dimensionality. By projecting the agent\u2019s state onto a low-dimensional manifold, we can represent the state space in a smaller and more efficient representation. By using this representation during learning, the agent can converge to a good policy much faster. We test this approach in the Mario Benchmarking Domain. When using dimensionality reduction in Mario, learning converges much faster to a good policy. But, there is a critical convergence-performance trade-off. By projecting onto a low-dimensional manifold, we are ignoring important data. In this paper, we explore this trade-off of convergence and performance. We find that learning in as few as 4 dimensions (instead of 9), we can improve performance past learning in the full dimensional space at a faster convergence rate.", "target": "Using PCA to Efficiently Represent State Spaces"}
{"id": "task1540-2075d92d561743e5b35810b16a71e579", "input": "Motivated by applications in computational advertising and systems biology, we consider the problem of identifying the best out of several possible soft interventions at a source node V in an acyclic causal directed graph, to maximize the expected value of a target node Y (located downstream of V ). Our setting imposes a fixed total budget for sampling under various interventions, along with cost constraints on different types of interventions. We pose this as a best arm identification bandit problem with K arms where each arm is a soft intervention at V, and leverage the information leakage among the arms to provide the first gap dependent error and simple regret bounds for this problem. Our results are a significant improvement over the traditional best arm identification results. We empirically show that our algorithms outperform the state of the art in the Flow Cytometry data-set, and also apply our algorithm for model interpretation of the Inception-v3 deep net that classifies images.", "target": "Identifying Best Interventions through Online Importance Sampling"}
{"id": "task1540-a33945746e114281a4614c4f16163912", "input": "s prominently include information that is relevant to the population group of interest, and intervention, comparison and disease of interest. The vector space model further measures the similarity between the query and citation based on concepts (as opposed to just terms or words themselves).", "target": "A Hybrid Citation Retrieval Algorithm for Evidence-based Clinical Knowledge Summarization: Combining Concept Extraction, Vector Similarity and Query Expansion for High Precision"}
{"id": "task1540-973a9c421b184a0b81a8bf0c9065a331", "input": "Recent works on word representations mostly rely on predictive models. Distributed word representations (aka word embeddings) are trained to optimally predict the contexts in which the corresponding words tend to appear. Such models have succeeded in capturing word similarities as well as semantic and syntactic regularities. Instead, we aim at reviving interest in a model based on counts. We present a systematic study of the use of the Hellinger distance to extract semantic representations from the word co-occurrence statistics of large text corpora. We show that this distance gives good performance on word similarity and analogy tasks, with a proper type and size of context, and a dimensionality reduction based on a stochastic low-rank approximation. Besides being both simple and intuitive, this method also provides an encoding function which can be used to infer unseen words or phrases. This becomes a clear advantage compared to predictive models which must train these new words.", "target": "Rehabilitation of Count-based Models for Word Vector Representations"}
{"id": "task1540-9cc313135c4343779eb3181b90258d3a", "input": "The objective of machine learning is to extract useful information from data, while privacy is preserved by concealing information. Thus it seems hard to reconcile these competing interests. However, they frequently must be balanced when mining sensitive data. For example, medical research represents an important application where it is necessary both to extract useful information and protect patient privacy. One way to resolve the conflict is to extract general characteristics of whole populations without disclosing the private information of individuals. In this paper, we consider differential privacy, one of the most popular and powerful definitions of privacy. We explore the interplay between machine learning and differential privacy, namely privacy-preserving machine learning algorithms and learning-based data release mechanisms. We also describe some theoretical results that address what can be learned differentially privately and upper bounds of loss functions for differentially", "target": "Differential Privacy and Machine Learning: a Survey and Review"}
{"id": "task1540-34abd95b88b445fc9e23ea2f9778988f", "input": "We develop a new model for Interactive Question Answering (IQA), using GatedRecurrent-Unit recurrent networks (GRUs) as encoders for statements and questions, and another GRU as a decoder for outputs. Distinct from previous work, our approach employs context-dependent word-level attention for more accurate statement representations and question-guided sentence-level attention for better context modeling. Employing these mechanisms, our model accurately understands when it can output an answer or when it requires generating a supplementary question for additional input. When available, user\u2019s feedback is encoded and directly applied to update sentence-level attention to infer the answer. Extensive experiments on QA and IQA datasets demonstrate quantitatively the effectiveness of our model with significant improvement over conventional QA models.", "target": "A CONTEXT-AWARE ATTENTION NETWORK FOR INTERACTIVE QUESTION ANSWERING"}
{"id": "task1540-b606834326eb4824a119df365f048631", "input": "In this paper, we present the DifferenceBased Causality Learner (DBCL), an algorithm for learning a class of discrete-time dynamic models that represents all causation across time by means of difference equations driving change in a system. We motivate this representation with real-world mechanical systems and prove DBCL\u2019s correctness for learning structure from time series data, an endeavour that is complicated by the existence of latent derivatives that have to be detected. We also prove that, under common assumptions for causal discovery, DBCL will identify the presence or absence of feedback loops, making the model more useful for predicting the effects of manipulating variables when the system is in equilibrium. We argue analytically and show empirically the advantages of DBCL over vector autoregression (VAR) and Granger causality models as well as modified forms of Bayesian and constraintbased structure discovery algorithms. Finally, we show that our algorithm can discover causal directions of alpha rhythms in human brains from EEG data.", "target": "Learning Why Things Change: The Difference-Based Causality Learner"}
{"id": "task1540-eeadd87c0a584b03a33ad3e7665b06fb", "input": "To cope with changing environments, recent developments in online learning have introduced the concepts of adaptive regret and dynamic regret independently. In this paper, we illustrate an intrinsic connection between these two concepts by showing that the dynamic regret can be expressed in terms of the adaptive regret and the functional variation. This observation implies that strongly adaptive algorithms can be directly leveraged to minimize the dynamic regret. As a result, we present a series of strongly adaptive algorithms whose dynamic regrets are minimax optimal for convex functions, exponentially concave functions, and strongly convex functions, respectively. To the best of our knowledge, this is the first time that such kind of dynamic regret bound is established for exponentially concave functions. Moreover, all of those adaptive algorithms do not need any prior knowledge of the functional variation, which is a significant advantage over previous specialized methods for minimizing dynamic regret.", "target": "Strongly Adaptive Regret Implies Optimally Dynamic Regret"}
{"id": "task1540-c71e783f43814d7e832c43257fcab71c", "input": "Determinantal point processes (DPPs) are popular probabilistic models that arise in many machine learning tasks, where distributions of diverse sets are characterized by matrix determinants. In this paper, we develop fast algorithms to find the most likely configuration (MAP) of large-scale DPPs, which is NP-hard in general. Due to the submodular nature of the MAP objective, greedy algorithms have been used with empirical success. Greedy implementations require computation of log-determinants, matrix inverses or solving linear systems at each iteration. We present faster implementations of the greedy algorithms by utilizing the complementary benefits of two log-determinant approximation schemes: (a) first-order expansions to the matrix log-determinant function and (b) high-order expansions to the scalar log function with stochastic trace estimators. In our experiments, our algorithms are orders of magnitude faster than their competitors, while sacrificing marginal accuracy.", "target": "Faster Greedy MAP Inference for Determinantal Point Processes"}
{"id": "task1540-ba2d272edd1b4c22a19541d2970d0ef1", "input": "Convolutional neural networks excel in image recognition tasks, but this comes at the cost of high computational and memory complexity. To tackle this problem, [1] developed a tensor factorization framework to compress fully-connected layers. In this paper, we focus on compressing convolutional layers. We show that while the direct application of the tensor framework [1] to the 4-dimensional kernel of convolution does compress the layer, we can do better. We reshape the convolutional kernel into a tensor of higher order and factorize it. We combine the proposed approach with the previous work to compress both convolutional and fully-connected layers of a network and achieve 80\u00d7 network compression rate with 1.1% accuracy drop on the CIFAR-10 dataset.", "target": "Ultimate tensorization: compressing convolutional and FC layers alike"}
{"id": "task1540-90569545bd7d479ba797ac7337143567", "input": "Machine learning systems trained on user-provided data are susceptible to data poisoning attacks, whereby malicious users inject false training data with the aim of corrupting the learned model. While recent work has proposed a number of attacks and defenses, little is understood about the worst-case loss of a defense in the face of a determined attacker. We address this by constructing approximate upper bounds on the loss across a broad family of attacks, for defenders that first perform outlier removal followed by empirical risk minimization. Our bound comes paired with a candidate attack that nearly realizes the bound, giving us a powerful tool for quickly assessing defenses on a given dataset. Empirically, we find that even under a simple defense, the MNIST-1-7 and Dogfish datasets are resilient to attack, while in contrast the IMDB sentiment dataset can be driven from 12% to 23% test error by adding only 3% poisoned data.", "target": "Certified Defenses for Data Poisoning Attacks"}
{"id": "task1540-af4d5b68f9204e099cb7005315b999cb", "input": "Sequence-to-sequence neural network models for generation of conversational responses tend to generate safe, commonplace responses (e.g., I don\u2019t know) regardless of the input. We suggest that the traditional objective function, i.e., the likelihood of output (responses) given input (messages) is unsuited to response generation tasks. Instead we propose using Maximum Mutual Information (MMI) as objective function in neural models. Experimental results demonstrate that the proposed objective function produces more diverse, interesting, and appropriate responses, yielding substantive gains in BLEU scores on two conversational datasets.", "target": "A Diversity-Promoting Objective Function for Neural Conversation Models"}
{"id": "task1540-e1c5ef6d1c7645d89f2ffb0c46dd4841", "input": "In many applications data is naturally presented in terms of orderings of some basic elements or symbols. Reasoning about such data requires a notion of similarity capable of handling sequences of different lengths. In this paper we describe a family of Mercer kernel functions for such sequentially structured data. The family is characterized by a decomposable structure in terms of symbol-level and structure-level similarities, representing a specific combination of kernels which allows for efficient computation. We provide an experimental evaluation on sequential classification tasks comparing kernels from our family of kernels to a state of the art sequence kernel called the Global Alignment kernel which has been shown to outperform Dynamic Time Warping.", "target": "On a Family of Decomposable Kernels on Sequences On a Family of Decomposable Kernels on Sequences"}
{"id": "task1540-cc972eb250c3497ba4d581d9eb8e801b", "input": "Programs to solve so-called constraint problems are complex pieces of software which require many design decisions to be made more or less arbitrarily by the implementer. These decisions affect the performance of the finished solver significantly [13]. Once a design decision has been made, it cannot easily be reversed, although a different decision may be more appropriate for a particular problem. We investigate using machine learning to make these decisions automatically depending on the problem to solve with the alldifferent constraint as an example. Our system is capable of making non-trivial, multi-level decisions that improve over always making a default choice.", "target": "Using machine learning to make constraint solver implementation decisions"}
{"id": "task1540-5d05e620fe7141299aceb11de78999df", "input": "This paper explores several techniques for enhancing coverage when parsing with HPSG grammars, determines appropriate evaluation methods, and uses them to compare performance. Depending on the dataset, baseline coverage gaps can be reduced by between 75% and 100%, while simultaneously improving EDM F1 scores.", "target": "A Comparison of Robust Parsing Methods for HPSG"}
{"id": "task1540-2c13b80e874841eca49c20cc4d7d8e67", "input": "Solving algebraic word problems requires executing a series of arithmetic operations\u2014a program\u2014to obtain a final answer. However, since programs can be arbitrarily complicated, inducing them directly from question-answer pairs is a formidable challenge. To make this task more feasible, we solve these problems by generating answer rationales, sequences of natural language and human-readable mathematical expressions that derive the final answer through a series of small steps. Although rationales do not explicitly specify programs, they provide a scaffolding for their structure via intermediate milestones. To evaluate our approach, we have created a new 100,000-sample dataset of questions, answers and rationales. Experimental results show that indirect supervision of program learning via answer rationales is a promising strategy for inducing arithmetic programs.", "target": "Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems"}
{"id": "task1540-ffacb99b228e4e36af6bb36a7bff45a3", "input": "Despite significant developments in Proof Theory, surprisingly little attention has been devoted to the concept of proof verifier. In particular, mathematical community may be interested in studying different types of proof verifiers (people, programs, oracles, communities, superintelligences, etc.) as mathematical objects, their properties, their powers and limitations (particularly in human mathematicians), minimum and maximum complexity, as well as selfverification and self-reference issues in verifiers. We propose an initial classification system for verifiers and provide some rudimentary analysis of solved and open problems in this important domain. Our main contribution is a formal introduction of the notion of unverifiability, for which the paper could serve as a general citation in domains of theorem proving, software and AI verification.", "target": "Verifier Theory from Axioms to Unverifiability of Mathematical Proofs, Software and AI"}
{"id": "task1540-b897672ecf784c0c800967cf87d510ac", "input": "The choice of architecture of artificial neuron network (ANN) is still a challenging task that users face every time. It greatly affects the accuracy of the built network. In fact there is no optimal method that is applicable to various implementations at the same time. In this paper we propose a method to construct ANN based on clustering, that resolves the problems of random and ad\u2019hoc approaches for multilayer ANN architecture. Our method can be applied to regression problems. Experimental results obtained with different datasets, reveals the efficiency of our method.", "target": "Towards a constructive multilayer perceptron for regression task using non-parametric clustering. A case study of Photo-Z redshift reconstruction"}
{"id": "task1540-2da3afa5771f41f38d57dbd354b59d32", "input": "Gaussian state space models have been used for decades as generative models of sequential data. They admit an intuitive probabilistic interpretation, have a simple functional form, and enjoy widespread adoption. We introduce a unified algorithm to efficiently learn a broad class of linear and non-linear state space models, including variants where the emission and transition distributions are modeled by deep neural networks. Our learning algorithm simultaneously learns a compiled inference network and the generative model, leveraging a structured variational approximation parameterized by recurrent neural networks to mimic the posterior distribution. We apply the learning algorithm to both synthetic and real-world datasets, demonstrating its scalability and versatility. We find that using the structured approximation to the posterior results in models with significantly higher held-out likelihood.", "target": "Structured Inference Networks for Nonlinear State Space Models"}
{"id": "task1540-e5472d1c71e24607a11d50ab76aea950", "input": "Recently, the problem of local minima in very high dimensional non-convex optimization has been challenged and the problem of saddle points has been introduced. This paper introduces a dynamic type of normalization that forces the system to escape saddle points. Unlike other saddle point escaping algorithms, second order information is not utilized, and the system can be trained with an arbitrary gradient descent learner. The system drastically improves learning in a range of deep neural networks on various data-sets in comparison to non-CPN neural networks. 1 SADDLE POINT PROBLEM 1.", "target": "CHARGED POINT NORMALIZATION AN EFFICIENT SOLUTION TO THE SADDLE POINT PROBLEM"}
{"id": "task1540-b50e8dbb1dc544aebfbcccb0a02488d8", "input": "Without discourse connectives, classifying implicit discourse relations is a challenging task and a bottleneck for building a practical discourse parser. Previous research usually makes use of one kind of discourse framework such as PDTB or RST to improve the classification performance on discourse relations. Actually, under different discourse annotation frameworks, there exist multiple corpora which have internal connections. To exploit the combination of different discourse corpora, we design related discourse classification tasks specific to a corpus, and propose a novel Convolutional Neural Network embedded multi-task learning system to synthesize these tasks by learning both unique and shared representations for each task. The experimental results on the PDTB implicit discourse relation classification task demonstrate that our model achieves significant gains over baseline systems.", "target": "Implicit Discourse Relation Classification via Multi-Task Neural Networks"}
{"id": "task1540-7351b9d8698f407aa0e5fe4ecda5be87", "input": "Stemming or suffix stripping, an important part of the modern Information Retrieval systems, is to find the root word (stem) out of a given cluster of words. Existing algorithms targeting this problem have been developed in a haphazard manner. In this work, we model this problem as an optimization problem. An Integer Program is being developed to overcome the shortcomings of the existing approaches. The sample results of the proposed method are also being compared with an established technique in the field for English language. An AMPL code for the same IP has also been given.", "target": "Suffix Stripping Problem as an Optimization Problem"}
{"id": "task1540-3c1f5eea4e5c499598666d254aeeb927", "input": "In this paper we introduce Refractor Importance Sampling (RIS), an improvement to reduce error variance in Bayesian network importance sampling propagation under evidential reasoning. We prove the existence of a collection of importance functions that are close to the optimal importance function under evidential reasoning. Based on this theoretic result we derive the RIS algorithm. RIS approaches the optimal importance function by applying localized arc changes to minimize the divergence between the evidence-adjusted importance function and the optimal importance function. The validity and performance of RIS is empirically tested with a large set of synthetic Bayesian networks and two realworld networks.", "target": "Refractor Importance Sampling"}
{"id": "task1540-325c395351a74790a9f74ea2b9b0cc4b", "input": "While end-to-end neural machine translation (NMT) has made remarkable progress recently, NMT systems only rely on parallel corpora for parameter estimation. Since parallel corpora are usually limited in quantity, quality, and coverage, especially for low-resource languages, it is appealing to exploit monolingual corpora to improve NMT. We propose a semisupervised approach for training NMT models on the concatenation of labeled (parallel corpora) and unlabeled (monolingual corpora) data. The central idea is to reconstruct the monolingual corpora using an autoencoder, in which the sourceto-target and target-to-source translation models serve as the encoder and decoder, respectively. Our approach can not only exploit the monolingual corpora of the target language, but also of the source language. Experiments on the ChineseEnglish dataset show that our approach achieves significant improvements over state-of-the-art SMT and NMT systems.", "target": "Semi-Supervised Learning for Neural Machine Translation"}
{"id": "task1540-1b95f71cecbf4f4d95b8aaedb87c496d", "input": "There has been a long history of using fuzzy language equivalence to compare the behavior of fuzzy systems, but the comparison at this level is too coarse. Recently, a finer behavioral measure, bisimulation, has been introduced to fuzzy finite automata. However, the results obtained are applicable only to finite-state systems. In this paper, we consider bisimulation for general fuzzy systems which may be infinite-state or infiniteevent, by modeling them as fuzzy transition systems. To help understand and check bisimulation, we characterize it in three ways by enumerating whole transitions, comparing individual transitions, and using a monotonic function. In addition, we address composition operations, subsystems, quotients, and homomorphisms of fuzzy transition systems and discuss their properties connected with bisimulation. The results presented here are useful for comparing the behavior of general fuzzy systems. In particular, this makes it possible to relate an infinite fuzzy system to a finite one, which is easier to analyze, with the same behavior.", "target": "Bisimulations for Fuzzy Transition Systems"}
{"id": "task1540-c217c2adf1d143c7ac8c41dd2925ad78", "input": "Sarcasm is considered one of the most difficult problem in sentiment analysis. In our ob-servation on Indonesian social media, for cer-tain topics, people tend to criticize something using sarcasm. Here, we proposed two additional features to detect sarcasm after a common sentiment analysis is conducted. The features are the negativity information and the number of interjection words. We also employed translated SentiWordNet in the sentiment classification. All the classifications were conducted with machine learning algorithms. The experimental results showed that the additional features are quite effective in the sarcasm detection. Keywords\u2014 Sentimen analysis, sarcasm, classification,", "target": "Indonesian Social Media Sentiment Analysis with Sarcasm Detection"}
{"id": "task1540-7a7c3c5b9e474aba8d92d7a75ccc63d3", "input": "Besides spoken words, speech signals also carry information about speaker gender, age, and emotional state which can be used in a variety of speech analysis applications. In this paper, a divide and conquer strategy for ensemble classification has been proposed to recognize emotions in speech. Intrinsic hierarchy in emotions has been utilized to construct an emotions tree, which assisted in breaking down the emotion recognition task into smaller sub tasks. The proposed framework generates predictions in three phases. Firstly, emotions are detected in the input speech signal by classifying it as neutral or emotional. If the speech is classified as emotional, then in the second phase, it is further classified into positive and negative classes. Finally, individual positive or negative emotions are identified based on the outcomes of the previous stages. Several experiments have been performed on a widely used benchmark dataset. The proposed method was able to achieve improved recognition rates as compared to several other approaches.", "target": "Divide-and-Conquer based Ensemble to Spot Emotions in Speech using MFCC and Random Forest"}
{"id": "task1540-ab9c809b82ad4b009ac9de83d16585f2", "input": "Recently, there have been several promising methods to generate realistic imagery from deep convolutional networks. These methods sidestep the traditional computer graphics rendering pipeline and instead generate imagery at the pixel level by learning from large collections of photos (e.g. faces or bedrooms). However, these methods are of limited utility because it is difficult for a user to control what the network produces. In this paper, we propose a deep adversarial image synthesis architecture that is conditioned on coarse sketches and sparse color strokes to generate realistic cars, bedrooms, or faces. We demonstrate a sketch based image synthesis system which allows users to scribble over the sketch to indicate preferred color for objects. Our network can then generate convincing images that satisfy both the color and the sketch constraints of user. The network is feed-forward which allows users to see the effect of their edits in real time. We compare to recent work on sketch to image synthesis and show that our approach can generate more realistic, more diverse, and more controllable outputs. The architecture is also effective at user-guided colorization of grayscale images.", "target": "Scribbler: Controlling Deep Image Synthesis with Sketch and Color"}
{"id": "task1540-d2c1928dc2694d2eb93ecd7eedcbda4d", "input": "This paper discusses SYNTAGMA, a rule based NLP system addressing the tricky issues of syntactic ambiguity reduction and word sense disambiguation as well as providing innovative and original solutions for constituent generation and constraints management. To provide an insight into how it operates, the system's general architecture and components, as well as its lexical, syntactic and semantic resources are described. After that, the paper addresses the mechanism that performs selective parsing through an interaction between syntactic and semantic information, leading the parser to a coherent and accurate interpretation of the input text.", "target": "Syntax-Semantics Interaction Parsing Strategies. Inside SYNTAGMA"}
{"id": "task1540-2e5991a9f4f9434388635d5590599ce7", "input": "Training recurrent neural networks to model long term dependencies is difficult. Hence, we propose to use external linguistic knowledge as an explicit signal to inform the model which memories it should utilize. Specifically, external knowledge is used to augment a sequence with typed edges between arbitrarily distant elements, and the resulting graph is decomposed into directed acyclic subgraphs. We introduce a model that encodes such graphs as explicit memory in recurrent neural networks, and use it to model coreference relations in text. We apply our model to several text comprehension tasks and achieve new state-of-the-art results on all considered benchmarks, including CNN, bAbi, and LAMBADA. On the bAbi QA tasks, our model solves 15 out of the 20 tasks with only 1000 training examples per task. Analysis of the learned representations further demonstrates the ability of our model to encode fine-grained entity information across a document.", "target": "Linguistic Knowledge as Memory for Recurrent Neural Networks"}
{"id": "task1540-23e9a48dd8c54a1c8f3c0beff9734f22", "input": "\u200bThe \u200bElectromyography (EMG) signal is the electrical manifestation of a neuromuscular activation that provides access to physiological processes which cause the muscle to generate force and produce movement. Non-invasive prostheses use such signals detected by electrodes placed on the user\u2019s stump, as input to generate hand posture movements according to the intentions of the prosthesis wearer. The aim of this pilot study is to explore the repeatability issue, i.e. the ability to classify 17 different hand postures, represented by EMG signal, across a time span of days by a control algorithm. Data collection experiments lasted four days and signals were collected from the forearm of a single subject. We find that Support Vector Machine (SVM) classification results are high enough to guarantee a correct classification of more than 10 postures in each moment of the considered time span.", "target": "Studying the control of non-invasive prosthetic hands over large time spans"}
{"id": "task1540-a48782df04f243d486325dbda5b2cacf", "input": "Recently, recurrent neural networks (RNNs) as powerful sequence models have re-emerged as a potential acoustic model for statistical parametric speech synthesis (SPSS). The long short-term memory (LSTM) architecture is particularly attractive because it addresses the vanishing gradient problem in standard RNNs, making them easier to train. Although recent studies have demonstrated that LSTMs can achieve significantly better performance on SPSS than deep feedforward neural networks, little is known about why. Here we attempt to answer two questions: a) why do LSTMs work well as a sequence model for SPSS; b) which component (e.g., input gate, output gate, forget gate) is most important. We present a visual analysis alongside a series of experiments, resulting in a proposal for a simplified architecture. The simplified architecture has significantly fewer parameters than an LSTM, thus reducing generation complexity considerably without degrading quality.", "target": "INVESTIGATING GATED RECURRENT NETWORKS FOR SPEECH SYNTHESIS"}
{"id": "task1540-b2e766ffac0a49929eebb0de7e18766d", "input": "Significant efforts have been made to understand and document knowledge related to scientific measurements. Many of those efforts resulted in one or more high-quality ontologies that describe some aspects of scientific measurements, but not in a comprehensive and coherently integrated manner. For instance, we note that many of these high-quality ontologies are not properly aligned, and more challenging, that they have different and often conflicting concepts and approaches for encoding knowledge about empirical measurements. As a result of this lack of an integrated view, it is often challenging for scientists to determine whether any two scientific measurements were taken in semantically compatible manners, thus making it difficult to decide whether measurements should be analyzed in combination or not. In this paper, we present the Human-Aware Sensor Network Ontology that is a comprehensive alignment and integration of a sensing infrastructure ontology and a provenance ontology. HASNetO has been under development for more than one year, and has been reviewed, shared and used by multiple scientific communities. The ontology has been in use to support the data management of a number of large-scale ecological monitoring activities (observations) and empirical experiments.", "target": "Human-Aware Sensor Network Ontology: Semantic Support for Empirical Data Collection"}
{"id": "task1540-d3fc79b1b3b045e890147fc2e13bd20e", "input": "Recently, the rapid development of word embedding and neural networks has brought new inspiration to various NLP and IR tasks. In this paper, we describe a staged hybrid model combining Recurrent Convolutional Neural Networks (RCNN) with highway layers. The highway network module is incorporated in the middle takes the output of the bidirectional Recurrent Neural Network (Bi-RNN) module in the first stage and provides the Convolutional Neural Network (CNN) module in the last stage with the input. The experiment shows that our model outperforms common neural network models (CNN, RNN, Bi-RNN) on a sentiment analysis task. Besides, the analysis of how sequence length influences the RCNN with highway layers shows that our model could learn good representation for the long text.", "target": "Learning text representation using recurrent convolutional neural network with highway layers"}
{"id": "task1540-2a3830a1ca7e49bb92cf78add39d0c38", "input": "The practicality of a video surveillance system is adversely limited by the amount of queries that can be placed on human resources and their vigilance in response. To transcend this limitation, a major effort under way is to include software that (fully or at least semi) automatically mines video footage, reducing the burden imposed to the system. Herein, we propose a semi-supervised incremental learning framework for evolving visual streams in order to develop a robust and flexible track classification system. Our proposed method learns from consecutive batches by updating an ensemble in each time. It tries to strike a balance between performance of the system and amount of data which needs to be labelled. As no restriction is considered, the system can address many practical problems in an evolving multi-camera scenario, such as concept drift, class evolution and various length of video streams which have not been addressed before. Experiments were performed on synthetic as well as real-world visual data in non-stationary environments, showing high accuracy with fairly little human collaboration.", "target": "Active Mining of Parallel Video Streams"}
{"id": "task1540-da0c02fa3eb444b19448066dfd331483", "input": "This paper addresses the task of AMR-totext generation by leveraging synchronous node replacement grammar. During training, graph-to-string rules are learned using a heuristic extraction algorithm. At test time, a graph transducer is applied to collapse input AMRs and generate output sentences. Evaluated on SemEval-2016 Task 8, our method gives a BLEU score of 25.62, which is the best reported so far.", "target": "AMR-to-text Generation with Synchronous Node Replacement Grammar"}
{"id": "task1540-7e6a9d1cefd54896ae272f32ff2c8571", "input": "This papers shows that using separators, which is a pair of two complementary contractors, we can easily and efficiently solve the localization problem of a robot with sonar measurements in an unstructured environment. We introduce separators associated with the Minkowski sum and the Minkowski difference in order to facilitate the resolution. A test-case is given in order to illustrate the principle of the approach.", "target": "Minkowski Operations of Sets with Application to Robot Localization"}
{"id": "task1540-a336b29f635e48f99177a60df1868022", "input": "In this paper, we propose a novel unsupervised deep learning model, called PCA-based Convolutional Network (PCN). The architecture of PCN is composed of several feature extraction stages and a nonlinear output stage. Particularly, each feature extraction stage includes two layers: a convolutional layer and a feature pooling layer. In the convolutional layer, the filter banks are simply learned by PCA. In the nonlinear output stage, binary hashing is applied. For the higher convolutional layers, the filter banks are learned from the feature maps that were obtained in the previous stage. To test PCN, we conducted extensive experiments on some challenging tasks, including handwritten digits recognition, face recognition and texture classification. The results show that PCN performs competitive with or even better than state-of-theart deep learning models. More importantly, since there is no back propagation for supervised finetuning, PCN is much more efficient than existing deep networks.", "target": "A PCA-Based Convolutional Network"}
{"id": "task1540-b266c478ea1d484a81495e4b491bfdbc", "input": "As food becomes an important part of modern life, recipes shared on the web are a great indicator of civilizations and culinary attitudes in different countries. Similarly, ingredients, flavors, and nutrition information are strong signals of the taste preferences of individuals from various parts of the world. Yet, we do not have a thorough understanding of these palate varieties. In this paper, we present a large-scale study of recipes published on the Web and their content, aiming to understand cuisines and culinary habits around the world. Using a database of more than 157K recipes from over 200 different cuisines, we analyze ingredients, flavors, and nutritional values which distinguish dishes from different regions, and use this knowledge to assess the predictability of recipes from different cuisines. We then use country health statistics to understand the relation between these factors and health indicators of different nations, such as obesity, diabetes, migration, and health expenditure. Our results confirm the strong effects of geographical and cultural similarities on recipes, health indicators, and culinary preferences between countries around the world.", "target": "Kissing Cuisines: Exploring Worldwide Culinary Habits on the Web"}
{"id": "task1540-d5364ab9f3a1460889ac182daa614eb3", "input": "Whereas acausal Bayesian networks rep\u00ad resent probabilistic independence, causal Bayesian networks represent causal relation\u00ad ships. In this paper, we examine Bayesian methods for learning both types of networks. Bayesian methods for learning acausal net\u00ad works are fairly well developed. These meth\u00ad ods often employ assumptions to facilitate the construction of priors, including the as\u00ad sumptions of parameter independence, pa\u00ad rameter modularity, and likelihood equiva\u00ad lence. We show that although these assump\u00ad tions also can be appropriate for learning causal networks, we need additional assump\u00ad tions in order to learn causal networks. We introduce two sufficient assumptions, called mechanism independence and component in\u00ad dependence. We show that these new as\u00ad sumptions, when combined with parame\u00ad ter independence, parameter modularity, and likelihood equivalence, allow us to apply methods for learning acausal networks to learn causal networks.", "target": "A Bayesian Approach to Learning Causal Networks"}
{"id": "task1540-dff8907153f74b5aa0b312e1bfa6da59", "input": "With the recent proliferation of large-scale learning problems, there have been a lot of interest on distributed machine learning algorithms, particularly those that are based on stochastic gradient descent (SGD) and its variants. However, existing algorithms either suffer from slow convergence due to the inherent variance of stochastic gradients, or have a fast linear convergence rate but at the expense of poorer solution quality. In this paper, we combine their merits together by proposing a distributed asynchronous SGD-based algorithm with variance reduction. A constant learning rate can be used, and it is also guaranteed to converge linearly to the optimal solution. Experiments on the Google Cloud Computing Platform demonstrate that the proposed algorithm outperforms state-of-the-art distributed asynchronous algorithms in terms of both wall clock time and solution quality.", "target": "Fast Distributed Asynchronous SGD with Variance Reduction"}
{"id": "task1540-2e8b7604ee134ac4a7507a4dc9dd7ebb", "input": "In the present paper we show that distributional information is particularly important when considering concept availability under implicit language learning conditions. Based on results from different behavioural experiments we argue that the implicit learnability of semantic regularities depends on the degree to which the relevant concept is reflected in language use. In our simulations, we train a VectorSpace model on either an English or a Chinese corpus and then feed the resulting representations to a feed-forward neural network. The task of the neural network was to find a mapping between the word representations and the novel words. Using datasets from four behavioural experiments, which used different semantic manipulations, we were able to obtain learning patterns very similar to those obtained by humans.", "target": "A Distributional Semantics Approach to Implicit Language Learning"}
{"id": "task1540-47d9a4ed8f2c4157a2b1463029513bca", "input": "In this paper, we introduce evidence propagation operations on influence diagrams and a concept of value of evidence, which measures the value of experimentation. Evidence propagation operations are critical for the computation of the value of evidence, general update and inference operations in normative expert systems which are based on the influence diagram (generalized Bayesian network) paradigm. The value of evidence allows us to compute directly an outcome sensitivity, a value of perfect information and a value of control which are used in decision analysis (the science of decision making under uncertainty). More specifically, the outcome sensitivity is the maximum difference among the values of evidence, the value of perfect information is the expected value of the values of evidence, and the value of control is the optimal value of the values of evidence. We also discuss an implementation and a relative computational efficiency issues related to the value of evidence and the value of perfect information.", "target": "Value of Evidence on Influence Diagrams"}
{"id": "task1540-06d39fca64db4dd88367cda0882fd995", "input": "Quality control at each stage of production in textile industry has become a key factor to retaining the existence in the highly competitive global market. Problems of manual fabric defect inspection are lack of accuracy and high time consumption, where early and accurate fabric defect detection is a significant phase of quality control. Computer vision based, i.e. automated fabric defect inspection systems are thought by many researchers of different countries to be very useful to resolve these problems. There are two major challenges to be resolved to attain a successful automated fabric defect inspection system. They are defect detection and defect classification. In this work, we discuss different techniques used for automated fabric defect classification, then show a survey of classifiers used in automated fabric defect inspection systems, and finally, compare these classifiers by using performance metrics. This work is expected to be very useful for the researchers in the area of automated fabric defect inspection to understand and evaluate the many potential options in this field.", "target": "AUTOMATED FABRIC DEFECT INSPECTION: A SURVEY OF CLASSIFIERS"}
{"id": "task1540-dfb8a8d0eb6c45b5bf5e3ef0ecbfb265", "input": "Recently proposed neural network activation functions such as rectified linear, maxout, and local winner-take-all have allowed for faster and more effective training of deep neural architectures on large and complex datasets. The common trait among these functions is that they implement local competition between small groups of units within a layer, so that only part of the network is activated for any given input pattern. In this paper, we attempt to visualize and understand this self-modularization, and suggest a unified explanation for the beneficial properties of such networks. We also show how our insights can be directly useful for efficiently performing retrieval over large datasets using neural networks. A version of this paper was submitted to NIPS 2014 on 06-06-2014", "target": "Understanding Locally Competitive Networks"}
{"id": "task1540-52ebd5fbbed74f7386a9e064493c2681", "input": "Various tasks in decision making and decision support systems require selecting a preferred subset of a given set of items. Here we focus on problems where the individual items are described using a set of characterizing attributes, and a generic preference specification is required, that is, a specification that can work with an arbitrary set of items. For example, preferences over the content of an online newspaper should have this form: At each viewing, the newspaper contains a subset of the set of articles currently available. Our preference specification over this subset should be provided offline, but we should be able to use it to select a subset of any currently available set of articles, e.g., based on their tags. We present a general approach for lifting formalisms for specifying preferences over objects with multiple attributes into ones that specify preferences over subsets of such objects. We also show how we can compute an optimal subset given such a specification in a relatively efficient manner. We provide an empirical evaluation of the approach as well as some worst-case complexity results.", "target": "Generic Preferences over Subsets of Structured Objects"}
{"id": "task1540-1d4f35ade0c54cd98d727d2c00621946", "input": "In this paper, we propose the deep reinforcement relevance network (DRRN), a novel deep architecture, for handling an unbounded action space with applications to language understanding for text-based games. For a particular class of games, a user must choose among a variable number of actions described by text, with the goal of maximizing long-term reward. In these games, the best action is typically that which fits the best to the current situation (modeled as a state in the DRRN), also described by text. Because of the exponential complexity of natural language with respect to sentence length, there is typically an unbounded set of unique actions. Therefore, it is very difficult to pre-define the action set as in the deep Q-network (DQN). To address this challenge, the DRRN extracts high-level embedding vectors from the texts that describe states and actions, respectively, and computes the inner products between the state and action embedding vectors to approximate the Q-function. We evaluate the DRRN on two popular text games, showing superior performance over the DQN.", "target": "DEEP REINFORCEMENT LEARNING WITH AN UNBOUNDED ACTION SPACE"}
{"id": "task1540-96a8f4dd2f9042f586f14595628befe3", "input": "Scientists often run experiments to distinguish competing theories. This requires patience, rigor, and ingenuity\u2014there is often a large space of possible experiments one could run. But we need not comb this space by hand\u2014if we represent our theories as formal models and explicitly declare the space of experiments, we can automate the search for good experiments, looking for those with high expected information gain. Here, we present a general and principled approach to experiment design based on probabilistic programming languages (PPLs). PPLs offer a clean separation between declaring problems and solving them, which means that the scientist can automate experiment design by simply declaring her model and experiment spaces in the PPL without having to worry about the details of calculating information gain. We demonstrate our system in two case studies drawn from cognitive psychology, where we use it to design optimal experiments in the domains of sequence prediction and categorization. We find strong empirical validation that our automatically designed experiments were indeed optimal. We conclude by discussing a number of interesting questions for future research.", "target": "Practical optimal experiment design with probabilistic programs"}
{"id": "task1540-84c4f29b917549e1973c81a8bdc585db", "input": "We describe how to use propositional model counting for a quantitative analysis of product configuration data. Our approach computes valuable meta information such as the total number of valid configurations or the relative frequency of components. This information can be used to assess the severity of documentation errors or to measure documentation quality. As an application example we show how we apply these methods to product documentation formulas of the Mercedes-Benz line of vehicles. In order to process these large formulas we developed and implemented a new model counter for non-CNF formulas. Our model counter can process formulas, whose CNF representations could not be processed up till now.", "target": "Model Counting in Product Configuration"}
{"id": "task1540-5a4f3c8025a743cd9039b08ba0dd3c00", "input": "Vector-space representations provide geometric tools for reasoning about the similarity of a set of objects and their relationships. Recent machine learning methods for deriving vectorspace embeddings of words (e.g., word2vec) have achieved considerable success in natural language processing. These vector spaces have also been shown to exhibit a surprising capacity to capture verbal analogies, with similar results for natural images, giving new life to a classic model of analogies as parallelograms that was first proposed by cognitive scientists. We evaluate the parallelogram model of analogy as applied to modern word embeddings, providing a detailed analysis of the extent to which this approach captures human relational similarity judgments in a large benchmark dataset. We find that that some semantic relationships are better captured than others. We then provide evidence for deeper limitations of the parallelogram model based on the intrinsic geometric constraints of vector spaces, paralleling classic results for first-order similarity.", "target": "Evaluating vector-space models of analogy"}
{"id": "task1540-42a3a536d0f047c3bc608afc14574f82", "input": "We analyze in this paper a random feature map based on a theory of invariance (I-theory) introduced in [1]. More specifically, a group invariant signal signature is obtained through cumulative distributions of group transformed random projections. Our analysis bridges invariant feature learning with kernel methods, as we show that this feature map defines an expected Haar integration kernel that is invariant to the specified group action. We show how this non-linear random feature map approximates this group invariant kernel uniformly on a set of N points. Moreover, we show that it defines a function space that is dense in the equivalent Invariant Reproducing Kernel Hilbert Space. Finally, we quantify error rates of the convergence of the empirical risk minimization, as well as the reduction in the sample complexity of a learning algorithm using such an invariant representation for signal classification, in a classical supervised learning setting.", "target": "Learning with Group Invariant Features: A Kernel Perspective"}
{"id": "task1540-6a340965ef7f43f5a7bfbe789a3071c8", "input": "In this paper, we describe a methodology to infer Bullish or Bearish sentiment towards companies/brands. More specifically, our approach leverages affective lexica and word embeddings in combination with convolutional neural networks to infer the sentiment of financial news headlines towards a target company. Such architecture was used and evaluated in the context of the SemEval 2017 challenge (task 5, subtask 2), in which it obtained the best performance.", "target": "Fortia-FBK at SemEval-2017 Task 5: Bullish or Bearish? Inferring Sentiment towards Brands from Financial News Headlines"}
{"id": "task1540-504a0b67d33f45bcb867c5908d406a3b", "input": "Many classification problems involve data instances that are interlinked with each other, such as webpages connected by hyperlinks. Techniques for collective classification (CC) often increase accuracy for such data graphs, but usually require a fully-labeled training graph. In contrast, we examine how to improve the semi-supervised learning of CC models when given only a sparsely-labeled graph, a common situation. We first describe how to use novel combinations of classifiers to exploit the different characteristics of the relational features vs. the non-relational features. We also extend the ideas of label regularization to such hybrid classifiers, enabling them to leverage the unlabeled data to bias the learning process. We find that these techniques, which are efficient and easy to implement, significantly increase accuracy on three real datasets. In addition, our results explain conflicting findings from prior related studies.", "target": "Semi-Supervised Collective Classification via Hybrid Label Regularization"}
{"id": "task1540-0b62fe1528b646a885f23d623d98ac0c", "input": "Deep neural networks have proved very successful in domains where large training sets are available, but when the number of training samples is small, their performance suffers from overfitting. Prior methods of reducing overfitting such as weight decay, Dropout and DropConnect are data-independent. This paper proposes a new method, GraphConnect, that is data-dependent, and is motivated by the observation that data of interest lie close to a manifold. The new method encourages the relationships between the learned decisions to resemble a graph representing the manifold structure. Essentially GraphConnect is designed to learn attributes that are present in data samples in contrast to weight decay, Dropout and DropConnect which are simply designed to make it more difficult to fit to random error or noise. Empirical Rademacher complexity is used to connect the generalization error of the neural network to spectral properties of the graph learned from the input data. This framework is used to show that GraphConnect is superior to weight decay. Experimental results on several benchmark datasets validate the theoretical analysis, and show that when the number of training samples is small, GraphConnect is able to significantly improve performance over weight decay.", "target": "GraphConnect: A Regularization Framework for Neural Networks"}
{"id": "task1540-0d660a89545140a9b297ed00049b4c03", "input": "We consider apprenticeship learning \u2014 i.e., having an agent learn a task by observing an expert demonstrating the task \u2014 in a partially observable environment when the model of the environment is uncertain. This setting is useful in applications where the explicit modeling of the environment is difficult, such as a dialogue system. We show that we can extract information about the environment model by inferring action selection process behind the demonstration, under the assumption that the expert is choosing optimal actions based on knowledge of the true model of the target environment. Proposed algorithms can achieve more accurate estimates of POMDP parameters and better policies from a short demonstration, compared to methods that learns only from the reaction from the environment.", "target": "Apprenticeship Learning for Model Parameters of  Partially Observable Environments"}
{"id": "task1540-a85a764b9674483696779a58e3476507", "input": "The ubiquity of metaphor in our everyday communication makes it an important problem for natural language understanding. Yet, the majority of metaphor processing systems to date rely on handengineered features and there is still no consensus in the field as to which features are optimal for this task. In this paper, we present the first deep learning architecture designed to capture metaphorical composition. Our results demonstrate that it outperforms the existing approaches in the metaphor identification task.", "target": "Grasping the Finer Point: A Supervised Similarity Network for Metaphor Detection"}
{"id": "task1540-038eb25aa1174da288d76d782031f5e3", "input": "A commonly used learning rule is to approximately minimize the average loss over the training set. Other learning algorithms, such as AdaBoost and hard-SVM, aim at minimizing the maximal loss over the training set. The average loss is more popular, particularly in deep learning, due to three main reasons. First, it can be conveniently minimized using online algorithms, that process few examples at each iteration. Second, it is often argued that there is no sense to minimize the loss on the training set too much, as it will not be reflected in the generalization loss. Last, the maximal loss is not robust to outliers. In this paper we describe and analyze an algorithm that can convert any online algorithm to a minimizer of the maximal loss. We prove that in some situations better accuracy on the training set is crucial to obtain good performance on unseen examples. Last, we propose robust versions of the approach that can handle outliers.", "target": "Minimizing the Maximal Loss: How and Why"}
{"id": "task1540-ec44d46c0532483092a2d81e15a6bae5", "input": "We consider the question of extending propositional logic to a logic of plausible reasoning, and posit four requirements that any such extension should satisfy. Each is a requirement that some property of classical propositional logic be preserved in the extended logic; as such, the requirements are simpler and less problematic than those used in Cox\u2019s Theorem and its variants. As with Cox\u2019s Theorem, our requirements imply that the extended logic must be isomorphic to (finite-set) probability theory. We also obtain specific numerical values for the probabilities, recovering the classical definition of probability as a theorem, with truth assignments that satisfy the premise playing the role of the \u201cpossible cases.\u201d", "target": "From Propositional Logic to Plausible Reasoning: A Uniqueness Theorem"}
{"id": "task1540-0c2cf6ff005b4da3b9e3986978703705", "input": "Distributed dense word vectors have been shown to be effective at capturing tokenlevel semantic and syntactic regularities in language, while topic models can form interpretable representations over documents. In this work, we describe lda2vec, a model that learns dense word vectors jointly with Dirichlet-distributed latent document-level mixtures of topic vectors. In contrast to continuous dense document representations, this formulation produces sparse, interpretable document mixtures through a non-negative simplex constraint. Our method is simple to incorporate into existing automatic differentiation frameworks and allows for unsupervised document representations geared for use by scientists while simultaneously learning word vectors and the linear relationships between them.", "target": "Mixing Dirichlet Topic Models and Word Embeddings to Make lda2vec"}
{"id": "task1540-dd7c30102b1a4516aa28229ada75dfe0", "input": "Automated Essay Scoring (AES) has been quite popular and is being widely used. However, lack of appropriate methodology for rating nonnative English speakers\u2019 essays has meant a lopsided advancement in this field. In this paper, we report initial results of our experiments with nonnative AES that learns from manual evaluation of nonnative essays. For this purpose, we conducted an exercise in which essays written by nonnative English speakers in test environment were rated both manually and by the automated system designed for the experiment. In the process, we experimented with a few features to learn about nuances linked to nonnative evaluation. The proposed methodology of automated essay evaluation has yielded a correlation coefficient of 0.750 with the manual evaluation.", "target": "Exploring Automated Essay Scoring for Nonnative English Speakers"}
{"id": "task1540-f3e6c4510e2c4825a802a3f5634a4022", "input": "Motivated by the previously developed multilevel aggregation method for solving structural analysis problems a novel two-level aggregation approach for efficient iterative solution of Principal Component Analysis (PCA) problems is proposed. The course aggregation model of the original covariance matrix is used in the iterative solution of the eigenvalue problem by a power iterations method. The method is tested on several data sets consisting of large number of text documents.", "target": "ITERATIVE AGGREGATION METHOD FOR SOLVING PRINCIPAL COMPONENT ANALYSIS PROBLEMS"}
{"id": "task1540-36d827426dee44e2ac96eb4186b26cc3", "input": "The challenge in engaging malware activities involves the correct identification and classification of different malware variants. Various malwares incorporate code obfuscation methods that alters their code signatures effectively countering antimalware detection techniques utilizing static methods and signature database. In this study, we utilized an approach of converting a malware binary into an image and use Random Forest to classify various malware families. The resulting accuracy of 0.9562 exhibits the effectivess of the method in detecting malware.", "target": "Random Forest for Malware Classification"}
{"id": "task1540-47307c899989439eb2b1bd300cf0e752", "input": "We present algorithms for generating alternative solutions for explicit acyclic AND/OR structures in non-decreasing order of cost. The proposed algorithms use a best first search technique and report the solutions using an implicit representation ordered by cost. In this paper, we present two versions of the search algorithm \u2013 (a) an initial version of the best first search algorithm, ASG, which may present one solution more than once while generating the ordered solutions, and (b) another version, LASG, which avoids the construction of the duplicate solutions. The actual solutions can be reconstructed quickly from the implicit compact representation used. We have applied the methods on a few test domains, some of them are synthetic while the others are based on well known problems including the search space of the 5-peg Tower of Hanoi problem, the matrix-chain multiplication problem and the problem of finding secondary structure of RNA. Experimental results show the efficacy of the proposed algorithms over the existing approach. Our proposed algorithms have potential use in various domains ranging from knowledge based frameworks to service composition, where the AND/OR structure is widely used for representing problems.", "target": "Algorithms for Generating Ordered Solutions for Explicit AND/OR Structures"}
{"id": "task1540-27337b91ce914945b49549a27c7b75fd", "input": "We consider log-supermodular models on binary variables, which are probabilistic models with negative log-densities which are submodular. These models provide probabilistic interpretations of common combinatorial optimization tasks such as image segmentation. In this paper, we focus primarily on parameter estimation in the models from known upper-bounds on the intractable log-partition function. We show that the bound based on separable optimization on the base polytope of the submodular function is always inferior to a bound based on \u201cperturb-and-MAP\u201d ideas. Then, to learn parameters, given that our approximation of the log-partition function is an expectation (over our own randomization), we use a stochastic subgradient technique to maximize a lower-bound on the log-likelihood. This can also be extended to conditional maximum likelihood. We illustrate our new results in a set of experiments in binary image denoising, where we highlight the flexibility of a probabilistic model to learn with missing data.", "target": "Parameter Learning for Log-supermodular Distributions"}
{"id": "task1540-4373e1458f57432db5419104cc9851e7", "input": "We examine three different algorithms that enable the collision certificate method from [1] to handle the case of a centralized multi-robot team. By taking advantage of symmetries in the configuration space of multi-robot teams, our methods can significantly reduce the number of collision checks vs. both [1] and standard collision checking implementations.", "target": "Fast Collision Checking: From Single Robots to Multi-Robot Teams"}
{"id": "task1540-030f602cd57d4ea693018d6631c7c326", "input": "We show that there is a largely unexplored class of functions (positive polymatroids) that can define proper discrete metrics over pairs of binary vectors and that are fairly tractable to optimize over. By exploiting submodularity, we are able to give hardness results and approximation algorithms for optimizing over such metrics. Additionally, we demonstrate empirically the effectiveness of these metrics and associated algorithms on both a metric minimization task (a form of clustering) and also a metric maximization task (generating diverse k-best lists).", "target": "Submodular Hamming Metrics"}
{"id": "task1540-35aa6f842f714be3a8651b3096bcd60d", "input": "Answer Set Programming (ASP) is a well-established formalism for nonmonotonic reasoning. An ASP program can have no answer set due to cyclic default negation. In this case, it is not possible to draw any conclusion, even if this is not intended. Recently, several paracoherent semantics have been proposed that address this issue, and several potential applications for these semantics have been identified. However, paracoherent semantics have essentially been inapplicable in practice, due to the lack of efficient algorithms and implementations. In this paper, this lack is addressed, and several different algorithms to compute semi-stable and semi-equilibrium models are proposed and implemented into an answer set solving framework. An empirical performance comparison among the new algorithms on benchmarks from ASP competitions is given as well.", "target": "On the Computation of Paracoherent Answer Sets"}
{"id": "task1540-cdf64ce21497467da4f849c62b0c3e5c", "input": "We develop a worst-case analysis of aggregation of binary classifier ensembles in a transductive setting, for a broad class of losses including but not limited to all convex surrogates. The result is a family of parameter-free ensemble aggregation algorithms, which are as efficient as linear learning and prediction for convex risk minimization but work without any relaxations whatsoever on many nonconvex losses like the 0-1 loss. The prediction algorithms take a familiar form, applying \u201clink functions\" to a generalized notion of ensemble margin, but without the assumptions typically made in margin-based learning \u2013 all this structure follows from a minimax interpretation of loss minimization.", "target": "Minimax Binary Classifier Aggregation with General Losses"}
{"id": "task1540-02fd3b4fb6514c1f88855ae0f61d1eab", "input": "We take another look at the general problem of selecting a preferred probability measure among those that comply with some given constraints. The dominant role that entropy maximization has obtained in this context is questioned by argu\u00ad ing that the minimum information principle on which it is based could be supplanted by an at least as plausible \"likelihood of evidence\" prin\u00ad ciple. We then review a method for turning given selection functions into representation indepen\u00ad dent variants, and discuss the tradeoffs involved in this transformation.", "target": "Measure Selection: Notions of Rationality and Representation Independence"}
{"id": "task1540-9dae648edef84406b523457e347b3003", "input": "Submodular functions describe a variety of discrete problems in machine learn-<lb>ing, signal processing, and computer vision. However, minimizing submodular<lb>functions poses a number of algorithmic challenges. Recent work introduced an<lb>easy-to-use, parallelizable algorithm for minimizing submodular functions that<lb>decompose as the sum of \u201csimple\u201d submodular functions. Empirically, this al-<lb>gorithm performs extremely well, but no theoretical analysis was given. In this<lb>paper, we show that the algorithm converges linearly, and we provide upper and<lb>lower bounds on the rate of convergence. Our proof relies on the geometry of<lb>submodular polyhedra and draws on results from spectral graph theory.", "target": "On the Convergence Rate of Decomposable Submodular Function Minimization"}
{"id": "task1540-0c92a1de0fe34584acc1249252c38d71", "input": "A language independent Stemmer has always been looked for. Single N-gram tokenization technique works well, however, it often generates stems that start with intermediate characters, rather than initial ones. We present a novel technique that takes the concept of N-grams one step ahead and compare our method with an established algorithm in the fieldPorter\u2019s Stemmer. Porter\u2019s Stemmer is language dependent, and performance of our proposed method is not inferior to it.", "target": "Generation, Implementation and Appraisal of a Language Independent Stemming Algorithm"}
{"id": "task1540-b0ddad2a938045888a8eef35d98a6964", "input": "In this thesis, we study the problem of recognizing video sequences of fingerspelled letters in American Sign Language (ASL). Fingerspelling comprises a significant but relatively understudied part of ASL, and recognizing it is challenging for a number of reasons: It involves quick, small motions that are often highly coarticulated; it exhibits significant variation between signers; and there has been a dearth of continuous fingerspelling data collected. In this work, we propose several types of recognition approaches, and explore the signer variation problem. Our best-performing models are segmental (semi-Markov) conditional random fields using deep neural network-based features. In the signer-dependent setting, our recognizers achieve up to about 8% letter error rates. The signer-independent setting is much more challenging, but with neural network adaptation we achieve up to 17% letter error rates. Thesis Supervisor: Karen Livescu Title: Assistant Professor", "target": "American Sign Language fingerspelling recognition from video: Methods for unrestricted recognition and signer-independence"}
{"id": "task1540-f956da5f1ee04a47a2fe289a17ea641d", "input": "Knowledge representation is a popular research field in IT. As mathematical knowledge is most formalized, its representation is important and interesting. Mathematical knowledge consists of various mathematical theories. In this paper we consider a deductive system that derives mathematical notions, axioms and theorems. All these notions, axioms and theorems can be considered a small mathematical theory. This theory will be represented as a semantic net. We start with the signature <Set; > where Set is the support set, \uf02dis the membership predicate. Using the MathSem program we build the signature <Set; \uf020\uf03e\uf020\uf02c\uf020\uf020\uf020\uf020 where \uf020\uf02d\uf020is set intersection,\uf020 \uf02d is set union, -is the Cartesian product of sets, and \uf02d\uf020is the subset relation.", "target": "Building the Signature of Set Theory Using the MathSem Program"}
{"id": "task1540-5d4b479a63a34b79b1baba4673fbef1d", "input": "We propose a new fast word embedding technique using hash functions. The method is a derandomization of a new type of random projections: By disregarding the classic constraint used in designing random projections (i.e., preserving pairwise distances in a particular normed space), our solution exploits extremely sparse non-negative random projections. Our experiments show that the proposed method can achieve competitive results, comparable to neural embedding learning techniques, however, with only a fraction of the computational complexity of these methods. While the proposed derandomization enhances the computational and space complexity of our method, the possibility of applying weighting methods such as positive pointwise mutual information (PPMI) to our models after their construction (and at a reduced dimensionality) imparts a high discriminatory power to the resulting embeddings. Obviously, this method comes with other known benefits of random projection-based techniques such as ease of update.", "target": "Sketching Word Vectors Through Hashing"}
{"id": "task1540-095a7b3aecf042c19d1dd484ece125b8", "input": "Deep reinforcement learning (RL) has achieved several high profile successes in difficult control problems. However, these algorithms typically require a huge amount of data before they reach reasonable performance. In fact, their performance during learning can be extremely poor. This may be acceptable for a simulator, but it severely limits the applicability of deep RL to many real-world tasks, where the agent must learn in the real environment. In this paper we study a setting where the agent may access data from previous control of the system. We present an algorithm, Deep Q-learning from Demonstrations (DQfD), that leverages this data to massively accelerate the learning process even from relatively small amounts of demonstration data. DQfD works by combining temporal difference updates with large-margin classification of the demonstrator\u2019s actions. We show that DQfD has better initial performance than Deep Q-Networks (DQN) on 40 of 42 Atari games and it receives more average rewards than DQN on 27 of 42 Atari games. We also demonstrate that DQfD learns faster than DQN even when given poor demonstration data.", "target": "Learning from Demonstrations for Real World Reinforcement Learning"}
{"id": "task1540-80a1b85f028c4f63bf89dadadb6e52a8", "input": "Data can be acquired, shared, and processed by an increasingly larger number of entities, in particular people. The distributed nature of this phenomenon has contributed to the development of many crowdsourcing projects. This scenario is prevalent in most forms of expert/non-expert group opinion and rating tasks (including many forms of internet or on-line user behavior), where a key element is the aggregation of observations-opinions from multiple sources.", "target": "Evaluating Crowdsourcing Participants in the Absence of Ground-Truth"}
{"id": "task1540-6d226de836b54a25b22b4fc2f71c1ed8", "input": "Neural Turing Machines (NTM) [2] contain memory component that simulates \u201cworking memroy\u201d in the brain to store and retrieve information to ease simple algorithms learning. So far, only linearly organized memory is proposed, and during experiments, we observed that the model does not always converge, and overfits easily when handling certain tasks. We think memory component is key to some faulty behaviors of NTM, and better organization of memory component could help fight those problems. In this paper, we propose several different structures of memory for NTM, and we proved in experiments that two of our proposed structured-memory NTMs could lead to better convergence, in term of speed and prediction accuracy on copy task and associative recall task as in [2].", "target": "Structured Memory for Neural Turing Machines"}
{"id": "task1540-75d7b0e35fa744d98c33fa64d0acdb62", "input": "An RDF data shape is a description of the expected contents of an RDF document (aka graph) or dataset. A major part of this description is the set of constraints that the document or dataset is required to satisfy. W3C recently (2014) chartered the RDF Data Shapes Working Group to define SHACL, a standard RDF data shape language. We refer to the ability to name and reference shape language elements as recursion. This article provides a precise definition of the meaning of recursion as used in Resource Shape 2.0. The definition of recursion presented in this article is largely independent of language-specific details. We speculate that it also applies to ShEx and to all three of the current proposals for SHACL. In particular, recursion is not permitted in the SHACL-SPARQL proposal, but we conjecture that recursion could be added by using the definition proposed here as a top-level control structure.", "target": "Recursion in RDF Data Shape Languages"}
{"id": "task1540-73852f12094f4a07b450fa782f6e9874", "input": "Achieving joint objectives by teams of cooperative planning agents requires significant coordination and communication efforts. For a singleagent system facing a plan failure in a dynamic environment, arguably, attempts to repair the failed plan in general do not straightforwardly bring any benefit in terms of time complexity. However, in multi-agent settings the communication complexity might be of a much higher importance, possibly a high communication overhead might be even prohibitive in certain domains. We hypothesize that in decentralized systems, where coordination is enforced to achieve joint objectives, attempts to repair failed multi-agent plans should lead to lower communication overhead than replanning from scratch. The contribution of the presented paper is threefold. Firstly, we formally introduce the multi-agent plan repair problem and formally present the core hypothesis underlying our work. Secondly, we propose three algorithms for multi-agent plan repair reducing the problem to specialized instances of the multi-agent planning problem. Finally, we present results of experimental validation confirming the core hypothesis of the paper.", "target": "Decentralized Multi-agent Plan Repair in Dynamic Environments\u2217"}
{"id": "task1540-1664deb0ca7b473281c97315252a7e21", "input": "This paper studies the applicability of evolutionary algorithms, particularly, the evolution strategies family to estimation of a degradation parameter (referred as kappa parameter) for the shear design of reinforced concrete beams, a problem which have an expensive computational cost and highly relevant in the design of pillars and reinforced concrete structures, which however, has not been covered extensively in the present literature.", "target": "\u201cANA\u0301LISIS E IMPLEMENTACIO\u0301N DE ALGORITMOS EVOLUTIVOS PARA LA OPTIMIZACIO\u0301N DE SIMULACIONES EN INGENIERI\u0301A CIVIL.\u201d"}
{"id": "task1540-144c4db3bff6473cbed47514f1728c32", "input": "Label propagation is a powerful and flexible semi-supervised learning technique on graphs. Neural network architectures, on the other hand, have proven track records in many supervised learning tasks. In this work, we propose a training objective for neural networks, Neural Graph Machines, for combining the power of neural networks and label propagation. The new objective allows the neural networks to harness both labeled and unlabeled data by: (a) allowing the network to train using labeled data as in the supervised setting, (b) biasing the network to learn similar hidden representations for neighboring nodes on a graph, in the same vein as label propagation. Such architectures with the proposed objective can be trained efficiently using stochastic gradient descent and scaled to large graphs. The proposed method is experimentally validated on a wide range of tasks (multilabel classification on social graphs, news categorization and semantic intent classification) using different architectures (NNs, CNNs, and LSTM RNNs).", "target": "NEURAL GRAPH MACHINES: LEARNING NEURAL NETWORKS USING GRAPHS"}
{"id": "task1540-989015f2063d48ec80e5ae81928887b0", "input": "The vocabulary mismatch problem is a long-standing problem in information retrieval. Semantic matching holds the promise of solving the problem. Recent advances in language technology have given rise to unsupervised neural models for learning representations of words as well as bigger textual units. Such representations enable powerful semantic matching methods. This survey is meant as an introduction to the use of neural models for semantic matching. To remain focused we limit ourselves to web search. We detail the required background and terminology, a taxonomy grouping the rapidly growing body of work in the area, and then survey work on neural models for semantic matching in the context of three tasks: query suggestion, ad retrieval, and document retrieval. We include a section on resources and best practices that we believe will help readers who are new to the area. We conclude with an assessment of the state-of-the-art and suggestions for future work.", "target": "Getting Started with Neural Models for Semantic Matching in Web Search"}
{"id": "task1540-2255609d69ba4ec1ab330fbe822f58c7", "input": "The understanding of the buildings operation has become a challenging task due to the large amount of data recorded in energy efficient buildings. Still, today the experts use visual tools for analyzing the data. In order to make the task realistic, a method has been proposed in this paper to automatically detect the different patterns in buildings. The K-Means clustering is used to automatically identify the ON (operational) cycles of the chiller. In the next step the ON cycles are transformed to symbolic representation by using Symbolic Aggregate Approximation (SAX) method. Then the SAX symbols are converted to bag of words representation for hierarchical clustering. Moreover, the proposed technique is applied to real life data of adsorption chiller. Additionally, the results from the proposed method and dynamic time warping (DTW) approach are also discussed and compared. Keywords\u2014 Building energy performance; Fault detection and diagnosis (FDD); clustering; symbolic aggregate approximation (SAX); Bag of words representation (BoWR); hierarchical clustering; Dynamic time warping (DTW); Coefficient of Performance (COP)", "target": "Finding the different patterns in buildings data using bag of words representation with clustering"}
{"id": "task1540-98e4951443494f24861f0b2a1e66888a", "input": "Given samples from a distribution, how many new elements should we expect to find if we continue sampling this distribution? This is an important and actively studied problem, with many applications ranging from unseen species estimation to genomics. We generalize this extrapolation and related unseen estimation problems to the multiple population setting, where population j has an unknown distributionDj from which we observe nj samples. We derive an optimal estimator for the total number of elements we expect to find among new samples across the populations. Surprisingly, we prove that our estimator\u2019s accuracy is independent of the number of populations. We also develop an efficient optimization algorithm to solve the more general problem of estimating multi-population frequency distributions. We validate our methods and theory through extensive experiments. Finally, on a real dataset of human genomes across multiple ancestries, we demonstrate how our approach for unseen estimation can enable cohort designs that can discover interesting mutations with greater efficiency.", "target": "Estimating the unseen from multiple populations"}
{"id": "task1540-2bd7c18b97184b4fb922eb641f7cb55b", "input": "Ensembling is a well-known technique in neural machine translation (NMT) to improve system performance. Instead of a single neural net, multiple neural nets with the same topology are trained separately, and the decoder generates predictions by averaging over the individual models. Ensembling often improves the quality of the generated translations drastically. However, it is not suitable for production systems because it is cumbersome and slow. This work aims to reduce the runtime to be on par with a single system without compromising the translation quality. First, we show that the ensemble can be unfolded into a single large neural network which imitates the output of the ensemble system. We show that unfolding can already improve the runtime in practice since more work can be done on the GPU. We proceed by describing a set of techniques to shrink the unfolded network by reducing the dimensionality of layers. On JapaneseEnglish we report that the resulting network has the size and decoding speed of a single NMT network but performs on the level of a 3-ensemble system.", "target": "Unfolding and Shrinking Neural Machine Translation Ensembles"}
{"id": "task1540-777e9345097d410e9ea4d85eb7114b07", "input": "The historic background of algorithmic processing with regard to etymology and methodology is translated into terms of mathematical logic and Computer Science. A formal logic structure is introduced by exemplary questions posed to Fiqh-chapters to define a logic query language. As a foundation, a generic algorithm for deciding Fiqhrulings is designed to enable and further leverage rule of law (vs. rule by law) with full transparency and complete algorithmic coverage of Islamic law eventually providing legal security, legal equality, and full legal accountability. This is implemented by disentangling and reinstating classic Fiqh-methodology (usul al-Fiqh) with the expressive power of subsets of First Order Logic (FOL) sustainably substituting ad hoc reasoning with falsifiable rational argumentation. The results are discussed in formal terms of completeness, decidability and complexity of formal Fiqh-systems. An Entscheidungsproblem for formal Fiqh-Systems is formulated and validated.", "target": "The Algorithm of Islamic Jurisprudence (Fiqh) with Validation of an Entscheidungsproblem"}
{"id": "task1540-10d25a05a31c4fbaa882a8d7ada2879c", "input": "Chinese word segmentation is a fundamental task for Chinese language processing. The granularity mismatch problem is the main cause of the errors. This paper showed that the binary tree representation can store outputs with different granularity. A binary tree based framework is also designed to overcome the granularity mismatch problem. There are two steps in this framework, namely tree building and tree pruning. The tree pruning step is specially designed to focus on the granularity problem. Previous work for Chinese word segmentation such as the sequence tagging can be easily employed in this framework. This framework can also provide quantitative error analysis methods. The experiments showed that after using a more sophisticated tree pruning function for a state-of-the-art conditional random field based baseline, the error reduction can be up to 20%.", "target": "Binary Tree based Chinese Word Segmentation"}
{"id": "task1540-7759e0091f6d4cf6bd6cdc070fe22687", "input": "This paper investigates how linguistic knowledge mined from large text corpora can aid the generation of natural language descriptions of videos. Specifically, we integrate both a neural language model and distributional semantics trained on large text corpora into a recent LSTM-based architecture for video description. We evaluate our approach on a collection of Youtube videos as well as two large movie description datasets showing significant improvements in grammaticality while modestly improving descriptive quality.", "target": "Improving LSTM-based Video Description with Linguistic Knowledge Mined from Text"}
{"id": "task1540-8acfd1b5c96c4713b8f04cd05fc28e29", "input": "This paper presents to the best of our knowledge the first end-to-end object tracking approach which directly maps from raw sensor input to object tracks in sensor space without requiring any feature engineering or system identification in the form of plant or sensor models. Specifically, our system accepts a stream of raw sensor data at one end and, in real-time, produces an estimate of the entire environment state at the output including even occluded objects. We achieve this by framing the problem as a deep learning task and exploit sequence models in the form of recurrent neural networks to learn a mapping from sensor measurements to object tracks. In particular, we propose a learning method based on a form of input dropout which allows learning in an unsupervised manner, only based on raw, occluded sensor data without access to ground-truth annotations. We demonstrate our approach using a synthetic dataset designed to mimic the task of tracking objects in 2D laser data \u2013 as commonly encountered in robotics applications \u2013 and show that it learns to track many dynamic objects despite occlusions and the presence of sensor noise.", "target": "Deep Tracking: Seeing Beyond Seeing Using Recurrent Neural Networks"}
{"id": "task1540-e63cbeddad744b8d9fc0dcdc9d1ea261", "input": "We propose a general method called truncated gradient to induce sparsity in the weights of online learning algorithms with convex loss functions. This method has several essential properties: 1. The degree of sparsity is continuous a parameter controls the rate of sparsi cation from no sparsi cation to total sparsi cation. 2. The approach is theoretically motivated, and an instance of it can be regarded as an online counterpart of the popular L1-regularization method in the batch setting. We prove that small rates of sparsi cation result in only small additional regret with respect to typical online learning guarantees. 3. The approach works well empirically. We apply the approach to several datasets and nd that for datasets with large numbers of features, substantial sparsity is discoverable.", "target": "Sparse Online Learning via Truncated Gradient"}
{"id": "task1540-855c5337ca37404ba8410884c38252da", "input": "This paper describes pre-processing phase of ontology graph generation system from Punjabi text documents of different domains. This research paper focuses on pre-processing of Punjabi text documents. Pre-processing is structured representation of the input text. Pre-processing of ontology graph generation includes allowing input restrictions to the text, removal of special symbols and punctuation marks, removal of duplicate terms, removal of stop words, extract terms by matching input terms with dictionary and gazetteer lists terms. KeywordsOntology, Pre-processing phase, Ontology Graph, Knowledge Representation, Natural Language Processing.", "target": "Pre-processing of Domain Ontology Graph Generation System in Punjabi"}
{"id": "task1540-17e703fb7ed445629a62f1779243d48e", "input": "This paper provides a global vision of the scientific publications related with the Systemic Lupus Erythematosus (SLE), taking as starting point abstracts of articles. Through the time, abstracts have been evolving towards higher complexity on used terminology, which makes necessary the use of sophisticated statistical methods and answering questions including: how vocabulary is evolving through the time? Which ones are most influential articles? And which one are the articles that introduced new terms and vocabulary? To answer these, we analyze a dataset composed by 506 abstracts and downloaded from 115 different journals and cover a 18 year-period.", "target": "How scientific literature has been evolving over the time? A novel statistical approach using tracking verbal-based methods"}
{"id": "task1540-5b7099fea3204afdbb21a56fc921f7d2", "input": "We propose Batch-Expansion Training (BET), a framework for running a batch optimizer on a gradually expanding dataset. As opposed to stochastic approaches, batches do not need to be resampled i.i.d. at every iteration, thus making BET more resource efficient in a distributed setting, and when disk-access is constrained. Moreover, BET can be easily paired with most batch optimizers, does not require any parameter-tuning, and compares favorably to existing stochastic and batch methods. We show that when the batch size grows exponentially with the number of outer iterations, BET achieves optimal \u00d5(1/\u01eb) data-access convergence rate for strongly convex objectives.", "target": "Batch-Expansion Training: An Efficient Optimization Paradigm for Machine Learning"}
{"id": "task1540-fd8593f18aff41d386c829a9eca6e9bf", "input": "Understanding language goes hand in hand with the ability to integrate complex contextual information obtained via perception. In this work, we present a novel task for grounded language understanding: disambiguating a sentence given a visual scene which depicts one of the possible interpretations of that sentence. To this end, we introduce a new multimodal corpus containing ambiguous sentences, representing a wide range of syntactic, semantic and discourse ambiguities, coupled with videos that visualize the different interpretations for each sentence. We address this task by extending a vision model which determines if a sentence is depicted by a video. We demonstrate how such a model can be adjusted to recognize different interpretations of the same underlying sentence, allowing to disambiguate sentences in a unified fashion across the different ambiguity types.", "target": "Do You See What I Mean? Visual Resolution of Linguistic Ambiguities"}
{"id": "task1540-37c84d36fd224c948ca8b126e4c02fbc", "input": "The UCT algorithm, which combines the UCB algorithm and Monte-Carlo Tree Search (MCTS), is currently the most widely used variant of MCTS. Recently, a number of investigations into applying other bandit algorithms to MCTS have produced interesting results. In this research, we will investigate the possibility of combining the improved UCB algorithm, proposed by Auer et al. [2], with MCTS. However, various characteristics and properties of the improved UCB algorithm may not be ideal for a direct application to MCTS. Therefore, some modifications were made to the improved UCB algorithm, making it more suitable for the task of game tree search. The Mi-UCT algorithm is the application of the modified UCB algorithm applied to trees. The performance of Mi-UCT is demonstrated on the games of 9\u00d7 9 Go and 9\u00d7 9 NoGo, and has shown to outperform the plain UCT algorithm when only a small number of playouts are given, and rougly on the same level when more playouts are available.", "target": "Adapting Improved Upper Confidence Bounds for Monte-Carlo Tree Search"}
{"id": "task1540-1157a41132744c2699d40f27e1426501", "input": "Semantic segmentation requires a detailed labeling of image pixels by object category. Information derived from local image patches is necessary to describe the detailed shape of individual objects. However, this information is ambiguous and can result in noisy labels. Global inference of image content can instead capture the general semantic concepts present. We advocate that holistic inference of image concepts provides valuable information for detailed pixel labeling. We propose a generic framework to leverage holistic information in the form of a LabelBank for pixellevel segmentation. We show the ability of our framework to improve semantic segmentation performance in a variety of settings. We learn models for extracting a holistic LabelBank from visual cues, attributes, and/or textual descriptions. We demonstrate improvements in semantic segmentation accuracy on standard datasets across a range of state-of-the-art segmentation architectures and holistic inference approaches.", "target": "LabelBank: Revisiting Global Perspectives for Semantic Segmentation"}
{"id": "task1540-6492baf7ce264b9f9184a5c02ac446d5", "input": "This paper presents complexity analysis and variational methods for inference in probabilistic description logics featuring Boolean operators, quantification, qualified number restrictions, nominals, inverse roles and role hierarchies. Inference is shown to be PEXP-complete, and variational methods are designed so as to exploit logical inference whenever possible.", "target": "Complexity Analysis and Variational Inference for Interpretation-based Probabilistic Description Logics"}
{"id": "task1540-549c48e7e22e41f585800d8cf7c63a86", "input": "5", "target": "Leveraging over priors for boosting control of prosthetic hands"}
{"id": "task1540-0d6c349d9b4940bbaf4861f739f9f5d9", "input": "It is well known that different solution strategies work well for different types of instances of hard combinatorial problems. As a consequence, most solvers for the propositional satisfiability problem (SAT) expose parameters that allow them to be customized to a particular family of instances. In the international SAT competition series, these parameters are ignored: solvers are run using a single default parameter setting (supplied by the authors) for all benchmark instances in a given track. While this competition format rewards solvers with robust default settings, it does not reflect the situation faced by a practitioner who only cares about performance on one particular application and can invest some time into tuning solver parameters for this application. The new Configurable SAT Solver Competition (CSSC) compares solvers in this latter setting, scoring each solver by the performance it achieved after a fully automated configuration step. This article describes the CSSC in more detail, and reports the results obtained in its two instantiations so far, CSSC 2013 and 2014.", "target": "The Configurable SAT Solver Challenge (CSSC)"}
{"id": "task1540-a9647036f93442c1aae2a3315afb515b", "input": "Measuring the naturalness of images is important to generate realistic images or to detect unnatural regions in images. Additionally, a method to measure naturalness can be complementary to Convolutional Neural Network (CNN) based features, which are known to be insensitive to the naturalness of images. However, most probabilistic image models have insufficient capability of modeling the complex and abstract naturalness that we feel because they are built directly on raw image pixels. In this work, we assume that naturalness can be measured by the predictability on high-level features during eye movement. Based on this assumption, we propose a novel method to evaluate the naturalness by building a variant of Recurrent Neural Network Language Models on pre-trained CNN representations. Our method is applied to two tasks, demonstrating that 1) using our method as a regularizer enables us to generate more understandable images from image features than existing approaches, and 2) unnaturalness maps produced by our method achieve state-of-the-art eye fixation prediction performance on two well-studied datasets.", "target": "Visual Language Modeling on CNN Image Representations"}
{"id": "task1540-74ae1b3396be46b2b2acd504e47ca0bc", "input": "The weighted Maximum Satisfiability problem (weighted MAX-SAT) is a NP-hard problem with numerous applications arising in artificial intelligence. As an efficient tool for heuristic design, the backbone has been applied to heuristics design for many NP-hard problems. In this paper, we investigated the computational complexity for retrieving the backbone in weighted MAX-SAT and developed a new algorithm for solving this problem. We showed that it is intractable to retrieve the full backbone under the assumption that NP P \uf0b9 . Moreover, it is intractable to retrieve a fixed fraction of the backbone as well. And then we presented a backbone guided local search (BGLS) with Walksat operator for weighted MAX-SAT. BGLS consists of two phases: the first phase samples the backbone information from local optima and the backbone phase conducts local search under the guideline of backbone. Extensive experimental results on the benchmark showed that BGLS outperforms the existing heuristics in both solution quality and runtime.", "target": "Approximating the Backbone in the Weighted Maximum Satisfiability Problem"}
{"id": "task1540-c2ca48b89a184c568cb0c99731e157dd", "input": "With applications to many disciplines, the traveling salesman problem (TSP) is a classical computer science optimization problem with applications to industrial engineering, theoretical computer science, bioinformatics, and several other disciplines [2]. In recent years, there have been a plethora of novel approaches for approximate solutions ranging from simplistic greedy to cooperative distributed algorithms derived from artificial intelligence. In this paper, we perform an evaluation and analysis of cornerstone algorithms for the Euclidean TSP. We evaluate greedy, 2opt, and genetic algorithms. We use several datasets as input for the algorithms including a small dataset, a mediumsized dataset representing cities in the United States, and a synthetic dataset consisting of 200 cities to test algorithm scalability. We discover that the greedy and 2-opt algorithms efficiently calculate solutions for smaller datasets. Genetic algorithm has the best performance for optimality for medium to large datasets, but generally have longer runtime. Our implementations is public available 1.", "target": "An Empirical Analysis of Approximation Algorithms for the Euclidean Traveling Salesman Problem"}
{"id": "task1540-ddf2c8ae366d436aa9d81fa6758f0e4a", "input": "A new neuro-fuzzy system\u2019s architecture and a learning method that adjusts its weights as well as automatically determines a number of neurons, centers\u2019 location of membership functions and the receptive field\u2019s parameters in an online mode with high processing speed is proposed in this paper. The basic idea of this approach is to tune both synaptic weights and membership functions with the help of the supervised learning and self-learning paradigms. The approach to solving the problem has to do with evolving online neuro-fuzzy systems that can process data under uncertainty conditions. The results proves the effectiveness of the developed architecture and the learning procedure.", "target": "An Evolving Neuro-Fuzzy System with Online Learning/Self-learning"}
{"id": "task1540-66df67429e3e4437b667a78d8f8af83e", "input": "Data preprocessing is a fundamental part of any machine learning application and frequently the most time-consuming aspect when developing a machine learning solution. Preprocessing for deep learning is characterized by pipelines that lazily load data and perform data transformation, augmentation, batching and logging. Many of these functions are common across applications but require different arrangements for training, testing or inference. Here we introduce a novel software framework named nuts-flow/ml that encapsulates common preprocessing operations as components, which can be flexibly arranged to rapidly construct efficient preprocessing pipelines for deep learning.", "target": "nuts-flow/ml : data pre-processing for deep learning"}
{"id": "task1540-1b459570f974418595ca85fd860fd428", "input": "To improve the efficiency of Monte Carlo estimation, practitioners are turning to biased Markov chain Monte Carlo procedures that trade off asymptotic exactness for computational speed. The reasoning is sound: a reduction in variance due to more rapid sampling can outweigh the bias introduced. However, the inexactness creates new challenges for sampler and parameter selection, since standard measures of sample quality like effective sample size do not account for asymptotic bias. To address these challenges, we introduce a new computable quality measure based on Stein\u2019s method that quantifies the maximum discrepancy between sample and target expectations over a large class of test functions. We use our tool to compare exact, biased, and deterministic sample sequences and illustrate applications to hyperparameter selection, convergence rate assessment, and quantifying bias-variance tradeoffs in posterior inference.", "target": "Measuring Sample Quality with Stein\u2019s Method"}
{"id": "task1540-8c7241a258e743daabe3508b7536a039", "input": "For Markov chain Monte Carlo methods, one of the greatest discrepancies between theory and system is the scan order \u2014 while most theoretical development on the mixing time analysis deals with random updates, real-world systems are implemented with systematic scans. We bridge this gap for models that exhibit a bipartite structure, including, most notably, the Restricted/Deep Boltzmann Machine. The de facto implementation for these models scans variables in a layer-wise fashion. We show that the Gibbs sampler with a layerwise alternating scan order has its relaxation time (in terms of epochs) no larger than that of a random-update Gibbs sampler (in terms of variable updates). We also construct examples to show that this bound is asymptotically tight. Through standard inequalities, our result also implies a comparison on the mixing times.", "target": "LAYERWISE SYSTEMATIC SCAN: DEEP BOLTZMANN MACHINES AND BEYOND"}
{"id": "task1540-48fbe36795714a40aa4dda4e769ef1df", "input": "In this paper, we describe our autonomous bidding agent, RoxyBot, who emerged victorious in the travel division of the 2006 Trading Agent Competition in a photo finish. At a high level, the design of many successful trading agents can be summarized as follows: (i) price prediction: build a model of market prices; and (ii) optimization: solve for an approximately optimal set of bids, given this model. To predict, RoxyBot builds a stochastic model of market prices by simulating simultaneous ascending auctions. To optimize, RoxyBot relies on the sample average approximation method, a stochastic optimization technique.", "target": "RoxyBot-06: Stochastic Prediction and Optimization in TAC Travel"}
{"id": "task1540-8cf8d216b2ef4eb6b71dcb2fe3ab2d49", "input": "In reinforcement learning, we often define goals by specifying rewards within desirable states. One problem with this approach is that we typically need to redefine the rewards each time the goal changes, which often requires some understanding of the solution in the agent\u2019s environment. When humans are learning to complete tasks, we regularly utilize alternative sources that guide our understanding of the problem. Such task representations allow one to specify goals on their own terms, thus providing specifications that can be appropriately interpreted across various environments. This motivates our own work, in which we represent goals in environments that are different from the agent\u2019s. We introduce Cross-Domain Perceptual Reward (CDPR) functions, learned rewards that represent the visual similarity between an agent\u2019s state and a cross-domain goal image. We report results for learning the CDPRs with a deep neural network and using them to solve two tasks with deep reinforcement learning.", "target": "Cross-Domain Perceptual Reward Functions"}
{"id": "task1540-4f3bd61f2b8a4fc790e415ce7608742b", "input": "In this information technology age, a convenient and user friendly interface is required to operate the computer system on very fast rate. In human being, speech being a natural mode of communication has potential to being a fast and convenient mode of interaction with computer. Speech recognition will play an important role in taking technology to them. It is the need of this era to access the information with in seconds. This paper describes the design and development of speaker independent and English command interpreted system for computer. HMM model is used to represent the phoneme like speech commands. Experiments have been done on real world data and system has been trained in normal condition for real world subject.", "target": "CONATION: English Command Input/Output System for Computers"}
{"id": "task1540-3064d771bbc2482b8b5912c8ccec0824", "input": "Learning useful information across long time lags is a critical and difficult problem for temporal neural models in tasks like language modeling. Existing architectures that address the issue are often complex and costly to train. The Delta Recurrent Neural Network (Delta-RNN) framework is a simple and highperforming design that unifies previously proposed gated neural models. The DeltaRNN models maintain longer-term memory by learning to interpolate between a fast-changing data-driven representation and a slowly changing, implicitly stable state. This requires hardly any more parameters than a classical simple recurrent network. The models outperform popular complex architectures, such as the Long Short Term Memory (LSTM) and the Gated Recurrent Unit (GRU) and achieve state-of-the art performance in language modeling at character and word levels and yield comparable performance at the subword level.", "target": "Learning Simpler Language Models with the Delta Recurrent Neural Network Framework"}
{"id": "task1540-bf4f46519f254b1c9c2fd1df37bb3eda", "input": "In this paper, we present a heuristic operator which aims at simultaneously optimizing the orientations of all the edges in an interme\u00ad diate Bayesian network structure during the search process. This is done by alternating between the space of directed acyclic graphs (DAGs) and the space of skeletons. The found orientations of the edges are based on a scoring function rather than on induced con\u00ad ditional independences. This operator can be used as an extension to commonly employed search strategies. It is evaluated in experi\u00ad ments with artificial and real-world data.", "target": "On the Use of Skeletons when Learning in Bayesian Networks"}
{"id": "task1540-cbfe8dda6aeb4eafb19386521cdfa9ca", "input": "We investigate the use of pivot languages for phrase-based statistical machine translation (PB-SMT) between related languages with limited parallel corpora. We show that subword-level pivot translation via a related pivot language is: (i) highly competitive with the best direct translation model and (ii) better than a pivot model which uses an unrelated pivot language, but has at its disposal large parallel corpora to build the source-pivot (S-P) and pivot-target (P-T) translation models. In contrast, pivot models trained at word and morpheme level are far inferior to their direct counterparts. We also show that using multiple related pivot languages can outperform a direct translation model. Thus, the use of subwords as translation units coupled with the use of multiple related pivot languages can compensate for the lack of a direct parallel corpus. Subword units make pivot models competitive by (i) utilizing lexical similarity to improve the underlying S-P and P-T translation models, and (ii) reducing loss of translation candidates during pivoting.", "target": "Utilizing Lexical Similarity for pivot translation involving resource-poor, related languages"}
{"id": "task1540-6294aa6da5d14df88e46fdad4b1762ee", "input": "Random utility theory models an agent\u2019s preferences on alternatives by drawing a real-valued score on each alternative (typically independently) from a parameterized distribution, and then ranking the alternatives according to scores. A special case that has received significant attention is the Plackett-Luce model, for which fast inference methods for maximum likelihood estimators are available. This paper develops conditions on general random utility models that enable fast inference within a Bayesian framework through MC-EM, providing concave loglikelihood functions and bounded sets of global maxima solutions. Results on both real-world and simulated data provide support for the scalability of the approach and capability for model selection among general random utility models including Plackett-Luce.", "target": "Random Utility Theory for Social Choice"}
{"id": "task1540-e0d7d009b4444c2cba6f24ce6fa018e7", "input": "SNOMED Clinical Terms (SNOMED CT) is one of the most widespread ontologies in the life sciences, with more than 300,000 concepts and relationships, but is distributed with no associated software tools. In this paper we present MySNOM, a web-based SNOMED CT browser. MySNOM allows organizations to browse their own distribution of SNOMED CT under a controlled environment, focuses on navigating using the structure of SNOMED CT, and has diagramming capabilities.", "target": "Are SNOMED CT Browsers Ready for Institutions? Introducing MySNOM"}
{"id": "task1540-27c046e1a9134557bf5b80faf5609fce", "input": "Topic models provide a useful method for dimensionality reduction and exploratory data<lb>analysis in large text corpora. Most approaches to topic model inference have been based on<lb>a maximum likelihood objective. Efficient algorithms exist that approximate this objective,<lb>but they have no provable guarantees. Recently, algorithms have been introduced that provide<lb>provable bounds, but these algorithms are not practical because they are inefficient and not ro-<lb>bust to violations of model assumptions. In this paper we present an algorithm for topic model<lb>inference that is both provable and practical. The algorithm produces results comparable to the<lb>best MCMC implementations while running orders of magnitude faster.", "target": "A Practical Algorithm for Topic Modeling with Provable Guarantees"}
{"id": "task1540-5245f0e1ac9d4176af7d67b1d3cfae02", "input": "Multi-step temporal-difference (TD) learning, where the update targets contain information from multiple time steps ahead, is one of the most popular forms of TD learning for linear function approximation. The reason is that multi-step methods often yield substantially better performance than their single-step counter-parts, due to a lower bias of the update targets. For non-linear function approximation, however, single-step methods appear to be the norm. Part of the reason could be that on many domains the popular multi-step methods TD(\u03bb) and Sarsa(\u03bb) do not perform well when combined with non-linear function approximation. In particular, they are very susceptible to divergence of value estimates. In this paper, we identify the reason behind this. Furthermore, based on our analysis, we propose a new multi-step TD method for non-linear function approximation that addresses this issue. We confirm the effectiveness of our method using two benchmark tasks with neural networks as function approximation.", "target": "Effective Multi-step Temporal-Difference Learning for Non-Linear Function Approximation"}
{"id": "task1540-decd6a8c01144d5694e988c55d55b18b", "input": "The chain-structured long short-term memory (LSTM) has showed to be effective in a wide range of problems such as speech recognition and machine translation. In this paper, we propose to extend it to tree structures, in which a memory cell can reflect the history memories of multiple child cells or multiple descendant cells in a recursive process. We call the model S-LSTM, which provides a principled way of considering long-distance interaction over hierarchies, e.g., language or image parse structures. We leverage the models for semantic composition to understand the meaning of text, a fundamental problem in natural language understanding, and show that it outperforms a state-of-theart recursive model by replacing its composition layers with the S-LSTM memory blocks. We also show that utilizing the given structures is helpful in achieving a performance better than that without considering the structures.", "target": "Long Short-Term Memory Over Tree Structures"}
{"id": "task1540-1234d0c669174457a87c9bb81cf6f717", "input": "We discuss representing and reasoning with knowledge about the time-dependent util\u00ad ity of an agent's actions. Time-dependent utility plays a crucial role in the interac\u00ad tion between computation and action under bounded resources. We present a semantics for time-dependent utility and describe the use of time-dependent information in deci\u00ad sion contexts. We illustrate our discussion with examples of time-pressured reasoning in Protos, a system constructed to explore the ideal control of inference by reasoners with limited abilities.", "target": "Time-Dependent Utility and Action Under Uncertainty"}
{"id": "task1540-68393b5ecf4d4f26b9de0ef7f0cb64df", "input": "The problem of sparse rewards is one of the hardest challenges in contemporary reinforcement learning. Hierarchical reinforcement learning (HRL) tackles this problem by using a set of temporally-extended actions, or options, each of which has its own subgoal. These subgoals are normally handcrafted for specific tasks. Here, though, we introduce a generic class of subgoals with broad applicability in the visual domain. Underlying our approach (in common with work using \u201cauxiliary tasks\u201d) is the hypothesis that the ability to control aspects of the environment is an inherently useful skill to have. We incorporate such subgoals in an end-to-end hierarchical reinforcement learning system and test two variants of our algorithm on a number of games from the Atari suite. We highlight the advantage of our approach in one of the hardest games \u2013 Montezuma\u2019s revenge \u2013 for which the ability to handle sparse rewards is key. Our agent learns several times faster than the current state-of-the-art HRL agent in this game, reaching a similar level of performance.", "target": "Feature Control as Intrinsic Motivation for Hierarchical Reinforcement Learning"}
{"id": "task1540-dbab2cc5d9a04dafa3921d49df7e149c", "input": "Two popular approaches for distributed training of SVMs on big data are parameter averaging and ADMM. Parameter averaging is efficient but suffers from loss of accuracy with increase in number of partitions, while ADMM in the feature space is accurate but suffers from slow convergence. In this paper, we report a hybrid approach called weighted parameter averaging (WPA), which optimizes the regularized hinge loss with respect to weights on parameters. The problem is shown to be same as solving SVM in a projected space. We also demonstrate an O( 1 N ) stability bound on final hypothesis given by WPA, using novel proof techniques. Experimental results on a variety of toy and real world datasets show that our approach is significantly more accurate than parameter averaging for high number of partitions. It is also seen the proposed method enjoys much faster convergence compared to ADMM in features space.", "target": "Distributed Weighted Parameter Averaging for SVM Training on Big Data"}
{"id": "task1540-459c915284d44708a3197bf6a6ebefe5", "input": "Language students are most engaged while reading texts at an appropriate difficulty level. However, existing methods of evaluating text difficulty focus mainly on vocabulary and do not prioritize grammatical features, hence they do not work well for language learners with limited knowledge of grammar. In this paper, we introduce grammatical templates, the expert-identified units of grammar that students learn from class, as an important feature of text difficulty evaluation. Experimental classification results show that grammatical template features significantly improve text difficulty prediction accuracy over baseline readability features by 7.4%. Moreover, we build a simple and human-understandable text difficulty evaluation approach with 87.7% accuracy, using only 5 grammatical template features.", "target": "Grammatical Templates: Improving Text Difficulty Evaluation for Language Learners"}
{"id": "task1540-5d6a31c7e045403ab0b05a80f80484c7", "input": "An interesting research problem in our age of Big Data is that of determining provenance. Granular evaluation of provenance of physical goods--e.g. tracking ingredients of a pharmaceutical or demonstrating authenticity of luxury goods--has often not been possible with today's items that are produced and transported in complex, inter-organizational, often internationally-spanning supply chains. Recent adoption of Internet of Things and Blockchain technologies give promise at better supply chain provenance. We are particularly interested in the blockchain as many favoured use cases of blockchain are for provenance tracking. We are also interested in applying ontologies as there has been some work done on knowledge provenance, traceability, and food provenance using ontologies. In this paper, we make a case for why ontologies can contribute to blockchain design. To support this case, we analyze a traceability ontology and translate some of its representations to smart contracts that execute a provenance trace and enforce traceability constraints on the Ethereum blockchain platform.", "target": "Towards an Ontology-Driven Blockchain Design for Supply Chain Provenance"}
{"id": "task1540-e84f81aacc43420abe81891ba5a05310", "input": "Monte Carlo Tree Search (MCTS) has improved the performance of game engines in domains such as Go, Hex, and general game playing. MCTS has been shown to outperform classic \u03b1\u03b2 search in games where good heuristic evaluations are difficult to obtain. In recent years, combining ideas from traditional minimax search in MCTS has been shown to be advantageous in some domains, such as Lines of Action, Amazons, and Breakthrough. In this paper, we propose a new way to use heuristic evaluations to guide the MCTS search by storing the two sources of information, estimated win rates and heuristic evaluations, separately. Rather than using the heuristic evaluations to replace the playouts, our technique backs them up implicitly during the MCTS simulations. These minimax values are then used to guide future simulations. We show that using implicit minimax backups leads to stronger play performance in Kalah, Breakthrough, and Lines of Action.", "target": "Monte Carlo Tree Search with Heuristic Evaluations using Implicit Minimax Backups"}
{"id": "task1540-ffb99b3cb8514a01b5127f3e9ce47a5f", "input": "The RoboCup 2D Simulation League incorporates several challenging features, setting a benchmark for Artificial Intelligence (AI). In this paper we describe some of the ideas and tools around the development of our team, Gliders2012. In our description, we focus on the evaluation function as one of our central mechanisms for action selection. We also point to a new framework for watching log files in a web browser that we release for use and further development by the RoboCup community. Finally, we also summarize results of the group and final matches we played during RoboCup 2012, with Gliders2012 finishing 4th out of 19 teams.", "target": "Gliders2012: Development and Competition Results"}
{"id": "task1540-11e1da6761c345b7ad20be120a49e043", "input": "Cumulative prospect theory (CPT) is known to model human decisions well, with substantial empirical evidence supporting this claim. CPT works by distorting probabilities and is more general than the classic expected utility and coherent risk measures. We bring this idea to a risk-sensitive reinforcement learning (RL) setting and design algorithms for both estimation and control. The estimation scheme that we propose uses the empirical distribution in order to estimate the CPT-value of a random variable. We then use this scheme in the inner loop of policy optimization procedures for a Markov decision process (MDP). We propose both gradient-based as well as gradient-free policy optimization algorithms. The former includes both first-order and second-order methods that are based on the well-known simulation optimization idea of simultaneous perturbation stochastic approximation (SPSA), while the latter is based on a reference distribution that concentrates on the global optima. Using an empirical distribution over the policy space in conjunction with Kullback-Leibler (KL) divergence to the reference distribution, we get a global policy optimization scheme. We provide theoretical convergence guarantees for all the proposed algorithms.", "target": "Cumulative Prospect Theory Meets Reinforcement Learning: Estimation and Control"}
{"id": "task1540-2da5d516f9a149a4ad2737d44ea5f2d3", "input": "A standard assumption in machine learning is the exchangeability of data, which is equivalent to assuming that the examples are generated from the same probability distribution independently. This paper is devoted to testing the assumption of exchangeability on-line: the examples arrive one by one, and after receiving each example we would like to have a valid measure of the degree to which the assumption of exchangeability has been falsified. Such measures are provided by exchangeability martingales. We extend known techniques for constructing exchangeability martingales and show that our new method is competitive with the martingales introduced before. Finally we investigate the performance of our testing method on two benchmark datasets, USPS and Statlog Satellite data; for the former, the known techniques give satisfactory results, but for the latter our new more flexible method becomes necessary.", "target": "Plug-in martingales for testing exchangeability on-line"}
{"id": "task1540-2c8fce12f8c54ba084083937a0e34e35", "input": "Trilateration-based localization (TBL) has become a corner stone of modern technology. This study formulates the concern on how wireless sensor networks can take advantage of the computational intelligent techniques using both singleand multi-objective particle swarm optimization (PSO) with an overall aim of concurrently minimizing the required time for localization, minimizing energy consumed during localization, and maximizing the number of nodes fully localized through the adjustment of wireless sensor transmission ranges while using TBL process. A parameter-study of the applied PSO variants is performed, leading to results that show algorithmic improvements of up to 32% in the evaluated objectives.", "target": "PARTICLE SWARM OPTIMIZED POWER CONSUMPTION OF TRILATERATION"}
{"id": "task1540-be90d75d13ce4d7da1d96697af98443c", "input": "Video captioning has been attracting broad research attention in multimedia community. However, most existing approaches either ignore temporal information among video frames or just employ local contextual temporal knowledge. In this work, we propose a novel video captioning framework, termed as Bidirectional Long-Short Term Memory (BiLSTM), which deeply captures bidirectional global temporal structure in video. Specifically, we first devise a joint visual modelling approach to encode video data by combining a forward LSTM pass, a backward LSTM pass, together with visual features from Convolutional Neural Networks (CNNs). Then, we inject the derived video representation into the subsequent language model for initialization. The benefits are in two folds: 1) comprehensively preserving sequential and visual information; and 2) adaptively learning dense visual features and sparse semantic representations for videos and sentences, respectively. We verify the effectiveness of our proposed video captioning framework on a commonlyused benchmark, i.e., Microsoft Video Description (MSVD) corpus, and the experimental results demonstrate that the superiority of the proposed approach as compared to several state-of-the-art methods.", "target": "Bidirectional Long-Short Term Memory for Video Description"}
{"id": "task1540-39209dad3a0b426992c84e1f0969555c", "input": "First-order factoid question answering assumes that the question can be answered by a single fact in a knowledge base (KB). While this does not seem like a challenging task, many recent attempts that apply either complex linguistic reasoning or deep neural networks achieve 35%\u201365% accuracy on benchmark sets. Our approach formulates the task as two machine learning problems: detecting the entities in the question, and classifying the question as one of the relation types in the KB. Based on this assumption of the structure, our simple yet effective approach trains two recurrent neural networks to outperform state of the art by significant margins \u2014 relative improvement reaches 16% for WebQuestions, and surpasses 38% for SimpleQuestions.", "target": "Simple and Effective Question Answering with Recurrent Neural Networks"}
{"id": "task1540-19b78eb552004c75879c5a2981b53d58", "input": "This paper tests the hypothesis that distinctive feature classifiers anchored at phonetic landmarks can be transferred crosslingually without loss of accuracy. Three consonant voicing classifiers were developed: (1) manually selected acoustic features anchored at a phonetic landmark, (2) MFCCs (either averaged across the segment or anchored at the landmark), and (3) acoustic features computed using a convolutional neural network (CNN). All detectors are trained on English data (TIMIT), and tested on English, Turkish, and Spanish (performance measured using F1 and accuracy). Experiments demonstrate that manual features outperform all MFCC classifiers, while CNN features outperform both. MFCC-based classifiers suffer an overall error rate increase of up to 96.1% when generalized from English to other languages. Manual features suffer only an up to 35.2% relative error rate increase, and CNN features actually perform the best on Turkish and Spanish, demonstrating that features capable of representing long-term spectral dynamics (CNN and landmark-based features) are able to generalize cross-lingually with little or no loss of accuracy.", "target": "LANDMARK-BASED CONSONANT VOICING DETECTION ON MULTILINGUAL CORPORA"}
{"id": "task1540-0e5785abbbce450086c1a19dac7627e9", "input": "When approximating binary similarity using the hamming distance between short binary hashes, we show that even if the similarity is symmetric, we can have shorter and more accurate hashes by using two distinct code maps. I.e. by approximating the similarity between x and x\u2032 as the hamming distance between f(x) and g(x\u2032), for two distinct binary codes f, g, rather than as the hamming distance between f(x) and f(x\u2032).", "target": "The Power of Asymmetry in Binary Hashing"}
{"id": "task1540-93c3140c49fb437182f988b04dc90153", "input": "In this paper we discuss and analyze some of the intelligent classifiers which allows for automatic detection and classification of networks attacks for any intrusion detection system. We will proceed initially with their analysis using the WEKA software to work with the classifiers on a well-known IDS (Intrusion Detection Systems) dataset like NSL-KDD dataset. The NSL-KDD dataset of network attacks was created in a military network by MIT Lincoln Labs. Then we will discuss and experiment some of the hybrid AI (Artificial Intelligence) classifiers that can be used for IDS, and finally we developed a Java software with three most efficient classifiers and compared it with other options. The outputs would show the detection accuracy and efficiency of the single and combined classifiers used.", "target": "Analysis of Intelligent Classifiers and Enhancing the Detection Accuracy for Intrusion Detection System"}
{"id": "task1540-23b27502242c4743999cd67bee630c04", "input": "Assessing network security is a complex and difficult task. Attack graphs have been proposed as a tool to help network administrators understand the potential weaknesses of their networks. However, a problem has not yet been addressed by previous work on this subject; namely, how to actually execute and validate the attack paths resulting from the analysis of the attack graph. In this paper we present a complete PDDL representation of an attack model, and an implementation that integrates a planner into a penetration testing tool. This allows to automatically generate attack paths for penetration testing scenarios, and to validate these attacks by executing the corresponding actions -including exploitsagainst the real target network. We present an algorithm for transforming the information present in the penetration testing tool to the planning domain, and we show how the scalability issues of attack graphs can be solved using current planners. We include an analysis of the performance of our solution, showing how our model scales to medium-sized networks and the number of actions available in current penetration testing tools.", "target": "Attack Planning in the Real World"}
{"id": "task1540-47fa90d852f94cf1af55fbad302a9961", "input": "Neural machine translation (NMT) offers a novel alternative formulation of translation that is potentially simpler than statistical approaches. However to reach competitive performance, NMT models need to be exceedingly large. In this paper we consider applying knowledge distillation approaches (Bucila et al., 2006; Hinton et al., 2015) that have proven successful for reducing the size of neural models in other domains to the problem of NMT. We demonstrate that standard knowledge distillation applied to word-level prediction can be effective for NMT, and also introduce two novel sequence-level versions of knowledge distillation that further improve performance, and somewhat surprisingly, seem to eliminate the need for beam search (even when applied on the original teacher model). Our best student model runs 10 times faster than its state-of-the-art teacher with only a decrease of 0.2 BLEU. It is also significantly better than a baseline model trained without knowledge distillation: by 4.2/1.7 BLEU with greedy decoding/beam search.", "target": "Sequence-Level Knowledge Distillation"}
{"id": "task1540-ecd1067e894a492980600713f13fa385", "input": "In this paper, we explore SPPIM-based text classification method, and the experiment reveals that the SPPIM method is equal to or even superior than SGNS method in text classification task on three international and standard text datasets, namely 20newsgroups, Reuters52 and WebKB. Comparing to SGNS, although SPPMI provides a better solution, it is not necessarily better than SGNS in text classification tasks.. Based on our analysis, SGNS takes into the consideration of weight calculation during decomposition process, so it has better performance than SPPIM in some standard datasets. Inspired by this, we propose a WL-SPPIM semantic model based on SPPIM model, and experiment shows that WL-SPPIM approach has better classification and higher scalability in the text classification task compared with LDA, SGNS and SPPIM approaches.", "target": "A WL-SPPIM Semantic Model for Document Classification"}
{"id": "task1540-8825b3e340a642828e6c49ec147392c0", "input": "Microsoft Kinect camera and its skeletal tracking capabilities have been embraced by many researchers and commercial developers in various applications of real-time human movement analysis. In this paper, we evaluate the accuracy of the human kinematic motion data in the first and second generation of the Kinect system, and compare the results with an optical motion capture system. We collected motion data in 12 exercises for 10 different subjects and from three different viewpoints. We report on the accuracy of the joint localization and bone length estimation of Kinect skeletons in comparison to the motion capture. We also analyze the distribution of the joint localization offsets by fitting a mixture of Gaussian and uniform distribution models to determine the outliers in the Kinect motion data. Our analysis shows that overall Kinect 2 has more robust and more accurate tracking of human pose as compared to Kinect 1.", "target": "Evaluation of Pose Tracking Accuracy in the First and Second Generations of Microsoft Kinect"}
{"id": "task1540-a3ab227b0aa64afaab4c7eb017218c91", "input": "Integrating vision and language has long been a dream in work on artificial intelligence (AI). In the past two years, we have witnessed an explosion of work that brings together vision and language from images to videos and beyond. The available corpora have played a crucial role in advancing this area of research. In this paper, we propose a set of quality metrics for evaluating and analyzing the vision & language datasets and classify them accordingly. Our analyses show that the most recent datasets have been using more complex language and more abstract concepts, however, there are different strengths and weaknesses in each.", "target": "On Available Corpora for Empirical Methods in Vision & Language"}
{"id": "task1540-c7760aad4d7c482db8e4acf7f16b1479", "input": "This paper describes a method for the automatic inference of structural transfer rules to be used in a shallow-transfer machine translation (MT) system from small parallel corpora. The structural transfer rules are based on alignment templates, like those used in statistical MT. Alignment templates are extracted from sentence-aligned parallel corpora and extended with a set of restrictions which are derived from the bilingual dictionary of the MT system and control their application as transfer rules. The experiments conducted using three different language pairs in the free/open-source MT platform Apertium show that translation quality is improved as compared to word-for-word translation (when no transfer rules are used), and that the resulting translation quality is close to that obtained using hand-coded transfer rules. The method we present is entirely unsupervised and benefits from information in the rest of modules of the MT system in which the inferred rules are applied.", "target": "Inferring Shallow-Transfer Machine Translation Rules from Small Parallel Corpora"}
{"id": "task1540-c234ca3491d14cfc829edd02db16a9f8", "input": "Abstract This paper reconsiders the problem of the absent-minded driver who must choose between alternatives with different payoff with imperfect recall and varying degrees of knowledge of the system. The classical absent-minded driver problem represents the case with limited information and it has bearing on the general area of communication and learning, social choice, mechanism design, auctions, theories of knowledge, belief, and rational agency. Within the framework of extensive games, this problem has applications to many artificial intelligence scenarios. It is obvious that the performance of the agent improves as information available increases. It is shown that a non-uniform assignment strategy for successive choices does better than a fixed probability strategy. We consider both classical and quantum approaches to the problem. We argue that the superior performance of quantum decisions with access to entanglement cannot be fairly compared to a classical algorithm. If the cognitive systems of agents are taken to have access to quantum resources, or have a quantum mechanical basis, then that can be leveraged into superior performance.", "target": "The Absent-Minded Driver Problem Redux"}
{"id": "task1540-e24336da753b4d069358fc7d310d88a1", "input": "The ability to accurately perceive whether a speaker is asking a question or is making a statement is crucial for any successful interaction. However, learning and classifying tonal patterns has been a challenging task for automatic speech recognition and for models of tonal representation, as tonal contours are characterized by significant variation. This paper provides a classification model of Cypriot Greek questions and statements. We evaluate two state-of-the-art network architectures: a Long Short-Term Memory (LSTM) network and a convolutional network (ConvNet). The ConvNet outperforms the LSTM in the classification task and exhibited an excellent performance with 95% classification accuracy.", "target": "Modelling prosodic structure using Artificial Neural Networks Modelling prosodic structure using Artificial Neural Networks"}
{"id": "task1540-8cdbea7ec4fb4a6b98e951450c9cc6a7", "input": "Algorithm portfolios represent a strategy of composing multiple heuristic algorithms, each suited to a different class of problems, within a single general solver that will choose the best suited algorithm for each input. This approach recently gained popularity especially for solving combinatoric problems, but optimization applications are still emerging. The COCO platform [6] [5] of the BBOB workshop series is the current standard way to measure performance of continuous black-box optimization algorithms. As an extension to the COCO platform, we present the Python-based COCOpf framework that allows composing portfolios of optimization algorithms and running experiments with different selection strategies. In our framework, we focus on black-box algorithm portfolio and online adaptive selection. As a demonstration, we measure the performance of stock SciPy [8] optimization algorithms and the popular CMA algorithm [4] alone and in a portfolio with two simple selection strategies. We confirm that even a naive selection strategy can provide improved performance across problem classes.", "target": "COCOpf: An Algorithm Portfolio Framework"}
{"id": "task1540-1453b45c6b82439db940cf6d8395115f", "input": "Rumour stance classification, the task that determines if each tweet in a collection discussing a rumour is supporting, denying, questioning or simply commenting on the rumour, has been attracting substantial interest. Here we introduce a novel approach that makes use of the sequence of transitions observed in tree-structured conversation threads in Twitter. The conversation threads are formed by harvesting users\u2019 replies to one another, which results in a nested tree-like structure. Previous work addressing the stance classification task has treated each tweet as a separate unit. Here we analyse tweets by virtue of their position in a sequence and test two sequential classifiers, Linear-Chain CRF and Tree CRF, each of which makes different assumptions about the conversational structure. We experiment with eight Twitter datasets, collected during breaking news, and show that exploiting the sequential structure of Twitter conversations achieves significant improvements over the non-sequential methods. Our work is the first to model Twitter conversations as a tree structure in this manner, introducing a novel way of tackling NLP tasks on Twitter conversations.", "target": "Stance Classification in Rumours as a Sequential Task Exploiting the Tree Structure of Social Media Conversations"}
{"id": "task1540-61a44742e9d14baab92dfbca1c2cfccb", "input": "The success of kernel methods has initiated the design of novel positive semidefinite functions, in particular for structured data. A leading design paradigm for this is the convolution kernel, which decomposes structured objects into their parts and sums over all pairs of parts. Assignment kernels, in contrast, are obtained from an optimal bijection between parts, which can provide a more valid notion of similarity. In general however, optimal assignments yield indefinite functions, which complicates their use in kernel methods. We characterize a class of base kernels used to compare parts that guarantees positive semidefinite optimal assignment kernels. These base kernels give rise to hierarchies from which the optimal assignment kernels are computed in linear time by histogram intersection. We apply these results by developing the Weisfeiler-Lehman optimal assignment kernel for graphs. It provides high classification accuracy on widely-used benchmark data sets improving over the original Weisfeiler-Lehman kernel.", "target": "On Valid Optimal Assignment Kernels and Applications to Graph Classification"}
{"id": "task1540-3d8dfe5709ed4496bc5b50442501da91", "input": "This report presents a general model of the architecture of information systems for the children\u2019s speech recognition. It presents a model of the speech data stream and how it works. The result of these studies and presented veins architectural model shows that research needs to be focused on acoustic-phonetic modeling in order to improve the quality of children's speech recognition and the sustainability of the systems to noise and changes in transmission environment. Another important aspect is the development of more accurate algorithms for modeling of spontaneous child speech.", "target": "On model architecture for a children\u2019s speech recognition interactive dialog system"}
{"id": "task1540-92872fa4a84649688148c7e1637fe14f", "input": "Riemannian geometry has been successfully used in many brain-computer interface (BCI) classification problems and demonstrated superior performance. In this paper, for the first time, it is applied to BCI regression problems, an important category of BCI applications. More specifically, we propose a new feature extraction approach for Electroencephalogram (EEG) based BCI regression problems: a spatial filter is first used to increase the signal quality of the EEG trials and also to reduce the dimensionality of the covariance matrices, and then Riemannian tangent space features are extracted. We validate the performance of the proposed approach in reaction time estimation from EEG signals measured in a large-scale sustained-attention psychomotor vigilance task, and show that compared with the traditional powerband features, the tangent space features can reduce the root mean square estimation error by 4.30-8.30%, and increase the estimation correlation coefficient by 6.59-11.13%.", "target": "EEG-Based User Reaction Time Estimation Using Riemannian Geometry Features"}
{"id": "task1540-6cc193e89adf4784805c6abccf6bdcd2", "input": "We introduce the lifted Generalized Belief Propagation (GBP) message passing algorithm, for the computation of sum-product queries in Probabilistic Relational Models (e.g. Markov logic network). The algorithm forms a compact region graph and establishes a modified version of message passing, which mimics the GBP behavior in a corresponding ground model. The compact graph is obtained by exploiting a graphical representation of clusters, which reduces cluster symmetry detection to isomorphism tests on small local graphs. The framework is thus capable of handling complex models, while remaining domain-size independent.", "target": "Lifted Message Passing for the Generalized Belief Propagation"}
{"id": "task1540-edbc878b32a54849b37549eab4890430", "input": "Large-scale multi-relational embedding refers to the task of learning the latent representations for entities and relations in large knowledge graphs. An effective and scalable solution for this problem is crucial for the true success of knowledgebased inference in a broad range of applications. This paper proposes a novel framework for optimizing the latent representations with respect to the analogical properties of the embedded entities and relations. By formulating the learning objective in a differentiable fashion, our model enjoys both theoretical power and computational scalability, and significantly outperformed a large number of representative baseline methods on benchmark datasets. Furthermore, the model offers an elegant unification of several well-known methods in multi-relational embedding, which can be proven to be special instantiations of our framework.", "target": "Analogical Inference for Multi-relational Embeddings"}
{"id": "task1540-5d19c735a6764be4a04904fda4bf797d", "input": "The First-Order Variable Elimination (FOVE) algorithm allows exact inference to be applied directly to probabilistic relational models, and has proven to be vastly superior to the application of standard inference methods on a grounded propositional model. Still, FOVE operators can be applied under restricted conditions, often forcing one to resort to propositional inference. This paper aims to extend the applicability of FOVE by providing two new model conversion operators: the first and the primary is joint formula conversion and the second is just-different counting conversion. These new operations allow efficient inference methods to be applied directly on relational models, where no existing efficient method could be applied hitherto. In addition, aided by these capabilities, we show how to adapt FOVE to provide exact solutions to Maximum Expected Utility (MEU) queries over relational models for decision under uncertainty. Experimental evaluations show our algorithms to provide significant speedup over the alternatives.", "target": "Extended Lifted Inference with Joint Formulas"}
{"id": "task1540-679ac02062f643dc97b35e8b4ebb044b", "input": "Path planning for multiple robots is well studied in the AI and robotics communities. For a given discretized environment, robots need to find collision-free paths to a set of specified goal locations. Robots can be fully anonymous, non-anonymous, or organized in groups. Although powerful solvers for this abstract problem exist, they make simplifying assumptions by ignoring kinematic constraints, making it difficult to use the resulting plans on actual robots. In this paper, we present a solution which takes kinematic constraints, such as maximum velocities, into account, while guaranteeing a user-specified minimum safety distance between robots. We demonstrate our approach in simulation and on real robots in 2D and 3D environments.", "target": "Path Planning With Kinematic Constraints For Robot Groups"}
{"id": "task1540-4ded55397a204932b8bbae216bfd8cf4", "input": "Graphs are a useful abstraction of image content. Not only can graphs represent details about individual objects in a scene but they can capture the interactions between pairs of objects. We present a method for training a convolutional neural network such that it takes in an input image and produces a full graph. This is done end-to-end in a single stage with the use of associative embeddings. The network learns to simultaneously identify all of the elements that make up a graph and piece them together. We benchmark on the Visual Genome dataset, and report a Recall@50 of 9.7% compared to the prior state-of-the-art at 3.4%, a nearly threefold improvement on the challenging task of scene graph generation.", "target": "Pixels to Graphs by Associative Embedding"}
{"id": "task1540-786c909d74f148abab2b9f613ded821d", "input": "We tackle a task where an agent learns to navigate in a 2D maze-like environment called XWORLD. In each session, the agent perceives a sequence of raw-pixel frames, a natural language command issued by a teacher, and a set of rewards. The agent learns the teacher\u2019s language from scratch in a grounded and compositional manner, such that after training it is able to correctly execute zero-shot commands: 1) the combination of words in the command never appeared before, and/or 2) the command contains new object concepts that are learned from another task but never learned from navigation. Our deep framework for the agent is trained end to end: it learns simultaneously the visual representations of the environment, the syntax and semantics of the language, and the action module that outputs actions. The zero-shot learning capability of our framework results from its compositionality and modularity with parameter tying. We visualize the intermediate outputs of the framework, demonstrating that the agent truly understands how to solve the problem. We believe that our results provide some preliminary insights on how to train an agent with similar abilities in a 3D environment.", "target": "A Deep Compositional Framework for Human-like Language Acquisition in Virtual Environment"}
{"id": "task1540-46539e08d9a3400a81d4fcb26b8df68c", "input": "Hidden Markov Models (HMMs) are learning methods for pattern recognition. The probabilistic HMMs have been one of the most used techniques based on the Bayesian model. First-order probabilistic HMMs were adapted to the theory of belief functions such that Bayesian probabilities were replaced with mass functions. In this paper, we present a second-order Hidden Markov Model using belief functions. Previous works in belief HMMs have been focused on the first-order HMMs. We extend them to the second-order model.", "target": "Second-order Belief Hidden Markov Models"}
{"id": "task1540-50f464a92fbe4340905c22d9e370442d", "input": "Ontologies usually suffer from the semantic heterogeneity when simultaneously used in information sharing, merging, integrating and querying processes. Therefore, the similarity identification between ontologies being used becomes a mandatory task for all these processes to handle the problem of semantic heterogeneity. In this paper, we propose an efficient technique for similarity measurement between two ontologies. The proposed technique identifies all candidate pairs of similar concepts without omitting any similar pair. The proposed technique can be used in different types of operations on ontologies such as merging, mapping and aligning. By analyzing its results a reasonable improvement in terms of completeness, correctness and overall quality of the results has been", "target": "An Efficient Technique for Similarity Identification between Ontologies"}
{"id": "task1540-f48074c48c2540f59b232db49106b0e0", "input": "Sequence prediction and classification are ubiquitous and challenging problems in machine learning that can require identifying complex dependencies between temporally distant inputs. Recurrent Neural Networks (RNNs) have the ability, in theory, to cope with these temporal dependencies by virtue of the short-term memory implemented by their recurrent (feedback) connections. However, in practice they are difficult to train successfully when the long-term memory is required. This paper introduces a simple, yet powerful modification to the standard RNN architecture, the Clockwork RNN (CW-RNN), in which the hidden layer is partitioned into separate modules, each processing inputs at its own temporal granularity, making computations only at its prescribed clock rate. Rather than making the standard RNN models more complex, CW-RNN reduces the number of RNN parameters, improves the performance significantly in the tasks tested, and speeds up the network evaluation. The network is demonstrated in preliminary experiments involving two tasks: audio signal generation and TIMIT spoken word classification, where it outperforms both RNN and LSTM networks.", "target": "A Clockwork RNN"}
{"id": "task1540-d09f43fa33de44ae8e8f40048ddf6260", "input": "We present an approach for the verification of feed-forward neural networks in which all nodes have a piece-wise linear activation function. Such networks are often used in deep learning and have been shown to be hard to verify for modern satisfiability modulo theory (SMT) and integer linear programming (ILP) solvers. The starting point of our approach is the addition of a global linear approximation of the overall network behavior to the verification problem that helps with SMT-like reasoning over the network behavior. We present a specialized verification algorithm that employs this approximation in a search process in which it infers additional node phases for the non-linear nodes in the network from partial node phase assignments, similar to unit propagation in classical SAT solving. We also show how to infer additional conflict clauses and safe node fixtures from the results of the analysis steps performed during the search. The resulting approach is evaluated on collision avoidance and handwritten digit recognition case studies.", "target": "Formal Verification of Piece-Wise Linear Feed-Forward Neural Networks"}
{"id": "task1540-add3e88a4a504e17956f0e8511c56a2d", "input": "Although traditionally used in the machine translation field, the encoder-decoder framework has been recently applied for the generation of video and image descriptions. The combination of Convolutional and Recurrent Neural Networks in these models has proven to outperform the previous state of the art, obtaining more accurate video descriptions. In this work we propose pushing further this model by introducing two contributions into the encoding stage. First, producing richer image representations by combining object and location information from Convolutional Neural Networks and second, introducing Bidirectional Recurrent Neural Networks for capturing both forward and backward temporal relationships in the input frames.", "target": "Video Description using Bidirectional Recurrent Neural Networks"}
{"id": "task1540-cec01a2896fd4216a03b6222376416e7", "input": "Living organisms intertwine soft (e.g., muscle) and hard (e.g., bones) materials, giving them an intrinsic flexibility and resiliency often lacking in conventional rigid robots. The emerging field of soft robotics seeks to harness these same properties in order to create resilient machines. The nature of soft materials, however, presents considerable challenges to aspects of design, construction, and control \u2013 and up until now, the vast majority of gaits for soft robots have been hand-designed through empirical trial-and-error. This manuscript describes an easy-to-assemble tensegritybased soft robot capable of highly dynamic locomotive gaits and demonstrating structural and behavioral resilience in the face of physical damage. Enabling this is the use of a machine learning algorithm able to discover novel gaits with a minimal number of physical trials. These results lend further credence to soft-robotic approaches that seek to harness the interaction of complex material dynamics in order to generate a wealth of dynamical behaviors.", "target": "Soft tensegrity robots"}
{"id": "task1540-8df1d98271264c0f9c10b329f750d361", "input": "Wikipedia is a useful knowledge source that benefits many applications in language processing and knowledge representation. An important feature of Wikipedia is that of categories. Wikipedia pages are assigned different categories according to their contents as human-annotated labels which can be used in information retrieval, ad hoc search improvements, entity ranking and tag recommendations. However, important pages are usually assigned too many categories, which makes it difficult to recognize the most important ones that give the best descriptions. In this paper, we propose an approach to recognize the most descriptive Wikipedia categories. We observe that historical figures in a precise category presumably are mutually similar and such categorical coherence could be evaluated via texts or Wikipedia links of corresponding members in the category. We rank descriptive level of Wikipedia categories according to their coherence and our ranking yield an overall agreement of 88.27% compared with human wisdom.", "target": "Recognizing Descriptive Wikipedia Categories for Historical Figures"}
{"id": "task1540-e8f884eab561447e9c7820bec868c5bf", "input": "We present a novel application of LSTM recurrent neural networks to multilabel classification of diagnoses given variable-length time series of clinical measurements. Our method outperforms a strong baseline on a variety of metrics.", "target": "Phenotyping of Clinical Time Series with LSTM Recurrent Neural Networks"}
{"id": "task1540-241bf521aa104e1b8b5f34a4c0adbb29", "input": "We propose a series of recurrent and contextual neural network models for multiple choice visual question answering on the Visual7W dataset. Motivated by divergent trends in model complexities in the literature, we explore the balance between model expressiveness and simplicity by studying incrementally more complex architectures. We start with LSTM-encoding of input questions and answers; build on this with context generation by LSTM-encodings of neural image and question representations and attention over images; and evaluate the diversity and predictive power of our models and the ensemble thereof. All models are evaluated against a simple baseline inspired by the current state-of-the-art, consisting of involving simple concatenation of bag-of-words and CNN representations for the text and images, respectively. Generally, we observe marked variation in image-reasoning performance between our models not obvious from their overall performance, as well as evidence of dataset bias. Our standalone models achieve accuracies up to 64.6%, while the ensemble of all models achieves the best accuracy of 66.67%, within 0.5% of the current state-of-the-art for Visual7W.", "target": "Recurrent and Contextual Models for Visual Question Answering"}
{"id": "task1540-0c24b86405b84c8d888f0e089f40ab1d", "input": "This paper reports the analysis of audio and visual features in predicting the emotion dimensions under the seventh Audio/Visual Emotion Subchallenge (AVEC 2017). For visual features we used the HOG (Histogram of Gradients) features, Fisher encodings of SIFT (Scale-Invariant Feature Transform) features based on Gaussian mixture model (GMM) and some pretrained Convolutional Neural Network layers as features; all these extracted for each video clip. For audio features we used the Bag-of-audio-words (BoAW) representation of the LLDs (low-level descriptors) generated by openXBOW provided by the organisers of the event. Then we trained fully connected neural network regression model on the dataset for all these different modalities. We applied multimodal fusion on the output models to get the Concordance correlation coefficient on Development set as well as Test set. Keywords\u2014HOG, SIFT, GMM, Fisher, Neural Networks.", "target": "Continuous Multimodal Emotion Recognition Approach for AVEC 2017"}
{"id": "task1540-c479a1c6c7d54ae0bfd79eedb76d51a2", "input": "This paper describes a new method for classifying a dataset that partitions elements into different categories. It has relations with neural networks but works in a different way, requiring only a single pass through the classifier to generate the weight sets. A grid structure is required and a novel idea of converting a row of real values into a 2-D or grid-like structure of value bands. Each cell in the band can then store a cell weight value and also a set of weights that represent its own importance to each of the output categories. For any input that needs to be categorised, all of the output weight lists for each relevant input cell can be retrieved and summed to produce a probability for what the correct output category is. So the relative importance of each input point to the output is distributed to each cell. The construction process itself can simply be the reinforcement of the weight values, without requiring an iterative adjustment process, making it potentially much faster.", "target": "A Single-Pass Classifier for Categorical Data"}
{"id": "task1540-c9f2c8254be94311ba2501637094ff7e", "input": "Leaf vein forms the basis of leaf characterization and classification. Different species have different leaf vein patterns. It is seen that leaf vein segmentation will help in maintaining a record of all the leaves according to their specific pattern of veins thus provide an effective way to retrieve and store information regarding various plant species in database as well as provide an effective means to characterize plants on the basis of leaf vein structure which is unique for every species. The algorithm proposes a new way of segmentation of leaf veins with the use of Odd Gabor filters and the use of morphological operations for producing a better output. The Odd Gabor filter gives an efficient output and is robust and scalable as compared with the existing techniques as it detects the fine fiber like veins present in leaves much more efficiently.", "target": "Leaf vein segmentation using Odd Gabor filters and morphological operations"}
{"id": "task1540-e89683cb9cce4bd6b7a0bc19c88d9a34", "input": "We present the AP16-OL7 database which was released as the training and test data for the oriental language recognition (OLR) challenge on APSIPA 2016. Based on the database, a baseline system was constructed on the basis of the i-vector model. We report the baseline results evaluated in various metrics defined by the AP16-OLR evaluation plan and demonstrate that AP16-OL7 is a reasonable data resource for multilingual research.", "target": "AP16-OL7: A Multilingual Database for Oriental Languages and A Language Recognition Baseline"}
{"id": "task1540-dde6666cf7d14a5a8344d1a75d07bf4f", "input": "We investigate the problem of sentence-level supporting argument detection from relevant documents for user-specified claims. A dataset containing claims and associated citation articles is collected from online debate website idebate.org. We then manually label sentence-level supporting arguments from the documents along with their types as STUDY, FACTUAL, OPINION, or REASONING. We further characterize arguments of different types, and explore whether leveraging type information can facilitate the supporting arguments detection task. Experimental results show that LambdaMART (Burges, 2010) ranker that uses features informed by argument types yields better performance than the same ranker trained without type information.", "target": "Understanding and Detecting Supporting Arguments of Diverse Types"}
{"id": "task1540-9cb2b001e8e44904a0375a92cc131819", "input": "The proposed algorithmic approach deals with finding the sense of a word in an electronic data. Now a day, in different communication mediums like internet, mobile services etc. people use few words, which are slang in nature. This approach detects those abusive words using supervised learning procedure. But in the real life scenario, the slang words are not used in complete word forms always. Most of the times, those words are used in different abbreviated forms like sounds alike forms, taboo morphemes etc. This proposed approach can detect those abbreviated forms also using semi supervised learning procedure. Using the synset and concept analysis of the text, the probability of a suspicious word to be a slang word is also evaluated.", "target": "Detection of Slang Words in e-Data using semi- Supervised Learning"}
{"id": "task1540-15d872494fdd4b3a99a1b745da81702c", "input": "Lipreading, i.e. speech recognition from visual-only recordings of a speaker\u2019s face, can be achieved with a processing pipeline based solely on neural networks, yielding significantly better accuracy than conventional methods. Feedforward and recurrent neural network layers (namely Long Short-Term Memory; LSTM) are stacked to form a single structure which is trained by back-propagating error gradients through all the layers. The performance of such a stacked network was experimentally evaluated and compared to a standard Support Vector Machine classifier using conventional computer vision features (Eigenlips and Histograms of Oriented Gradients). The evaluation was performed on data from 19 speakers of the publicly available GRID corpus. With 51 different words to classify, we report a best word accuracy on held-out evaluation speakers of 79.6% using the end-toend neural network-based solution (11.6% improvement over the best feature-based solution evaluated).", "target": "LIPREADING WITH LONG SHORT-TERM MEMORY"}
{"id": "task1540-11f68d771a8b4db890148e1a83eb037f", "input": "In this paper, we present a transfer learning approach for music classification and regression tasks. We propose to use a pre-trained convnet feature, a concatenated feature vector using the activations of feature maps of multiple layers in a trained convolutional network. We show how this convnet feature can serve as general-purpose music representation. In the experiments, a convnet is trained for music tagging and then transferred to other music-related classification and regression tasks. The convnet feature outperforms the baseline MFCC feature in all the considered tasks and several previous approaches that are aggregating MFCCs as well as lowand high-level music features.", "target": "TRANSFER LEARNING FOR MUSIC CLASSIFICATION AND REGRESSION TASKS"}
{"id": "task1540-20f46f51ffbb4b8bbc37a920cedbdc6e", "input": "Several generic summarization algorithms were developed in the past and successfully applied in fields such as text and speech summarization. In this paper, we review and apply these algorithms to music. To evaluate this summarization\u2019s performance, we adopt an extrinsic approach: we compare a Fado Genre Classifier\u2019s performance using truncated contiguous clips against the summaries extracted with those algorithms on 2 different datasets. We show that Maximal Marginal Relevance (MMR), LexRank and Latent Semantic Analysis (LSA) all improve classification performance in both datasets used for testing.", "target": "On the Application of Generic Summarization Algorithms to Music"}
{"id": "task1540-38af7d151e844905be39fba66c710f7d", "input": "Normalized web distance (NWD) is a similarity or normalized semantic distance based on the World Wide Web or any other large electronic database, for instance Wikipedia, and a search engine that returns reliable aggregate page counts. For sets of search terms the NWD gives a similarity on a scale from 0 (identical) to 1 (completely different). The NWD approximates the similarity according to all (upper semi)computable properties. We develop the theory and give applications. The derivation of the NWD method is based on Kolmogorov complexity.", "target": "Web Similarity"}
{"id": "task1540-25956d8644224911ac3cfcc0aae34761", "input": "Machine-learning techniques have been recently used with spectacular results to generate artefacts such as music or text. However, these techniques are still unable to capture and generate artefacts that are convincingly structured. In this paper we present an approach to generate structured musical sequences. We introduce a mechanism for sampling efficiently variations of musical sequences. Given a input sequence and a statistical model, this mechanism samples a set of sequences whose distance to the input sequence is approximately within specified bounds. This mechanism is implemented as an extension of belief propagation, and uses local fields to bias the generation. We show experimentally that sampled sequences are indeed closely correlated to the standard musical similarity measure defined by Mongeau and Sankoff. We then show how this mechanism can used to implement composition strategies that enforce arbitrary structure on a musical lead sheet generation problem.", "target": "Sampling Variations of Lead Sheets"}
{"id": "task1540-6c0356917ef5456dbade601bdced33d4", "input": "Machine learning has been used to detect new malware in recent years, while malware authors have strong motivation to attack such algorithms.Malware authors usually have no access to the detailed structures and parameters of the machine learning models used by malware detection systems, and therefore they can only perform black-box attacks. This paper proposes a generative adversarial network (GAN) based algorithm named MalGAN to generate adversarial malware examples, which are able to bypass black-box machine learning based detection models. MalGAN uses a substitute detector to fit the black-box malware detection system. A generative network is trained to minimize the generated adversarial examples\u2019 malicious probabilities predicted by the substitute detector. The superiority of MalGAN over traditional gradient based adversarial example generation algorithms is that MalGAN is able to decrease the detection rate to nearly zero and make the retraining based defensive method against adversarial examples hard to work.", "target": "Generating Adversarial Malware Examples for Black-Box Attacks Based on GAN"}
{"id": "task1540-9ec7afb4ffa24e3a9b77c6fb715cdd06", "input": "We present a neural architecture for sequence processing. The ByteNet is a stack of two dilated convolutional neural networks, one to encode the source sequence and one to decode the target sequence, where the target network unfolds dynamically to generate variable length outputs. The ByteNet has two core properties: it runs in time that is linear in the length of the sequences and it preserves the sequences\u2019 temporal resolution. The ByteNet decoder attains state-of-the-art performance on character-level language modelling and outperforms the previous best results obtained with recurrent neural networks. The ByteNet also achieves a performance on raw character-level machine translation that approaches that of the best neural translation models that run in quadratic time. The implicit structure learnt by the ByteNet mirrors the expected alignments between the sequences.", "target": "Neural Machine Translation in Linear Time"}
{"id": "task1540-fe04cf65869e4c08b9c8f598c6eec265", "input": "In Bayesian networks, a Most Probable Explanation (MPE) is a complete variable instantiation with the highest probability given the current evidence. In this paper, we discuss the problem of finding robustness conditions of the MPE under single parameter changes. Specifically, we ask the question: How much change in a single network parameter can we afford to apply while keeping the MPE unchanged? We will describe a procedure, which is the first of its kind, that computes this answer for all parameters in the Bayesian network in time O(n exp(w)), where n is the number of network variables and w is its treewidth.", "target": "On the Robustness of Most Probable Explanations"}
{"id": "task1540-3fb6f13f1a6540408e52b2d0e57920ec", "input": "We describe methods for managing the com\u00ad plexity of information displayed to people responsible for making high-stakes, time\u00ad critical decisions. The techniques provide tools for real-time control of the configura\u00ad tion and quantity of information displayed to a user, and a methodology for designing flexible human-computer interfaces for mon\u00ad itoring applications. After defining a proto\u00ad typical set of display decision problems, we introduce the expected value of revealed in\u00ad formation (EVRI) and the related measure of expected value of displayed information (EVDI) . We describe how these measures can be used to enhance computer displays used for monitoring complex systems. We moti\u00ad vate the presentation by discussing our ef\u00ad forts to employ decision-theoretic control of displays for a time-critical monitoring appli\u00ad cation at the NASA Mission Control Center in Houston.", "target": "Display of Information for Time-Critical Decision Making"}
{"id": "task1540-4deeaf2c51024ed2a0917edaddba6c40", "input": "This paper describes the architecture and implementation of a rule-based grapheme to phoneme converter for Turkish. The system accepts surface form as input, outputs SAMPA mapping of the all parallel pronounciations according to the morphological analysis together with stress positions. The system has been implemented in Python.", "target": "Towards Turkish ASR: Anatomy of a rule-based Turkish g2p"}
{"id": "task1540-5cf927a8fcf649eba162baf82b1e1f9e", "input": "In Recommender Systems research, algorithms are often characterized as either Collaborative Filtering (CF) or Content Based (CB). CF algorithms are trained using a dataset of user explicit or implicit preferences while CB algorithms are typically based on item profiles. These approaches harness very different data sources hence the resulting recommended items are generally also very different. This paper presents a novel model that serves as a bridge from items content into their CF representations. We introduce a multiple input deep regression model to predict the CF latent embedding vectors of items based on their textual description and metadata. We showcase the effectiveness of the proposed model by predicting the CF vectors of movies and apps based on their textual descriptions. Finally, we show that the model can be further improved by incorporating metadata such as the movie release year and tags which contribute to a higher accuracy.", "target": "Microsoft Word - cb2cf_arxiv.docx"}
{"id": "task1540-e45b5f3133fb4923b6f36a1056792eb6", "input": "Possibilistic logic bases and possibilistic graphs are two different frameworks of interest for representing knowledge. The former stratifies the pieces of knowledge (expressed by logical formulas) accor?i\ufffdg to their level of certainty, while the latter exhibits relationships between variables. The two types of representations are semantically equivalent when they lead to the same possibility distribution (which rank\u00ad orders the possible interpretations). A possibility distribution can be decomposed using a chain rule which may be based on two different kinds of conditioning which exist in possibility theory (one based on product in a numerical setting, one based on minimum operation in a qualitative setting). These two types of conditioning induce two kin_ds of possibilistic graphs. In both cases, a translatiOn of these graphs into possibilistic bases is provided. The converse translation from a possibilistic knowledge base into a min-based graph is also described.", "target": "Possibilistic logic bases and possibilistic graphs"}
{"id": "task1540-a8cb77345d234fb382918686275e772b", "input": "We present a token-level decision summarization framework that utilizes the latent topic structures of utterances to identify \u201csummaryworthy\u201d words. Concretely, a series of unsupervised topic models is explored and experimental results show that fine-grained topic models, which discover topics at the utterance-level rather than the document-level, can better identify the gist of the decisionmaking process. Moreover, our proposed token-level summarization approach, which is able to remove redundancies within utterances, outperforms existing utterance ranking based summarization methods. Finally, context information is also investigated to add additional relevant information to the summary.", "target": "Unsupervised Topic Modeling Approaches to Decision Summarization in Spoken Meetings"}
{"id": "task1540-9a93baa38ee8407e8d4b7089e56dfb8e", "input": "We consider the quantified constraint satisfaction problem (QCSP) which is to decide, given a structure and a first-order sentence (not assumed here to be in prenex form) built from conjunction and quantification, whether or not the sentence is true on the structure. We present a proof system for certifying the falsity of QCSP instances and develop its basic theory; for instance, we provide an algorithmic interpretation of its behavior. Our proof system places the established Q-resolution proof system in a broader context, and also allows us to derive QCSP tractability results.", "target": "BEYOND Q-RESOLUTION AND PRENEX FORM: A PROOF SYSTEM FOR QUANTIFIED CONSTRAINT SATISFACTION"}
{"id": "task1540-9193e6d264704b098b3d2e84cbe88f26", "input": "The goal of open information extraction (OIE) is to extract surface relations and their arguments from naturallanguage text in an unsupervised, domainindependent manner. In this paper, we explore how overly-specific extractions can be reduced in OIE systems without producing uninformative or inaccurate results. We propose MinIE, an OIE system that produces minimized, annotated extractions. At its heart, MinIE rewrites OIE extractions by (1) identifying and removing parts that are considered overly specific; (2) representing information about polarity, modality, attribution, and quantities with suitable annotations instead of in the actual extraction. We conducted an experimental study with several real-world datasets and found that MinIE achieves competitive or higher precision and recall than most prior systems, while at the same time producing much shorter extractions.", "target": "MinIE: Minimizing Facts in Open Information Extraction"}
{"id": "task1540-4562ac714e1046e48fef17a6c9fa6617", "input": "This paper proposes an efficient algorithm (HOLRR) to handle regression tasks where the outputs have a tensor structure. We formulate the regression problem as the minimization of a least square criterion under a multilinear rank constraint, a difficult non convex problem. HOLRR computes efficiently an approximate solution of this problem, with solid theoretical guarantees. A kernel extension is also presented. Experiments on synthetic and real data show that HOLRR outperforms multivariate and multilinear regression methods and is considerably faster than existing tensor methods.", "target": "Higher-Order Low-Rank Regression"}
{"id": "task1540-a49724dba444448ea6508d2f04cc7292", "input": "We consider emphatic temporal-difference learning algorithms for policy evaluation in discounted Markov decision processes with finite spaces. Such algorithms were recently proposed by Sutton, Mahmood, and White (2015) as an improved solution to the problem of divergence of off-policy temporal-difference learning with linear function approximation. We present in this paper the first convergence proofs for two emphatic algorithms, ETD(\u03bb) and ELSTD(\u03bb). We prove, under general off-policy conditions, the convergence in L for ELSTD(\u03bb) iterates, and the almost sure convergence of the approximate value functions calculated by both algorithms using a single infinitely long trajectory. Our analysis involves new techniques with applications beyond emphatic algorithms leading, for example, to the first proof that standard TD(\u03bb) also converges under off-policy training for \u03bb sufficiently large.", "target": "On Convergence of Emphatic Temporal-Difference Learning\u2217"}
{"id": "task1540-949339e9a46f4bff8f41252d8997dcdc", "input": "The family of temporal difference (TD) methods span a spectrum from computationally frugal linear methods like TD(\u03bb) to data efficient least squares methods. Least square methods make the best use of available data directly computing the TD solution and thus do not require tuning a typically highly sensitive learning rate parameter, but require quadratic computation and storage. Recent algorithmic developments have yielded several sub-quadratic methods that use an approximation to the least squares TD solution, but incur bias. In this paper, we propose a new family of accelerated gradient TD (ATD) methods that (1) provide similar data efficiency benefits to least-squares methods, at a fraction of the computation and storage (2) significantly reduce parameter sensitivity compared to linear TD methods, and (3) are asymptotically unbiased. We illustrate these claims with a proof of convergence in expectation and experiments on several benchmark domains and a large-scale industrial energy allocation domain.", "target": "Accelerated Gradient Temporal Difference Learning"}
{"id": "task1540-f34a392208eb46438f8e9f44170b148d", "input": "Despite their success, convolutional neural networks are computationally expensive because they must examine all image locations. Stochastic attention-based models have been shown to improve computational efficiency at test time, but they remain difficult to train because of intractable posterior inference and high variance in the stochastic gradient estimates. Borrowing techniques from the literature on training deep generative models, we present the Wake-Sleep Recurrent Attention Model, a method for training stochastic attention networks which improves posterior inference and which reduces the variability in the stochastic gradients. We show that our method can greatly speed up the training time for stochastic attention networks in the domains of image classification and caption generation.", "target": "Learning Wake-Sleep Recurrent Attention Models"}
{"id": "task1540-d0e7da6a49404cf2913eee43b43a2991", "input": "Despite interest in using cross-lingual knowledge to learn word embeddings for various tasks, a systematic comparison of the possible approaches is lacking in the literature. We perform an extensive evaluation of four popular approaches of inducing cross-lingual embeddings, each requiring a different form of supervision, on four typographically different language pairs. Our evaluation setup spans four different tasks, including intrinsic evaluation on mono-lingual and cross-lingual similarity, and extrinsic evaluation on downstream semantic and syntactic applications. We show that models which require expensive cross-lingual knowledge almost always perform better, but cheaply supervised models often prove competitive on certain tasks.", "target": "Cross-lingual Models of Word Embeddings: An Empirical Comparison"}
{"id": "task1540-cb7e158a340941dcb5e13a44e3a4027c", "input": "Despite recent advances, the remaining bottlenecks in deep generative models are necessity of extensive training and difficulties with generalization from small number of training examples. Both problems may be addressed by conditional generative models that are trained to adapt the generative distribution to additional input data. So far this idea was explored only under certain limitations such as restricting the input data to be a single object or multiple objects representing the same concept. In this work we develop a new class of deep generative model called generative matching networks which is inspired by the recently proposed matching networks for one-shot learning in discriminative tasks and the ideas from meta-learning. By conditioning on the additional input dataset, generative matching networks may instantly learn new concepts that were not available during the training but conform to a similar generative process, without explicit limitations on the number of additional input objects or the number of concepts they represent. Our experiments on the Omniglot dataset demonstrate that generative matching networks can significantly improve predictive performance on the fly as more additional data is available to the model and also adapt the latent space which is beneficial in the context of feature extraction.", "target": "GENERATIVE MATCHING NETWORKS"}
{"id": "task1540-22667e06f21c451e83cea7f14cd08631", "input": "The role of sentiment analysis is increasingly emerging to study software developers\u2019 emotions by mining crowdgenerated content within social software engineering tools. However, off-the-shelf sentiment analysis tools have been trained on non-technical domains and general-purpose social media, thus resulting in misclassifications of technical jargon and problem reports. Here, we present Senti4SD, a classifier specifically trained to support sentiment analysis in developers\u2019 communication channels. Senti4SD is trained and validated using a gold standard of Stack Overflow questions, answers, and comments manually annotated for sentiment polarity. It exploits a suite of both lexiconand keyword-based features, as well as semantic features based on word embedding. With respect to a mainstream off-the-shelf tool, which we use as a baseline, Senti4SD reduces the misclassifications of neutral and positive posts as emotionally negative. To encourage replications, we release a lab package including the classifier, the word embedding space, and the gold standard with annotation guidelines.", "target": "Sentiment Polarity Detection for Software Development"}
{"id": "task1540-5df771a616cf47409f5eb9d4d80e4fb4", "input": "The increasing amount of available Linked Data resources is laying the foundations for more advanced Semantic Web applications. One of their main limitations, however, remains the general low level of data quality. In this paper we focus on a measure of quality which is negatively affected by the increase of the available resources. We propose a measure of semantic richness of Linked Data concepts and we demonstrate our hypothesis that the more a concept is reused, the less semantically rich it becomes. This is a significant scalability issue, as one of the core aspects of Linked Data is the propagation of semantic information on the Web by reusing common terms. We prove our hypothesis with respect to our measure of semantic richness and we validate our model empirically. Finally, we suggest possible future directions to address this scalability problem.", "target": "A Linked Data Scalability Challenge: Concept Reuse Leads to Semantic Decay"}
{"id": "task1540-775cc45d5f8a4ee5bc73a25f3724465c", "input": "Many deep Convolutional Neural Networks (CNN) make incorrect predictions on adversarial samples obtained by imperceptible perturbations of clean samples. We hypothesize that this is caused by a failure to suppress unusual signals within network layers. As remedy we propose the use of Symmetric Activation Functions (SAF) in non-linear signal transducer units. These units suppress signals of exceptional magnitude. We prove that SAF networks can perform classification tasks to arbitrary precision in a simplified situation. In practice, rather than use SAFs alone, we add them into CNNs to improve their robustness. The modified CNNs can be easily trained using popular strategies with the moderate training load. Our experiments on MNIST and CIFAR-10 show that the modified CNNs perform similarly to plain ones on clean samples, and are remarkably more robust against adversarial and nonsense samples.", "target": "Suppressing the Unusual: towards Robust CNNs using Symmetric Activation Functions"}
{"id": "task1540-14fac6770ea54756bfe7350c435856c5", "input": "Classification performance is often not uniform over the data. Some areas in the input space are easier to classify than others. Features that hold information about the \u201ddifficulty\u201d of the data may be nondiscriminative and are therefore disregarded in the classification process. We propose a meta-learning approach where performance may be improved by post-processing. This improvement is done by establishing a dynamic threshold on the base-classifier results. Since the base-classifier is treated as a \u201cblack box\u201d the method presented can be used on any state of the art classifier in order to try an improve its performance. We focus our attention on how to better control the true-positive/false-positive tradeoff known as the ROC curve. We propose an algorithm for the derivation of optimal thresholds by redistributing the error depending on features that hold information about difficulty. We demonstrate the resulting benefit on both synthetic and real-life data.", "target": "Bending the Curve: Improving the ROC Curve Through Error Redistribution"}
{"id": "task1540-29c6a144f3644b2e84024d9bd1c4e0b5", "input": "The basic features of some of the most versatile and popular open source frameworks for machine learning (TensorFlow, Deep Learning4j, and H2O) are considered and compared. Their comparative analysis was performed and conclusions were made as to the advantages and disadvantages of these platforms. The performance tests for the de facto standard MNIST data set were carried out on H2O framework for deep learning algorithms designed for CPU and GPU platforms for single-threaded and multithreaded modes of operation. Keywords\u2014machine learning; deep learning; TensorFlow; Deep Learning4j; H2O; MNIST; multicore CPU; GPU.", "target": "Comparative Analysis of Open Source Frameworks for Machine Learning with Use Case in Single- Threaded and Multi-Threaded Modes"}
{"id": "task1540-1c7e702e917640c6bf28c04eefb38ab6", "input": "Or\u2019s of And\u2019s (OA) models are comprised of a small number of disjunctions of conjunctions, also called disjunctive normal form. An example of an OA model is as follows: If (x1 = \u2018blue\u2019 AND x2 = \u2018middle\u2019) OR (x1 = \u2018yellow\u2019), then predict Y = 1, else predict Y = 0. Or\u2019s of And\u2019s models have the advantage of being interpretable to human experts, since they are a set of conditions that concisely capture the characteristics of a specific subset of data. We present two optimization-based machine learning frameworks for constructing OA models, Optimized OA (OOA) and its faster version, Optimized OA with Approximations (OOAx). We prove theoretical bounds on the properties of patterns in an OA model. We build OA models as a diagnostic screening tool for obstructive sleep apnea, that achieves high accuracy with a substantial gain in interpretability over other methods.", "target": "Learning Optimized Or\u2019s of And\u2019s"}
{"id": "task1540-382340f342d14392939a1e8707c9d794", "input": "Domain similarity measures can be used<lb>to gauge adaptability and select suitable<lb>data for transfer learning, but existing ap-<lb>proaches define ad hoc measures that are deemed suitable for respective tasks. In-<lb>spired by work on curriculum learning, we<lb>propose to learn data selection measures<lb>using Bayesian Optimization and evaluate<lb>them across models, domains and tasks. Our learned measures outperform existing<lb>domain similarity measures significantly<lb>on three tasks: sentiment analysis, part-<lb>of-speech tagging, and parsing. We show<lb>the importance of complementing similarity with diversity, and that learned mea-<lb>sures are\u2014to some degree\u2014transferable<lb>across models, domains, and even tasks.", "target": "Learning to select data for transfer learning with Bayesian Optimization"}
{"id": "task1540-b884b2b941584ff19c6823a7b673c04d", "input": "Previous studies have demonstrated that encoding a Bayesian network into a SAT formula and then performing weighted model counting using a backtracking search algorithm can be an effective method for exact inference. In this paper, we present techniques for improving this approach for Bayesian networks with noisy-OR and noisy-MAX relations\u2014 two relations that are widely used in practice as they can dramatically reduce the number of probabilities one needs to specify. In particular, we present two SAT encodings for noisy-OR and two encodings for noisy-MAX that exploit the structure or semantics of the relations to improve both time and space efficiency, and we prove the correctness of the encodings. We experimentally evaluated our techniques on large-scale real and randomly generated Bayesian networks. On these benchmarks, our techniques gave speedups of up to two orders of magnitude over the best previous approaches for networks with noisyOR/MAX relations and scaled up to larger networks. As well, our techniques extend the weighted model counting approach for exact inference to networks that were previously intractable for the approach.", "target": "Exploiting Structure in Weighted Model Counting Approaches to Probabilistic Inference"}
{"id": "task1540-55fe65513e834a23a2217ac30885da8f", "input": "One major deficiency of most semantic representation techniques is that they usually model a word type as a single point in the semantic space, hence conflating all the meanings that the word can have. Addressing this issue by learning distinct representations for individual meanings of words has been the subject of several research studies in the past few years. However, the generated sense representations are either not linked to any sense inventory or are unreliable for infrequent word senses. We propose a technique that tackles these problems by de-conflating the representations of words based on the deep knowledge it derives from a semantic network. Our approach provides multiple advantages in comparison to the past work, including its high coverage and the ability to generate accurate representations even for infrequent word senses. We carry out evaluations on six datasets across two semantic similarity tasks and report state-of-the-art results on most of them.", "target": "De-Conflated Semantic Representations"}
{"id": "task1540-62cb6500eed246a49e05f3f2dd8a99a6", "input": "In the problem of edge sign prediction, we are given a directed graph (representing a social network), and our task is to predict the binary labels of the edges (i.e., the positive or negative nature of the social relationships). Many successful heuristics for this problem are based on the troll-trust features, estimating at each node the fraction of outgoing and incoming positive/negative edges. We show that these heuristics can be understood, and rigorously analyzed, as approximators to the Bayes optimal classifier for a simple probabilistic model of the edge labels. We then show that the maximum likelihood estimator for this model approximately corresponds to the predictions of a Label Propagation algorithm run on a transformed version of the original social graph. Extensive experiments on a number of real-world datasets show that this algorithm is competitive against state-ofthe-art classifiers in terms of both accuracy and scalability. Finally, we show that trolltrust features can also be used to derive online learning algorithms which have theoretical guarantees even when edges are adversarially labeled.", "target": "On the Troll-Trust Model for Edge Sign Prediction in Social Networks"}
{"id": "task1540-eacfb44c73e145f09f8027c2e96d8a18", "input": "Learning in probabilistic models is often severely hampered by the general intractability of the normalization factor and its derivatives. Here we propose a new learning technique that obviates the need to compute an intractable normalization factor or sample from the equilibrium distribution of the model. This is achieved by establishing dynamics that would transform the observed data distribution into the model distribution, and then setting as the objective the minimization of the initial flow of probability away from the data distribution. Score matching, minimum velocity learning, and certain forms of contrastive divergence are shown to be special cases of this learning technique. We demonstrate the application of minimum probability flow learning to parameter estimation in Ising models, deep belief networks, multivariate Gaussian distributions and a continuous model with a highly general energy function defined as a power series. In the Ising model case, minimum probability flow learning outperforms current state of the art techniques by approximately two orders of magnitude in learning time, with comparable error in the recovered parameters. It is our hope that this technique will alleviate existing restrictions on the classes of probabilistic models that are practical for use.", "target": "Minimum Probability Flow Learning"}
{"id": "task1540-b81f06c60fa94326afb7c59edba36bc9", "input": "Recent works on end-to-end neural network-based architectures for machine translation have shown promising results for English-French and English-German translation. Unlike these language pairs, however, in the majority of scenarios, there is a lack of high quality parallel corpora. In this work, we focus on applying neural machine translation to challenging/low-resource languages Turkish and low-resource domains such as parallel corpora of Chinese chat messages. In particular, we investigated how to leverage abundant monolingual data for these low-resource translation tasks. Without the use of external alignment tools, we obtained up to a 1.96 BLEU score improvement with our proposed method compared to the previous best result in Turkish-to-English translation on the IWLST 2014 dataset. On Chinese-toEnglish translation by using the OpenMT 2015 dataset, we were able to obtain up to a 1.59 BLEU score improvement over phrase-based and hierarchical phrase-based baselines.", "target": "On Using Monolingual Corpora in Neural Machine Translation"}
{"id": "task1540-d3456466c16647029187eb4068c5ef70", "input": "We propose a method for learning expressive energy-based policies for continuous states and actions, which has been feasible only in tabular domains before. We apply our method to learning maximum entropy policies, resulting into a new algorithm, called soft Q-learning, that expresses the optimal policy via a Boltzmann distribution. We use the recently proposed amortized Stein variational gradient descent to learn a stochastic sampling network that approximates samples from this distribution. The benefits of the proposed algorithm include improved exploration and compositionality that allows transferring skills between tasks, which we confirm in simulated experiments with swimming and walking robots. We also draw a connection to actorcritic methods, which can be viewed performing approximate inference on the corresponding energy-based model.", "target": "Reinforcement Learning with Deep Energy-Based Policies"}
{"id": "task1540-0b90f62d080c4f868b13b296bd61d949", "input": "Recent research shows that most Brazilian students have serious problems regarding their reading skills. The full development of this skill is key for the academic and professional future of every citizen. Tools for classifying the complexity of reading materials for children aim to improve the quality of the model of teaching reading and text comprehension. For English, Feng\u2019s work [11] is considered the state-of-art in grade level prediction and achieved 74% of accuracy in automatically classifying 4 levels of textual complexity for close school grades. There are no classifiers for nonfiction texts for close grades in Portuguese. In this article, we propose a scheme for manual annotation of texts in 5 grade levels, which will be used for customized reading to avoid the lack of interest by students who are more advanced in reading and the blocking of those that still need to make further progress. We obtained 52% of accuracy in classifying texts into 5 levels and 74% in 3 levels. The results prove to be promising when compared to the state-of-art work.", "target": "Automatic Classification of the Complexity of Nonfiction Texts in Portuguese for Early School Years"}
{"id": "task1540-6130dad97f7a429181f65055befa7c8c", "input": "We present DEFEXT, an easy to use semi supervised Definition Extraction Tool. DEFEXT is designed to extract from a target corpus those textual fragments where a term is explicitly mentioned together with its core features, i.e. its definition. It works on the back of a Conditional Random Fields based sequential labeling algorithm and a bootstrapping approach. Bootstrapping enables the model to gradually become more aware of the idiosyncrasies of the target corpus. In this paper we describe the main components of the toolkit as well as experimental results stemming from both automatic and manual evaluation. We release DEFEXT as open source along with the necessary files to run it in any Unix machine. We also provide access to training and test data for immediate use.", "target": "DEFEXT: A Semi Supervised Definition Extraction Tool"}
{"id": "task1540-5c4b20d7f3504494b9bab707a31f605b", "input": "The paper addresses the problem of learning a regression model parameterized by a fixedrank positive semidefinite matrix. The focus is on the nonlinear nature of the search space and on scalability to high-dimensional problems. The mathematical developments rely on the theory of gradient descent algorithms adapted to the Riemannian geometry that underlies the set of fixed-rank positive semidefinite matrices. In contrast with previous contributions in the literature, no restrictions are imposed on the range space of the learned matrix. The resulting algorithms maintain a linear complexity in the problem size and enjoy important invariance properties. We apply the proposed algorithms to the problem of learning a distance function parameterized by a positive semidefinite matrix. Good performance is observed on classical benchmarks.", "target": "Regression on fixed-rank positive semidefinite matrices: a Riemannian approach"}
{"id": "task1540-d9f3d414d59544e89bd042d0120c9d71", "input": "A number of visual question answering approaches have been proposed recently, aiming at understanding the visual scenes by answering the natural language questions. While the image question answering has drawn significant attention, video question answering is largely unexplored. Video-QA is different from Image-QA since the information and the events are scattered among multiple frames. In order to better utilize the temporal structure of the videos and the phrasal structures of the answers, we propose two mechanisms: the re-watching and the re-reading mechanisms and combine them into the forgettable-watcher model. Then we propose a TGIF-QA dataset for video question answering with the help of automatic question generation. Finally, we evaluate the models on our dataset. The experimental results show the effectiveness of our proposed models.", "target": "The Forgettable-Watcher Model for Video Question Answering"}
{"id": "task1540-1a04dc91a4fd4c0d8db35e356ab50c4c", "input": "We introduce the nonparametric metadata dependent relational (NMDR) model, a Bayesian nonparametric stochastic block model for network data. The NMDR allows the entities associated with each node to have mixed membership in an unbounded collection of latent communities. Learned regression models allow these memberships to depend on, and be predicted from, arbitrary node metadata. We develop efficient MCMC algorithms for learning NMDR models from partially observed node relationships. Retrospective MCMC methods allow our sampler to work directly with the infinite stickbreaking representation of the NMDR, avoiding the need for finite truncations. Our results demonstrate recovery of useful latent communities from real-world social and ecological networks, and the usefulness of metadata in link prediction tasks.", "target": "The Nonparametric Metadata Dependent Relational Model"}
{"id": "task1540-c75e1ea32ffc421a881e646e0e4aeee7", "input": "This paper aims to introduces a new algorithm for automatic speech-to-text summarization based on statistical divergences of probabilities and graphs. The input is a text from speech conversations with noise, and the output a compact text summary. Our results, on the pilot task CCCS Multiling 2015 French corpus are very encouraging.", "target": "LIA-RAG: a system based on graphs and divergence of probabilities applied to Speech-To-Text Summarization"}
{"id": "task1540-d0ec64e485504e7b9f8193f305b19d30", "input": "We compare the effectiveness of four different syntactic CCG parsers for a semantic slotfilling task to explore how much syntactic supervision is required for downstream semantic analysis. This extrinsic, task-based evaluation also provides a unique window into the semantics captured (or missed) by unsupervised grammar induction systems.", "target": "Evaluating Induced CCG Parsers on Grounded Semantic Parsing"}
{"id": "task1540-e2d034bd59af4ff39456fede2530c9aa", "input": "The pre-image problem has to be solved during inference by most structured output predictors. For string kernels, this problem corresponds to finding the string associated to a given input. An algorithm capable of solving or finding good approximations to this problem would have many applications in computational biology and other fields. This work uses a recent result on combinatorial optimization of linear predictors based on string kernels to develop, for the pre-image, a low complexity upper bound valid for many string kernels. This upper bound is used with success in a branch and bound searching algorithm. Applications and results in the discovery of druggable peptides are presented and discussed.", "target": "On the String Kernel Pre-Image Problem with Applications in Drug Discovery"}
{"id": "task1540-88f380c99a7242fb833b0b966b9c69e7", "input": "In this paper, we investigate whether an a priori disambiguation of word senses is strictly necessary or whether the meaning of a word in context can be disambiguated through composition alone. We evaluate the performance of off-the-shelf singlevector and multi-sense vector models on a benchmark phrase similarity task and a novel task for word-sense discrimination. We find that single-sense vector models perform as well or better than multi-sense vector models despite arguably less clean elementary representations. Our findings furthermore show that simple composition functions such as pointwise addition are able to recover sense specific information from a single-sense vector model remark-", "target": "One Representation per Word \u2014 Does it make Sense for Composition?"}
{"id": "task1540-8e8cd3a7d8c746b4a5c2b46f9fbdd0dc", "input": "Sustainable and economical generation of electrical power is an essential and mandatory component of infrastructure in today\u2019s world. Optimal generation (generator subset selection) of power requires a careful evaluation of various factors like type of source, generation, transmission & storage capacities, congestion among others which makes this a difficult task. We created a grid to simulate various conditions including stimuli like generator supply, weather and load demand using Siemens PSS/E software and this data is trained using deep learning methods and subsequently tested. The results are highly encouraging. As per our knowledge, this is the first paper to propose a working and scalable deep learning model for this problem.", "target": "Intelligent Subset Selection of Power Generators for Economic Dispatch"}
{"id": "task1540-dbb4e941afab44e4bae8e9e6a3cd6527", "input": "Previous work has modeled the compositionality of words by creating characterlevel models of meaning, reducing problems of sparsity for rare words. However, in many writing systems compositionality has an effect even on the character-level: the meaning of a character is derived by the sum of its parts. In this paper, we model this effect by creating embeddings for characters based on their visual characteristics, creating an image for the character and running it through a convolutional neural network to produce a visual character embedding. Experiments on a text classification task demonstrate that such model allows for better processing of instances with rare characters in languages such as Chinese, Japanese, and Korean. Additionally, qualitative analyses demonstrate that our proposed model learns to focus on the parts of characters that carry categorical content which resulting in embeddings that are coherent in visual space.", "target": "Learning Character-level Compositionality with Visual Features"}
{"id": "task1540-3006bbb7bcfc41d4a8c032d998d3f487", "input": "We first observe a potential weakness of continuous vector representations of symbols in neural machine translation. That is, the continuous vector representation, or a word embedding vector, of a symbol encodes multiple dimensions of similarity, equivalent to encoding more than one meaning of the word. This has the consequence that the encoder and decoder recurrent networks in neural machine translation need to spend substantial amount of their capacity in disambiguating source and target words based on the context which is defined by a source sentence. Based on this observation, in this paper we propose to contextualize the word embedding vectors using a nonlinear bag-of-words representation of the source sentence. Additionally, we propose to represent special tokens (such as numbers, proper nouns and acronyms) with typed symbols to facilitate translating those words that are not well-suited to be translated via continuous vectors. The experiments on En-Fr and En-De reveal that the proposed approaches of contextualization and symbolization improves the translation quality of neural machine translation systems significantly.", "target": "Context-Dependent Word Representation for Neural Machine Translation"}
{"id": "task1540-b93e2ef23feb4d948c0a49c9169a33fc", "input": "Learning neural network architectures is a way to discover new highly predictive models. We propose to focus on this problem from a different perspective where the goal is to discover architectures efficient in terms of both prediction quality and computation cost, e.g time in milliseconds, number of operations... For instance, our approach is able to solve the following task: find the best neural network architecture (in a very large set of possible architectures) able to predict well in less than 100 milliseconds on my mobile phone. Our contribution is based on a new family of models called Budgeted Super Networks that are learned using reinforcement-learning inspired techniques applied to a budgeted learning objective function which includes the computation cost during disk/memory operations at inference. We present a set of experiments on computer vision problems and show the ability of our method to discover efficient architectures in terms of both predictive quality and computation time.", "target": "Learning Time-Efficient Deep Architectures with Budgeted Super Networks"}
{"id": "task1540-37e0c85ba5764b30a295eb5c9fdce38f", "input": "We establish optimal bounds on the number of nested propagation steps in k-consistency tests. It is known that local consistency algorithms such as arc-, pathand k-consistency are not efficiently parallelizable. Their inherent sequential nature is caused by long chains of nested propagation steps, which cannot be executed in parallel. This motivates the question \u201cWhat is the minimum number of nested propagation steps that have to be performed by k-consistency algorithms on (binary) constraint networks with n variables and domain size d?\u201d It was known before that 2-consistency requires \u03a9(nd) and 3-consistency requires \u03a9(n) sequential propagation steps. We answer the question exhaustively for every k \u2265 2: there are binary constraint networks where any k-consistency procedure has to perform \u03a9(nk\u22121dk\u22121) nested propagation steps before local inconsistencies were detected. This bound is tight, because the overall number of propagation steps performed by k-consistency is at most nk\u22121dk\u22121.", "target": "The Propagation Depth of Local Consistency"}
{"id": "task1540-5ab0cb9e59f74ae2bbb548cd692a4da4", "input": "Artificial intelligence is commonly defined as the ability to achieve goals in the world. In the reinforcement learning framework, goals are encoded as reward functions that guide agent behaviour, and the sum of observed rewards provide a notion of progress. However, some domains have no such reward signal, or have a reward signal so sparse as to appear absent. Without reward feedback, agent behaviour is typically random, often dithering aimlessly and lacking intentionality. In this paper we present an algorithm capable of learning purposeful behaviour in the absence of rewards. The algorithm proceeds by constructing temporally extended actions (options), through the identification of purposes that are \u201cjust out of reach\u201d of the agents current behaviour. These purposes establish intrinsic goals for the agent to learn, ultimately resulting in a suite of behaviours that encourage the agent to visit different parts of the state space. Moreover, the approach is particularly suited for settings where rewards are very sparse, and such behaviours can help in the exploration of the environment until reward is observed.", "target": "Learning Purposeful Behaviour in the Absence of Rewards"}
{"id": "task1540-37abdcf3a2204c2b9fed4e6e9e0624be", "input": "imbalanced-learn is an open-source python toolbox aiming at providing a wide range of methods to cope with the problem of imbalanced dataset frequently encountered in machine learning and pattern recognition. The implemented state-of-the-art methods can be categorized into 4 groups: (i) under-sampling, (ii) over-sampling, (iii) combination of overand under-sampling, and (iv) ensemble learning methods. The proposed toolbox only depends on numpy, scipy, and scikit-learn and is distributed under MIT license. Furthermore, it is fully compatible with scikit-learn and is part of the scikit-learn-contrib supported project. Documentation, unit tests as well as integration tests are provided to ease usage and contribution. The toolbox is publicly available in GitHub https://github.com/scikit-learn-contrib/imbalanced-learn.", "target": "Imbalanced-learn: A Python Toolbox to Tackle the Curse of Imbalanced Datasets in Machine Learning"}
{"id": "task1540-67cea3dc682248768099fff2b7e65d56", "input": "We present a self-training approach to unsupervised dependency parsing that reuses existing supervised and unsupervised parsing algorithms. Our approach, called \u2018iterated reranking\u2019 (IR), starts with dependency trees generated by an unsupervised parser, and iteratively improves these trees using the richer probability models used in supervised parsing that are in turn trained on these trees. Our system achieves 1.8% accuracy higher than the stateof-the-part parser of Spitkovsky et al. (2013) on the WSJ corpus.", "target": "Unsupervised Dependency Parsing: Let\u2019s Use Supervised Parsers"}
{"id": "task1540-79ac5a82201645fe8ee129523ff9ccca", "input": "In this paper, we present algorithms that perform gradient ascent of the average reward in a partially observable Markov decision process (POMDP). These algorithms are based on GPOMDP, an algorithm introduced in a companion paper (Baxter & Bartlett, 2001), which computes biased estimates of the performance gradient in POMDPs. The algorithm\u2019s chief advantages are that it uses only one free parameter 2 [0; 1), which has a natural interpretation in terms of bias-variance trade-off, it requires no knowledge of the underlying state, and it can be applied to infinite state, control and observation spaces. We show how the gradient estimates produced by GPOMDP can be used to perform gradient ascent, both with a traditional stochastic-gradient algorithm, and with an algorithm based on conjugate-gradients that utilizes gradient information to bracket maxima in line searches. Experimental results are presented illustrating both the theoretical results of Baxter and Bartlett (2001) on a toy problem, and practical aspects of the algorithms on a number of more realistic problems.", "target": "Experiments with Infinite-Horizon, Policy-Gradient Estimation"}
{"id": "task1540-882605f965784277b31edb392b8ad700", "input": "There are many declarative frameworks that allow us to implement code formatters relatively easily for any specific language, but constructing them is cumbersome. The first problem is that \u201ceverybody\u201d wants to format their code differently, leading to either many formatter variants or a ridiculous number of configuration options. Second, the size of each implementation scales with a language\u2019s grammar size, leading to hundreds of rules. In this paper, we solve the formatter construction problem using a novel approach, one that automatically derives formatters for any given language without intervention from a language expert. We introduce a code formatter called CODEBUFF that uses machine learning to abstract formatting rules from a representative corpus, using a carefully designed feature set. Our experiments on Java, SQL, and ANTLR grammars show that CODEBUFF is efficient, has excellent accuracy, and is grammar invariant for a given language. It also generalizes to a 4th language tested during manuscript preparation.", "target": "Technical Report: Towards a Universal Code Formatter through Machine Learning"}
{"id": "task1540-e1bab8fbcfa94fbbb3acd66565648f0f", "input": "With the advancement of web technology and its growth, there is a huge volume of data present in the web for internet users and a lot of data is generated too. Internet has become a platform for online learning, exchanging ideas and sharing opinions. Social networking sites like Twitter, Facebook, Google+ are rapidly gaining popularity as they allow people to share and express their views about topics,have discussion with different communities, or post messages across the world. There has been lot of work in the field of sentiment analysis of twitter data. This survey focuses mainly on sentiment analysis of twitter data which is helpful to analyze the information in the tweets where opinions are highly unstructured, heterogeneous and are either positive or negative, or neutral in some cases. In this paper, we provide a survey and a comparative analyses of existing techniques for opinion mining like machine learning and lexicon-based approaches, together with evaluation metrics. Using various machine learning algorithms like Naive Bayes, Max Entropy, and Support Vector Machine, we provide a research on twitter data streams. We have also discussed general challenges and applications of Sentiment Analysis on Twitter.", "target": "Sentiment Analysis of Twitter Data :A Survey of Techniques"}
{"id": "task1540-93fb814c935040cb99340cf9e2da52f5", "input": "We present WIKIREADING, a large-scale natural language understanding task and publicly-available dataset with 18 million instances. The task is to predict textual values from the structured knowledge base Wikidata by reading the text of the corresponding Wikipedia articles. The task contains a rich variety of challenging classification and extraction sub-tasks, making it well-suited for end-to-end models such as deep neural networks (DNNs). We compare various state-of-the-art DNNbased architectures for document classification, information extraction, and question answering. We find that models supporting a rich answer space, such as word or character sequences, perform best. Our best-performing model, a word-level sequence to sequence model with a mechanism to copy out-of-vocabulary words, obtains an accuracy of 71.8%.", "target": "WIKIREADING: A Novel Large-scale Language Understanding Task over Wikipedia"}
{"id": "task1540-6f35377c993a417fbaab2a147bedf2c9", "input": "We study the responses of two tactile sensors, the fingertip sensor from the iCub and the BioTac under different external stimuli. The question of interest is to which degree both sensors i) allow the estimation of force exerted on the sensor and ii) enable the recognition of differing degrees of curvature. Making use of a force controlled linear motor affecting the tactile sensors we acquire several high-quality data sets allowing the study of both sensors under exactly the same conditions. We also examined the structure of the representation of tactile stimuli in the recorded tactile sensor data using t-SNE embeddings. The experiments show that both the iCub and the BioTac excel in different settings.", "target": "ML-based tactile sensor calibration: A universal approach"}
{"id": "task1540-352b1623bfcc434cbb81419e71ffac51", "input": "The language that we produce reflects our personality, and various personal and demographic characteristics can be detected in natural language texts. We focus on one particular personal trait of the author, gender, and study how it is manifested in original texts and in translations. We show that author\u2019s gender has a powerful, clear signal in originals texts, but this signal is obfuscated in human and machine translation. We then propose simple domainadaptation techniques that help retain the original gender traits in the translation, without harming the quality of the translation, thereby creating more personalized machine translation systems.", "target": "Personalized Machine Translation: Preserving Original Author Traits"}
{"id": "task1540-dc6936a212c44b12bf7aaea17cc6f4b0", "input": "The ability to train large-scale neural networks has resulted in state-of-the-art performance in many areas of computer vision. These results have largely come from computational break throughs of two forms: model parallelism, e.g. GPU accelerated training, which has seen quick adoption in computer vision circles, and data parallelism, e.g. A-SGD, whose large scale has been used mostly in industry. We report early experiments with a system that makes use of both model parallelism and data parallelism, we call GPU A-SGD. We show using GPU A-SGD it is possible to speed up training of large convolutional neural networks useful for computer vision. We believe GPU A-SGD will make it possible to train larger networks on larger training sets in a reasonable amount of time.", "target": "GPU Asynchronous Stochastic Gradient Descent to Speed Up Neural Network Training"}
{"id": "task1540-c34ac15f71124e5b8abd433de0c475b7", "input": "We present a new algorithm to model and investigate the learning process of a learner mastering a set of grammatical rules from an inconsistent source. The compelling interest of human language acquisition is that the learning succeeds in virtually every case, despite the fact that the input data are formally inadequate to explain the success of learning. Our model explains how a learner can successfully learn from or even surpass its imperfect source without possessing any additional biases or constraints about the types of patterns that exist in the language. We use the data collected by Singleton & Newport (2004) on the performance of a 7-year boy Simon, who mastered the American Sign Language (ASL) by learning it from his parents, both of whom were imperfect speakers of ASL. We show that the algorithm possesses a frequency-boosting property, whereby the frequency of the most common form of the source is increased by the learner. We also explain several key features of Simon\u2019s ASL.", "target": "When learners surpass their models: mathematical modeling of learning from an inconsistent source"}
{"id": "task1540-67405f12c9a6446599532ea41f5b5399", "input": "Bellemare et al. (2016) introduced the notion of a pseudo-count to generalize count-based exploration to non-tabular reinforcement learning. This pseudo-count is derived from a density model which effectively replaces the count table used in the tabular setting. Using an exploration bonus based on this pseudo-count and a mixed Monte Carlo update applied to a DQN agent was sufficient to achieve state-of-the-art on the Atari 2600 game Montezuma\u2019s Revenge.", "target": "Count-Based Exploration with Neural Density Models"}
{"id": "task1540-2aff2ff7839143f99b70c95935f942bf", "input": "Recommender systems often use latent features to explain the behaviors of users and capture the properties of items. As users interact with different items over time, user and item features can influence each other, evolve and co-evolve over time. To accurately capture the fine grained nonlinear coevolution of these features, we propose a recurrent coevolutionary feature embedding process model, which combines recurrent neural network (RNN) with a multidimensional point process model. The RNN learns a nonlinear representation of user and item features which take into account mutual influence between user and item features, and the feature evolution over time. We also develop an efficient stochastic gradient algorithm for learning the model parameters, which can readily scale up to millions of events. Experiments on diverse real-world datasets demonstrate significant improvements in user behavior prediction compared to state-of-the-arts.", "target": "Recurrent Coevolutionary Feature Embedding Processes for Recommendation"}
{"id": "task1540-b11e513d5c7c4c149db54a6be8b8a4a9", "input": "Optunity is a free software package dedicated to hyperparameter optimization. It contains various types of solvers, ranging from undirected methods to direct search, particle swarm and evolutionary optimization. The design focuses on ease of use, flexibility, code clarity and interoperability with existing software in all machine learning environments. Optunity is written in Python and contains interfaces to environments such as R and MATLAB. Optunity uses a BSD license and is freely available online at http://www.optunity.net.", "target": "Easy Hyperparameter Search Using Optunity"}
{"id": "task1540-988a6dbb54a740d9aa6f581fee1f98ad", "input": "The capability to store data about business processes execution in so-called Event Logs has brought to the diffusion of tools for the analysis of process executions and for the assessment of the goodness of a process model. Nonetheless, these tools are often very rigid in dealing with with Event Logs that include incomplete information about the process execution. Thus, while the ability of handling incomplete event data is one of the challenges mentioned in the process mining manifesto, the evaluation of compliance of an execution trace still requires an endto-end complete trace to be performed. This paper exploits the power of abduction to provide a flexible, yet computationally effective, framework to deal with different forms of incompleteness in an Event Log. Moreover it proposes a refinement of the classical notion of compliance into strong and conditional compliance to take into account incomplete logs. Finally, performances evaluation in an experimental setting shows the feasibility of the presented approach.", "target": "Abducing Compliance of Incomplete Event Logs"}
{"id": "task1540-1f6cd367ffc947c5a3110646257e5c98", "input": "Sequence labeling for extraction of medical events and their attributes from unstructured text in Electronic Health Record (EHR) notes is a key step towards semantic understanding of EHRs. It has important applications in health informatics including pharmacovigilance and drug surveillance. The state of the art supervised machine learning models in this domain are based on Conditional Random Fields (CRFs) with features calculated from fixed context windows. In this application, we explored recurrent neural network frameworks and show that they significantly outperformed the CRF models.", "target": "Bidirectional RNN for Medical Event Detection in Electronic Health Records"}
{"id": "task1540-f0e374c0198040188889952c2873b007", "input": "Authors are encouraged to submit new papers to INFORMS journals by means of a style file template, which includes the journal title. However, use of a template does not certify that the paper has been accepted for publication in the named journal. INFORMS journal templates are for the exclusive purpose of submitting to an INFORMS journal and should not be used to distribute the papers in print or online or to submit the papers to another publication.", "target": "A Dynamic Near-Optimal Algorithm for Online Linear Programming"}
{"id": "task1540-3d111207d9184b05b64502395d10dc87", "input": "We study the generalization properties of stochastic gradient methods for learning with convex loss functions and linearly parameterized functions. We show that, in the absence of penalizations or constraints, the stability and approximation properties of the algorithm can be controlled by tuning either the step-size or the number of passes over the data. In this view, these parameters can be seen to control a form of implicit regularization. Numerical results complement the theoretical findings.", "target": "Generalization Properties and Implicit Regularization for Multiple Passes SGM"}
{"id": "task1540-37d19fa314bd4bf08516da57b9ca913d", "input": "We study the use of greedy feature selection methods for morphosyntactic tagging under a number of different conditions. We compare a static ordering of features to a dynamic ordering based on mutual information statistics, and we apply the techniques to standalone taggers as well as joint systems for tagging and parsing. Experiments on five languages show that feature selection can result in more compact models as well as higher accuracy under all conditions, but also that a dynamic ordering works better than a static ordering and that joint systems benefit more than standalone taggers. We also show that the same techniques can be used to select which morphosyntactic categories to predict in order to maximize syntactic accuracy in a joint system. Our final results represent a substantial improvement of the state of the art for several languages, while at the same time reducing both the number of features and the running time by up to 80% in some cases.", "target": "Static and Dynamic Feature Selection in Morphosyntactic Analyzers"}
{"id": "task1540-f1ddf0fb9d0949cc9216263c9c2cfa64", "input": "We propose a statistical model applicable to character level language modeling and show that it is a good fit for both, program source code and English text. The model is parameterized by a program from a domain-specific language (DSL) that allows expressing non-trivial data dependencies. Learning is done in two phases: (i) we synthesize a program from the DSL, essentially learning a good representation for the data, and (ii) we learn parameters from the training data \u2013 the process is done via counting, as in simple language models such as n-gram. Our experiments show that the precision of our model is comparable to that of neural networks while sharing a number of advantages with n-gram models such as fast query time and the capability to quickly add and remove training data samples. Further, the model is parameterized by a program that can be manually inspected, understood and updated, addressing a major problem of neural networks.", "target": "PROGRAM SYNTHESIS FOR CHARACTER LEVEL LANGUAGE MODELING"}
{"id": "task1540-f410ed0fcdca4629b201203d22dafd55", "input": "Our work addresses two important issues with recurrent neural networks: (1) they are over-parameterized, and (2) the recurrence matrix is ill-conditioned. The former increases the sample complexity of learning and the training time. The latter causes the vanishing and exploding gradient problem. We present a flexible recurrent neural network model called Kronecker Recurrent Units (KRU). KRU achieves parameter efficiency in RNNs through a Kronecker factored recurrent matrix. It overcomes the ill-conditioning of the recurrent matrix by enforcing soft unitary constraints on the factors. Thanks to the small dimensionality of the factors, maintaining these constraints is computationally efficient. Our experimental results on five standard data-sets reveal that KRU can reduce the number of parameters by three orders of magnitude in the recurrent weight matrix compared to the existing recurrent models, without trading the statistical performance. These results in particular show that while there are advantages in having a high dimensional recurrent space, the capacity of the recurrent part of the model can be dramatically reduced.", "target": "Kronecker Recurrent Units"}
{"id": "task1540-c5161b9824264450a4b7603519d83dad", "input": "Multiclass prediction is the problem of classifying an object into a relevant target class. We consider the problem of learning a multiclass predictor that uses only few features, and in particular, the number of used features should increase sub-linearly with the number of possible classes. This implies that features should be shared by several classes. We describe and analyze the ShareBoost algorithm for learning a multiclass predictor that uses few shared features. We prove that ShareBoost efficiently finds a predictor that uses few shared features (if such a predictor exists) and that it has a small generalization error. We also describe how to use ShareBoost for learning a non-linear predictor that has a fast evaluation time. In a series of experiments with natural data sets we demonstrate the benefits of ShareBoost and evaluate its success relatively to other state-of-the-art approaches.", "target": "ShareBoost: Efficient Multiclass Learning with Feature Sharing"}
{"id": "task1540-7856a6f5bbd047fb8f313f51f959dd04", "input": "This paper summarizes the recent progress we have made for the computer vision technologies in physical therapy with the accessible and affordable devices. We first introduce the remote health coaching system we build with Microsoft Kinect. Since the motion data captured by Kinect is noisy, we investigate the data accuracy of Kinect with respect to the high accuracy motion capture system. We also propose an outlier data removal algorithm based on the data distribution. In order to generate the kinematic parameter from the noisy data captured by Kinect, we propose a kinematic filtering algorithm based on Unscented Kalman Filter and the kinematic model of human skeleton. The proposed algorithm can obtain smooth kinematic parameter with reduced noise compared to the kinematic parameter generated from the raw motion data from Kinect.", "target": "Remote Health Coaching System and Human Motion Data Analysis for Physical Therapy with Microsoft Kinect"}
{"id": "task1540-199a361140e440b9aad1a39538c49b20", "input": "In Chile, does not exist an independent entity that publishes quantitative or qualitative surveys to   understand   the   traditional   media   environment   and   its   adaptation   on   the   Social  Web. Nowadays, Chilean newsreaders are increasingly using social web platforms as their primary source of   information,  among which Twitter  plays  a central   role.  Historical  media and pure players  are developing  different  strategies   to   increase  their  audience  and  influence on  this platform. In this article, we propose a methodology based on data mining techniques to provide a first level of analysis of the new Chilean media environment. We use a crawling technique to mine news streams of  37 different  Chilean media actively  presents on Twitter  and propose several  indicators  to compare  them. We analyze their  volumes of production,   their  potential audience, and using NLP techniques, we explore the content of their production: their editorial line and their geographic coverage.", "target": "Diagnosing editorial strategies of Chilean media on Twitter using an automatic news classifier"}
{"id": "task1540-dedde9ae19114f3c830a0b6c80df543e", "input": "Topic Models have been reported to be beneficial for aspect-based sentiment analysis. This paper reports a simple topic model for sarcasm detection, a first, to the best of our knowledge. Designed on the basis of the intuition that sarcastic tweets are likely to have a mixture of words of both sentiments as against tweets with literal sentiment (either positive or negative), our hierarchical topic model discovers sarcasm-prevalent topics and topic-level sentiment. Using a dataset of tweets labeled using hashtags, the model estimates topic-level, and sentiment-level distributions. Our evaluation shows that topics such as \u2018work\u2019, \u2018gun laws\u2019, \u2018weather\u2019 are sarcasm-prevalent topics. Our model is also able to discover the mixture of sentiment-bearing words that exist in a text of a given sentiment-related label. Finally, we apply our model to predict sarcasm in tweets. We outperform two prior work based on statistical classifiers with specific features, by around 25%.", "target": "\u2018Who would have thought of that!\u2019: A Hierarchical Topic Model for Extraction of Sarcasm-prevalent Topics and Sarcasm Detection"}
{"id": "task1540-9f6bd4d6092f44b69baf1659ad38b803", "input": "Lifelong reinforcement learning provides a promising framework for developing versatile agents that can accumulate knowledge over a lifetime of experience and rapidly learn new tasks by building upon prior knowledge. However, current lifelong learning methods exhibit non-vanishing regret as the amount of experience increases, and include limitations that can lead to suboptimal or unsafe control policies. To address these issues, we develop a lifelong policy gradient learner that operates in an adversarial setting to learn multiple tasks online while enforcing safety constraints on the learned policies. We demonstrate, for the first time, sublinear regret for lifelong policy search, and validate our algorithm on several benchmark dynamical systems and an application to quadrotor control.", "target": "Safe Policy Search for Lifelong Reinforcement Learning with Sublinear Regret"}
{"id": "task1540-b2d44bcd5ec144f4990869912017343e", "input": "We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment.", "target": "ADADELTA: AN ADAPTIVE LEARNING RATE METHOD"}
{"id": "task1540-15578ed50c304241b4cfa484d21e4a78", "input": "Botnets, which consist of thousands of compromised machines, can cause significant threats to other systems by launching Distributed Denial of Service (DDoS) attacks, keylogging, and backdoors. In response to these threats, new effective techniques are needed to detect the presence of botnets. In this paper, we have used an interception technique to monitor Windows Application Programming Interface (API) functions calls made by communication applications and store these calls with their arguments in log files. Our algorithm detects botnets based on monitoring abnormal activity by correlating the changes in log file sizes from different hosts. Keywords-IRC; DDoS; Bots; Botnets; API function calls", "target": "Detecting Botnets Through Log Correlation"}
{"id": "task1540-14dbe0950f814d48b7972219a8b6c7ee", "input": "In this paper we present our exploratory findings related to extracting knowledge and experiences from a community of senior tourists. By using tools of qualitative analysis as well as review of literature, we managed to verify a set of hypotheses related to the content created by senior tourists when participating in on-line communities. We also produced a codebook, representing various themes one may encounter in such communities. This codebook, derived from our own qualitative research, as well a literature review will serve as a basis for further development of automated tools of knowledge extraction. We also managed to find that older adults more often than other poster in tourists forums, mention their age in discussion, more often share their experiences and motivation to travel, however they do not differ in relation to describing barriers encountered while traveling.", "target": "Golden Years, Golden Shores: A Study of Elders in Online Travel Communities"}
{"id": "task1540-42426d3ccd814aed8c97e834908ce0d0", "input": "Rare diseases are very difficult to identify among large number of other possible diagnoses. Better availability of patient data and improvement in machine learning algorithms empower us to tackle this problem computationally. In this paper, we target one such rare disease \u2013 cardiac amyloidosis. We aim to automate the process of identifying potential cardiac amyloidosis patients with the help of machine learning algorithms and also learn most predictive factors. With the help of experienced cardiologists, we prepared a gold standard with 73 positive (cardiac amyloidosis) and 197 negative instances. We achieved high average cross-validation F1 score of 0.98 using an ensemble machine learning classifier. Some of the predictive variables were: Age and Diagnosis of cardiac arrest, chest pain, congestive heart failure, hypertension, prim open angle glaucoma, and shoulder arthritis. Further studies are needed to validate the accuracy of the system across an entire health system and its generalizability for other diseases.", "target": "A Bootstrap Machine Learning Approach to Identify Rare Disease Patients from Electronic Health Records"}
{"id": "task1540-47f359ef376b4b4dbd7498b230baaddc", "input": "We address the problem of contour detection via per-pixel classifications of edge point. To facilitate the process, the proposed approach leverages with DenseNet, an efficient implementation of multiscale convolutional neural networks (CNNs), to extract an informative feature vector for each pixel and uses an SVM classifier to accomplish contour detection. The main challenge lies in adapting a pre-trained per-image CNN model for yielding per-pixel image features. We propose to base on the DenseNet architecture to achieve pixelwise fine-tuning and then consider a cost-sensitive strategy to further improve the learning with a small dataset of edge and non-edge image patches. In the experiment of contour detection, we look into the effectiveness of combining per-pixel features from different CNN layers and obtain comparable performances to the state-of-the-art on BSDS500.", "target": "CONTOUR DETECTION USING COST-SENSITIVE CON-"}
{"id": "task1540-e5c9adcc7a704f9eb2353d8e8bd50cd7", "input": "Predicting the outcomes of future events is a challenging problem for which a variety of solution methods have been explored and attempted. We present an empirical comparison of a variety of online and offline adaptive algorithms for aggregating experts\u2019 predictions of the outcomes of five years of US National Football League games (1319 games) using expert probability elicitations obtained from an Internet contest called ProbabilitySports. We find that it is difficult to improve over simple averaging of the predictions in terms of prediction accuracy, but that there is room for improvement in quadratic loss. Somewhat surprisingly, a Bayesian estimation algorithm which estimates the variance of each expert\u2019s prediction exhibits the most consistent superior performance over simple averaging among our collection of algorithms.", "target": "An Empirical Comparison of Algorithms for Aggregating Expert Predictions"}
{"id": "task1540-afade53ff2d14cc8bf3c02c4cb6a5611", "input": "We document a connection between constraint reasoning and probabilistic reasoning. We present an algorithm, called probabilistic arc consistency, which is both a generalization of a well known algorithm for arc consistency used in constraint reasoning, and a specialization of the belief updating algorithm for singly-connected networks. Our algorithm is exact for singly\u00ad connected constraint problems, but can work well as an approximation for arbitrary problems. We briefly discuss some empirical results, and re\u00ad lated methods.", "target": "Probabilistic Arc Consistency: A Connection between Constraint Reasoning and Probabilistic Reasoning"}
{"id": "task1540-e29e32fce6344342b447706688ec6810", "input": "We introduce a novel latent vector space model that jointly learns<lb>the latent representations of words, e-commerce products and a<lb>mapping between the two without the need for explicit annotations.<lb>The power of the model lies in its ability to directly model the dis-<lb>criminative relation between products and a particular word. We<lb>compare our method to existing latent vector space models (LSI,<lb>LDA and word2vec) and evaluate it as a feature in a learning to<lb>rank setting. Our latent vector space model achieves its enhanced<lb>performance as it learns better product representations. Further-<lb>more, the mapping from words to products and the representations<lb>of words benefit directly from the errors propagated back from the<lb>product representations during parameter estimation. We provide<lb>an in-depth analysis of the performance of our model and analyze<lb>the structure of the learned representations.", "target": "Learning Latent Vector Spaces for Product Search"}
{"id": "task1540-7430e9f95f914384a8947e01a1c3276b", "input": "We approach the challenging problem of generating highlights from sports broadcasts utilizing audio information only. A language-independent, multi-stage classification approach is employed for detection of key acoustic events which then act as a platform for summarization of highlight scenes. Objective results and human experience indicate that our system is highly efficient.", "target": "Sports highlights generation based on acoustic events detection: A rugby case study"}
{"id": "task1540-779749e2297648e1997a055a87d354b1", "input": "Bridging the \u2018reality gap\u2019 that separates simulated robotics from experiments on hardware could accelerate robotic research through improved data availability. This paper explores domain randomization, a simple technique for training models on simulated images that transfer to real images by randomizing rendering in the simulator. With enough variability in the simulator, the real world may appear to the model as just another variation. We focus on the task of object localization, which is a stepping stone to general robotic manipulation skills. We find that it is possible to train a real-world object detector that is accurate to 1.5 cm and robust to distractors and partial occlusions using only data from a simulator with non-realistic random textures. To demonstrate the capabilities of our detectors, we show they can be used to perform grasping in a cluttered environment. To our knowledge, this is the first successful transfer of a deep neural network trained only on simulated RGB images (without pre-training on real images) to the real world for the purpose of robotic control.", "target": "Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World"}
{"id": "task1540-38b1194cf0ed4ede90fa35ba66e04715", "input": "The follow the leader (FTL) algorithm, perhaps the simplest of all online learning algorithms, is known to perform well when the loss functions it is used on are convex and positively curved. In this paper we ask whether there are other \u201clucky\u201d settings when FTL achieves sublinear, \u201csmall\u201d regret. In particular, we study the fundamental problem of linear prediction over a non-empty convex, compact domain. Amongst other results, we prove that the curvature of the boundary of the domain can act as if the losses were curved: In this case, we prove that as long as the mean of the loss vectors have positive lengths bounded away from zero, FTL enjoys a logarithmic growth rate of regret, while, e.g., for polytope domains and stochastic data it enjoys finite expected regret. Building on a previously known meta-algorithm, we also get an algorithm that simultaneously enjoys the worst-case guarantees and the bound available for FTL.", "target": "Following the Leader and Fast Rates in Linear Prediction: Curved Constraint Sets and Other Regularities\u2217"}
{"id": "task1540-0f9d665bcb0b4a1b86e3b80ed9b28248", "input": "We consider an agent\u2019s uncertainty about its environment and the problem of generalizing this uncertainty across observations. Specifically, we focus on the problem of exploration in non-tabular reinforcement learning. Drawing inspiration from the intrinsic motivation literature, we use sequential density models to measure uncertainty, and propose a novel algorithm for deriving a pseudo-count from an arbitrary sequential density model. This technique enables us to generalize count-based exploration algorithms to the non-tabular case. We apply our ideas to Atari 2600 games, providing sensible pseudo-counts from raw pixels. We transform these pseudo-counts into intrinsic rewards and obtain significantly improved exploration in a number of hard games, including the infamously difficult MONTEZUMA\u2019S REVENGE.", "target": "Unifying Count-Based Exploration and Intrinsic Motivation"}
{"id": "task1540-89f815b4c7ae44f4a68deda205128000", "input": "This paper is an empirical study of the distributed deep learning for a question answering subtask: answer selection. Comparison studies of SGD, MSGD, DOWNPOUR and EASGD/EAMSGD algorithms have been presented. Experimental results show that the message passing interface based distributed framework can accelerate the convergence speed at a sublinear scale. This paper demonstrates the importance of distributed training: with 120 workers, an 83x speedup is achievable and running time is decreased from 107.9 hours to 1.3 hours, which will benefit the productivity significantly.", "target": "Distributed Deep Learning for Answer Selection"}
{"id": "task1540-4414f1b3a4a542ac9683b1bdf74221e9", "input": "In real-time strategy games like StarCraft, skilled players often block the entrance to their base with buildings to prevent the opponent\u2019s units from getting inside. This technique, called \u201cwalling-in\u201d, is a vital part of player\u2019s skill set, allowing him to survive early aggression. However, current artificial players (bots) do not possess this skill, due to numerous inconveniences surfacing during its implementation in imperative languages like C++ or Java. In this text, written as a guide for bot programmers, we address the problem of finding an appropriate building placement that would block the entrance to player\u2019s base, and present a ready to use declarative solution employing the paradigm of answer set programming (ASP). We also encourage the readers to experiment with different declarative approaches to this problem.", "target": "Implementing a Wall-In Building Placement in StarCraft with Declarative Programming"}
{"id": "task1540-75dfc79490324f1e8afbe6662295ecb3", "input": "We study the helpful product reviews identification problem in this paper. We observe that the evidence-conclusion discourse relations, also known as arguments, often appear in product reviews, and we hypothesise that some argumentbased features, e.g. the percentage of argumentative sentences, the evidencesconclusions ratios, are good indicators of helpful reviews. To validate this hypothesis, we manually annotate arguments in 110 hotel reviews, and investigate the effectiveness of several combinations of argument-based features. Experiments suggest that, when being used together with the argument-based features, the state-of-the-art baseline features can enjoy a performance boost (in terms of F1) of 11.01% in average.", "target": "Using Argument-based Features to Predict and Analyse Review Helpfulness"}
{"id": "task1540-80ff2311b20f4552aac6b83648368c31", "input": "Chain graphs combine directed and undi\u00ad rected graphs and their underlying mathe\u00ad matics combines properties of the two. This paper gives a simplified definition of chain graphs based on a hierarchical combination of Bayesian (directed) and Markov (undirected) networks. Examples of a chain graph are multivariate feed-forward networks, cluster\u00ad ing with conditional interaction between vari\u00ad ables, and forms of Bayes classifiers. Chain graphs are then extended using the notation of plates so that samples and data analysis problems can be represented in a graphical model as well. Implications for learning are discussed in the conclusion.", "target": "Chain graphs for learning"}
{"id": "task1540-99a329a54b3646579cac2472fccf4ba6", "input": "Learning both hierarchical and temporal representation has been among the longstanding challenges of recurrent neural networks. Multiscale recurrent neural networks have been considered as a promising approach to resolve this issue, yet there has been a lack of empirical evidence showing that this type of models can actually capture the temporal dependencies by discovering the latent hierarchical structure of the sequence. In this paper, we propose a novel multiscale approach, called the hierarchical multiscale recurrent neural network, which can capture the latent hierarchical structure in the sequence by encoding the temporal dependencies with different timescales using a novel update mechanism. We show some evidence that our proposed multiscale architecture can discover underlying hierarchical structure in the sequences without using explicit boundary information. We evaluate our proposed model on character-level language modelling and handwriting sequence generation.", "target": "Hierarchical Multiscale Recurrent Neural Networks"}
{"id": "task1540-775dd81d3f3249d1a91caf1ec106bbb0", "input": "In this paper we present REG, a graph-based approach for study a fundamental problem of Natural Language Processing (NLP): the automatic text summarization. The algorithm maps a document as a graph, then it computes the weight of their sentences. We have applied this approach to summarize documents in three languages.", "target": "Un re\u0301sumeur a\u0300 base de graphes, inde\u0301pe\u0301ndant de la langue"}
{"id": "task1540-4ea591ec125d47d191b8625e34cb181f", "input": "The areas of machine learning and communication technology are converging. Today\u2019s communications systems generate a huge amount of traffic data, which can help to significantly enhance the design and management of networks and communication components when combined with advanced machine learning methods. Furthermore, recently developed end-to-end training procedures offer new ways to jointly optimize the components of a communication system. Also in many emerging application fields of communication technology, e.g., smart cities or internet of things, machine learning methods are of central importance. This paper gives an overview over the use of machine learning in different areas of communications and discusses two exemplar applications in wireless networking. Furthermore, it identifies promising future research topics and discusses their potential impact.", "target": "THE CONVERGENCE OF MACHINE LEARNING AND COMMUNICATIONS"}
{"id": "task1540-1aec9763b7f44b1192c8a53b45adda4e", "input": "One weakness of machine-learned NLP models is that they typically perform poorly on out-of-domain data. In this work, we study the task of identifying products being bought and sold in online cybercrime forums, which exhibits particularly challenging cross-domain effects. We formulate a task that represents a hybrid of slot-filling information extraction and named entity recognition and annotate datasets consisting of data from four different forums. Each of these forums constitutes its own \u201cfine-grained domain\u201d in that the forums cover different market sectors with different properties, even though all forums are in the broad domain of cybercrime. We characterize these domain differences in the context of a learning-based system: supervised models see decreased accuracy when applied to new forums, and standard techniques for semisupervised learning and domain adaptation have limited effectiveness on this data, which suggests the need to improve these techniques. We release a dataset of 93,924 posts from across 4 forums, with annotations for 1,938 posts.", "target": "Identifying Products in Online Cybercrime Marketplaces: A Dataset and Fine-grained Domain Adaptation Task"}
{"id": "task1540-8882878db1684745a8719dce731a953b", "input": "We show that relation extraction can be reduced to answering simple reading comprehension questions, by associating one or more natural-language questions with each relation slot. This reduction has several advantages: we can (1) learn high-quality relation-extraction models by extending recent neural reading-comprehension techniques, (2) build very large training sets for those models by combining relation-specific crowd-sourced questions with distant supervision, and even (3) do zero-shot learning by extracting new relations that are only specified at test-time, for which we have no labeled training examples. Experiments on a Wikipedia slot-filling task demonstrate that the approach can generalize to new questions for known relations with high accuracy, and that zero-shot generalization to unseen relations is possible, at lower accuracy levels, setting the bar for future work on this task.", "target": "Zero-Shot Relation Extraction via Reading Comprehension"}
{"id": "task1540-b5011c1339814fa8b16688528d63c956", "input": "This paper proposes a hierarchical attentional neural translation model which focuses on enhancing source-side hierarchical representations by covering both local and global semantic information using a bidirectional tree-based encoder. To maximize the predictive likelihood of target words, a weighted variant of an attention mechanism is used to balance the attentive information between lexical and phrase vectors. Using a tree-based rare word encoding, the proposed model is extended to sub-word level to alleviate the out-of-vocabulary (OOV) problem. Empirical results reveal that the proposed model significantly outperforms sequence-to-sequence attention-based and tree-based neural translation models in English-Chinese translation tasks.", "target": "Towards Bidirectional Hierarchical Representations for Attention-Based Neural Machine Translation"}
{"id": "task1540-d072b2dc576941288943867497088077", "input": "Syllogism is a type of deductive reasoning involving quantified statements. The syllogistic reasoning scheme in the classical Aristotelian framework involves three crisp term sets and four linguistic quantifiers, for which the main support is the linguistic properties of the quantifiers. A number of fuzzy approaches for defining an approximate syllogism have been proposed for which the main support is cardinality calculus. In this paper we analyze fuzzy syllogistic models previously described by Zadeh and Dubois et al. and compare their behavior with that of the classical Aristotelian framework to check which of the 24 classical valid syllogistic reasoning patterns (called moods) are particular crisp cases of these fuzzy approaches. This allows us to assess to what extent these approaches can be considered as either plausible extensions of the classical crisp syllogism or a basis for a general approach to the problem of approximate syllogism.", "target": "On the analysis of set-based fuzzy quantified reasoning using classical syllogistics"}
{"id": "task1540-dda9ddc2550c4f299b48d9042c8a2cea", "input": "In the fashion industry, order scheduling focuses on the assignment of production orders to appropriate production lines. In reality, before a new order can be put into production, a series of activities known as pre-production events need to be completed. In addition, in real production process, owing to various uncertainties, the daily production quantity of each order is not always as expected. In this research, by considering the pre-production events and the uncertainties in the daily production quantity, robust order scheduling problems in the fashion industry are investigated with the aid of a multi-objective evolutionary algorithm (MOEA) called nondominated sorting adaptive differential evolution (NSJADE). The experimental results illustrate that it is of paramount importance to consider pre-production events in order scheduling problems in the fashion industry. We also unveil that the existence of the uncertainties in the daily production quantity heavily affects the order scheduling.", "target": "Robust Order Scheduling in the Fashion Industry: A Multi-Objective Optimization Approach"}
{"id": "task1540-4c2d273743d64a67a483c5af12b53f05", "input": "Building on recent advances in image caption generation and optical character recognition (OCR), we present a generalpurpose, deep learning-based system to decompile an image into presentational markup. While this task is a wellstudied problem in OCR, our method takes an inherently different, data-driven approach. Our model does not require any knowledge of the underlying markup language, and is simply trained end-to-end on real-world example data. The model employs a convolutional network for text and layout recognition in tandem with an attention-based neural machine translation system. To train and evaluate the model, we introduce a new dataset of real-world rendered mathematical expressions paired with LaTeX markup, as well as a synthetic dataset of web pages paired with HTML snippets. Experimental results show that the system is surprisingly effective at generating accurate markup for both datasets. While a standard domainspecific LaTeX OCR system achieves around 25% accuracy, our model reproduces the exact rendered image on 75% of examples.", "target": "What You Get Is What You See: A Visual Markup Decompiler"}
{"id": "task1540-ded63d723aa2463baf14e5c453b3a120", "input": "Circumscription is a representative example of a nonmonotonic reasoning inference technique. Circumscription has often been studied for first order theories, but its propositional version has also been the subject of extensive research, having been shown equivalent to extended closed world assumption (ECWA). Moreover, entailment in propositional circumscription is a well-known example of a decision problem in the second level of the polynomial hierarchy. This paper proposes a new Boolean Satisfiability (SAT)-based algorithm for entailment in propositional circumscription that explores the relationship of propositional circumscription to minimal models. The new algorithm is inspired by ideas commonly used in SAT-based model checking, namely counterexample guided abstraction refinement. In addition, the new algorithm is refined to compute the theory closure for generalized close world assumption (GCWA). Experimental results show that the new algorithm can solve problem instances that other solutions are unable to solve.", "target": "Counterexample Guided Abstraction Refinement Algorithm for Propositional Circumscription"}
{"id": "task1540-1891161a4d164ea395c1012551ca967b", "input": "Backward simulation is an approximate inference technique for Bayesian belief networks. It differs from existing simulation methods in that it starts simulation from the known evidence and works backward (i.e., contrary to the direction of the arcs). The technique's focus on the evidence leads to improved convergence in situations where the posterior beliefs are dominated by the evidence rather than by the prior probabilities. Since this class of situations is large, the technique may make practical the application of approximate inference in Bayesian belief networks to many real\ufffdworld problems.", "target": "Backward Simulation in Bayesian Networks"}
{"id": "task1540-a42b973c58784a34acf1a75c20f80ddb", "input": "We develop a general duality between neural networks and compositional kernels, striving towards a better understanding of deep learning. We show that initial representations generated by common random initializations are sufficiently rich to express all functions in the dual kernel space. Hence, though the training objective is hard to optimize in the worst case, the initial weights form a good starting point for optimization. Our dual view also reveals a pragmatic and aesthetic perspective of neural networks and underscores their expressive power. \u2217Email: amitdaniely@google.com \u2020Email: rf@cs.stanford.edu. Work performed at Google. \u2021Email: singer@google.com ar X iv :1 60 2. 05 89 7v 1 [ cs .L G ] 1 8 Fe b 20 16", "target": "Toward Deeper Understanding of Neural Networks: The Power of Initialization and a Dual View on Expressivity"}
{"id": "task1540-1a5ca36713c94a07af0dc549374a51db", "input": "The first ever human vs. computer no-limit Texas hold \u2019em competition took place from April 24\u2013 May 8, 2015 at River\u2019s Casino in Pittsburgh, PA. In this article I present my thoughts on the competition design, agent architecture, and lessons learned.", "target": "My Reflections on the First Man vs. Machine No-Limit Texas Hold \u2019em Competition\u2217"}
{"id": "task1540-0c63f6f23fc14232b3d16b4cf1f01761", "input": "Graph matching is a challenging problem with very important applications in a wide range of fields, from image and video analysis to biological and biomedical problems. We propose a robust graph matching algorithm inspired in sparsityrelated techniques. We cast the problem, resembling group or collaborative sparsity formulations, as a non-smooth convex optimization problem that can be efficiently solved using augmented Lagrangian techniques. The method can deal with weighted or unweighted graphs, as well as multimodal data, where different graphs represent different types of data. The proposed approach is also naturally integrated with collaborative graph inference techniques, solving general network inference problems where the observed variables, possibly coming from different modalities, are not in correspondence. The algorithm is tested and compared with state-of-the-art graph matching techniques in both synthetic and real graphs. We also present results on multimodal graphs and applications to collaborative inference of brain connectivity from alignment-free functional magnetic resonance imaging (fMRI) data. The code is publicly available.", "target": "Robust Multimodal Graph Matching: Sparse Coding Meets Graph Matching"}
{"id": "task1540-0bf7c55a806d427dbebdbb8ab27e1ce1", "input": "Constrained sampling and counting are two fundamental problems in artificial intelligence with a diverse range of applications, spanning probabilistic reasoning and planning to constrained-random verification. While the theory of these problems was thoroughly investigated in the 1980s, prior work either did not scale to industrial size instances or gave up correctness guarantees to achieve scalability. Recently, we proposed a novel approach that combines universal hashing and SAT solving and scales to formulas with hundreds of thousands of variables without giving up correctness guarantees. This paper provides an overview of the key ingredients of the approach and discusses challenges that need to be overcome to handle larger real-world instances.", "target": "Constrained Sampling and Counting: Universal Hashing Meets SAT Solving\u2217"}
{"id": "task1540-2520bac07fdd48ec81773c775964ec9e", "input": "In collective decision making, where a voting rule is used to take a collective decision among a group of agents, manipulation by one or more agents is usually considered negative behavior to be avoided, or at least to be made computationally difficult for the agents to perform. However, there are scenarios in which a restricted form of manipulation can instead be beneficial. In this paper we consider the iterative version of several voting rules, where at each step one agent is allowed to manipulate by modifying his ballot according to a set of restricted manipulation moves which are computationally easy and require little information to be performed. We prove convergence of iterative voting rules when restricted manipulation is allowed, and we present experiments showing that most iterative voting rules have a higher Condorcet efficiency than their non-iterative version.", "target": "Restricted Manipulation in Iterative Voting: Convergence and Condorcet Efficiency"}
{"id": "task1540-1bb3c9d8542647718972fa96555d0fa5", "input": "Indian languages have long history in World Natural languages. Panini was the first to define Grammar for Sanskrit language with about 4000 rules in fifth century. These rules contain uncertainty information. It is not possible to Computer processing of Sanskrit language with uncertain information. In this paper, fuzzy logic and fuzzy reasoning are proposed to deal to eliminate uncertain information for reasoning with Sanskrit grammar. The Sanskrit language processing is also discussed in this paper. .", "target": "Fuzzy Modeling and Natural Language Processing for Panini\u2019s Sanskrit Grammar"}
{"id": "task1540-2af09b1096f7439daf6743c7f2a6d944", "input": "In this paper we study multi robot cooperative task allocation issue in a situation where a swarm of robots is deployed in a confined unknown environment where the number of colored spots which represent tasks and the ratios of them are unknown. The robots should cover this spots as far as possible to do cleaning and sampling actions desirably. It means that they should discover the spots cooperatively and spread proportional to the spots area and avoid from remaining idle. We proposed 4 self-organized distributed methods which are called hybrid methods for coping with this scenario. In two different experiments the performance of the methods is analyzed. We compared them with each other and investigated their scalability and robustness in term of single", "target": "Task Allocation in Robotic Swarms: Explicit Communication Based Approaches"}
{"id": "task1540-f1e9b5ef18d84a1da23a81ae21b932b4", "input": "The human intelligence lies in the algorithm, the nature of algorithms lies in the classification, and the classification is equal to outlier detection. This paper is based on its past unpublished edition (2009), which discussed an application of \u05e4 (pe) algorithm in outlier detection for time series data. The \u05e4 algorithm, also named as RDD algorithm, is originated from the study on general AI. United with it, designed modules can be used to realize kinds of tasks. A primary framework concerned with the mind through \u05e4 algorithm has been constructed in prior works. In this concise paper, we neglect background and minor description of the early edition, and directly discuss the main contents include longest k\u2013turn subsequence problem, curve type outliers, futural directions and related comments. In section \u201cPast Present\u201d, we keep all prior conclusions, though a little might be out of date.", "target": "\u05e4 Algorithm: its past present, futue present and comments"}
{"id": "task1540-174e35023db04c7289dfac69660b199b", "input": "Encoding finite linear CSPs as Boolean formulas and solving them by using modern SAT solvers has proven to be highly effective, as exemplified by the award-winning sugar system. We here develop an alternative approach based on ASP. This allows us to use first-order encodings providing us with a high degree of flexibility for easy experimentation with different implementations. The resulting system aspartame re-uses parts of sugar for parsing and normalizing CSPs. The obtained set of facts is then combined with an ASP encoding that can be grounded and solved by off-the-shelf ASP systems. We establish the competitiveness of our approach by empirically contrasting aspartame and sugar.", "target": "Aspartame: Solving Constraint Satisfaction Problems with Answer Set Programming"}
{"id": "task1540-ff61c065bc084efeb028fe28e1d00314", "input": "We propose a methodology for designing dependable Artificial Neural Networks (ANN) by extending the concepts of understandability, correctness, and validity that are crucial ingredients in existing certification standards. We apply the concept in a concrete case study in designing a high-way ANNbased motion predictor to guarantee safety properties such as impossibility for the ego vehicle to suggest moving to the right lane if there exists another vehicle on its right.", "target": "Neural Networks for Safety-Critical Applications - Challenges, Experiments and Perspectives"}
{"id": "task1540-22db314146a64f4cb43ec12676d54949", "input": "Convolutional Neural Networks (CNNs) were recently shown to provide state-of-theart results for object category viewpoint estimation. However different ways of formulating this problem have been proposed and the competing approaches have been explored with very different design choices. This paper presents a comparison of these approaches in a unified setting as well as a detailed analysis of the key factors that impact performance. Followingly, we present a new joint training method with the detection task and demonstrate its benefit. We also highlight the superiority of classification approaches over regression approaches, quantify the benefits of deeper architectures and extended training data, and demonstrate that synthetic data is beneficial even when using ImageNet training data. By combining all these elements, we demonstrate an improvement of approximately 5% mAVP over previous state-of-the-art results on the Pascal3D+ dataset [29]. In particular for their most challenging 24 view classification task we improve the results from 31.1% to 36.1% mAVP.", "target": "Crafting a multi-task CNN for viewpoint estimation"}
{"id": "task1540-4423cbc8801944b7b4f67a457da19f76", "input": "The strength with which a statement is made can have a significant impact on the audience. For example, international relations can be strained by how the media in one country describes an event in another; and papers can be rejected because they overstate or understate their findings. It is thus important to understand the effects of statement strength. A first step is to be able to distinguish between strong and weak statements. However, even this problem is understudied, partly due to a lack of data. Since strength is inherently relative, revisions of texts that make claims are a natural source of data on strength differences. In this paper, we introduce a corpus of sentence-level revisions from academic writing. We also describe insights gained from our annotation efforts for this task.", "target": "A Corpus of Sentence-level Revisions in Academic Writing: A Step towards Understanding Statement Strength in Communication"}
{"id": "task1540-e6bf71ed73944893908955a7550c16f4", "input": "Abundant data is the key to successful machine learning. However, supervised learning requires annotated data that are often hard to obtain. In a classification task with limited resources, Active Learning (AL) promises to guide annotators to examples that bring the most value for a classifier. AL can be successfully combined with self-training, i.e., extending a training set with the unlabelled examples for which a classifier is the most certain. We report our experiences on using AL in a systematic manner to train an SVM classifier for Stack Overflow posts discussing performance of software components. We show that the training examples deemed as the most valuable to the classifier are also the most difficult for humans to annotate. Despite carefully evolved annotation criteria, we report low inter-rater agreement, but we also propose mitigation strategies. Finally, based on one annotator\u2019s work, we show that self-training can improve the classification accuracy. We conclude the paper by discussing implication for future text miners aspiring to use AL and self-training. Keywords-text mining, classification, active learning, selftraining, human annotation.", "target": "On Using Active Learning and Self-Training when Mining Performance Discussions on Stack Overflow"}
{"id": "task1540-817e8e0c6bcd455cb566244d77b13d2f", "input": "The paper investigates parameterized approximate message-passing schemes that are based on bounded inference and are inspired by Pearl\u2019s belief propagation algorithm (BP). We start with the bounded inference mini-clustering algorithm and then move to the iterative scheme called Iterative Join-Graph Propagation (IJGP), that combines both iteration and bounded inference. Algorithm IJGP belongs to the class of Generalized Belief Propagation algorithms, a framework that allowed connections with approximate algorithms from statistical physics and is shown empirically to surpass the performance of mini-clustering and belief propagation, as well as a number of other stateof-the-art algorithms on several classes of networks. We also provide insight into the accuracy of iterative BP and IJGP by relating these algorithms to well known classes of constraint propagation schemes.", "target": "Join-Graph Propagation Algorithms"}
{"id": "task1540-d6131cf9d11a4ab1a4bb941827096a93", "input": "In machine translation it is common phenomenon that machine-readable dictionaries and standard parsing rules are not enough to ensure accuracy in parsing and translating English phrases into Korean language, which is revealed in misleading translation results due to consequent structural and semantic ambiguities. This paper aims to suggest a solution to structural and semantic ambiguities due to the idiomaticity and non-grammaticalness of phrases commonly used in English language by applying bilingual phrase database in English-Korean Machine Translation (EKMT). This paper firstly clarifies what the phrase unit in EKMT is based on the definition of the English phrase, secondly clarifies what kind of language unit can be the target of the phrase database for EKMT, thirdly suggests a way to build the phrase database by presenting the format of the phrase database with examples, and finally discusses briefly the method to apply this bilingual phrase database to the EKMT for structural and semantic disambiguation.", "target": "Phrase database Approach to structural and semantic disambiguation in English-Korean Machine Translation"}
{"id": "task1540-619ae3e2960b4afd8cbc356eb9747ee6", "input": "Implicit discourse relation recognition is a crucial component for automatic discourse-level analysis and nature language understanding. Previous studies exploit discriminative models that are built on either powerful manual features or deep discourse representations. In this paper, instead, we explore generative models and propose a variational neural discourse relation recognizer. We refer to this model as VIRILE. VIRILE establishes a directed probabilistic model with a latent continuous variable that generates both a discourse and the relation between the two arguments of the discourse. In order to perform efficient inference and learning, we introduce a neural discourse relation model to approximate the posterior of the latent variable, and employ this approximated posterior to optimize a reparameterized variational lower bound. This allows VIRILE to be trained with standard stochastic gradient methods. Experiments on the benchmark data set show that VIRILE can achieve competitive results against state-of-the-art baselines.", "target": "Variational Neural Discourse Relation Recognizer"}
{"id": "task1540-8d84395fb96e4ddc8a1e79c177091485", "input": "Languages for open-universe probabilistic models (OUPMs) can represent situations with an unknown number of objects and identity uncertainty. While such cases arise in a wide range of important real-world applications, existing general purpose inference methods for OUPMs are far less efficient than those available for more restricted languages and model classes. This paper goes some way to remedying this deficit by introducing, and proving correct, a generalization of Gibbs sampling to partial worlds with possibly varying model structure. Our approach draws on and extends previous generic OUPM inference methods, as well as auxiliary variable samplers for nonparametric mixture models. It has been implemented for BLOG, a well-known OUPM language. Combined with compile-time optimizations, the resulting algorithm yields very substantial speedups over existing methods on several test cases, and substantially improves the practicality of OUPM languages generally.", "target": "Gibbs Sampling in Open-Universe Stochastic Languages"}
{"id": "task1540-7ae29e7c615f443d9cb3abf5c5233f08", "input": "In this work, answer-set programs that specify repairs of databases are used as a basis for solving computational and reasoning problems about causes for query answers from databases.", "target": "The Causality/Repair Connection in Databases: Causality-Programs\u22c6"}
{"id": "task1540-9c0fe4e5b4f941bea54c033ace32cff4", "input": "Cluster analysis plays an important role in decision making process for many knowledge-based systems. There exist a wide variety of different approaches for clustering applications including the heuristic techniques, probabilistic models, and traditional hierarchical algorithms. In this paper, a novel heuristic approach based on big bang-big crunch algorithm is proposed for clustering problems. The proposed method not only takes advantage of heuristic nature to alleviate typical clustering algorithms such as k-means, but it also benefits from the memory based scheme as compared to its similar heuristic techniques. Furthermore, the performance of the proposed algorithm is investigated based on several benchmark test functions as well as on the well-known datasets. The experimental results shows the significant superiority of the proposed method over the similar algorithms.", "target": "Memory Enriched Big Bang Big Crunch Optimization Algorithm for Data Clustering"}
{"id": "task1540-9041e4e341ff49ecb742d9f19341d75e", "input": "Opinion mining aims at extracting useful subjective information from reliable amounts of text. Opinion mining holder recognition is a task that has not been considered yet in Arabic Language. This task essentially requires deep understanding of clauses structures. Unfortunately, the lack of a robust, publicly available, Arabic parser further complicates the research. This paper presents a leading research for the opinion holder extraction in Arabic news independent from any lexical parsers. We investigate constructing a comprehensive feature set to compensate the lack of parsing structural outcomes. The proposed feature set is tuned from English previous works coupled with our proposed semantic field and named entities features. Our feature analysis is based on Conditional Random Fields (CRF) and semi-supervised pattern recognition techniques. Different research models are evaluated via cross-validation experiments achieving 54.03 F-measure. We publicly release our own research outcome corpus and lexicon for opinion mining community to encourage further research.", "target": "A MACHINE LEARNING APPROACH FOR OPINION HOLDER EXTRACTION IN ARABIC LANGUAGE"}
{"id": "task1540-b334b6bd76e64dc8976f2a8a6aa92a4b", "input": "We introduce openXBOW, an open-source toolkit for the generation of bag-of-words (BoW) representations from multimodal input. In the BoW principle, word histograms were first used as features in document classification, but the idea was and can easily be adapted to, e. g., acoustic or visual low-level descriptors, introducing a prior step of vector quantisation. The openXBOW toolkit supports arbitrary numeric input features and text input and concatenates computed subbags to a final bag. It provides a variety of extensions and options. To our knowledge, openXBOW is the first publicly available toolkit for the generation of crossmodal bags-of-words. The capabilities of the tool are exemplified in two sample scenarios: time-continuous speech-based emotion recognition and sentiment analysis in tweets where improved results over other feature representation forms were observed.", "target": "openXBOW \u2013 Introducing the Passau Open-Source Crossmodal Bag-of-Words Toolkit"}
{"id": "task1540-13a41322319041c58a6f0934b4288e6d", "input": "We introduce polyglot language models, recurrent neural network models trained to predict symbol sequences in many different languages using shared representations of symbols and conditioning on typological information about the language to be predicted. We apply these to the problem of modeling phone sequences\u2014a domain in which universal symbol inventories and cross-linguistically shared feature representations are a natural fit. Intrinsic evaluation on held-out perplexity, qualitative analysis of the learned representations, and extrinsic evaluation in two downstream applications that make use of phonetic features show (i) that polyglot models better generalize to held-out data than comparable monolingual models and (ii) that polyglot phonetic feature representations are of higher quality than those learned monolingually.", "target": "Polyglot Neural Language Models: A Case Study in Cross-Lingual Phonetic Representation Learning"}
{"id": "task1540-17b9b356bdfd4a13a767ca119ef7c45d", "input": "We present a first step towards a framework for defining and manipulating normative documents or contracts described as ContractOriented (C-O) Diagrams. These diagrams provide a visual representation for such texts, giving the possibility to express a signatory\u2019s obligations, permissions and prohibitions, with or without timing constraints, as well as the penalties resulting from the non-fulfilment of a contract. This work presents a CNL for verbalising C-O Diagrams, a web-based tool allowing editing in this CNL, and another for visualising and manipulating the diagrams interactively. We then show how these proof-ofconcept tools can be used by applying them to a small example.", "target": "A CNL for Contract-Oriented Diagrams"}
{"id": "task1540-9e54e6214fda4a3c9d7dbc8a0d2b896e", "input": "Natural language generation (NLG) is a critical component of spoken dialogue and it has a significant impact both on usability and perceived quality. Most NLG systems in common use employ rules and heuristics and tend to generate rigid and stylised responses without the natural variation of human language. They are also not easily scaled to systems covering multiple domains and languages. This paper presents a statistical language generator based on a semantically controlled Long Short-term Memory (LSTM) structure. The LSTM generator can learn from unaligned data by jointly optimising sentence planning and surface realisation using a simple cross entropy training criterion, and language variation can be easily achieved by sampling from output candidates. With fewer heuristics, an objective evaluation in two differing test domains showed the proposed method improved performance compared to previous methods. Human judges scored the LSTM system higher on informativeness and naturalness and overall preferred it to the other systems.", "target": "Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems"}
{"id": "task1540-e10d0734c1c7464ba7e844697cfcfc59", "input": "In this paper, we present a system to visualize RDF knowledge graphs. These graphs are obtained from a knowledge extraction system designed by GEOLSemantics. This extraction is performed using natural language processing and trigger detection. The user can visualize subgraphs by selecting some ontology features like concepts or individuals. The system is also multilingual, with the use of the annotated ontology in English, French, Arabic and Chinese.", "target": "RDF Knowledge Graph Visualization From a Knowledge Extraction System"}
{"id": "task1540-0b790ea2fc2042c2b35729b6b974cd9c", "input": "The ability to simultaneously leverage multiple modes of sensor information is critical for perception of an automated vehicle\u2019s physical surroundings. Spatio-temporal alignment of registration of the incoming information is often a prerequisite to analyzing the fused data. The persistence and reliability of multi-modal registration is therefore the key to the stability of decision support systems ingesting the fused information. LiDAR-video systems like on those many driverless cars are a common example of where keeping the LiDAR and video channels registered to common physical features is important. We develop a deep learning method that takes multiple channels of heterogeneous data, to detect the misalignment of the LiDARvideo inputs. A number of variations were tested on the Ford LiDAR-video driving test data set and will be discussed. To the best of our knowledge the use of multi-modal deep convolutional neural networks for dynamic real-time LiDAR-video registration has not been presented.", "target": "Multi-modal Sensor Registration for Vehicle Perception via Deep Neural Networks"}
{"id": "task1540-848b6f949e3144048efdf2a2b6fd2f19", "input": "In most common settings of Markov Decision Process (MDP), an agent evaluate a policy based on expectation of (discounted) sum of rewards. However in many applications this criterion might not be suitable from two perspective: first, in risk aversion situation expectation of accumulated rewards is not robust enough, this is the case when distribution of accumulated reward is heavily skewed; another issue is that many applications naturally take several objective into consideration when evaluating a policy, for instance in autonomous driving an agent needs to balance speed and safety when choosing appropriate decision. In this paper, we consider evaluating a policy based on a sequence of quantiles it induces on a set of target states, our idea is to reformulate the original problem into a multi-objective MDP problem with lexicographic preference naturally defined. For computation of finding an optimal policy, we proposed an algorithm FLMDP that could solve general multi-objective MDP with lexicographic reward preference.", "target": "Solving Multi-Objective MDP with Lexicographic Preference: An application to stochastic planning with multiple quantile objective"}
{"id": "task1540-4bba0a975eb44debbc1cf3c70e83c8a0", "input": "A two\u2013step Christoffel function based solution is proposed to distribution regression problem. On the first step, to model distribution of observations inside a bag, build Christoffel function for each bag of observations. Then, on the second step, build outcome variable Christoffel function, but use the bag\u2019s Christoffel function value at given point as the weight for the bag\u2019s outcome. The approach allows the result to be obtained in closed form and then to be evaluated numerically. While most of existing approaches minimize some kind an error between outcome and prediction, the proposed approach is conceptually different, because it uses Christoffel function for knowledge representation, what is conceptually equivalent working with probabilities only. To receive possible outcomes and their probabilities Gauss quadrature for second\u2013step measure can be built, then the nodes give possible outcomes and normalized weights \u2013 outcome probabilities. A library providing numerically stable polynomial basis for these calculations is available, what make the proposed approach practical.", "target": "Multiple\u2013Instance Learning: Christoffel Function Approach to Distribution Regression Problem"}
{"id": "task1540-504f877684da4527acac2832177e550a", "input": "Using current reinforcement learning methods, it has recently become possible to learn to play unknown 3D games from raw pixels. In this work, we study the challenges that arise in such complex environments, and summarize current methods to approach these. We choose a task within the Doom game, that has not been approached yet. The goal for the agent is to fight enemies in a 3D world consisting of five rooms. We train the DQN and LSTMA3C algorithms on this task. Results show that both algorithms learn sensible policies, but fail to achieve high scores given the amount of training. We provide insights into the learned behavior, which can serve as a valuable starting point for further research in the Doom domain.", "target": "Deep Reinforcement Learning From Raw Pixels in Doom"}
{"id": "task1540-8301816a4ab24212959c41f80dca2e6f", "input": "With the fast development of deep learning, people have started to train very big neural networks using massive data. Asynchronous Stochastic Gradient Descent (ASGD) is widely used to fulfill this task, which, however, is known to suffer from the problem of delayed gradient. That is, when a local worker adds the gradient it calculates to the global model, the global model may have been updated by other workers and this gradient becomes \u201cdelayed\u201d. We propose a novel technology to compensate this delay, so as to make the optimization behavior of ASGD closer to that of sequential SGD. This is done by leveraging Taylor expansion of the gradient function and efficient approximators to the Hessian matrix of the loss function. We call the corresponding new algorithm Delay Compensated ASGD (DC-ASGD). We evaluated the proposed algorithm on CIFAR-10 and ImageNet datasets, and experimental results demonstrate that DC-ASGD can outperform both synchronous SGD and ASGD, and nearly approaches the performance of sequential SGD.", "target": "Asynchronous Stochastic Gradient Descent with Delay Compensation for Distributed Deep Learning"}
{"id": "task1540-845cfebca39a442b9f9cbab196eaacd9", "input": "This paper develops a model that addresses syntactic embedding for machine comprehension, a key task of natural language understanding. Our proposed model, structural embedding of syntactic trees (SEST), takes each word in a sentence, constructs a sequence of syntactic nodes extracted from syntactic parse trees, and encodes the sequence into a vector representation. The learned vector is then incorporated into neural attention models, which allows learning the mapping of syntactic structures between question and context pairs. We evaluate our approach on SQuAD dataset and demonstrate that our model can accurately identify the syntactic boundaries of the sentences and to extract answers that are syntactically coherent over the baseline methods.", "target": "Structural Embedding of Syntactic Trees for Machine Comprehension"}
{"id": "task1540-22d71f422db645a297058914b913e956", "input": "This paper presents a five-valued representation of bifuzzy sets. This representation is related to a five-valued logic that uses the following values: true, false, inconsistent, incomplete and ambiguous. In the framework of fivevalued representation, formulae for similarity, entropy and syntropy of bifuzzy sets are constructed.", "target": "Entropy and Syntropy in the Context of Five-Valued Logics"}
{"id": "task1540-8070986dbadd4638bfcae30fb3d0fd0d", "input": "Neural machine translation (NMT) approaches have improved the state of the art in many machine translation settings over the last couple of years, but they require large amounts of training data to produce sensible output. We demonstrate that NMT can be used for low-resource languages as well, by introducing more local dependencies and using word alignments to learn sentence reordering during translation. In addition to our novel model, we also present an empirical evaluation of low-resource phrase-based statistical machine translation (SMT) and NMT to investigate the lower limits of the respective technologies. We find that while SMT remains the best option for low-resource settings, our method can produce acceptable translations with only 70 000 tokens of training data, a level where the baseline NMT system fails completely.", "target": "Neural machine translation for low-resource languages"}
{"id": "task1540-24a82da2a2ac4e85bc7a548189b8f1c1", "input": "Accurate prediction of suitable discourse connectives (however, furthermore, etc.) is a key component of any system aimed at building coherent and fluent discourses from shorter sentences and passages. As an example, a dialog system might assemble a long and informative answer by sampling passages extracted from different documents retrieved from the web. We formulate the task of discourse connective prediction and release a dataset of 2.9M sentence pairs separated by discourse connectives for this task. Then, we evaluate the hardness of the task for human raters, apply a recently proposed decomposable attention (DA) model to this task and observe that the automatic predictor has a higher F1 than human raters (32 vs. 30). Nevertheless, under specific conditions the raters still outperform the DA model, suggesting that there is headroom for future improvements. Finally, we further demonstrate the usefulness of the connectives dataset by showing that it improves implicit discourse relation prediction when used for model pre-training.", "target": "Automatic Prediction of Discourse Connectives"}
{"id": "task1540-fa4c6bd8277e43cf952db592e74d9c37", "input": "Our hypothesis is that by equipping certain agents in a multi-agent system controlling an intelligent building with automated decision support, two important factors will be in\u00ad creased. The first is energy saving in the building. The second is customer value-how the people in the building experience the ef\u00ad fects of the actions of the agents. We give evi\u00ad dence for the truth of this hypothesis through experimental findings related to tools for arti\u00ad ficial decision making. A number of assump\u00ad tions related to agent control, through moni\u00ad toring and delegation of tasks to other kinds of agents, of rooms at a test site are relaxed. Each assumption controls at least one un\u00ad certainty that complicates considerably the procedures for selecting actions part of each such agent. We show that in realistic deci\u00ad sion situations, room-controlling agents can make bounded rational decisions even under dynamic real-time constraints. This result can be, and has been, generalized to other domains with even harsher time constraints.", "target": "Artificial Decision Making Under Uncertainty in Intelligent Buildings"}
{"id": "task1540-ba8e92f1e3034f33bbb4ad8799a6cba7", "input": "Finding inclusion-minimal hitting sets for a given collection of sets is a fundamental combinatorial problem with applications in domains as diverse as Boolean algebra, computational biology, and data mining. Much of the algorithmic literature focuses on the problem of recognizing the collection of minimal hitting sets; however, in many of the applications, it is more important to generate these hitting sets. We survey twenty algorithms from across a variety of domains, considering their history, classification, useful features, and computational performance on a variety of synthetic and real-world inputs. We also provide a suite of implementations of these algorithms with a ready-to-use, platform-agnostic interface based on Docker containers and the AlgoRun framework, so that interested computational scientists can easily perform similar tests with inputs from their own research areas on their own computers or through a convenient Web interface.", "target": "The minimal hitting set generation problem: algorithms and computation"}
{"id": "task1540-a8f4c643c0864d55a449ab35d5463a6c", "input": "For any deep computational processing of language we need evidences, and one such set of evidences is corpus. This paper describes the development of a textbased corpus for the Bishnupriya Manipuri language. A Corpus is considered as a building block for any language processing tasks. Due to the lack of awareness like other Indian languages, it is also studied less frequently. As a result the language still lacks a good corpus and basic language processing tools. As per our knowledge this is the first effort to develop a corpus for Bishnupriya Manipuri language. KeywordsCorpus, language, Bishnupriya Manipuri, word analysis, word frequency", "target": "Towards The Development of a Bishnupriya Manipuri Corpus"}
{"id": "task1540-c5b57adf6c274c3f80037a511217a141", "input": "In this report, we describe a Theano-based AlexNet (Krizhevsky et al., 2012) implementation and its naive data parallelism on multiple GPUs. Our performance on 2 GPUs is comparable with the state-of-art Caffe library (Jia et al., 2014) run on 1 GPU. To the best of our knowledge, this is the first open-source Python-based AlexNet implementation to-date.", "target": "TION WITH MULTIPLE GPUS"}
{"id": "task1540-bc382d6c574d4a5197ca639be5a7aff7", "input": "Constrained counting is important in domains ranging from artificial intelligence to software analysis. There are already a few approaches for counting models over various types of constraints. Recently, hashing-based approaches achieve both theoretical guarantees and scalability, but still rely on solution enumeration. In this paper, a new probabilistic polynomial time approximate model counter is proposed, which is also a hashing-based universal framework, but with only satisfiability queries. A variant with a dynamic stopping criterion is also presented. Empirical evaluation over benchmarks on propositional logic formulas and SMT(BV) formulas shows that the approach is promising.", "target": "A New Probabilistic Algorithm for Approximate Model Counting"}
{"id": "task1540-e3b7f72fb1cf4bdfac43f3ae5b2970d1", "input": "Two-timescale Stochastic Approximation (SA) algorithms are widely used in Reinforcement Learning (RL). Their iterates have two parts that are updated with distinct stepsizes. In this work we provide a recipe for analyzing two-timescale SA. Using it, we develop the first convergence rate result for them. From this result we extract key insights on stepsize selection. As an application, we obtain convergence rates for two-timescale RL algorithms such as GTD(0), GTD2, and TDC.", "target": "Two-Timescale Stochastic Approximation Convergence Rates with Applications to Reinforcement Learning"}
{"id": "task1540-895300b581de4e669717f443e168577e", "input": "<lb>We consider a situation in which we see samples Xn \u2208 R drawn i.i.d. from some<lb>distribution with mean zero and unknown covariance A. We wish to compute the<lb>top eigenvector of A in an incremental fashion with an algorithm that maintains<lb>an estimate of the top eigenvector in O(d) space, and incrementally adjusts the<lb>estimate with each new data point that arrives. Two classical such schemes are<lb>due to Krasulina (1969) and Oja (1983). We give finite-sample convergence rates<lb>for both.", "target": "The Fast Convergence of Incremental PCA"}
{"id": "task1540-3396e9a8972c40ccba1f8f4f7a4c4c3d", "input": "This is a machine learning application paper involving big data. We present high-accuracy prediction methods of rare events in semi-structured machine log files, which are produced at high velocity and high volume by NORC\u2019s computer-assisted telephone interviewing (CATI) network for conducting surveys. We judiciously apply natural language processing (NLP) techniques and data-mining strategies to train effective learning and prediction models for classifying uncommon error messages in the log\u2014without access to source code, updated documentation or dictionaries. In particular, our simple but effective approach of features preallocation for learning from imbalanced data coupled with naive Bayes classifiers can be conceivably generalized to supervised or semisupervised learning and prediction methods for other critical events such as cyberattack detection.", "target": "Machine Learning for Machine Data from a CATI Network"}
{"id": "task1540-0b5b1eee6e264a25a7295e09fdcfa6ff", "input": "How fake news goes viral via social media? How does its propagation pattern differ from real stories? In this paper, we attempt to address the problem of identifying rumors, i.e., fake information, out of microblog posts based on their propagation structure. We firstly model microblog posts diffusion with propagation trees, which provide valuable clues on how an original message is transmitted and developed over time. We then propose a kernel-based method called Propagation Tree Kernel, which captures high-order patterns differentiating different types of rumors by evaluating the similarities between their propagation tree structures. Experimental results on two real-world datasets demonstrate that the proposed kernel-based approach can detect rumors more quickly and accurately than state-ofthe-art rumor detection models.", "target": "Detect Rumors in Microblog Posts Using Propagation Structure via Kernel Learning"}
{"id": "task1540-30cc6f0565614a608ec6fabbf402f7b3", "input": "We propose a novel approach to reduce memory consumption of the backpropagation through time (BPTT) algorithm when training recurrent neural networks (RNNs). Our approach uses dynamic programming to balance a trade-off between caching of intermediate results and recomputation. The algorithm is capable of tightly fitting within almost any user-set memory budget while finding an optimal execution policy minimizing the computational cost. Computational devices have limited memory capacity and maximizing a computational performance given a fixed memory budget is a practical use-case. We provide asymptotic computational upper bounds for various regimes. The algorithm is particularly effective for long sequences. For sequences of length 1000, our algorithm saves 95% of memory usage while using only one third more time per iteration than the standard BPTT.", "target": "Memory-Efficient Backpropagation Through Time"}
{"id": "task1540-7bb59760be8746948bcdfec5cad43b9c", "input": "I. Abstract This paper attempts multi-label classification by extending the idea of independent binary classification models for each output label, and exploring how the inherent correlation between output labels can be used to improve predictions. Logistic Regression, Naive Bayes, Random Forest, and SVM models were constructed, with SVM giving the best results: an improvement of 12.9% over binary models was achieved for hold out cross validation by augmenting with pairwise correlation probabilities of the labels.", "target": "Exploring Correlation between Labels to improve Multi-Label Classification"}
{"id": "task1540-14a51577e706408890dbdbcf715f0f8d", "input": "The goal of argumentation mining, an evolving research field in computational linguistics, is to design methods capable of analyzing people\u2019s argumentation. In this article, we go beyond the state of the art in several ways. (i) We deal with actual Web data and take up the challenges given by the variety of registers, multiple domains, and unrestricted noisy user-generated Web discourse. (ii) We bridge the gap between normative argumentation theories and argumentation phenomena encountered in actual data by adapting an argumentation model tested in an extensive annotation study. (iii) We create a new gold standard corpus (90k tokens in 340 documents) and experiment with several machine learning methods to identify argument components. We offer the data, source codes, and annotation guidelines to the community under free licenses. Our findings show that argumentation mining in user-generated Web discourse is a feasible but challenging task.", "target": "Argumentation Mining in User-Generated Web Discourse"}
{"id": "task1540-72231651c504413e884458ea096c8480", "input": "A currently successful approach to computational semantics is to represent words as embeddings in a machine-learned vector space. We present an ensemble method that combines embeddings produced by GloVe (Pennington et al., 2014) and word2vec (Mikolov et al., 2013) with structured knowledge from the semantic networks ConceptNet (Speer and Havasi, 2012) and PPDB (Ganitkevitch et al., 2013), merging their information into a common representation with a large, multilingual vocabulary. The embeddings it produces achieve state-of-the-art performance on many word-similarity evaluations. Its score of \u03c1 = .596 on an evaluation of rare words (Luong et al., 2013) is 16% higher than the previous best known system.", "target": "An Ensemble Method to Produce High-Quality Word Embeddings"}
{"id": "task1540-58965754189f467ea00aebe1963f64cc", "input": "We formalize Simplified Boardgames language, which describes a subclass of arbitrary board games. The language structure is based on the regular expressions, which makes the rules easily machine-processable while keeping the rules concise and fairly human-readable.", "target": "SIMPLIFIED BOARDGAMES"}
{"id": "task1540-9a7d1cfe81dc445ea1813f85c874e9a3", "input": "This is a working paper summarizing results of an ongoing research project whose aim is to uniquely characterize the uncertainty mea\u00ad sure for the Dempster-Shafer Theory. A set of intuitive axiomatic requirements is pre\u00ad sented, some of their implications are shown, and the proof is given of the minimality of re\u00ad cently proposed measure AU among all mea\u00ad sures satisfying the proposed requirements.", "target": "Toward a Characterization of Uncertainty Measure for the Dempster-Shafer Theory"}
{"id": "task1540-8038635cd51d4628a91c065a45ccef42", "input": "Recent studies on knowledge base completion, the task of recovering missing relationships based on recorded relations, demonstrate the importance of learning embeddings from multi-step relations. However, due to the size of knowledge bases, learning multi-step relations directly on top of observed instances could be costly. In this paper, we propose Implicit ReasoNets (IRNs), which is designed to perform large-scale inference implicitly through a search controller and shared memory. Unlike previous work, IRNs use training data to learn to perform multi-step inference through the shared memory, which is also jointly updated during training. While the inference procedure is not operating on top of observed instances for IRNs, our proposed model outperforms all previous approaches on the popular FB15k benchmark by more than 5.7%.", "target": "IMPLICIT REASONET: MODELING LARGE-SCALE STRUCTURED RELATIONSHIPS WITH SHARED MEM- ORY"}
{"id": "task1540-61ddb76475b34061ac5ab48b476b0ce5", "input": "In this study, we introduce a new approach for learning language models by training them to estimate word-context pointwise mutual information (PMI), and then deriving the desired conditional probabilities from PMI at test time. Specifically, we show that with minor modifications to word2vec\u2019s algorithm, we get principled language models that are closely related to the well-established Noise Contrastive Estimation (NCE) based language models. A compelling aspect of our approach is that our models are trained with the same simple negative sampling objective function that is commonly used in word2vec to learn word embeddings.", "target": "A Simple Language Model based on PMI Matrix Approximations"}
{"id": "task1540-299c248f533042a1b62b1df78a56207d", "input": "Natural language understanding and dialogue policy learning are both essential in conversational systems that predict the next system actions in response to a current user utterance. Conventional approaches aggregate separate models of natural language understanding (NLU) and system action prediction (SAP) as a pipeline that is sensitive to noisy outputs of error-prone NLU. To address the issues, we propose an end-to-end deep recurrent neural network with limited contextual dialogue memory by jointly training NLU and SAP on DSTC4 multi-domain human-human dialogues. Experiments show that our proposed model significantly outperforms the state-of-the-art pipeline models for both NLU and SAP, which indicates that our joint model is capable of mitigating the affects of noisy NLU outputs, and NLU model can be refined by error flows backpropagating from the extra supervised signals of system actions.", "target": "END-TO-END JOINT LEARNING OF NATURAL LANGUAGE UNDERSTANDING AND DIALOGUE MANAGER"}
{"id": "task1540-02420e623d10421990ab8a75b36b94d6", "input": "Neural networks have proven effective at solving difficult problems but designing their architectures can be challenging, even for image classification problems alone. Evolutionary algorithms provide a technique to discover such networks automatically. Despite significant computational requirements, we show that evolving models that rival large, hand-designed architectures is possible today. We employ simple evolutionary techniques at unprecedented scales to discover models for the CIFAR-10 and CIFAR-100 datasets, starting from trivial initial conditions. To do this, we use novel and intuitive mutation operators that navigate large search spaces. We stress that no human participation is required once evolution starts and that the output is a fully-trained model. Throughout this work, we place special emphasis on the repeatability of results, the variability in the outcomes and the computational requirements.", "target": "Large-Scale Evolution of Image Classifiers"}
{"id": "task1540-1e2b527d63e14aa5b7c4e7b9659de44f", "input": "Embeddings are generic representations that are useful for many NLP tasks. In this paper, we introduce DENSIFIER, a method that learns an orthogonal transformation of the embedding space that focuses the information relevant for a task in an ultradense subspace of a dimensionality that is smaller by a factor of 100 than the original space. We show that ultradense embeddings generated by DENSIFIER reach state of the art on a lexicon creation task in which words are annotated with three types of lexical information \u2013 sentiment, concreteness and frequency. On the SemEval2015 10B sentiment analysis task we show that no information is lost when the ultradense subspace is used, but training is an order of magnitude more efficient due to the compactness of the ultradense space.", "target": "Ultradense Word Embeddings by Orthogonal Transformation"}
{"id": "task1540-19c8871f37a347faa1d936b2a9e6f55e", "input": "Several authors have recently developed risk-sensitive policy gradient methods<lb>that augment the standard expected cost minimization problem with a measure of<lb>variability in cost. These studies have focused on specific risk-measures, such as<lb>the variance or conditional value at risk (CVaR). In this work, we extend the pol-<lb>icy gradient method to the whole class of coherent risk measures, which is widely<lb>accepted in finance and operations research, among other fields. We consider<lb>both static and time-consistent dynamic risk measures. For static risk measures,<lb>our approach is in the spirit of policy gradient algorithms and combines a standard<lb>sampling approach with convex programming. For dynamic risk measures, our ap-<lb>proach is actor-critic style and involves explicit approximation of value function.<lb>Most importantly, our contribution presents a unified approach to risk-sensitive<lb>reinforcement learning that generalizes and extends previous results.", "target": "Policy Gradient for Coherent Risk Measures"}
{"id": "task1540-f95b666ff03049d4a8ad9d60abcffe75", "input": "Radiomics has proven to be a powerful prognostic tool for cancer detection, and has previously been applied in lung, breast, prostate, and head-and-neck cancer studies with great success. However, these radiomics-driven methods rely on pre-defined, hand-crafted radiomic feature sets that can limit their ability to characterize unique cancer traits. In this study, we introduce a novel discovery radiomics framework where we directly discover custom radiomic features from the wealth of available medical imaging data. In particular, we leverage novel StochasticNet radiomic sequencers for extracting custom radiomic features tailored for characterizing unique cancer tissue phenotype. Using StochasticNet radiomic sequencers discovered using a wealth of lung CT data, we perform binary classification on 42,340 lung lesions obtained from the CT scans of 93 patients in the LIDC-IDRI dataset. Preliminary results show significant improvement over previous state-of-the-art methods, indicating the potential of the proposed discovery radiomics framework for improving cancer screening and diagnosis.", "target": "Discovery Radiomics via StochasticNet Sequencers for Cancer Detection"}
{"id": "task1540-ee9db39feb5346f5a38dd5cc47935e3b", "input": "Since its inception, the modus operandi of multi-task learning (MTL) has been to minimize the task-wise mean of the empirical risks. We introduce a generalized loss-compositional paradigm for MTL that includes a spectrum of formulations as a subfamily. One endpoint of this spectrum is minimax MTL: a new MTL formulation that minimizes the maximum of the tasks\u2019 empirical risks. Via a certain relaxation of minimax MTL, we obtain a continuum of MTL formulations spanning minimax MTL and classical MTL. The full paradigm itself is loss-compositional, operating on the vector of empirical risks. It incorporates minimax MTL, its relaxations, and many new MTL formulations as special cases. We show theoretically that minimax MTL tends to avoid worst case outcomes on newly drawn test tasks in the learning to learn (LTL) test setting. The results of several MTL formulations on synthetic and real problems in the MTL and LTL test settings are encouraging.", "target": "Minimax Multi-Task Learning and a Generalized Loss-Compositional Paradigm for MTL"}
{"id": "task1540-cb40e0150ead4f7fa1641f9fd491e35b", "input": "Sarcasm occurring due to the presence of numerical portions in text has been quoted as an error made by automatic sarcasm detection approaches in the past. We present a first study in detecting sarcasm in numbers, as in the case of the sentence \u2018Love waking up at 4 am\u2019. We analyze the challenges of the problem, and present Rulebased, Machine Learning and Deep Learning approaches to detect sarcasm in numerical portions of text. Our Deep Learning approach outperforms four past works for sarcasm detection and Rule-based and Machine learning approaches on a dataset of tweets, obtaining an F1-score of 0.93. This shows that special attention to text containing numbers may be useful to improve state-of-the-art in sarcasm detection.", "target": "\u201cHaving 2 hours to write a paper is fun!\u201d: Detecting Sarcasm in Numerical Portions of Text"}
{"id": "task1540-76e8fbd2bc42455597c4dc9a6b7fc68b", "input": "Generative models defining joint distributions over parse trees and sentences are useful for parsing and language modeling, but impose restrictions on the scope of features and are often outperformed by discriminative models. We propose a framework for parsing and language modeling which marries a generative model with a discriminative recognition model in an encoder-decoder setting. We provide interpretations of the framework based on expectation maximization and variational inference, and show that it enables parsing and language modeling within a single implementation. On the English Penn Treenbank, our framework obtains competitive performance on constituency parsing while matching the state-of-the-art singlemodel language modeling score.", "target": "A Generative Parser with a Discriminative Recognition Algorithm"}
{"id": "task1540-07ab0af9d33348909d8f453965273ab3", "input": "Indefinite similarity measures can be frequently found in bio-informatics by means of alignment scores, but are also common in other fields like shape measures in image retrieval. Lacking an underlying vector space, the data are given as pairwise similarities only. The few algorithms available for such data do not scale to larger datasets. Focusing on probabilistic batch classifiers, the Indefinite Kernel Fisher Discriminant (iKFD) and the Probabilistic Classification Vector Machine (PCVM) are both effective algorithms for this type of data but, with cubic complexity. Here we propose an extension of iKFD and PCVM such that linear runtime and memory complexity is achieved for low rank indefinite kernels. Employing the Nystr\u00f6m approximation for indefinite kernels, we also propose a new almost parameter free approach to identify the landmarks, restricted to a supervised learning problem. Evaluations at several larger similarity data from various domains show that the proposed methods provides similar generalization capabilities while being easier to parametrize and substantially faster for large scale data.", "target": "Probabilistic classifiers with low rank indefinite kernels"}
{"id": "task1540-413b88af89ab44dfb6b23c357919d216", "input": "We study the design of interactive clustering algorithms for data sets satisfying natural stability assumptions. Our algorithms start with any initial clustering and only make local changes in each step; both are desirable features in many applications. We show that in this constrained setting one can still design provably efficient algorithms that produce accurate clusterings. We also show that our algorithms perform well on real-world data.", "target": "Local algorithms for interactive clustering"}
{"id": "task1540-2246461daffb45e9b7fbefe4e1ea6ff7", "input": "We introduce the value iteration network: a fully differentiable neural network with a \u2018planning module\u2019 embedded within. Value iteration networks are suitable for making predictions about outcomes that involve planning-based reasoning, such as predicting a desired trajectory from an observation of a map. Key to our approach is a novel differentiable approximation of the valueiteration algorithm, which can be represented as a convolutional neural network, and trained endto-end using standard backpropagation. We evaluate our value iteration networks on the task of predicting optimal obstacle-avoiding trajectories from an image of a landscape, both on synthetic data, and on challenging raw images of the Mars terrain.", "target": "Value Iteration Networks"}
{"id": "task1540-ada8703572234204b4103a84325b45f6", "input": "Deep neural networks are capable of modelling highly nonlinear functions by capturing different levels of abstraction of data hierarchically. While training deep networks, first the system is initialized near a good optimum by greedy layer-wise unsupervised pre-training. However, with burgeoning data and increasing dimensions of the architecture, the time complexity of this approach becomes enormous. Also, greedy pre-training of the layers often turns detrimental by over-training a layer causing it to lose harmony with the rest of the network. In this paper a synchronized parallel algorithm for pre-training deep networks on multi-core machines has been proposed. Different layers are trained by parallel threads running on different cores with regular synchronization. Thus the pre-training process becomes faster and chances of overtraining are reduced. This is experimentally validated using a stacked autoencoder for dimensionality reduction of MNIST handwritten digit database. The proposed algorithm achieved 26% speed-up compared to greedy layer-wise pre-training for achieving the same reconstruction accuracy substantiating its potential as an alternative.", "target": "Faster learning of deep stacked autoencoders on multi-core systems using synchronized layer-wise pre-training"}
{"id": "task1540-72f6ef8038144b6a85ce48d9bece8b7b", "input": "Exposing latent knowledge in geospatial trajectories has the potential to provide a better understanding of the movements of individuals and groups. Motivated by such a desire, this work presents the context tree, a new hierarchical data structure that summarises the context behind user actions in a single model. We propose a method for context tree construction that augments geospatial trajectories with land usage data to identify such contexts. Through evaluation of the construction method and analysis of the properties of generated context trees, we demonstrate the foundation for understanding and modelling behaviour a\u21b5orded. Summarising user contexts into a single data structure gives easy access to information that would otherwise remain latent, providing the basis for better understanding and predicting the actions and behaviours of individuals and groups. Finally, we also present a method for pruning context trees, for use in applications where it is desirable to reduce the size of the tree while retaining useful information.", "target": "Context Trees: Augmenting Geospatial Trajectories with Context"}
{"id": "task1540-805718f88b1d4676a8acf712cf11048d", "input": "We propose a new active learning (AL) method for text classification based on convolutional neural networks (CNNs). In AL, one selects the instances to be manually labeled with the aim of maximizing model performance with minimal effort. Neural models capitalize on word embeddings as features, tuning these to the task at hand. We argue that AL strategies for neural text classification should focus on selecting instances that most affect the embedding space (i.e., induce discriminative word representations). This is in contrast to traditional AL approaches (e.g., uncertainty sampling), which specify higher level objectives. We propose a simple approach that selects instances containing words whose embeddings are likely to be updated with the greatest magnitude, thereby rapidly learning discriminative, task-specific embeddings. Empirical results show that our method outperforms baseline AL approaches.", "target": "Active Discriminative Word Embedding Learning"}
{"id": "task1540-48852ec0347642f69cd1a08b28a76f30", "input": "Fully connected network has been widely used in deep learning, and its computation efficiency is highly benefited from the matrix multiplication algorithm with cuBLAS on GPU. However, We found that, there exist some drawbacks of cuBLAS in calculating matrix A multiplies the transpose of matrix B (i.e., NT operation). To reduce the impact of NT operation by cuBLAS, we exploit the out-of-place transpose of matrix B to avoid using NT operation, and then we apply our method to Caffe, which is a popular deep learning tool. Our contribution is two-fold. First, we propose a naive method (TNN) and model-based method (MTNN) to increase the performance in calculating A \u00d7 B , and it achieves about 4.7 times performance enhancement in our tested cases on GTX1080 card. Second, we integrate MTNN method into Caffe to enhance the efficiency in training fully connected networks, which achieves about 70% speedup compared to the original Caffe in our configured fully connected networks on GTX1080 card.", "target": "Improving the Performance of Fully Connected Neural Networks by Out-of-Place Matrix Transpose"}
{"id": "task1540-ffe2b70709a54a979ae4e3b32c30c806", "input": "In recent years, the crucial importance of metrics in machine learning algorithms has led to an increasing interest for optimizing distance and similarity functions. Most of the state of the art focus on learning Mahalanobis distances (requiring to fulfill a constraint of positive semi-definiteness) for use in a local k-NN algorithm. However, no theoretical link is established between the learned metrics and their performance in classification. In this paper, we make use of the formal framework of (\u01eb, \u03b3, \u03c4)-good similarities introduced by Balcan et al. to design an algorithm for learning a non PSD linear similarity optimized in a nonlinear feature space, which is then used to build a global linear classifier. We show that our approach has uniform stability and derive a generalization bound on the classification error. Experiments performed on various datasets confirm the effectiveness of our approach compared to stateof-the-art methods and provide evidence that (i) it is fast, (ii) robust to overfitting and (iii) produces very sparse classifiers.", "target": "Similarity Learning for Provably AccurateSparse Linear Classification"}
{"id": "task1540-16cdd1eceee14979aa6228abb07ae33c", "input": "A long-standing dream of Artificial Intelligence (AI) has pursued to enrich computer programs with commonsense knowledge enabling machines to reason about our world. This paper offers a new practical insight towards the automation of commonsense reasoning with first-order logic (FOL) ontologies. We propose a new black-box testing methodology of FOL SUMO-based ontologies by exploiting WordNet and its mapping into SUMO. Our proposal includes a method for the (semi-)automatic creation of a very large set of tests and a procedure for its automated evaluation by using automated theorem provers (ATPs). Applying our testing proposal, we are able to successfully evaluate a) the competency of several translations of SUMO into FOL and b) the performance of various automated ATPs. In addition, we are also able to evaluate the resulting set of tests according to different quality criteria.", "target": "Black-box Testing of First-Order Logic Ontologies Using WordNet"}
{"id": "task1540-284639171e324548bf13a23c2b43a82d", "input": "LSTD is a popular algorithm for value function approximation. Whenever the number of features is larger than the number of samples, it must be paired with some form of regularization. In particular, `1-regularization methods tend to perform feature selection by promoting sparsity, and thus, are wellsuited for high\u2013dimensional problems. However, since LSTD is not a simple regression algorithm, but it solves a fixed\u2013point problem, its integration with `1-regularization is not straightforward and might come with some drawbacks (e.g., the P-matrix assumption for LASSO-TD). In this paper, we introduce a novel algorithm obtained by integrating LSTD with the Dantzig Selector. We investigate the performance of the proposed algorithm and its relationship with the existing regularized approaches, and show how it addresses some of their drawbacks.", "target": "A Dantzig Selector Approach to Temporal Difference Learning"}
{"id": "task1540-1f1b6c6ee01a4da0b36d980ff689834a", "input": "In this paper, we study the impact of selection methods in the context of on-line on-board distributed evolutionary algorithms. We propose a variant of the mEDEA algorithm in which we add a selection operator, and we apply it in a task-driven scenario. We evaluate four selection methods that induce different intensity of selection pressure in a multi-robot navigation with obstacle avoidance task and a collective foraging task. Experiments show that a small intensity of selection pressure is sufficient to rapidly obtain good performances on the tasks at hand. We introduce different measures to compare the selection methods, and show that the higher the selection pressure, the better the performances obtained, especially for the more challenging food foraging task.", "target": "Comparison of Selection Methods in On-line Distributed Evolutionary Robotics"}
{"id": "task1540-fe180377011e4f90a775624ad3272b36", "input": "Inspired by the hierarchical hidden Markov models (HHMM), we present the hierarchical semi-Markov conditional random field (HSCRF), a generalisation of embedded undirected Markov chains to model complex hierarchical, nested Markov processes. It is parameterised in a discriminative framework and has polynomial time algorithms for learning and inference. Importantly, we consider partiallysupervised learning and propose algorithms for generalised partially-supervised learning and constrained inference. We demonstrate the HSCRF in two applications: (i) recognising human activities of daily living (ADLs) from indoor surveillance cameras, and (ii) noun-phrase chunking. We show that the HSCRF is capable of learning rich hierarchical models with reasonable accuracy in both fully and partially observed data cases.", "target": "Hierarchical Semi-Markov Conditional Random Fields for Recursive Sequential Data"}
{"id": "task1540-91f918d4a38d49c6b5650175c6a09afa", "input": "Open forms of global constraints allow the addition of new variables to an argument during the execution of a constraint program. Such forms are needed for difficult constraint programming problems where problem construction and problem solving are interleaved, and fit naturally within constraint logic programming. However, in general, filtering that is sound for a global constraint can be unsound when the constraint is open. This paper provides a simple characterization, called contractibility, of the constraints where filtering remains sound when the constraint is open. With this characterization we can easily determine whether a constraint has this property or not. In the latter case, we can use it to derive a contractible approximation to the constraint. We demonstrate this work on both hard and soft constraints. In the process, we formulate two general classes of soft constraints. Under consideration in Theory and Practice of Logic Programming (TPLP).", "target": "Contractibility for Open Global Constraints"}
{"id": "task1540-aa0134a0621b4e7db53deb557d3ce78c", "input": "For an artificial creative agent, an essential driver of the search for novelty is a value function which is often provided by the system designer or users. We argue that an important barrier for progress in creativity research is the inability of these systems to develop their own notion of value for novelty. We propose a notion of knowledge-driven creativity that circumvent the need for an externally imposed value function, allowing the system to explore based on what it has learned from a set of referential objects. The concept is illustrated by a specific knowledge model provided by a deep generative autoencoder. Using the described system, we train a knowledge model on a set of digit images and we use the same model to build coherent sets of new digits that do not belong to known", "target": "Digits that are not: Generating new types through deep neural nets"}
{"id": "task1540-51aeeccc70344474a0ba4d1be0eb741c", "input": "We carefully study how well minimizing convex surrogate loss functions corresponds to minimizing the misclassification error rate for the problem of binary classification with linear predictors. We consider the agnostic setting, and investigate guarantees on the misclassification error of the loss-minimizer in terms of the margin error rate of the best predictor. We show that, aiming for such a guarantee, the hinge loss is essentially optimal among all convex losses.", "target": "Minimizing The Misclassification Error Rate  Using a Surrogate Convex Loss"}
{"id": "task1540-b879a327f2ad4144b5cf972cd8123178", "input": "We consider the problem of Bayesian parameter estimation for deep neural networks, which is important in problem settings where we may have little data, and/ or where we need accurate posterior predictive densities p(y|x,D), e.g., for applications involving bandits or active learning. One simple approach to this is to use online Monte Carlo methods, such as SGLD (stochastic gradient Langevin dynamics). Unfortunately, such a method needs to store many copies of the parameters (which wastes memory), and needs to make predictions using many versions of the model (which wastes time). We describe a method for \u201cdistilling\u201d a Monte Carlo approximation to the posterior predictive density into a more compact form, namely a single deep neural network. We compare to two very recent approaches to Bayesian neural networks, namely an approach based on expectation propagation [HLA15] and an approach based on variational Bayes [BCKW15]. Our method performs better than both of these, is much simpler to implement, and uses less computation at test time.", "target": "Bayesian Dark Knowledge"}
{"id": "task1540-6eb334155da04e7f816785a3c02372a3", "input": "Adequate representation of natural language semantics requires access to vast amounts of common sense and domain-specific world knowledge. Prior work in the field was based on purely statistical techniques that did not make use of background knowledge, on limited lexicographic knowledge bases such as WordNet, or on huge manual efforts such as the CYC project. Here we propose a novel method, called Explicit Semantic Analysis (ESA), for fine-grained semantic interpretation of unrestricted natural language texts. Our method represents meaning in a high-dimensional space of concepts derived from Wikipedia, the largest encyclopedia in existence. We explicitly represent the meaning of any text in terms of Wikipedia-based concepts. We evaluate the effectiveness of our method on text categorization and on computing the degree of semantic relatedness between fragments of natural language text. Using ESA results in significant improvements over the previous state of the art in both tasks. Importantly, due to the use of natural concepts, the ESA model is easy to explain to human users.", "target": "Wikipedia-based Semantic Interpretation for Natural Language Processing"}
{"id": "task1540-7383c70b42c04cc7baa744a28260ce9c", "input": "In this article we introduce how to put vague hyperprior on Dirichlet distribution, and we update the parameter of it by adaptive rejection sampling (ARS). Finally we analyze this hyperprior in an over-fitted mixture model by some synthetic experiments.", "target": "Hyperprior on symmetric Dirichlet distribution"}
{"id": "task1540-1e7c7514c81c468ba632f41f20fd685a", "input": "Recent work on embedding ontology concepts has relied on either expensive manual annotation or automated concept tagging methods that ignore the textual contexts around concepts. We propose a novel method for jointly learning concept, phrase, and word embeddings from an unlabeled text corpus, by using the representative phrases for ontology concepts as distant supervision. We learn embeddings for medical concepts in the Unified Medical Language System and generaldomain concepts in YAGO, using a variety of corpora. Our embeddings show performance competitive with existing methods on concept similarity and relatedness tasks, while requiring no human corpus annotation and demonstrating more than 3x coverage in the vocabulary size.", "target": "A Weakly-Supervised Method for Jointly Embedding Concepts, Phrases, and Words"}
{"id": "task1540-4b0334d6d105432b8ee1048031131698", "input": "In a recent paper, we have shown that Plan Recognition over STRIPS can be formulated and solved using Classical Planning heuristics and algorithms (Ramirez and Geffner 2009). In this work, we show that this formulation subsumes the standard formulation of Plan Recognition over libraries through a compilation of libraries into STRIPS theories. The libraries correspond to AND/OR graphs that may be cyclic and where children of AND nodes may be partially ordered. These libraries include Context-Free Grammars as a special case, where the Plan Recognition problem becomes a parsing with missing tokens problem. Plan Recognition over the standard libraries become Planning problems that can be easily solved by any modern planner, while recognition over more complex libraries, including Context\u2013Free Grammars (CFGs), illustrate limitations of current Planning heuristics and suggest improvements that may be relevant in other Planning problems too.", "target": "Heuristics for Planning, Plan Recognition and Parsing (Written: June 2009, Published: May 2016)"}
{"id": "task1540-46b220a6abb343039287e5001e0643e0", "input": "We present an approach for the detection of coordinateterm relationships between entities from the software domain, that refer to Java classes. Usually, relations are found by examining corpus statistics associated with text entities. In some technical domains, however, we have access to additional information about the real-world objects named by the entities, suggesting that coupling information about the \u201cgrounded\u201d entities with corpus statistics might lead to improved methods for relation discovery. To this end, we develop a similarity measure for Java classes using distributional information about how they are used in software, which we combine with corpus statistics on the distribution of contexts in which the classes appear in text. Using our approach, cross-validation accuracy on this dataset can be improved dramatically, from around 60% to 88%. Human labeling results show that our classifier has an F1 score of 86% over the top 1000 predicted pairs.", "target": "Grounded Discovery of Coordinate Term Relationships between Software Entities"}
{"id": "task1540-bd76278da64c43c7baba3013c0eb6180", "input": "This report presents a plan to develop an intelligent Meta search engine,<lb>iral. This Meta search engine is aimed to provide comprehensive, efficient and<lb>relevant search results for given queries by combining results from different search<lb>engines. This will provide users to cover a larger database of World Wide Web for<lb>their queries and to get more relevant search results. On the other hand this Meta<lb>search engine will be creating a larger user base and attracting online marketing for<lb>Abster-iT. This will provide a new business set-up for Abster-iT.", "target": "Intelligent Search Optimization using Artificial fuzzy logic"}
{"id": "task1540-18633a974e0743ed909d0ae9d54d92ca", "input": "We study the problem of structured output learning from a regression perspective. We first provide a general formulation of the kernel dependency estimation (KDE) approach to this problem using operator-valued kernels. Our formulation overcomes the two main limitations of the original KDE approach, namely the decoupling between outputs in the image space and the inability to use a joint feature space. We then propose a covariance-based operator-valued kernel that allows us to take into account the structure of the kernel feature space. This kernel operates on the output space and only encodes the interactions between the outputs without any reference to the input space. To address this issue, we introduce a variant of our KDE method based on the conditional covariance operator that in addition to the correlation between the outputs takes into account the effects of the input variables. Finally, we evaluate the performance of our KDE approach on three structured output problems, and compare it to the state-of-the-art kernelbased structured output regression methods.", "target": "A Generalized Kernel Approach to Structured Output Learning"}
{"id": "task1540-40859a933c954811a97c885cc0966ae8", "input": "Network data mining has become an important area of study due to the large number of problems it can be applied to. This paper presents NOESIS, an open source framework for network data mining that provides a large collection of network analysis techniques, including the analysis of network structural properties, community detection methods, link scoring, and link prediction, as well as network visualization algorithms. It also features a complete stand\u2013alone graphical user interface that facilitates the use of all these techniques. The NOESIS framework has been designed using solid object\u2013oriented design principles and structured parallel programming. As a lightweight library with minimal external dependencies and a permissive software license, NOESIS can be incorporated into other software projects. Released under a BSD license, it is available from http://noesis.ikor.org.", "target": "The NOESIS Network-Oriented Exploration, Simulation, and Induction System"}
{"id": "task1540-5fd388839a9f48da928b6197389b5f39", "input": "Word embeddings are used with success for a variety of tasks involving lexical semantic similarities between individual words. Using unsupervised methods and just cosine similarity, encouraging results were obtained for analogical similarities. In this paper, we explore the potential of pre-trained word embeddings to identify generic types of semantic relations in an unsupervised experiment. We propose a new relational similarity measure based on the combination of word2vec\u2019s CBOW input and output vectors which outperforms concurrent vector representations, when used for unsupervised clustering on SemEval 2010 Relation Classification data.", "target": "Exploring Vector Spaces for Semantic Relations"}
{"id": "task1540-02a1e357ca194cb2866b1eac6118c556", "input": "Human vision greatly benefits from the information about sizes of objects. The role of size in several visual reasoning tasks has been thoroughly explored in human perception and cognition. However, the impact of the information about sizes of objects is yet to be determined in AI. We postulate that this is mainly attributed to the lack of a comprehensive repository of size information. In this paper, we introduce a method to automatically infer object sizes, leveraging visual and textual information from web. By maximizing the joint likelihood of textual and visual observations, our method learns reliable relative size estimates, with no explicit human supervision. We introduce the relative size dataset and show that our method outperforms competitive textual and visual baselines in reasoning about size comparisons.", "target": "Are Elephants Bigger than Butterflies? Reasoning about Sizes of Objects"}
{"id": "task1540-01c881ac4a7b429d90c2b67f034c117b", "input": "In this paper, we model the trajectory of sea vessels and provide a service that predicts in near-real time the position of any given vessel in 4\u2019, 10\u2019, 20\u2019 and 40\u2019 time intervals. We explore the necessary tradeoffs between accuracy, performance and resource utilization are explored given the large volume and update rates of input data. We start with building models based on well-established machine learning algorithms using static datasets and multi-scan training approaches and identify the best candidate to be used in implementing a single-pass predictive approach, under real-time constraints. The results are measured in terms of accuracy and performance and are compared against the baseline kinematic equations. Results show that it is possible to efficiently model the trajectory of multiple vessels using a single model, which is trained and evaluated using an adequately large, static dataset, thus achieving a significant gain in terms of resource usage while not compromising accuracy.", "target": "Employing traditional machine learning algorithms for big data streams analysis: the case of object trajectory prediction"}
{"id": "task1540-5832db8e13f94799b4ba558ec85e95e5", "input": "Recent work has proposed several generative neural models for constituency parsing that achieve state-of-the-art results. Since direct search in these generative models is difficult, they have primarily been used to rescore candidate outputs from base parsers in which decoding is more straightforward. We first present an algorithm for direct search in these generative models. We then demonstrate that the rescoring results are at least partly due to implicit model combination rather than reranking effects. Finally, we show that explicit model combination can improve performance even further, resulting in new state-of-the-art numbers on the PTB of 94.25 F1 when training only on gold data and 94.66 F1 when using external data.", "target": "Improving Neural Parsing by Disentangling Model Combination and Reranking Effects"}
{"id": "task1540-3d6e84b2bf1e48dc80869dfc4ba9100b", "input": "We develop T-SKIRT: a temporal, structuredknowledge, IRT-based method for predicting student responses online. By explicitly accounting for student learning and employing a structured, multidimensional representation of student proficiencies, the model outperforms standard IRTbased methods on an online response prediction task when applied to real responses collected from students interacting with diverse pools of educational content.", "target": "T-SKIRT: Online Estimation of Student Proficiency in an Adaptive Learning System"}
{"id": "task1540-899a286ce110410fb8f2976c685e0907", "input": "In this work we present a method for using Deep Q-Networks (DQNs) in multi-objective tasks. Deep Q-Networks provide remarkable performance in single objective tasks learning from high-level visual perception. However, in many scenarios (e.g in robotics), the agent needs to pursue multiple objectives simultaneously. We propose an architecture in which separate DQNs are used to control the agent\u2019s behaviour with respect to particular objectives. In this architecture we use signal suppression, known from the (Brooks) subsumption architecture, to combine outputs of several DQNs into a single action. Our architecture enables the decomposition of the agent\u2019s behaviour into controllable and replaceable sub-behaviours learned by distinct modules. To evaluate our solution we used a game-like simulator in which an agent provided with high-level visual input pursues multiple objectives in a 2D world. Our solution provides benefits of modularity, while its performance is comparable to the monolithic approach.", "target": "Multi-Objective Deep Q-Learning with Subsumption Architecture"}
{"id": "task1540-f13b803a37514caf8c2e64167ef46331", "input": "Nowadays ontologies present a growing interest in Data Fusion applications. As a matter of fact, the ontologies are seen as a semantic tool for describing and reasoning about sensor data, objects, relations and general domain theories. In addition, uncertainty is perhaps one of the most important characteristics of the data and information handled by Data Fusion. However, the fundamental nature of ontologies implies that ontologies describe only asserted and veracious facts of the world. Different probabilistic, fuzzy and evidential approaches already exist to fill this gap; this paper recaps the most popular tools. However none of the tools meets exactly our purposes. Therefore, we constructed a Dempster-Shafer ontology that can be imported into any specific domain ontology and that enables us to instantiate it in an uncertain manner. We also developed a Java application that enables reasoning about these uncertain ontological instances.", "target": "Uncertainty in Ontologies: Dempster-Shafer Theory for Data Fusion Applications"}
{"id": "task1540-35d5471287c14bd1b55f9ed18bcda302", "input": "We present an annotation schema as part of an effort to create a manually annotated corpus for Arabic dialogue language understanding including spoken dialogue and written \u201echat\u201f dialogue for inquiryanswer domain. The proposed schema handles mainly the request and response acts that occurs frequently in inquiry-answer debate conversations expressing request services, suggests, and offers. We applied the proposed schema on 83 Arabic inquiry-answer dialogues.", "target": "Arabic Inquiry-Answer Dialogue Acts Annotation Schema"}
{"id": "task1540-3e2bc2449dcb44c183ae1db177c21dc3", "input": "We investigate the pertinence of methods from algebraic topology for text data analysis. These methods enable the development of mathematically-principled isometric-invariant mappings from a set of vectors to a document embedding, which is stable with respect to the geometry of the document in the selected metric space. In this work, we evaluate the utility of these topology-based document representations in traditional NLP tasks, specifically document clustering and sentiment classification. We find that the embeddings do not benefit text analysis. In fact, performance is worse than simple techniques like tf-idf, indicating that the geometry of the document does not provide enough variability for classification on the basis of topic or sentiment in the chosen datasets.", "target": "Does the Geometry of Word Embeddings Help Document Classification? A Case Study on Persistent Homology Based Representations"}
{"id": "task1540-34823de39b9443d6aa0b1703e9187f13", "input": "In this paper, we introduce a notion of backdoors to Reiter\u2019s propositional default logic and study structural properties of it. Also we consider the problems of backdoor detection (parameterised by the solution size) as well as backdoor evaluation (parameterised by the size of the given backdoor), for various kinds of target classes (cnf, horn, krom, monotone, positive-unit). We show that backdoor detection is fixed-parameter tractable for the considered target classes, and backdoor evaluation is either fixed-parameter tractable, in para-\u2206P2 , or in para-NP, depending on the target class.", "target": "Strong Backdoors for Default Logic"}
{"id": "task1540-5d61aab168334d4bb4d502eee98a75b5", "input": "This paper addresses the question of how language use affects community reaction to comments in online discussion forums, and the relative importance of the message vs. the messenger. A new comment ranking task is proposed based on community annotated karma in Reddit discussions, which controls for topic and timing of comments. Experimental work with discussion threads from six subreddits shows that the importance of different types of language features varies with the community of interest.", "target": "Talking to the crowd: What do people react to in online discussions?"}
{"id": "task1540-c28b2c32b81c47bb86b6fa4ce4da16b6", "input": "This paper proposes DRL-Sense\u2014a multisense word representation learning model, to address the word sense ambiguity issue, where a sense selection module and a sense representation module are jointly learned in a reinforcement learning fashion. A novel reward passing procedure is proposed to enable joint training on the selection and representation modules. The modular design implements pure senselevel representation learning with linear time sense selection (decoding). We further develop a non-parametric learning algorithm and a sense exploration mechanism for better flexibility and robustness. The experiments on benchmark data show that the proposed approach achieves the state-of-the-art performance on contextual word similarities and comparable performance with Google\u2019s word2vec while using much less training data.", "target": "DRL-Sense: Deep Reinforcement Learning for Multi-Sense Word Representations"}
{"id": "task1540-869ff1ae9b0f40b1a713712fa13b754e", "input": "Dependency parsers are among the most crucial tools in natural language processing as they have many important applications in downstream tasks such as information retrieval, machine translation and knowledge acquisition. We introduce the Yara Parser, a fast and accurate open-source dependency parser based on the arc-eager algorithm and beam search. It achieves an unlabeled accuracy of 93.32 on the standard WSJ test set which ranks it among the top dependency parsers. At its fastest, Yara can parse about 4000 sentences per second when in greedy mode (1 beam). When optimizing for accuracy (using 64 beams and Brown cluster features), Yara can parse 45 sentences per second. The parser can be trained on any syntactic dependency treebank and different options are provided in order to make it more flexible and tunable for specific tasks. It is released with the Apache version 2.0 license and can be used for both commercial and academic purposes. The parser can be found at https: //github.com/yahoo/YaraParser.", "target": "Yara Parser: A Fast and Accurate Dependency Parser"}
{"id": "task1540-90be022139354474a69cc8f49d785f87", "input": "A model checker can produce a trace of counterexample, for a erroneous program, which is often long and difficult to understand. In general, the part about the loops is the largest among the instructions in this trace. This makes the location of errors in loops critical, to analyze errors in the overall program. In this paper, we explore the scalability capabilities of LocFaults, our error localization approach exploiting paths of CFG(Control Flow Graph) from a counterexample to calculate the MCDs (Minimal Correction Deviations), and MCSs (Minimal Correction Subsets) from each MCD found. We present the times of our approach on programs with While-loops unfolded b times, and a number of diverted conditions ranging from 0 to n. Our preliminary results show that the times of our approach, constraintbased and flow-driven, are better compared to BugAssist which is based on SAT and transforms the entire program to a Boolean formula, although the information provided by LocFaults is more expressive for the user.", "target": "Exploration de la scalabilite\u0301 de LocFaults"}
{"id": "task1540-bf74d80bf37147848e164ba5b50fa927", "input": "Exploration has been a crucial part of reinforcement learning, yet several important questions concerning exploration efficiency are still not answered satisfactorily by existing analytical frameworks. These questions include exploration parameter setting, situation analysis, and hardness of MDPs, all of which are unavoidable for practitioners. To bridge the gap between the theory and practice, we propose a new analytical framework called the success probability of exploration. We show that those important questions of exploration above can all be answered under our framework, and the answers provided by our framework meet the needs of practitioners better than the existing ones. More importantly, we introduce a concrete and practical approach to evaluating the success probabilities in certain MDPs without the need of actually running the learning algorithm. We then provide empirical results to verify our approach, and demonstrate how the success probability of exploration can be used to analyse and predict the behaviours and possible outcomes of exploration, which are the keys to the answer of the important questions of exploration.", "target": "Success Probability of Exploration: a Concrete Analysis of Learning Efficiency"}
{"id": "task1540-82363befa41e45808ac49de2c2defdbe", "input": "Recent research on deep neural networks has focused primarily on improving accuracy. For a given accuracy level, is typically possible to identify multiple DNN architectures that achieve that accuracy level. With equivalent accuracy, smaller DNN architectures offer at least three advantages: (1) Smaller DNNs require less communication across servers during distributed training. (2) Smaller DNNs require less bandwidth to export a new model from the cloud to an autonomous car. (3) Smaller DNNs are more feasible to deploy on FPGAs and other hardware with limited memory. To provide all of these advantages, we propose a small DNN architecture called SqueezeNet. SqueezeNet achieves AlexNetlevel accuracy on ImageNet with 50x fewer parameters. Additionally, with model compression techniques we are able to compress SqueezeNet to less than 1MB (461x smaller than AlexNet). The SqueezeNet architecture is available for download here: https://github.com/DeepScale/SqueezeNet", "target": "SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <1MB model size"}
{"id": "task1540-4bb09ec8796c44b987cf521f60910372", "input": "We describe an open-source toolkit for neural machine translation (NMT). The toolkit prioritizes efficiency, modularity, and extensibility with the goal of supporting NMT research into model architectures, feature representations, and source modalities, while maintaining competitive performance and reasonable training requirements. The toolkit consists of modeling and translation support, as well as detailed pedagogical documentation about the underlying techniques.", "target": "OpenNMT: Open-Source Toolkit for Neural Machine Translation"}
{"id": "task1540-58d0382ed34c4140aebbd61709577a1a", "input": "We provide the first extensive evaluation of how using different types of context to learn skip-gram word embeddings affects performance on a wide range of intrinsic and extrinsic NLP tasks. Our results suggest that while intrinsic tasks tend to exhibit a clear preference to particular types of contexts and higher dimensionality, more careful tuning is required for finding the optimal settings for most of the extrinsic tasks that we considered. Furthermore, for these extrinsic tasks, we find that once the benefit from increasing the embedding dimensionality is mostly exhausted, simple concatenation of word embeddings, learned with different context types, can yield further performance gains. As an additional contribution, we propose a new variant of the skip-gram model that learns word embeddings from weighted contexts of substitute words.", "target": "The Role of Context Types and Dimensionality in Learning Word Embeddings"}
{"id": "task1540-aa9f42530cec46ffabb86ba92fec5b61", "input": "This work proposes a low complexity nonlinearity model and develops adaptive algorithms over it. The model is based on the decomposable\u2014or rank-one, in tensor language\u2014 Volterra kernels. It may also be described as a product of FIR filters, which explains its low-complexity. The rank-one model is also interesting because it comes from a well-posed problem in approximation theory. The paper uses such model in an estimation theory context to develop an exact gradienttype algorithm, from which adaptive algorithms such as the least mean squares (LMS) filter and its data-reuse version\u2014the TRUE-LMS\u2014are derived. Stability and convergence issues are addressed. The algorithms are then tested in simulations, which show its good performance when compared to other nonlinear processing algorithms in the literature.", "target": "Nonlinear Adaptive Algorithms on Rank-One Tensor Models"}
{"id": "task1540-f3777e50a8994b09afd95891aae6795e", "input": "We propose a new algorithm for minimizing regularized empirical loss: Stochastic Dual Newton Ascent (SDNA). Our method is dual in nature: in each iteration we update a random subset of the dual variables. However, unlike existing methods such as stochastic dual coordinate ascent, SDNA is capable of utilizing all curvature information contained in the examples, which leads to striking improvements in both theory and practice \u2013 sometimes by orders of magnitude. In the special case when an L2-regularizer is used in the primal, the dual problem is a concave quadratic maximization problem plus a separable term. In this regime, SDNA in each step solves a proximal subproblem involving a random principal submatrix of the Hessian of the quadratic function; whence the name of the method. If, in addition, the loss functions are quadratic, our method can be interpreted as a novel variant of the recently introduced Iterative Hessian Sketch.", "target": "SDNA: Stochastic Dual Newton Ascent for Empirical Risk Minimization"}
{"id": "task1540-12be216e8bc342b6acada964cb753379", "input": "In this paper, we propose a novel multi-label learning framework, called Multi-Label Self-Paced Learning (MLSPL), in an attempt to incorporate the self-paced learning strategy into multi-label learning regime. In light of the benefits of adopting the easy-to-hard strategy proposed by self-paced learning, the devised MLSPL aims to learn multiple labels jointly by gradually including label learning tasks and instances into model training from the easy to the hard. We first introduce a self-paced function as a regularizer in the multi-label learning formulation, so as to simultaneously rank priorities of the label learning tasks and the instances in each learning iteration. Considering that different multi-label learning scenarios often need different self-paced schemes during optimization, we thus propose a general way to find the desired self-paced functions. Experimental results on three benchmark datasets suggest the state-of-the-art performance of our approach.", "target": "A Self-Paced Regularization Framework for Multi-Label Learning"}
{"id": "task1540-eb47bc43994944e5abf51d3b93308542", "input": "We propose a transfer deep learning (TDL) framework that can transfer the knowledge obtained from a single-modal neural network to a network with a different modality. Specifically, we show that we can leverage speech data to fine-tune the network trained for video recognition, given an initial set of audio-video parallel dataset within the same semantics. Our approach first learns the analogypreserving embeddings between the abstract representations learned from intermediate layers of each network, allowing for semantics-level transfer between the source and target modalities. We then apply our neural network operation that fine-tunes the target network with the additional knowledge transferred from the source network, while keeping the topology of the target network unchanged. While we present an audio-visual recognition task as an application of our approach, our framework is flexible and thus can work with any multimodal dataset, or with any already-existing deep networks that share the common underlying semantics. In this work in progress report, we aim to provide comprehensive results of different configurations of the proposed approach on two widely used audiovisual datasets, and we discuss potential applications of the proposed approach.", "target": "Multimodal Transfer Deep Learning with Applications in Audio-Visual Recognition"}
{"id": "task1540-8ab4083d11c449ccbf63b922fd06ad41", "input": "Progress in language and image understanding by machines has sparkled the interest of the research community in more open-ended, holistic tasks, and refueled an old AI dream of building intelligent machines. We discuss a few prominent challenges that characterize such holistic tasks and argue for \u201cquestion answering about images\u201d as a particular appealing instance of such a holistic task. In particular, we point out that it is a version of a Turing Test that is likely to be more robust to over-interpretations and contrast it with tasks like grounding and generation of descriptions. Finally, we discuss tools to measure progress in this field.", "target": "Hard to Cheat: A Turing Test based on Answering Questions about Images"}
{"id": "task1540-12e394c379534287aa7e41c07df55c12", "input": "This paper presents a systematic evaluation of Neural Network (NN) for classification of real-world data. In the field of machine learning, it is often seen that a single parameter that is \u2018predictive accuracy\u2019 is being used for evaluating the performance of a classifier model. However, this parameter might not be considered reliable given a dataset with very high level of skewness. To demonstrate such behavior, seven different types of datasets have been used to evaluate a Multilayer Perceptron (MLP) using twelve(12) different parameters which include microand macro-level estimation. In the present study, the most common problem of prediction called \u2018multiclass\u2019 classification has been considered. The results that are obtained for different parameters for each of the dataset could demonstrate interesting findings to support the usability of these set of performance evaluation parameters.", "target": "Reliable Evaluation of Neural Network for Multiclass Classification of Real-world Data"}
{"id": "task1540-16c6b8e829524408a5560ba53a3efe9a", "input": "Latent Dirichlet Allocation (LDA) mining thematic structure of documents plays an important role in nature language processing and machine learning areas. However, the probability distribution from LDA only describes the statistical relationship of occurrences in the corpus and usually in practice, probability is not the best choice for feature representations. Recently, embedding methods have been proposed to represent words and documents by learning essential concepts and representations, such as Word2Vec and Doc2Vec. The embedded representations have shown more effectiveness than LDA-style representations in many tasks. In this paper, we propose the Topic2Vec approach which can learn topic representations in the same semantic vector space with words, as an alternative to probability. The experimental results show that Topic2Vec achieves interesting and meaningful results.", "target": "Topic2Vec: Learning Distributed Representations of Topics"}
{"id": "task1540-58daef9ec51342098d35bd5b7656d3aa", "input": "We demonstrate that a continuous relaxation of the argmax operation can be used to create a differentiable approximation to greedy decoding for sequence-tosequence (seq2seq) models. By incorporating this approximation into the scheduled sampling training procedure (Bengio et al., 2015)\u2013a well-known technique for correcting exposure bias\u2013we introduce a new training objective that is continuous and differentiable everywhere and that can provide informative gradients near points where previous decoding decisions change their value. In addition, by using a related approximation, we demonstrate a similar approach to sampled-based training. Finally, we show that our approach outperforms cross-entropy training and scheduled sampling procedures in two sequence prediction tasks: named entity recognition and machine translation.", "target": "Differentiable Scheduled Sampling for Credit Assignment"}
{"id": "task1540-955100045f0c4d38a88bd3f5f7555225", "input": "Training Recurrent Neural Networks is more troublesome than feedforward ones because of the vanishing and exploding gradient problems detailed in Bengio et al. (1994). In this paper we attempt to understand the fundamental issues underlying the exploding gradient problem by exploring it from an analytical, a geometric and a dynamical system perspective. Our analysis is used to justify the simple yet effective solution of norm clipping the exploded gradient. In the experimental section, the comparison between this heuristic solution and standard SGD provides empirical evidence towards our hypothesis as well as it shows that such a heuristic is required to reach state of the art results on a character prediction task and a polyphonic music prediction one.", "target": "Understanding the exploding gradient problem"}
{"id": "task1540-1a6b5860c48a4c5795b23c28bca80a88", "input": "This paper describes a new approach allowing the generation of a simplified Biped gait. This approach combines a classical dynamic modeling with an inverse kinematics\u2019 solver based on particle swarm optimization, PSO. First, an inverted pendulum, IP, is used to obtain a simplified dynamic model of the robot and to compute the target position of a key point in biped locomotion, the Centre Of Mass, COM. The proposed algorithm, called IK-PSO, Inverse Kinematics PSO, returns and inverse kinematics solution corresponding to that COM respecting the joints constraints. In This paper the inertia weight PSO variant is used to generate a possible solution according to the stability based fitness function and a set of joints motions constraints. The method is applied with success to a leg motion generation. Since based on a precalculated COM, that satisfied the biped stability, the proposal allowed also to plan a walk with application on a small size biped robot. General Terms Robotics, Robotic Modeling, Computational Intelligence", "target": "IK-PSO, PSO Inverse Kinematics Solver with Application to Biped Gait Generation"}
{"id": "task1540-ba90e1c2121d4cc39c4983a9200018e2", "input": "Derivational morphology is a fundamental and complex characteristic of language. In this paper we propose the new task of predicting the derivational form of a given base-form lemma that is appropriate for a given context. We present an encoder\u2013 decoder style neural network to produce a derived form character-by-character, based on its corresponding character-level representation of the base form and the context. We demonstrate that our model is able to generate valid context-sensitive derivations from known base forms, but is less accurate under a lexicon agnostic setting.", "target": "Context-Aware Prediction of Derivational Word-forms"}
{"id": "task1540-b233467408234a5ebeeed5c55b599d76", "input": "Modern Learning Classifier Systems can be characterized by their use of rule accuracy as the utility metric for the search algorithm(s) discovering useful rules. Such searching typically takes place within the restricted space of co-active rules for efficiency. This paper gives an historical overview of the evolution of such systems.", "target": "A Brief History of Learning Classifier Systems: From CS-1 to XCS"}
{"id": "task1540-f539aca39e59405394ecd759c42657c2", "input": "In data analysis, latent variables play a central role because they help provide powerful insights into a wide variety of phenomena, ranging from biological to human sciences. The latent tree model, a particular type of probabilistic graphical models, deserves attention. Its simple structure a tree allows simple and efficient inference, while its latent variables capture complex relationships. In the past decade, the latent tree model has been subject to significant theoretical and methodological developments. In this review, we propose a comprehensive study of this model. First we summarize key ideas underlying the model. Second we explain how it can be efficiently learned from data. Third we illustrate its use within three types of applications: latent structure discovery, multidimensional clustering, and probabilistic inference. Finally, we conclude and give promising directions for future researches in this field.", "target": "A Survey on Latent Tree Models and Applications"}
{"id": "task1540-3ae9d6bb878748d8845c61f84966d53b", "input": "Non-technical losses (NTL) such as electricity theft cause significant harm to our economies, as in some countries they may range up to 40% of the total electricity distributed. Detecting NTLs requires costly on-site inspections. Accurate prediction of NTLs for customers using machine learning is therefore crucial. To date, related research largely ignore that the two classes of regular and non-regular customers are highly imbalanced, that NTL proportions may change and mostly consider small data sets, often not allowing to deploy the results in production. In this paper, we present a comprehensive approach to assess three NTL detection models for different NTL proportions in large real world data sets of 100Ks of customers: Boolean rules, fuzzy logic and Support Vector Machine. This work has resulted in appreciable results that are about to be deployed in a leading industry solution. We believe that the considerations and observations made in this contribution are necessary for future smart meter research in order to report their effectiveness on imbalanced and large real world data sets.", "target": "Large-Scale Detection of Non-Technical Losses in Imbalanced Data Sets"}
{"id": "task1540-b04a1c8f83554fcf94253d85f57a3c2a", "input": "This work introduces a method to tune a sequence-based generative model for molecular de novo design that through augmented episodic likelihood can learn to generate structures with certain specified desirable properties. We demonstrate how this model can execute a range of tasks such as generating analogues to a query structure and generating compounds predicted to be active against a biological target. As a proof of principle, the model is first trained to generate molecules that do not contain sulphur. As a second example, the model is trained to generate analogues to the drug Celecoxib, a technique that could be used for scaffold hopping or library expansion starting from a single molecule. Finally, when tuning the model towards generating compounds predicted to be active against the dopamine receptor D2, the model generates structures of which more than 95% are predicted to be active, including experimentally confirmed actives that have not been included in either the generative model nor the activity prediction model.", "target": "Molecular De-Novo Design through Deep Reinforcement Learning"}
{"id": "task1540-13594451ad034d66a0bafc6cb4877788", "input": "A branch-and-bound approach to solving influence diagrams has been previously proposed in the literature, but appears to have never been implemented and evaluated \u2013 apparently due to the difficulties of computing effective bounds for the branch-and-bound search. In this paper, we describe how to efficiently compute effective bounds, and we develop a practical implementation of depth-first branch-and-bound search for influence diagram evaluation that outperforms existing methods for solving influence diagrams with multiple stages.", "target": "Solving Multistage Influence Diagrams using Branch-and-Bound Search"}
{"id": "task1540-8434df10591e4edc8e9328d72ea401ac", "input": "Most current word prediction systems make use of n-gram language models (LM) to estimate the probability of the following word in a phrase. In the past years there have been many attempts to enrich such language models with further syntactic or semantic information. We want to explore the predictive powers of Latent Semantic Analysis (LSA), a method that has been shown to provide reliable information on long-distance semantic dependencies between words in a context. We present and evaluate here several methods that integrate LSA-based information with a standard language model: a semantic cache, partial reranking, and different forms of interpolation. We found that all methods show significant improvements, compared to the 4gram baseline, and most of them to a simple cache model as well.", "target": "Methods to integrate a language model with semantic information for a word prediction component"}
{"id": "task1540-fc928ca4fa934e7bab53f5938940c6c5", "input": "We introduce Deep Neural Programs (DNP), a novel programming paradigm for writing adaptive controllers for cyber-physical systems (CPS). DNP replace if and while statements, whose discontinuity is responsible for undecidability in CPS analysis, intractability in CPS design, and frailness in CPS implementation, with their smooth, neural nif and nwhile counterparts. This not only makes CPS analysis decidable and CPS design tractable, but also allows to write robust and adaptive CPS code. In DNP the connection between the sigmoidal guards of the nif and nwhile statements has to be given as a Gaussian Bayesian network, which reflects the partial knowledge, the CPS program has about its environment. To the best of our knowledge, DNP are the first approach linking neural networks to programs, in a way that makes explicit the meaning of the network. In order to prove and validate the usefulness of DNP, we use them to write and learn an adaptive CPS controller for the parallel parking of the Pioneer rovers available in our CPS lab.", "target": "Deep Neural Programs for Adaptive Control in Cyber-Physical Systems"}
{"id": "task1540-0a1310dd7c5d4aaa920b9fbd75830909", "input": "Stochastic local search (SLS) algorithms have exhibited great effectiveness in finding models of random instances of the Boolean satisfiability problem (SAT). As one of the most widely known and used SLS algorithm, WalkSAT plays a key role in the evolutions of SLS for SAT, and also hold stateof-the-art performance on random instances. This work proposes a novel implementation for WalkSAT which decreases the redundant calculations leading to a dramatically speeding up, thus dominates the latest version of WalkSAT including its advanced variants.", "target": "An Efficient Implementation for WalkSAT"}
{"id": "task1540-4a944d61503f4bbdb496023825b7c03e", "input": "Identifying topics of discussions in online health communities (OHC) is critical to various applications, but can be difficult because topics of OHC content are usually heterogeneous and domaindependent. In this paper, we provide a multi-class schema, an annotated dataset, and supervised classifiers based on convolutional neural network (CNN) and other models for the task of classifying discussion topics. We apply the CNN classifier to the most popular breast cancer online community, and carry out a longitudinal analysis to show topic distributions and topic changes throughout members\u2019 participation. Our experimental results suggest that CNN outperforms other classifiers in the task of topic classification, and that certain trajectories can be detected with respect to topic changes.", "target": "Longitudinal Analysis of Discussion Topics in an Online Breast Cancer Community using Convolutional Neural Networks"}
{"id": "task1540-6362f5785df040d7b4b83775d85170d8", "input": "This work studies the representational mapping across multimodal data such that given a piece of the raw data in one modality the corresponding semantic description in terms of the raw data in another modality is immediately obtained. Such a representational mapping can be found in a wide spectrum of real-world applications including image/video retrieval, object recognition, action/behavior recognition, and event understanding and prediction. To that end, we introduce a simplified training objective for learning multimodal embeddings using the skip-gram architecture by introducing convolutional \u201cpseudowords:\u201d embeddings composed of the additive combination of distributed word representations and image features from convolutional neural networks projected into the multimodal space. We present extensive results of the representational properties of these embeddings on various word similarity benchmarks to show the promise of this approach.", "target": "MULTIMODAL SKIP-GRAM USING CONVOLUTIONAL PSEUDOWORDS"}
{"id": "task1540-81cb1770b5184e3bb8f1d4792d33da0f", "input": "The current trends in next-generation exascale systems go towards integrating a wide range of specialized (co-)processors into traditional supercomputers. Due to the efficiency of heterogeneous systems in terms of Watts and FLOPS per surface unit, opening the access of heterogeneous platforms to a wider range of users is an important problem to be tackled. However, heterogeneous platforms limit the portability of the applications and increase development complexity due to the programming skills required. Program transformation can help make programming heterogeneous systems easier by defining a step-wise transformation process that translates a given initial code into a semantically equivalent final code, but adapted to a specific platform. Program transformation systems require the definition of efficient transformation strategies to tackle the combinatorial problem that emerges due to the large set of transformations applicable at each step of the process. In this paper we propose a machine learning-based approach to learn heuristics to define program transformation strategies. Our approach proposes a novel combination of reinforcement learning and classification methods to efficiently tackle the problems inherent to this type of systems. Preliminary results demonstrate the suitability of this approach.", "target": "Towards Automatic Learning of Heuristics for Mechanical Transformations of Procedural Code\u2217"}
{"id": "task1540-95b61fa951174f29bc73f149baa012d3", "input": "An exhaustive study on neural network language modeling (NNLM) is performed in this paper. Different architectures of basic neural network language models are described and examined. A number of different improvements over basic neural network language models, including importance sampling, word classes, caching and bidirectional recurrent neural network (BiRNN), are studied separately, and the advantages and disadvantages of every technique are evaluated. Then, the limits of neural network language modeling are explored from the aspects of model architecture and knowledge representation. Part of the statistical information from a word sequence will loss when it is processed word by word in a certain order, and the mechanism of training neural network by updating weight matrixes and vectors imposes severe restrictions on any significant enhancement of NNLM. For knowledge representation, the knowledge represented by neural network language models is the approximate probabilistic distribution of word sequences from a certain training data set rather than the knowledge of a language itself or the information conveyed by word sequences in a natural language. Finally, some directions for improving neural network language modeling further is discussed.", "target": "A Study on Neural Network Language Modeling"}
{"id": "task1540-7400ef453f0449228bf964f281ad8e86", "input": "Graph-based representations of images have recently acquired an important role for classification purposes within the context of machine learning approaches. The underlying idea is to consider that relevant information of an image is implicitly encoded into the relationships between more basic entities that compose by themselves the whole image. The classification problem is then reformulated in terms of an optimization problem usually solved by a gradient-based search procedure. Vario-eta through structure is an approximate second order stochastic optimization technique that achieves a good trade-off between speed of convergence and the computational effort required. However, the robustness of this technique for large scale problems has not been yet assessed. In this paper we firstly provide a theoretical justification of the assumptions made by this optimization procedure. Secondly, a complexity analysis of the algorithm is performed to prove its suitability for large scale learning problems.", "target": "Complexity Analysis of Vario-eta through Structure"}
{"id": "task1540-367416033dab41eb88dded29b79496c6", "input": "This paper proposes a movie genreprediction based on multinomial probability model. To the best of our knowledge, this problem has not been addressed yet in the field of recommender system. The prediction of a movies genre has many practical applications including complementing the items categories given by experts and providing a surprise effect in the recommendations given to a user. We employ mulitnomial event model to estimate a likelihood of a movie given genre and the Bayes rule to evaluate the posterior probability of a genre given a movie. Experiments with the MovieLens dataset validate our approach. We achieved 70% prediction rate using only 15% of the whole set for training. Keywords\u2014Recommender system, category prediction, multinomial model, Naive Bayes classifier.", "target": "A multinomial probabilistic model for movie genre predictions"}
{"id": "task1540-08f4c67a3c8844e486469815e5949dc1", "input": "In this paper we describe an extension of the Variable Neighbourhood Search (VNS) which integrates the basic VNS with other complementary approaches from machine learning, statistics and experimental algorithmic, in order to produce high-quality performance and to completely automate the resulting optimization strategy. The resulting intelligent VNS has been successfully applied to a couple of optimization problems where the solution space consists of the subsets of a finite reference set. These problems are the labelled spanning tree and forest problems that are formulated on an undirected labelled graph; a graph where each edge has a label in a finite set of labels L. The problems consist on selecting the subset of labels such that the subgraph generated by these labels has an optimal spanning tree or forest, respectively. These problems have several applications in the real-world, where one aims to ensure connectivity by means of homogeneous connections.", "target": "An intelligent extension of Variable Neighbourhood Search for labelling graph problems"}
{"id": "task1540-7841119445a548aab917213efb5ada0f", "input": "This paper considers the problem of learning, from samples, the depen-<lb>dency structure of a system of linear stochastic differential equations,<lb>when some of the variables are latent. In particular, we observe the time<lb>evolution of some variables, and never observe other variables; from this,<lb>we would like to find the dependency structure between the observed vari-<lb>ables \u2013 separating out the spurious interactions caused by the (marginal-<lb>izing out of the) latent variables\u2019 time series. We develop a new method,<lb>based on convex optimization, to do so in the case when the number of<lb>latent variables is smaller than the number of observed ones. For the case<lb>when the dependency structure between the observed variables is sparse,<lb>we theoretically establish a high-dimensional scaling result for structure re-<lb>covery. We verify our theoretical result with both synthetic and real data<lb>(from the stock market).", "target": "Learning with Latent Factors in Time Series"}
{"id": "task1540-454bddb7124e4e2a85e060f9bb2e2113", "input": "Motivated by value function estimation in reinforcement learning, we study statistical linear inverse problems, i.e., problems where the coefficients of a linear system to be solved are observed in noise. We consider penalized estimators, where performance is evaluated using a matrix-weighted two-norm of the defect of the estimator measured with respect to the true, unknown coefficients. Two objective functions are considered depending whether the error of the defect measured with respect to the noisy coefficients is squared or unsquared. We propose simple, yet novel and theoretically well-founded data-dependent choices for the regularization parameters for both cases that avoid datasplitting. A distinguishing feature of our analysis is that we derive deterministic error bounds in terms of the error of the coefficients, thus allowing the complete separation of the analysis of the stochastic properties of these errors. We show that our results lead to new insights and bounds for linear value function estimation in reinforcement learning.", "target": "Statistical linear estimation with penalized estimators: an application to reinforcement learning"}
{"id": "task1540-6cd5d0d543f2474ca8299c5ad71de115", "input": "Click prediction is one of the fundamental problems in sponsored search. Most of existing studies took advantage of machine learning approaches to predict ad click for each event of ad view independently. However, as observed in the real-world sponsored search system, user\u2019s behaviors on ads yield high dependency on how the user behaved along with the past time, especially in terms of what queries she submitted, what ads she clicked or ignored, and how long she spent on the landing pages of clicked ads, etc. Inspired by these observations, we introduce a novel framework based on Recurrent Neural Networks (RNN). Compared to traditional methods, this framework directly models the dependency on user\u2019s sequential behaviors into the click prediction process through the recurrent structure in RNN. Large scale evaluations on the click-through logs from a commercial search engine demonstrate that our approach can significantly improve the click prediction accuracy, compared to sequence-independent approaches.", "target": "Sequential Click Prediction for Sponsored Search with Recurrent Neural Networks"}
{"id": "task1540-5a7a0815f9c44f319e0a5900aa76316b", "input": "We present Exponentiated Gradient LINUCB, an algorithm for contextual multi-armed bandits. This algorithm uses Exponentiated Gradient to find the optimal exploration of the LINUCB. Within a deliberately designed offline simulation framework we conduct evaluations with real online event log data. The experimental results demonstrate that our algorithm outperforms surveyed algorithms.", "target": "Exponentiated Gradient LINUCB for Contextual Multi- Armed Bandits"}
{"id": "task1540-7e2ac7aa7d60480f9f79a163e8e24d90", "input": "Event factuality identification plays an important role in deep NLP applications. In this paper, we propose a deep learning framework for this task which first extracts essential information from raw texts as the inputs and then identifies the factuality of events via a deep neural network with a proper combination of Bidirectional Long Short-Term Memory (BiLSTM) neural network and Convolutional Neural Network (CNN). The experimental results on FactBank show that our framework significantly outperforms several state-of-the-art baselines.", "target": "Event Factuality Identification via Deep Neural Networks"}
{"id": "task1540-31610604c33c4a259d823f5ae6f8cdbd", "input": "The speed of convergence of the Expectation Maximization (EM) algorithm for Gaussian mixture model fitting is known to be dependent on the amount of overlap among the mixture components. In this paper, we study the impact of mixing coefficients on the convergence of EM. We show that when the mixture components exhibit some overlap, the convergence of EM becomes slower as the dynamic range among the mixing coefficients increases. We propose a deterministic anti-annealing algorithm, that significantly improves the speed of convergence of EM for such mixtures with unbalanced mixing coefficients. The proposed algorithm is compared against other standard optimization techniques like BFGS, Conjugate Gradient, and the traditional EM algorithm. Finally, we propose a similar deterministic antiannealing based algorithm for the Dirichlet process mixture model and demonstrate its advantages over the conventional variational Bayesian approach.", "target": "Convergence of the EM Algorithm for Gaussian Mixtures with Unbalanced Mixing Coefficients"}
{"id": "task1540-f3243e0a5d6d4f1c9b1272e8d003e78d", "input": "Expert systems prove to be suitable replacement for human experts when human experts are unavailable for different reasons. Various expert system has been developed for wide range of application. Although some expert systems in the field of fishery and aquaculture has been developed but a system that aids user in process of selecting a new addition to their aquarium tank never been designed. This paper proposed an expert system that suggests new addition to an aquarium tank based on current environmental condition of aquarium and currently existing fishes in aquarium. The system suggest the best fit for aquarium condition and most compatible to other", "target": "An expert system for recommending suitable ornamental fish addition to an aquarium based on aquarium condition"}
{"id": "task1540-7310c4a297a846dfa4cf65fab91f560f", "input": "In 2014, Amin, Heidari, and Kearns proved that tree networks can be learned by observing only the infected set of vertices of the contagion process under the independent cascade model, in both the active and passive query models. They also showed empirically that simple extensions of their algorithms work on sparse networks. In this work, we focus on the active model. We prove that a simple modification of Amin et al.\u2019s algorithm works on more general classes of networks, namely (i) networks with large girth and low path growth rate, and (ii) networks with bounded degree. This also provides partial theoretical explanation for Amin et al.\u2019s experiments on sparse networks.", "target": "Learning Network Structures from Contagion"}
{"id": "task1540-10bc44438a52415aa488dbe575806097", "input": "Reordering poses a major challenge in machine translation (MT) between two languages with significant differences in word order. In this paper, we present a novel reordering approach utilizing sparse features based on dependency word pairs. Each instance of these features captures whether two words, which are related by a dependency link in the source sentence dependency parse tree, follow the same order or are swapped in the translation output. Experiments on Chinese-to-English translation show a statistically significant improvement of 1.21 BLEU point using our approach, compared to a state-of-the-art statistical MT system that incorporates prior reordering approaches.", "target": "To Swap or Not to Swap? Exploiting Dependency Word Pairs for Reordering in Statistical Machine Translation"}
{"id": "task1540-c9bd862da02b44bc9c7ddc9dfd354bb7", "input": "There are around a hundred installed apps on an average smartphone. The high number of apps and the limited number of app icons that can be displayed on the device\u2019s screen requires a new paradigm to address their visibility to the user. In this paper we propose a new online algorithm for dynamically predicting a set of apps that the user is likely to use. The algorithm runs on the user\u2019s device and constantly learns the user\u2019s habits at a given time, location, and device state. It is designed to actively help the user to navigate to the desired app as well as to provide a personalized feeling, and hence is aimed at maximizing the AUC. We show both theoretically and empirically that the algorithm maximizes the AUC, and yields good results on a set of 1,000 devices.", "target": "Context-Based Prediction of App Usage"}
{"id": "task1540-a5f16201612e46d7a0d4f34b229b77d6", "input": "Within the framework of ADABOOST.MH, we propose to train vector-valued decision trees to optimize the multi-class edge without reducing the multi-class problem toK binary one-againstall classifications. The key element of the method is a vector-valued decision stump, factorized into an input-independent vector of length K and label-independent scalar classifier. At inner tree nodes, the label-dependent vector is discarded and the binary classifier can be used for partitioning the input space into two regions. The algorithm retains the conceptual elegance, power, and computational efficiency of binary ADABOOST. In experiments it is on par with support vector machines and with the best existing multi-class boosting algorithm AOSOLOGITBOOST, and it is significantly better than other known implementations of ADABOOST.MH.", "target": "The return of ADABOOST.MH: multi-class Hamming trees"}
{"id": "task1540-5420a01df2fa403eac95635bc7f0b205", "input": "We study expressive power of shallow and deep neural networks with piece-wise linear activation functions. We establish new rigorous upper and lower bounds for the network complexity in the setting of approximations in Sobolev spaces. In particular, we prove that deep ReLU networks more efficiently approximate smooth functions than shallow networks. In the case of approximations of 1D Lipschitz functions we describe adaptive depth-6 network architectures more efficient than the standard shallow architecture.", "target": "Error bounds for approximations with deep ReLU networks"}
{"id": "task1540-76ceb5fe2c8d4749a9b5c876ae437d99", "input": "The aim of the paper is to provide an exact approach for generating a Poisson process sampled from a hierarchical CRM, without having to instantiate the infinitely many atoms of the random measures. We use completely random measures (CRM) and hierarchical CRM to define a prior for Poisson processes. We derive the marginal distribution of the resultant point process, when the underlying CRM is marginalized out. Using well known properties unique to Poisson processes, we were able to derive an exact approach for instantiating a Poisson process with a hierarchical CRM prior. Furthermore, we derive Gibbs sampling strategies for hierarchical CRM models based on Chinese restaurant franchise sampling scheme. As an example, we present the sum of generalized gamma process (SGGP), and show its application in topicmodelling. We show that one can determine the power-law behaviour of the topics and words in a Bayesian fashion, by defining a prior on the parameters of SGGP.", "target": "On collapsed representation of hierarchical Completely Random Measures"}
{"id": "task1540-0389c7b4c13a4fef939916fff9cfd6c6", "input": "We consider the problem of rank loss minimization in the setting of multilabel classification, which is usually tackled by means of convex surrogate losses defined on pairs of labels. Very recently, this approach was put into question by a negative result showing that commonly used pairwise surrogate losses, such as exponential and logistic losses, are inconsistent. In this paper, we show a positive result which is arguably surprising in light of the previous one: the simpler univariate variants of exponential and logistic surrogates (i.e., defined on single labels) are consistent for rank loss minimization. Instead of directly proving convergence, we give a much stronger result by deriving regret bounds and convergence rates. The proposed losses suggest efficient and scalable algorithms, which are tested experimentally.", "target": "Consistent Multilabel Ranking through Univariate Loss Minimization"}
{"id": "task1540-e7152074f76948fb8ec3050b1d30bac0", "input": "This paper presents a Bayesian optimization method with exponential convergence without the need of auxiliary optimization and without the \u03b4-cover sampling. Most Bayesian optimization methods require auxiliary optimization: an additional non-convex global optimization problem, which can be time-consuming and hard to implement in practice. Also, the existing Bayesian optimization method with exponential convergence [1] requires access to the \u03b4-cover sampling, which was considered to be impractical [1, 2]. Our approach eliminates both requirements and achieves an exponential convergence rate.", "target": "Bayesian Optimization with Exponential Convergence"}
{"id": "task1540-bf9c2e1b3c1045e4ab40b9d0b23670d7", "input": "This paper reveals the results of an analysis of the accuracy of developed software for automatic lemmatization for the Bulgarian language. This lemmatization software is written entirely in Java and is distributed as a GATE plugin. Certain statistical methods are used to define the accuracy of this software. The results of the analysis show 95% lemmatization accuracy.", "target": "Evaluation of the Accuracy of the BGLemmatizer"}
{"id": "task1540-bbdd07964d924be8af8c7522920b531e", "input": "Deep learning tools have recently gained much attention in applied machine learning. However such tools for regression and classification do not allow us to capture model uncertainty. Bayesian models offer us the ability to reason about model uncertainty, but usually come with a prohibitive computational cost. We show that dropout in multilayer perceptron models (MLPs) can be interpreted as a Bayesian approximation. Results are obtained for modelling uncertainty for dropout MLP models \u2013 extracting information that has been thrown away so far, from existing models. This mitigates the problem of representing uncertainty in deep learning without sacrificing computational performance or test accuracy. We perform an exploratory study of the dropout uncertainty properties. Various network architectures and non-linearities are assessed on tasks of extrapolation, interpolation, and classification. We show that model uncertainty is important for classification tasks using MNIST as an example, and use the model\u2019s uncertainty in a Bayesian pipeline, with deep reinforcement learning as a concrete example.", "target": "Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning"}
{"id": "task1540-7c82c22884aa4101805bc993027873d7", "input": "The expected supremum of a Gaussian process indexed by the image of an index set under a function class is bounded in terms of separate properties of the index set and the function class. The bound is relevant to the estimation of nonlinear transformations or the analysis of learning algorithms whenever hypotheses are chosen from composite classes, as is the case for multi-layer models.", "target": "A chain rule for the expected suprema of Gaussian processes"}
{"id": "task1540-d90deef2bf014cc5ab3e76390a709f75", "input": "This paper demonstrates a data-driven control approach for demand response in real-life residential buildings. The objective is to optimally schedule the heating cycles of the Domestic Hot Water (DHW) buffer to maximize the selfconsumption of the local photovoltaic (PV) production. A modelbased reinforcement learning technique is used to tackle the underlying sequential decision-making problem. The proposed algorithm learns the stochastic occupant behavior, predicts the PV production and takes into account the dynamics of the system. A real-life experiment with six residential buildings is performed using this algorithm. The results show that the self-consumption of the PV production is significantly increased, compared to the default thermostat control. Reinforcement Learning, Demand Response, Domestic Hot Water, Field Experiment March 1, 2017", "target": "Using Reinforcement Learning for Demand Response of Domestic Hot Water Buffers: a Real-Life Demonstration"}
{"id": "task1540-74d3a5a6e55e47aeaa8eb7d9f0057a4d", "input": "Medical errors are leading causes of death in the US and as such, prevention of these errors is paramount to promoting healthcare. Patient Safety Event reports are narratives describing potential adverse events to the patients and are important in identifying, and preventing medical errors. We present a neural network architecture for identifying the type of safety events which is the first step in understanding these narratives. Our proposed model is based on a soft neural attention model to improve the effectiveness of encoding long sequences. Empirical results on two large-scale real-world datasets of patient safety reports demonstrate the effectiveness of our method with significant improvements over existing methods.", "target": "A Neural Attention Model for Categorizing Patient Safety Events"}
{"id": "task1540-1e79f937bffb49c5a6c6a7b0598eb256", "input": "We extend previous work on efficiently training linear models by applying stochastic updates to non-zero features only, lazily bringing weights current as needed. To date, only the closed form updates for the l1, l\u221e, and the rarely used l2 norm have been described. We extend this work by showing the proper closed form updates for the popular l22 and elastic net regularized models. We show a dynamic programming algorithm to calculate the proper elastic net update with only one constant-time subproblem computation per update. Our algorithm handles both fixed and decreasing learning rates and we derive the result for both stochastic gradient descent (SGD) and forward backward splitting (FoBoS). We empirically validate the algorithm, showing that on a bag-of-words dataset with 260, 941 features and 88 nonzero features on average per example, our method trains a logistic regression classifier with elastic net regularization 612 times faster than an otherwise identical implementation with dense updates.", "target": "Efficient Elastic Net Regularization for Sparse Linear Models"}
{"id": "task1540-251d3c07e0e341279812330806b179a8", "input": "Recent non-linear feature selection approaches employing greedy optimisation of Centred Kernel Target Alignment(KTA) exhibit strong results in terms of generalisation accuracy and sparsity. However, they are computationally prohibitive for large datasets. We propose randSel, a randomised feature selection algorithm, with attractive scaling properties. Our theoretical analysis of randSel provides probabilistic guarantees for correct identification of relevant features under reasonable assumptions. RandSel\u2019s characteristics make it an ideal candidate for identifying informative learned representations. We\u2019ve conducted experimentation to establish the performance of this approach, and present encouraging results, including a 3rd position result in the recent ICML black box learning challenge as well as competitive results for signal peptide prediction, an important problem in bioinformatics.", "target": "Learning Non-Linear Feature Maps With An Application To Representation Learning"}
{"id": "task1540-18e4ee6275d747f3b9612f857ee17f46", "input": "We study the problem of structured prediction under test-time budget constraints. We propose a novel approach applicable to a wide range of structured prediction problems in computer vision and natural language processing. Our approach seeks to adaptively generate computationally costly features during test-time in order to reduce the computational cost of prediction while maintaining prediction performance. We show that training the adaptive feature generation system can be reduced to a series of structured learning problems, resulting in efficient training using existing structured learning algorithms. This framework provides theoretical justification for several existing heuristic approaches found in literature. We evaluate our proposed adaptive system on two structured prediction tasks, optical character recognition (OCR) and dependency parsing and show strong performance in reduction of the feature costs without degrading accuracy.", "target": "Resource Constrained Structured Prediction"}
{"id": "task1540-e7774178381148b7ad0a131a79a85ea9", "input": "This paper presents an approach to identify efficient techniques used in Web Search Engine Optimization (SEO). Understanding SEO factors which can influence page\u2019s ranking in search engine is significant for webmasters who wish to attract large number of users to their website. Different from previous relevant research, in this study we developed an intelligent Meta search engine which aggregates results from various search engines and ranks them based on several important SEO parameters. The research tries to establish that using more SEO parameters in ranking algorithms helps in retrieving better search results thus increasing user satisfaction. Initial results generated from Meta search engine outperformed existing search engines in terms of better retrieved search results with high precision.", "target": "An Innovative Approach for online Meta Search Engine Optimization"}
{"id": "task1540-c21aaefd190e42b698c7181029be4408", "input": "We propose online unsupervised domain adaptation (DA), which is performed incrementally as data comes in and is applicable when batch DA is not possible. In a part-of-speech (POS) tagging evaluation, we find that online unsupervised DA performs as well as batch DA.", "target": "Online Updating of Word Representations for Part-of-Speech Tagging"}
{"id": "task1540-db37a8571ca8447e96ef79c966482d19", "input": "Argumentation is a promising model for reasoning with uncertain and inconsistent knowledge. The key concept of acceptability enables to differentiate arguments and defeaters: The certainty of a proposition can then be evaluated through the most acceptable arguments for that proposition. In this paper, we investigate different complementary points of view: an acceptability based on the existence of direct defeaters and an acceptability based on the existence of defenders. Pursuing previous work on preference-based argumentation principles, we enforce both points of view by taking into account preference orderings for comparing arguments. Our approach is illustrated in the context of reasoning with stratified knowledge bases.", "target": "On the Acceptability of Arguments in Preference-based Argumentation"}
{"id": "task1540-8001e579f4924d738e6658402c9d3c89", "input": "Multitask learning algorithms are typically designed assuming some fixed, a priori known latent structure shared by all the tasks. However, it is usually unclear what type of latent task structure is the most appropriate for a given multitask learning problem. Ideally, the \u201cright\u201d latent task structure should be learned in a data-driven manner. We present a flexible, nonparametric Bayesian model that posits a mixture of factor analyzers structure on the tasks. The nonparametric aspect makes the model expressive enough to subsume many existing models of latent task structures (e.g, meanregularized tasks, clustered tasks, low-rank or linear/non-linear subspace assumption on tasks, etc.). Moreover, it can also learn more general task structures, addressing the shortcomings of such models. We present a variational inference algorithm for our model. Experimental results on synthetic and realworld datasets, on both regression and classification problems, demonstrate the effectiveness of the proposed method.", "target": "Flexible Modeling of Latent Task Structures in Multitask Learning"}
{"id": "task1540-945fcd2ac9bf49389cec457a29365a22", "input": "In practice, a ranking of objects with respect to given set of criteria is of considerable importance. However, due to lack of knowledge, information of time pressure, decision makers might not be able to provide a (crisp) ranking of objects from the top to the bottom. Instead, some objects might be ranked equally, or better than other objects only to some degree. In such cases, a generalization of crisp rankings to fuzzy rankings can be more useful. The aim of the article is to introduce the notion of a fuzzy ranking and to discuss its several properties, namely orderings, similarity and indecisiveness. The proposed approach can be used both for group decision making or multiple criteria decision making when uncertainty is involved.", "target": "Fuzzy Rankings: Properties and Applications"}
{"id": "task1540-d161509d5dcb41a6a73f55e2c995593f", "input": "We present a method for learning treewidthbounded Bayesian networks from data sets containing thousands of variables. Bounding the treewidth of a Bayesian greatly reduces the complexity of inferences. Yet, being a global property of the graph, it considerably increases the difficulty of the learning process. We propose a novel algorithm for this task, able to scale to large domains and large treewidths. Our novel approach consistently outperforms the state of the art on data sets with up to ten thousand variables.", "target": "Learning Bounded Treewidth Bayesian Networks with Thousands of Variables"}
{"id": "task1540-128ee05bbe3d4131bd00667eac9f4ad6", "input": "We demonstrate that an attention-based encoder-decoder model can be used for sentence-level grammatical error identification for the Automated Evaluation of Scientific Writing (AESW) Shared Task 2016. The attention-based encoder-decoder models can be used for the generation of corrections, in addition to error identification, which is of interest for certain end-user applications. We show that a character-based encoder-decoder model is particularly effective, outperforming other results on the AESW Shared Task on its own, and showing gains over a word-based counterpart. Our final model\u2014a combination of three character-based encoder-decoder models, one word-based encoder-decoder model, and a sentence-level CNN\u2014is the highest performing system on the AESW 2016 binary prediction Shared Task.", "target": "Sentence-Level Grammatical Error Identification as Sequence-to-Sequence Correction"}
{"id": "task1540-98ff0aaa1bc944f18dd1bc3fbfade992", "input": "The combinatorial stochastic semi-bandit problem is an extension of the classical multi-armed bandit problem in which an algorithm pulls more than one arm at each stage and the rewards of all pulled arms are revealed. One difference with the single arm variant is that the dependency structure of the arms is crucial. Previous works on this setting either used a worst-case approach or imposed independence of the arms. We introduce a way to quantify the dependency structure of the problem and design an algorithm that adapts to it. The algorithm is based on linear regression and the analysis develops techniques from the linear bandit literature. By comparing its performance to a new lower bound, we prove that it is optimal, up to a poly-logarithmic factor in the number of pulled arms.", "target": "Combinatorial semi-bandit with known covariance"}
{"id": "task1540-e71349e02d164f97889fe3a9dd90c2fc", "input": "Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.", "target": "Scikit-learn: Machine Learning in Python"}
{"id": "task1540-702f9d9de4324d5bbe8fb2f7586e7373", "input": "The Generative Adversarial Network (GAN) has achieved great success in generating realistic (realvalued) synthetic data. However, convergence issues and difficulties dealing with discrete data hinder the applicability of GAN to text. We propose a framework for generating realistic text via adversarial training. We employ a long shortterm memory network as generator, and a convolutional network as discriminator. Instead of using the standard objective of GAN, we propose matching the high-dimensional latent feature distributions of real and synthetic sentences, via a kernelized discrepancy metric. This eases adversarial training by alleviating the mode-collapsing problem. Our experiments show superior performance in quantitative evaluation, and demonstrate that our model can generate realistic-looking sentences.", "target": "Adversarial Feature Matching for Text Generation"}
{"id": "task1540-e16577736b5349ddbc1c14fbaae27712", "input": "We introduce an online neural sequence to sequence model that learns to alternate between encoding and decoding segments of the input as it is read. By independently tracking the encoding and decoding representations our algorithm permits exact polynomial marginalization of the latent segmentation during training, and during decoding beam search is employed to find the best alignment path together with the predicted output sequence. Our model tackles the bottleneck of vanilla encoder-decoders that have to read and memorize the entire input sequence in their fixedlength hidden states before producing any output. It is different from previous attentive models in that, instead of treating the attention weights as output of a deterministic function, our model assigns attention weights to a sequential latent variable which can be marginalized out and permits online generation. Experiments on abstractive sentence summarization and morphological inflection show significant performance gains over the baseline encoder-decoders.", "target": "Online Segment to Segment Neural Transduction"}
{"id": "task1540-255d121a3dac4e74a493eadd6dc4f454", "input": "\u00a9 Scott A. Hale 2016. This is the author\u2019s version of the work. It is posted here for your personal use. Not for redistribution. The definitive version was published in CHI EA 2016, http://dx.doi.org/10.1145/2851581.2892466. Abstract The number of user reviews of tourist attractions, restaurants, mobile apps, etc. is increasing for all languages; yet, research is lacking on how reviews in multiple languages should be aggregated and displayed. Speakers of different languages may have consistently different experiences, e.g., different information available in different languages at tourist attractions or different user experiences with software due to internationalization/localization choices. This paper assesses the similarity in the ratings given by speakers of different languages to London tourist attractions on TripAdvisor. The correlations between different languages are generally high, but some language pairs are more correlated than others. The results question the common practice of computing average ratings from reviews in many languages.", "target": "User Reviews and Language: How Language Influences Ratings"}
{"id": "task1540-ae187f213f1847cdaff56a4d205000e3", "input": "We propose a novel deep layer cascade (LC) method to improve the accuracy and speed of semantic segmentation. Unlike the conventional model cascade (MC) that is composed of multiple independent models, LC treats a single deep model as a cascade of several sub-models. Earlier sub-models are trained to handle easy and confident regions, and they progressively feed-forward harder regions to the next sub-model for processing. Convolutions are only calculated on these regions to reduce computations. The proposed method possesses several advantages. First, LC classifies most of the easy regions in the shallow stage and makes deeper stage focuses on a few hard regions. Such an adaptive and \u2018difficulty-aware\u2019 learning improves segmentation performance. Second, LC accelerates both training and testing of deep network thanks to early decisions in the shallow stage. Third, in comparison to MC, LC is an endto-end trainable framework, allowing joint learning of all sub-models. We evaluate our method on PASCAL VOC and Cityscapes datasets, achieving state-of-the-art performance and fast speed.", "target": "Not All Pixels Are Equal: Difficulty-Aware Semantic Segmentation via Deep Layer Cascade"}
{"id": "task1540-55830b4c88f549d5923f6e4e1817984e", "input": "We discuss the feasibility of the following learning problem: given unmatched samples from two domains and nothing else, learn a mapping between the two, which preserves semantics. Due to the lack of paired samples and without any definition of the semantic information, the problem might seem ill-posed. Specifically, in typical cases, it seems possible to build infinitely many alternative mappings from every target mapping. This apparent ambiguity stands in sharp contrast to the recent empirical success in solving this problem. A theoretical framework for measuring the complexity of compositions of functions is developed in order to show that the target mapping is of lower complexity than all other mappings. The measured complexity is directly related to the depth of the neural networks being learned and the semantic mapping could be captured simply by learning using architectures that are not much bigger than the minimal architecture.", "target": "Unsupervised Learning of Semantic Mappings"}
{"id": "task1540-7fcd4b468e674094bf1eb97600589e6e", "input": "The recently introduced series of description logics under the common moniker \u2018DLLite\u2019 has attracted attention of the description logic and semantic web communities due to the low computational complexity of inference, on the one hand, and the ability to represent conceptual modeling formalisms, on the other. The main aim of this article is to carry out a thorough and systematic investigation of inference in extensions of the original DL-Lite logics along five axes: by (i) adding the Boolean connectives and (ii) number restrictions to concept constructs, (iii) allowing role hierarchies, (iv) allowing role disjointness, symmetry, asymmetry, reflexivity, irreflexivity and transitivity constraints, and (v) adopting or dropping the unique name assumption. We analyze the combined complexity of satisfiability for the resulting logics, as well as the data complexity of instance checking and answering positive existential queries. Our approach is based on embedding DL-Lite logics in suitable fragments of the one-variable first-order logic, which provides useful insights into their properties and, in particular, computational behavior.", "target": "The DL-Lite Family and Relations"}
{"id": "task1540-8fa4dbe46fcd4f82bf2fdcf7bcce62b6", "input": "Epistemic logic with non-standard knowledge operators, especially the \u201cknowing-value\u201d operator, has recently gathered much attention. With the \u201cknowing-value\u201d operator, we can express knowledge of individual variables, but not of the relations between them in general. In this paper, we propose a new operator Kf to express knowledge of the functional dependencies between variables. The semantics of this Kf operator uses a function domain which imposes a constraint on what counts as a functional dependency relation. By adjusting this function domain, different interesting logics arise, and in this paper we axiomatize three such logics in a single agent setting. Then we show how these three logics can be unified by allowing the function domain to vary relative to different agents and possible worlds. A multiagent axiomatization is given in this case.", "target": "Epistemic Logic with Functional Dependency Operator"}
{"id": "task1540-1b91d52d17d248739b086e8f07d39a6c", "input": "In this paper we present a new way of predicting the performance of a reinforcement learning policy given historical data that may have been generated by a different policy. The ability to evaluate a policy from historical data is important for applications where the deployment of a bad policy can be dangerous or costly. We show empirically that our algorithm produces estimates that often have orders of magnitude lower mean squared error than existing methods\u2014it makes more efficient use of the available data. Our new estimator is based on two advances: an extension of the doubly robust estimator (Jiang & Li, 2015), and a new way to mix between model based estimates and importance sampling based estimates.", "target": "Data-Efficient Off-Policy Policy Evaluation for Reinforcement Learning"}
{"id": "task1540-1560c5df33cc46269bab0834e5fcc753", "input": "Automatically evaluating the quality of dialogue responses for unstructured domains is a challenging problem. Unfortunately, existing automatic evaluation metrics are biased and correlate very poorly with human judgements of response quality (Liu et al., 2016). Yet having an accurate automatic evaluation procedure is crucial for dialogue research, as it allows rapid prototyping and testing of new models with fewer expensive human evaluations. In response to this challenge, we formulate automatic dialogue evaluation as a learning problem. We present an evaluation model (ADEM) that learns to predict human-like scores to input responses, using a new dataset of human response scores. We show that the ADEM model\u2019s predictions correlate significantly, and at a level much higher than word-overlap metrics such as BLEU, with human judgements at both the utterance and system-level. We also show that ADEM can generalize to evaluating dialogue models unseen during training, an important step for automatic dialogue evaluation.", "target": "Towards an Automatic Turing Test: Learning to Evaluate Dialogue Responses"}
{"id": "task1540-c591989ed61148d497ee97db43ff563e", "input": "We present a software tool that employs state-ofthe-art natural language processing (NLP) and machine learning techniques to help newspaper editors compose effective headlines for online publication. The system identifies the most salient keywords in a news article and ranks them based on both their overall popularity and their direct relevance to the article. The system also uses a supervised regression model to identify headlines that are likely to be widely shared on social media. The user interface is designed to simplify and speed the editor\u2019s decision process on the composition of the headline. As such, the tool provides an efficient way to combine the benefits of automated predictors of engagement and search-engine optimization (SEO) with human judgments of overall headline quality.", "target": "Helping News Editors Write Better Headlines: A Recommender to Improve the Keyword Contents & Shareability of News Headlines"}
{"id": "task1540-dbf7c1904fa34265ad4ddeced345238f", "input": "We consider the task of obtaining the maximum a posteriori estimate of discrete pairwise random fields with arbitrary unary potentials and semimetric pairwise potentials. For this problem, we propose an accurate hierarchical move making strategy where each move is computed efficiently by solving an st-MINCUT problem. Unlike previous move making approaches, e.g. the widely used \u03b1-expansion algorithm, our method obtains the guarantees of the standard linear programming (LP) relaxation for the important special case of metric labeling. Unlike the existing LP relaxation solvers, e.g. interior-point algorithms or tree-reweighted message passing, our method is significantly faster as it uses only the efficient st-MINCUT algorithm in its design. Using both synthetic and real data experiments, we show that our technique outperforms several commonly used algorithms.", "target": "MAP Estimation of Semi-Metric MRFs via Hierarchical Graph Cuts"}
{"id": "task1540-357d5fa4a54747d19276ce803af44680", "input": "Humans develop a common sense of style compatibility between items based on their attributes. We seek to automatically answer questions like \u201cDoes this shirt go well with that pair of jeans?\u201d In order to answer these kinds of questions, we attempt to model human sense of style compatibility in this paper. The basic assumption of our approach is that most of the important attributes for a product in an online store are included in its title description. Therefore it is feasible to learn style compatibility from these descriptions. We design a Siamese Convolutional Neural Network architecture and feed it with title pairs of items, which are either compatible or incompatible. Those pairs will be mapped from the original space of symbolic words into some embedded style space. Our approach takes only words as the input with few preprocessing and there is no laborious and expensive feature engineering.", "target": "Deep Style Match for Complementary Recommendation"}
{"id": "task1540-02db4c4791c64027a10c58e77942d1ac", "input": "We argue that the optimization plays a crucial role in generalization of deep learning models through implicit regularization. We do this by demonstrating that generalization ability is not controlled by network size but rather by some other implicit control. We then demonstrate how changing the empirical optimization procedure can improve generalization, even if actual optimization quality is not affected. We do so by studying the geometry of the parameter space of deep networks, and devising an optimization algorithm attuned to this geometry.", "target": "Geometry of Optimization and Implicit Regularization in Deep Learning"}
{"id": "task1540-e82b6e804ec74c509db7c0f4e7243195", "input": "A dictionary defines words in terms of other words. Definitions can tell you the meanings of words you don\u2019t know, but only if you know the meanings of the defining words. How many words do you need to know (and which ones) in order to be able to learn all the rest from definitions? We reduced dictionaries to their \u201cgrounding kernels\u201d (GKs), about 10% of the dictionary, from which all the other words could be defined. The GK words turned out to have psycholinguistic correlates: they were learned at an earlier age and more concrete than the rest of the dictionary. But one can compress still more: the GK turns out to have internal structure, with a strongly connected \u201ckernel core\u201d (KC) and a surrounding layer, from which a hierarchy of definitional distances can be derived, all the way out to the periphery of the full dictionary. These definitional distances, too, are correlated with psycholinguistic variables (age of acquisition, concreteness, imageability, oral and written frequency) and hence perhaps with the \u201cmental lexicon\u201d in each of our heads.", "target": "Hierarchies in Dictionary Definition Space"}
{"id": "task1540-056e97dad445452ebd9033e69c171f67", "input": "The Abstract Meaning Representation (AMR) is a representation for opendomain rich semantics, with potential use in fields like event extraction and machine translation. Node generation, typically done using a simple dictionary lookup, is currently an important limiting factor in AMR parsing. We propose a small set of actions that derive AMR subgraphs by transformations on spans of text, which allows for more robust learning of this stage. Our set of construction actions generalize better than the previous approach, and can be learned with a simple classifier. We improve on the previous state-of-the-art result for AMR parsing, boosting end-to-end performance by 3 F1 on both the LDC2013E117 and LDC2014T12 datasets.", "target": "Robust Subgraph Generation Improves Abstract Meaning Representation Parsing"}
{"id": "task1540-ed6b3860764442999d77675fe5c6fccd", "input": "An open challenge in constructing dialogue systems is developing methods for automatically learning dialogue strategies from large amounts of unlabelled data. Recent work has proposed NextUtterance-Classification (NUC) as a surrogate task for building dialogue systems from text data. In this paper we investigate the performance of humans on this task to validate the relevance of NUC as a method of evaluation. Our results show three main findings: (1) humans are able to correctly classify responses at a rate much better than chance, thus confirming that the task is feasible, (2) human performance levels vary across task domains (we consider 3 datasets) and expertise levels (novice vs experts), thus showing that a range of performance is possible on this type of task, (3) automated dialogue systems built using state-of-the-art machine learning methods have similar performance to the human novices, but worse than the experts, thus confirming the utility of this class of tasks for driving further research in automated dialogue systems.", "target": "On the Evaluation of Dialogue Systems with Next Utterance Classification"}
{"id": "task1540-d406a837568541b4a84f77b8e4d74e26", "input": "Kernel approximation using randomized feature maps has recently gained a lot of interest. In this work, we identify that previous approaches for polynomial kernel approximation create maps that are rank deficient, and therefore do not utilize the capacity of the projected feature space effectively. To address this challenge, we propose compact random feature maps (CRAFTMaps) to approximate polynomial kernels more concisely and accurately. We prove the error bounds of CRAFTMaps demonstrating their superior kernel reconstruction performance compared to the previous approximation schemes. We show how structured random matrices can be used to efficiently generate CRAFTMaps, and present a single-pass algorithm using CRAFTMaps to learn non-linear multi-class classifiers. We present experiments on multiple standard data-sets with performance competitive with state-of-the-art results.", "target": "Compact Random Feature Maps"}
{"id": "task1540-3b8883ec8e4047338f7d7c1aa9be8790", "input": "It is natural and efficient to use Natural Language (NL) for transferring knowledge from a human to a robot. Recently, research on using NL to support human-robot cooperation (HRC) has received increasing attention in several domains such as robotic daily assistance, robotic health caregiving, intelligent manufacturing, autonomous navigation and robot social accompany. However, a high-level review that can reveal the realization process and the latest methodologies of using NL to facilitate HRC is missing. In this review, a comprehensive summary about the methodology development of natural-language-facilitated human-robot cooperation (NLC) has been made. We first analyzed driving forces for NLC developments. Then, with a temporal realization order, we reviewed three main steps of NLC: human NL understanding, knowledge representation, and knowledge-world mapping. Last, based on our paper review and perspectives, potential research trends in NLC were discussed.", "target": "Methodologies realizing natural-language-facilitated human-robot cooperation: A review"}
{"id": "task1540-ef99eeb7948048c0b49ec30e036e2ab7", "input": "A landmark based heuristic is investigated for reducing query phase run-time of the probabilistic roadmap (PRM) motion planning method. The heuristic is generated by storing minimum spanning trees from a small number of vertices within the PRM graph and using these trees to approximate the cost of a shortest path between any two vertices of the graph. The intermediate step of preprocessing the graph increases the time and memory requirements of the classical motion planning technique in exchange for speeding up individual queries making the method advantageous in multi-query applications. This paper investigates these trade-offs on PRM graphs constructed in randomized environments as well as a practical manipulator simulation. We conclude that the method is preferable to Dijkstra\u2019s algorithm or the A\u2217 algorithm with conventional heuristics in multi-query applications.", "target": "Landmark Guided Probabilistic Roadmap Queries"}
{"id": "task1540-3d98e7f64c6240db8b08bae81083f58d", "input": "The preimage of the activities of all the nodes at a certain level of a deep network is the set of inputs that result in the same node activity. For fully connected multi layer rectifier networks we demonstrate how to compute the preimages of activities at arbitrary levels from knowledge of the parameters in a deep rectifying network by disregarding the effects of max-pooling. If the preimage set of a certain activity in the network contains elements from more than one class it means that these classes are irreversibly mixed. This implies that preimage sets which are piecewise linear manifolds are building blocks for describing the input manifolds specific classes, i.e. all preimages should ideally be from the same class. We believe that the knowledge of how to compute preimages will be valuable in understanding the efficiency displayed by deep learning networks and could potentially be used in designing more efficient training algorithms", "target": "THE PREIMAGE OF RECTIFIER NETWORK ACTIVITIES"}
{"id": "task1540-47f4b63ba40a44efb9f0a662704c228c", "input": "A new computationally efficient dependence measure, and an adaptive statistical test of independence, are proposed. The dependence measure is the difference between analytic embeddings of the joint distribution and the product of the marginals, evaluated at a finite set of locations (features). These features are chosen so as to maximize a lower bound on the test power, resulting in a test that is data-efficient, and that runs in linear time (with respect to the sample size n). The optimized features can be interpreted as evidence to reject the null hypothesis, indicating regions in the joint domain where the joint distribution and the product of the marginals differ most. Consistency of the independence test is established, for an appropriate choice of features. In real-world benchmarks, independence tests using the optimized features perform comparably to the state-of-the-art quadratictime HSIC test, and outperform competing O(n) and O(n log n) tests.", "target": "An Adaptive Test of Independence with Analytic Kernel Embeddings"}
{"id": "task1540-318177eb622642bc9b5e6f6ce460689c", "input": "Multiple different approaches of generating adversarial examples have been proposed to attack deep neural networks. These approaches involve either directly computing gradients with respect to the image pixels, or directly solving an optimization on the image pixels. In this work, we present a fundamentally new method for generating adversarial examples that is fast to execute and provides exceptional diversity of output. We efficiently train feed-forward neural networks in a self-supervised manner to generate adversarial examples against a target network or set of networks. We call such a network an Adversarial Transformation Network (ATN). ATNs are trained to generate adversarial examples that minimally modify the classifier\u2019s outputs given the original input, while constraining the new classification to match an adversarial target class. We present methods to train ATNs and analyze their effectiveness targeting a variety of MNIST classifiers as well as the latest state-of-the-art ImageNet classifier Inception ResNet v2.", "target": "Adversarial Transformation Networks: Learning to Generate Adversarial Examples "}
{"id": "task1540-bfc7ecd07c7d465e9c95ef6d5d989f48", "input": "This paper provides a theoretical explanation on the clustering aspect of nonnegative matrix factorization (NMF). We prove that even without imposing orthogonality nor sparsity constraint on the basis and/or coefficient matrix, NMF still can give clustering results, thus providing a theoretical support for many works, e.g., Xu et al. [1] and Kim et al. [2], that show the superiority of the standard NMF as a clustering method. Keywords\u2014bound-constrained optimization, clustering method, non-convex optimization, nonnegative matrix factorization", "target": "On the clustering aspect of nonnegative matrix factorization"}
{"id": "task1540-30acd15fa5dd4033888d6d25a2aa157a", "input": "In this paper we investigate the problem of localizing a mobile device based on readings from its embedded sensors utilizing machine learning methodologies. We consider a realworld environment, collect a large dataset of 3110 datapoints, and examine the performance of a substantial number of machine learning algorithms in localizing a mobile device. We have found algorithms that give a mean error as accurate as 0.76 meters, outperforming other indoor localization systems reported in the literature. We also propose a hybrid instance-based approach that results in a speed increase by a factor of ten with no loss of accuracy in a live deployment over standard instance-based methods, allowing for fast and accurate localization. Further, we determine how smaller datasets collected with less density affect accuracy of localization, important for use in real-world environments. Finally, we demonstrate that these approaches are appropriate for real-world deployment by evaluating their performance in an online, in-motion experiment.", "target": "Machine Learning for Indoor Localization Using Mobile Phone-Based Sensors"}
{"id": "task1540-ee1d059d4fde4af2acf1c12f2beaac0d", "input": "Speech analysis had been taken to a new level with the discovery of Reverse Speech (RS). RS is the discovery of hidden messages, referred as reversals, in normal speech. Works are in progress for exploiting the relevance of RS in different real world applications such as investigation, medical field etc. In this paper we represent an innovative method for preparing a reliable Software Requirement Specification (SRS) document with the help of reverse speech. As SRS act as the backbone for the successful completion of any project, a reliable method is needed to overcome the inconsistencies. Using RS such a reliable method for SRS documentation was developed. Keywords\u2014 Reverse Speech, Software Requirement Specification (SRS), Speech Enhancement, Speech Recognition.", "target": "Software Requirement Specification Using Reverse Speech Technology"}
{"id": "task1540-84b3a122e1bf41f39ae66ae261b7861f", "input": "Semantic parsing has made significant progress, but most current semantic parsers are extremely slow (CKY-based) and rather primitive in representation. We introduce three new techniques to tackle these problems. First, we design the first linear-time incremental shift-reduce-style semantic parsing algorithm which is more efficient than conventional cubic-time bottom-up semantic parsers. Second, our parser, being type-driven instead of syntax-driven, uses type-checking to decide the direction of reduction, which eliminates the need for a syntactic grammar such as CCG. Third, to fully exploit the power of type-driven semantic parsing beyond simple types (such as entities and truth values), we borrow from programming language theory the concepts of subtype polymorphism and parametric polymorphism to enrich the type system in order to better guide the parsing. Our system learns very accurate parses in GEOQUERY, JOBS and ATIS domains.", "target": "Type-Driven Incremental Semantic Parsing with Polymorphism"}
{"id": "task1540-3ca0b32d199c4910907512abb7a06762", "input": "The large scale of Q&A archives accumulated in community based question answering (CQA) servivces are important information and knowledge resource on the web. Question and answer matching task has been attached much importance to for its ability to reuse knowledge stored in these systems: it can be useful in enhancing user experience with recurrent questions. In this paper, a Word Embedding based Correlation (WEC) model is proposed by integrating advantages of both the translation model and word embedding. Given a random pair of words, WEC can score their co-occurrence probability in Q&A pairs, while it can also leverage the continuity and smoothness of continuous space word representation to deal with new pairs of words that are rare in the training parallel text. An experimental study on Yahoo! Answers dataset and Baidu Zhidao dataset shows this new method\u2019s promising", "target": "Word Embedding Based Correlation Model for Question/Answer Matching"}
{"id": "task1540-ab1200d3ff1146c2b8cb0823b7498ab0", "input": "Programming languages themselves have a limited number of reserved keywords and character based tokens that define the language specification. However, programmers have a rich use of natural language within their code through comments, text literals and naming entities. The programmer defined names that can be found in source code are a rich source of information to build a high level understanding of the project. The goal of this paper is to apply topic modeling to names used in over 13.6 million repositories and perceive the inferred topics. One of the problems in such a study is the occurrence of duplicate repositories not officially marked as forks (obscure forks). We show how to address it using the same identifiers which are extracted for topic modeling. We open with a discussion on naming in source code, we then elaborate on our approach to remove exact duplicate and fuzzy duplicate repositories using Locality Sensitive Hashing on the bag-of-words model and then discuss our work on topic modeling; and finally present the results from our data analysis together with open-access to the source code, tools and datasets.", "target": "Topic modeling of public repositories at scale using names in source code"}
{"id": "task1540-24f1bf92e88e4f6281d6d5f0587c416d", "input": "We formalize synthesis of shared control protocols with correctness guarantees for temporal logic specifications. More specifically, we introduce a modeling formalism in which both a human and an autonomy protocol can issue commands to a robot towards performing a certain task. These commands are blended into a joint input to the robot. The autonomy protocol is synthesized using an abstraction of possible human commands accounting for randomness in decisions caused by factors such as fatigue or incomprehensibility of the problem at hand. The synthesis is designed to ensure that the resulting robot behavior satisfies given safety and performance specifications, e.g., in temporal logic. Our solution is based on nonlinear programming and we address the inherent scalability issue by presenting alternative methods. We assess the feasibility and the scalability of the approach by an experimental evaluation.", "target": "Synthesis of Shared Control Protocols with Provable Safety and Performance Guarantees"}
{"id": "task1540-02d29b69f1134ff1b18a84a4eb83ae3e", "input": "We show that collaborative filtering can be viewed as a sequence prediction problem, and that given this interpretation, recurrent neural networks offer very competitive approach. In particular we study how the long short-term memory (LSTM) can be applied to collaborative filtering, and how it compares to standard nearest neighbors and matrix factorization methods on movie recommendation. We show that the LSTM is competitive in all aspects, and largely outperforms other methods in terms of item coverage and short term predictions.", "target": "Collaborative Filtering with Recurrent Neural Networks"}
{"id": "task1540-7a289acba635489b9e4819ad9744fe48", "input": "Toby Walsh in \u201cThe Singularity May Never Be Near\u201d gives six arguments to support his point of view that technological singularity may happen but that it is unlikely. In this paper, we provide analysis of each one of his arguments and arrive at similar conclusions, but with more weight given to the \u201clikely to happen\u201d probability.", "target": "The Singularity May Be Near"}
{"id": "task1540-1ea759e119224c3d9d2cd69a9a511476", "input": "We present a Bayesian scheme for the approximate diagonalisation of several square matrices which are not necessarily symmetric. A Gibbs sampler is derived to simulate samples of the common eigenvectors and the eigenvalues for these matrices. Several synthetic examples are used to illustrate the performance of the proposed Gibbs sampler and we then provide comparisons to several other joint diagonalization algorithms, which shows that the Gibbs sampler achieves the state-of-theart performance on the examples considered. As a byproduct, the output of the Gibbs sampler could be used to estimate the log marginal likelihood, however we employ the approximation based on the Bayesian information criterion (BIC) which in the synthetic examples considered correctly located the number of common eigenvectors. We then succesfully applied the sampler to the source separation problem as well as the common principal component analysis and the common spatial pattern analysis problems.", "target": "A Bayesian Approach to Approximate Joint Diagonalization of Square Matrices"}
{"id": "task1540-16e779655b254850ab921e9fb9550aa0", "input": "The classification of opinion texts in positive and negative is becoming a subject of great interest in sentiment analysis. The existence of many labeled opinions motivates the use of statistical and machine-learning methods. First-order statistics have proven to be very limited in this field. The Opinum approach is based on the order of the words without using any syntactic and semantic information. It consists of building one probabilistic model for the positive and another one for the negative opinions. Then the test opinions are compared to both models and a decision and confidence measure are calculated. In order to reduce the complexity of the training corpus we first lemmatize the texts and we replace most namedentities with wildcards. Opinum presents an accuracy above 81% for Spanish opinions in the financial products domain. In this work we discuss which are the most important factors that have an impact on the classification performance.", "target": "Statistical sentiment analysis performance in Opinum"}
{"id": "task1540-3ad5137416fe444a8d8e8988b89d3512", "input": "Reference is a crucial property of language that allows us to connect linguistic expressions to the world. Modeling it requires handling both continuous and discrete aspects of meaning. Data-driven models excel at the former, but struggle with the latter, and the reverse is true for symbolic models. This paper (a) introduces a concrete referential task to test both aspects, called cross-modal entity tracking; (b) proposes a neural network architecture that uses external memory to build an entity library inspired in the DRSs of DRT, with a mechanism to dynamically introduce new referents or add information to referents that are already in the library. Our model shows promise: it beats traditional neural network architectures on the task. However, it is still outperformed by Memory Networks, another model with external memory.", "target": "Living a discrete life in a continuous world: Reference in cross-modal entity tracking"}
{"id": "task1540-80581f087cff4701acf6150360610305", "input": "Parameterized algorithms are a way to solve hard problems more efficiently, given that a specific parameter of the input is small. In this paper, we apply this idea to the field of answer set programming (ASP). To this end, we propose two kinds of graph representations of programs to exploit their treewidth as a parameter. Treewidth roughly measures to which extent the internal structure of a program resembles a tree. Our main contribution is the design of parameterized dynamic programming algorithms, which run in linear time if the treewidth and weights of the given program are bounded. Compared to previous work, our algorithms handle the full syntax of ASP. Finally, we report on an empirical evaluation that shows good runtime behaviour for benchmark instances of low treewidth, especially for counting answer sets.", "target": "Answer Set Solving with Bounded Treewidth Revisited\u2217"}
{"id": "task1540-8cd3e436d35b474780c427ae0db3fb90", "input": "We introduce a novel algorithmic approach to content recommendation based on adaptive clustering of exploration-exploitation (\u201cbandit\u201d) strategies. We provide a sharp regret analysis of this algorithm in a standard stochastic noise setting, demonstrate its scalability properties, and prove its effectiveness on a number of artificial and real-world datasets. Our experiments show a significant increase in prediction performance over state-of-the-art methods for bandit problems.", "target": "Online Clustering of Bandits"}
{"id": "task1540-8d8ecad6e04a4af0814c293465e17ca1", "input": "In this paper, we discuss how machine learning could be used to produce a systematic and more objective political discourse analysis. Political footprints are vector space models (VSMs) applied to political discourse. Each of their vectors represents a word, and is produced by training the English lexicon on large text corpora. This paper presents a simple implementation of political footprints, some heuristics on how to use them, and their application to four cases: the U.N. Kyoto Protocol and Paris Agreement, and two U.S. presidential elections. The reader will be offered a number of reasons to believe that political footprints produce meaningful results, along with some suggestions on how to improve their implementation.", "target": "Political Footprints: Political Discourse Analysis using Pre-Trained Word Vectors"}
{"id": "task1540-a3c3c647cc7f43aca7ad32d49f2e7aaa", "input": "We introduce a method for constructing skills capable of solving tasks drawn from a distribution of parameterized reinforcement learning problems. The method draws example tasks from a distribution of interest and uses the corresponding learned policies to estimate the topology of the lower-dimensional piecewise-smooth manifold on which the skill policies lie. This manifold models how policy parameters change as task parameters vary. The method identifies the number of charts that compose the manifold and then applies non-linear regression in each chart to construct a parameterized skill by predicting policy parameters from task parameters. We evaluate our method on an underactuated simulated robotic arm tasked with learning to accurately throw darts at a parameterized target location.", "target": "Learning Parameterized Skills"}
{"id": "task1540-34c24945941d4640b37ac06b76da2b64", "input": "Deep neural networks have dramatically advanced the state of the art for many<lb>areas of machine learning. Recently they have been shown to have a remarkable<lb>ability to generate highly complex visual artifacts such as images and text rather<lb>than simply recognize them.<lb>In this work we use neural networks to effectively invert low-dimensional face<lb>embeddings while producing realistically looking consistent images. Our contri-<lb>bution is twofold, first we show that a gradient ascent style approaches can be<lb>used to reproduce consistent images, with a help of a guiding image. Second, we<lb>demonstrate that we can train a separate neural network to effectively solve the<lb>minimization problem in one pass, and generate images in real-time. We then<lb>evaluate the loss imposed by using a neural network instead of the gradient descent<lb>by comparing the final values of the minimized loss function.", "target": "Inverting face embeddings with convolutional neural networks"}
{"id": "task1540-077906c4aeda40f8aec357006bc2696f", "input": "Most Software Defined Networks (SDN) traffic engineering applications use excessive and frequent global monitoring in order to find the optimal Quality-of-Service (QoS) paths for the current state of the network. In this work, we present the motivations, architecture and initial evaluation of a SDN application called Cognitive Routing Engine (CRE) which is able to find near-optimal paths for a user-specified QoS while using a very small monitoring overhead compared to global monitoring which is required to guarantee that optimal paths are found. Smaller monitoring overheads bring the advantage of smaller response time for the SDN controllers and switches. The initial evaluation of CRE on a SDN representation of the GEANT academic network shows that it is possible to find near-optimal paths with a small optimality gap of 1.65% while using 9.5 times less monitoring.", "target": "Towards a Cognitive Routing Engine for Software Defined Networks"}
{"id": "task1540-69f724d7f8fd4e37b86f0150870fd1ee", "input": "GPU activity prediction is an important and complex problem. This is due to the high level of contention among thousands of parallel threads. This problem was mostly addressed using heuristics. We propose a representation learning approach to address this problem. We model any performance metric as a temporal function of the executed instructions with the intuition that the flow of instructions can be identified as distinct activities of the code. Our experiments show high accuracy and non-trivial predictive power of representation learning on a benchmark.", "target": "GPU Activity Prediction using Representation Learning"}
{"id": "task1540-a107194479ce4f20aa9ecb3334e2225f", "input": "In this paper, we use evidence-specific value ab\u00ad straction for speeding Bayesian networks infer\u00ad ence. This is done by grouping variable val\u00ad ues and treating the combined values as a sin\u00ad gle entity. As we show, such abstractions can ex\u00ad ploit regularities in conditional probability distri\u00ad butions and also the specific values of observed variables. To formally justify value abstraction, we defi ne the notion of safe value abstraction and devise inference algorithms that use it to re\u00ad duce the cost of inference. Our procedure is par\u00ad ticularly useful for learning complex networks with many hidden variables. In such cases, re\u00ad peated likelihood computations are required for E M or other parameter optimization techniques. Since these computations are repeated with re\u00ad spect to the same evidence set, our methods can provide signifi cant speedup to the learning pro\u00ad cedure. We demonstrate the algorithm on genetic linkage problems where the use of value abstrac\u00ad tion sometimes differentiates between a feasible and non-feasible solution.", "target": "Likelihood Computations Using Value Abstraction"}
{"id": "task1540-e635effc833a4e568fe55406bc6a2cc9", "input": "The number of word forms in agglutinative languages is theoretically infinite and this variety in word forms introduces sparsity in many natural language processing tasks. Part-of-speech tagging (PoS tagging) is one of these tasks that often suffers from sparsity. In this paper, we present an unsupervised Bayesian model using Hidden Markov Models (HMMs) for joint PoS tagging and stemming for agglutinative languages. We use stemming to reduce sparsity in PoS tagging. Two tasks are jointly performed to provide a mutual benefit in both tasks. Our results show that joint POS tagging and stemming improves PoS tagging scores. We present results for Turkish and Finnish as agglutinative languages and English as a morphologically poor language.", "target": "Joint PoS Tagging and Stemming for Agglutinative Languages"}
{"id": "task1540-39ea820350b3401795f2a9ccff5ac52f", "input": "We consider the adaptive shortest-path routing problem in wireless networks under unknown and stochastically varying link states. In this problem, we aim to optimize the quality of communication between a source and a destination through adaptive path selection. Due to the randomness and uncertainties in the network dynamics, the quality of each link varies over time according to a stochastic process with unknown distributions. After a path is selected for communication, the aggregated quality of all links on this path (e.g., total path delay) is observed. The quality of each individual link is not observable. We formulate this problem as a multi-armed bandit with dependent arms. We show that by exploiting arm dependencies, a regret polynomial with network size can be achieved while maintaining the optimal logarithmic order with time. This is in sharp contrast with the exponential regret order with network size offered by a direct application of the classic MAB policies that ignore arm dependencies. Furthermore, our results are obtained under a general model of link-quality distributions (including heavy-tailed distributions) and find applications in cognitive radio and ad hoc networks with unknown and dynamic communication environments.", "target": "Adaptive Shortest-Path Routing under Unknown and Stochastically Varying Link States"}
{"id": "task1540-089834d542624c858b6f7f85220cba33", "input": "Information hierarchies are organizational structures that often used to organize and present large and complex information as well as provide a mechanism for effective human navigation. Fortunately, many statistical and computational models exist that automatically generate hierarchies; however, the existing approaches do not consider linkages in information networks that are increasingly common in real-world scenarios. Current approaches also tend to present topics as an abstract probably distribution over words, etc rather than as tangible nodes from the original network. Furthermore, the statistical techniques present in many previous works are not yet capable of processing data at Web-scale. In this paper we present the Hierarchical Document Topic Model (HDTM), which uses a distributed vertex-programming process to calculate a nonparametric Bayesian generative model. Experiments on three medium size data sets and the entire Wikipedia dataset show that HDTM can infer accurate hierarchies even over large information networks.", "target": "Scalable Models for Computing Hierarchies in Information Networks"}
{"id": "task1540-65d425dcca0d41da917e66b9355c1f82", "input": "We apply the principle of maximum entropy to select a unique joint probability distribution from the set of all joint probability distributions speci\u00ad fied by a credal network. In detail, we start by showing that the unique joint distribution of a Bayesian tree coincides with the maximum en\u00ad tropy model of its conditional distributions. This result, however, does not hold anymore for gen\u00ad eral Bayesian networks. We thus present a new kind of maximum entropy models, which are computed sequentially. We then show that for all general Bayesian networks, the sequential max\u00ad imum entropy model coincides with the unique joint distribution. Moreover, we apply the new principle of sequential maximum entropy to in\u00ad terval Bayesian networks and more generally to credal networks. We especially show that this ap\u00ad plication is equivalent to a number of small local entropy maximizations.", "target": "Credal Networks under Maximum Entropy"}
{"id": "task1540-7ae2de528d4241988eeb7a31db70a582", "input": "Recurrent neural network(RNN) has been broadly applied to natural language processing(NLP) problems. This kind of neural network is designed for modeling sequential data and has been testified to be quite efficient in sequential tagging tasks. In this paper, we propose to use bi-directional RNN with long short-term memory(LSTM) units for Chinese word segmentation, which is a crucial preprocess task for modeling Chinese sentences and articles. Classical methods focus on designing and combining hand-craft features from context, whereas bi-directional LSTM network(BLSTM) does not need any prior knowledge or pre-designing, and it is expert in keeping the contextual information in both directions. Experiment result shows that our approach gets stateof-the-art performance in word segmentation on both traditional Chinese datasets and simplified Chinese datasets.", "target": "Bi-directional LSTM Recurrent Neural Network for Chinese Word Segmentation"}
{"id": "task1540-600f7163fed440d0a1f50cb657cc7b51", "input": "Alcohol abuse may lead to unsociable behavior such as crime, drunk driving, or privacy leaks. We introduce automatic drunk-texting prediction as the task of identifying whether a text was written when under the influence of alcohol. We experiment with tweets labeled using hashtags as distant supervision. Our classifiers use a set of N-gram and stylistic features to detect drunk tweets. Our observations present the first quantitative evidence that text contains signals that can be exploited to detect drunk-texting.", "target": "A Computational Approach to Automatic Prediction of Drunk-Texting"}
{"id": "task1540-48425edfdf5e478ea3b38ffd46b40913", "input": "Deep Neural Networks (DNNs) are analyzed via the theoretical framework of the information bottleneck (IB) principle. We first show that any DNN can be quantified by the mutual information between the layers and the input and output variables. Using this representation we can calculate the optimal information theoretic limits of the DNN and obtain finite sample generalization bounds. The advantage of getting closer to the theoretical limit is quantifiable both by the generalization bound and by the network\u2019s simplicity. We argue that both the optimal architecture, number of layers and features/connections at each layer, are related to the bifurcation points of the information bottleneck tradeoff, namely, relevant compression of the input layer with respect to the output layer. The hierarchical representations at the layered network naturally correspond to the structural phase transitions along the information curve. We believe that this new insight can lead to new optimality bounds and deep learning algorithms.", "target": "Deep Learning and the Information Bottleneck Principle"}
{"id": "task1540-398c6dfa30af4adebae8ed723f48c00c", "input": "Given the advantage and recent success of English character-level and subword-unit models in several NLP tasks, we consider the equivalent modeling problem for Chinese. Chinese script is logographic and many Chinese logograms are composed of common substructures that provide semantic, phonetic and syntactic hints. In this work, we propose to explicitly incorporate the visual appearance of a character\u2019s glyph in its representation, resulting in a novel glyph-aware embedding of Chinese characters. Being inspired by the success of convolutional neural networks in computer vision, we use them to incorporate the spatio-structural patterns of Chinese glyphs as rendered in raw pixels. In the context of two basic Chinese NLP tasks of language modeling and word segmentation, the model learns to represent each character\u2019s task-relevant semantic and syntactic information in the character-level embedding.", "target": "Glyph-aware Embedding of Chinese Characters"}
{"id": "task1540-fc8556c5ff3647f686192a04f28acffc", "input": "In this paper, we provide all information to participate to the Second International Nurse Rostering Competition (INRC-II). First, we describe the problem formulation, which, differently from INRC-I, is a multi-stage procedure. Second, we illustrate all the necessary infrastructure do be used together with the participant\u2019s solver, including the testbed, the file formats, and the validation/simulation tools. Finally, we state the rules of the competition. All update-to-date information about the competition is available at http://mobiz.vives.be/inrc2/.", "target": "Second International Nurse Rostering Competition (INRC-II) \u2014 Problem Description and Rules \u2014"}
{"id": "task1540-41bbeb6cd54b48d98dcf73d795c3befc", "input": "We propose a structured prediction architecture for images centered around deep recurrent neural networks. The proposed network, called ReSeg, is based on the recently introduced ReNet model for object classification. We modify and extend it to perform object segmentation, noting that the avoidance of pooling can greatly simplify pixel-wise tasks for images. The ReSeg layer is composed of four recurrent neural networks that sweep the image horizontally and vertically in both directions, along with a final layer that expands the prediction back to the original image size. ReSeg combines multiple ReSeg layers with several possible input layers as well as a final layer which expands the prediction back to the original image size, making it suitable for a variety of structured prediction tasks. We evaluate ReSeg on the specific task of object segmentation with three widely-used image segmentation datasets, namely Weizmann Horse, Fashionista and Oxford Flower. The results suggest that ReSeg can challenge the state of the art in object segmentation, and may have further applications in structured prediction at large.", "target": "RESEG: A RECURRENT NEURAL NETWORK FOR OBJECT SEGMENTATION"}
{"id": "task1540-23353a0a7c4d43f5a18fbb91ee25f5cc", "input": "A widely-used tool for binary classification is the Support Vector Machine (SVM), a supervised learning technique that finds the \u201cmaximum margin\u201d linear separator between the two classes. While SVMs have been well studied in the batch (offline) setting, there is considerably less work on the streaming (online) setting, which requires only a single pass over the data using sub-linear space. Existing streaming algorithms are not yet competitive with the batch implementation. In this paper, we use the formulation of the SVM as a minimum enclosing ball (MEB) problem to provide a streaming SVM algorithm based off of the blurred ball cover originally proposed by Agarwal and Sharathkumar. Our implementation consistently outperforms existing streaming SVM approaches and provides higher accuracies than libSVM on several datasets, thus making it competitive with the standard SVM batch implementation.", "target": "Accurate Streaming Support Vector Machines"}
{"id": "task1540-5257e9e6bde9458a89ee92dd957d43a5", "input": "Translated texts are distinctively different from original ones, to the extent that supervised text classification methods can distinguish between them with high accuracy. These differences were proven useful for statistical machine translation. However, it has been suggested that the accuracy of translation detection deteriorates when the classifier is evaluated outside the domain it was trained on. We show that this is indeed the case, in a variety of evaluation scenarios. We then show that unsupervised classification is highly accurate on this task. We suggest a method for determining the correct labels of the clustering outcomes, and then use the labels for voting, improving the accuracy even further. Moreover, we suggest a simple method for clustering in the challenging case of mixed-domain datasets, in spite of the dominance of domainrelated features over translation-related ones. The result is an effective, fully-unsupervised method for distinguishing between original and translated texts that can be applied to new domains with reasonable accuracy.", "target": "Unsupervised Identification of Translationese"}
{"id": "task1540-6b1918dec9f44eadbe43b52deb651219", "input": "Chinese characters can be compared to a molecular structure: a character is analogous to a molecule, radicals are like atoms, calligraphic strokes correspond to elementary particles, and when characters form compounds, they are like molecular structures. In chemistry the conjunction of all of these structural levels produces what we perceive as matter. In language, the conjunction of strokes, radicals, characters, and compounds produces meaning. But when does meaning arise? We all know that radicals are, in some sense, the basic semantic components of Chinese script, but what about strokes? Considering the fact that many characters are made by adding individual strokes to (combinations of) radicals, we can legitimately ask the question whether strokes carry meaning, or not. In this talk I will present my project of extending traditional NLP techniques to radicals and strokes, aiming to obtain a deeper understanding of the way ideographic languages model the world.", "target": "Seeking Meaning in a Space Made out of Strokes, Radicals, Characters and Compounds"}
{"id": "task1540-4dbe9ca1991f434fb44e2efc9f2db33e", "input": "Problem solving in Answer Set Programming consists of two steps, a first grounding phase, systematically replacing all variables by terms, and a second solving phase computing the stable models of the obtained ground program. An intricate part of both phases is the treatment of aggregates, which are popular language constructs that allow for expressing properties over sets. In this paper, we elaborate upon the treatment of aggregates during grounding in gringo series 4. Consequently, our approach is applicable to grounding based on semi-naive database evaluation techniques. In particular, we provide a series of algorithms detailing the treatment of recursive aggregates and illustrate this by a running example.", "target": "Grounding Recursive Aggregates: Preliminary Report"}
{"id": "task1540-faed27de8a474ba58a7de281225f79a8", "input": "Variational autoencoders are a powerful framework for unsupervised learning. However, previous work has been restricted to shallow models with one or two layers of fully factorized stochastic latent variables, limiting the flexibility of the latent representation. We propose three advances in training algorithms of variational autoencoders, for the first time allowing to train deep models of up to five stochastic layers, (1) using a structure similar to the Ladder network as the inference model, (2) warm-up period to support stochastic units staying active in early training, and (3) use of batch normalization. Using these improvements we show state-of-the-art log-likelihood results for generative modeling on several benchmark datasets.", "target": "How to Train Deep Variational Autoencoders and Probabilistic Ladder Networks"}
{"id": "task1540-4da0f039d55f4c418565628c678405d2", "input": "Artificial Intelligence (AI) is an effective science which employs strong enough approaches, methods, and techniques to solve unsolvable real-world based problems. Because of its unstoppable rise towards the future, there are also some discussions about its ethics and safety. Shaping an AI-friendly environment for people and a people-friendly environment for AI can be a possible answer for finding a shared context of values for both humans and robots. In this context, objective of this paper is to address the ethical issues of AI and explore the moral dilemmas that arise from ethical algorithms, from pre-set or acquired values. In addition, the paper will also focus on the subject of AI safety. As general, the paper will briefly analyze the concerns and potential solutions to solving the ethical issues presented and increase readers\u2019 awareness on AI safety as another related research interest.", "target": "Ethical Artificial Intelligence - An Open Question"}
{"id": "task1540-78b6b6504ab94b88ac658b699c39f550", "input": "To improve user satisfaction, mobile app developers are interested in relevant user opinions such as complaints or suggestions. An important source for such opinions is user reviews on online app markets. However, manual review analysis for useful opinions is often challenging due to the large amount and the noisy-nature of user reviews. To address this problem, we propose M.A.R.K, a keyword-based framework for semiautomated review analysis. The key task of M.A.R.K is to analyze reviews for keywords of potential interest which developers can use to search for useful opinions. We have developed several techniques for that task including: 1) keyword extracting with customized regularization algorithms; 2) keyword grouping with distributed representation; and 3) keyword ranking with ratings and frequencies analysis. Our empirical evaluation and case studies show that M.A.R.K can identify keywords of high interest and provide developers with useful user opinions. Keywords\u2014App Review, Opinion Mining, Keyword", "target": "Mining User Opinions in Mobile App Reviews: A Keyword-based Approach"}
{"id": "task1540-cec49fb48ebf4acc881015fc818a0f33", "input": "Solving sequential decision making problems, such as text parsing, robotic<lb>control, and game playing, requires a combination of planning policies and gen-<lb>eralisation of those plans. In this paper, we present Expert Iteration, a novel al-<lb>gorithm which decomposes the problem into separate planning and generalisation<lb>tasks. Planning new policies is performed by tree search, while a deep neural net-<lb>work generalises those plans. In contrast, standard Deep Reinforcement Learning<lb>algorithms rely on a neural network not only to generalise plans, but to discover<lb>them too. We show that our method substantially outperforms Policy Gradients in<lb>the board game Hex, winning 84.4% of games against it when trained for equal<lb>time.", "target": "Thinking Fast and Slow with Deep Learning and Tree Search"}
{"id": "task1540-9f3232c999994666bf4fb5d8a720556c", "input": "We present a supervised sequence to sequence transduction model with a hard attention mechanism which combines the more traditional statistical alignment methods with the power of recurrent neural networks. We evaluate the model on the task of morphological inflection generation and show that it provides state of the art results in various setups compared to the previous neural and non-neural approaches. Eventually we present an analysis of the learned representations for both hard and soft attention models, shedding light on the features such models extract in order to solve the task.", "target": "HARD MONOTONIC ATTENTION"}
{"id": "task1540-dce2bcd7beed4379af01a8d2ea92aeb3", "input": "Robotic commands in natural language usually contain various spatial descriptions that are semantically similar but syntactically different. Mapping such syntactic variants into semantic concepts that can be understood by robots is challenging due to the high flexibility of natural language expressions. To tackle this problem, we collect robotic commands for navigation and manipulation tasks using crowdsourcing. We further define a robot language and use a generative machine translation model to translate robotic commands from natural language to robot language. The main purpose of this paper is to simulate the interaction process between human and robots using crowdsourcing platforms, and investigate the possibility of translating natural language to robot language with paraphrases.", "target": "Learning Lexical Entries for Robotic Commands using Crowdsourcing"}
{"id": "task1540-e3e4a3fa540f416bb31fdaec187fbc69", "input": "We develop a probabilistic latent-variable model to discover semantic frames\u2014types of events and their participants\u2014from corpora. We present a Dirichlet-multinomial model in which frames are latent categories that explain the linking of verb-subject-object triples, given document-level sparsity. We analyze what the model learns, and compare it to FrameNet, noting it learns some novel and interesting frames. This document also contains a discussion of inference issues, including concentration parameter learning; and a small-scale error analysis of syntactic parsing accuracy. Note: this work was originally posted online October 2012 as part of CMU MLD\u2019s Data Analysis Project requirement. This version has no new experiments or results, but has added some discussion of new related work.", "target": "Learning Frames from Text with an Unsupervised Latent Variable Model"}
{"id": "task1540-85208e2b6a6243faaa1d8ff4784e2a56", "input": "Framing is a political strategy in which politicians carefully word their statements in order to control public perception of issues. Previous works exploring political framing typically analyze frame usage in longer texts, such as Congressional speeches. We present a collection of weakly supervised models which harness collective classification to predict the frames used in political discourse on the microblogging platform, Twitter. Our global probabilistic models show that by combining both lexical features of tweets and network-based behavioral features of Twitter, we are able to increase the average, unsupervised F1 score by 21.52 points over a lexical baseline alone.", "target": "Leveraging Behavioral and Social Information for Weakly Supervised Collective Classification of Political Discourse on Twitter"}
{"id": "task1540-e217c3cf126e4a79903beb88dee5d9ee", "input": "The challenge stated in the title can be divided into two main problems. The first problem is to reliably mimic the way that users interact with user interfaces. The second problem is to build an instructible agent, i.e. one that can be taught to execute tasks expressed as previously unseen natural language commands. This paper proposes a solution to the second problem, a system we call Helpa. End-users can teach Helpa arbitrary new tasks whose level of complexity is similar to the tasks available from today\u2019s most popular virtual assistants. Teaching Helpa does not involve any programming. Instead, users teach Helpa by providing just one example of a command paired with a demonstration of how to execute that command. Helpa does not rely on any pre-existing domain-specific knowledge. It is therefore completely domain-independent. Our usability study showed that end-users can teach Helpa many new tasks in less than a minute each, often much less.", "target": "Towards A Virtual Assistant That Can Be Taught New Tasks In Any Domain By Its End-Users"}
{"id": "task1540-0fbb72b334cd44aabae34fcbd0f75565", "input": "Theoretical analyses of the Dendritic Cell Algorithm (DCA) have yielded several criticisms about its underlying structure and operation. As a result, several alterations and fixes have been suggested in the literature to correct for these findings. A contribution of this work is to investigate the effects of replacing the classification stage of the DCA (which is known to be flawed) with a traditional machine learning technique. This work goes on to question the merits of those unique properties of the DCA that are yet to be thoroughly analysed. If none of these properties can be found to have a benefit over traditional approaches, then \u201cfixing\u201d the DCA is arguably less efficient than simply creating a new algorithm. This work examines the dynamic filtering property of the DCA and questions the utility of this unique feature for the anomaly detection problem. It is found that this feature, while advantageous for noisy, time-ordered classification, is not as useful as a traditional static filter for processing a synthetic dataset. It is concluded that there are still unique features of the DCA left to investigate. Areas that may be of benefit to the Artificial Immune Systems community are suggested.", "target": "Quiet in Class : Classification, Noise and the Dendritic Cell Algorithm"}
{"id": "task1540-23c7e71ccf4f48de90c838e226d73d79", "input": "Feature selection refers to the problem of selecting relevant features which produce the most predictive outcome. In particular, feature selection task is involved in datasets containing huge number of features. Rough set theory has been one of the most successful methods used for feature selection. However, this method is still not able to find optimal subsets. This paper proposes a new feature selection method based on Rough set theory hybrid with Bee Colony Optimization (BCO) in an attempt to combat this. This proposed work is applied in the medical domain to find the minimal reducts and experimentally compared with the Quick Reduct, Entropy Based Reduct, and other hybrid Rough Set methods such as Genetic Algorithm (GA), Ant Colony Optimization (ACO) and Particle Swarm Optimization (PSO).", "target": "A Novel Rough Set Reduct Algorithm for Medical Domain Based on Bee Colony Optimization"}
{"id": "task1540-13bafeef944b4b84a190a6cdfc6e8e11", "input": "Determinantal point processes (DPPs) are distributions over sets of items that model diversity using kernels. Their applications in machine learning include summary extraction and recommendation systems. Yet, the cost of sampling from a DPP is prohibitive in large-scale applications, which has triggered an effort towards efficient approximate samplers. We build a novel MCMC sampler that combines ideas from combinatorial geometry, linear programming, and Monte Carlo methods to sample from DPPs with a fixed sample cardinality, also called projection DPPs. Our sampler leverages the ability of the hit-and-run MCMC kernel to efficiently move across convex bodies. Previous theoretical results yield a fast mixing time of our chain when targeting a distribution that is close to a projection DPP, but not a DPP in general. Our empirical results demonstrate that this extends to sampling projection DPPs, i.e., our sampler is more sample-efficient than previous approaches which in turn translates to faster convergence when dealing with costlyto-evaluate functions, such as summary extraction in our experiments.", "target": "Zonotope Hit-and-run for Efficient Sampling from Projection DPPs"}
{"id": "task1540-64aa3df80ddd406baa044148ee51c14d", "input": "The dueling bandits problem is an online learning framework for learning from pairwise preference feedback, and is particularly wellsuited for modeling settings that elicit subjective or implicit human feedback. In this paper, we study the problem of multi-dueling bandits with dependent arms, which extends the original dueling bandits setting by simultaneously dueling multiple arms as well as modeling dependencies between arms. These extensions capture key characteristics found in many realworld applications, and allow for the opportunity to develop significantly more efficient algorithms than were possible in the original setting. We propose the SELFSPARRING algorithm, which reduces the multi-dueling bandits problem to a conventional bandit setting that can be solved using a stochastic bandit algorithm such as Thompson Sampling, and can naturally model dependencies using a Gaussian process prior. We present a no-regret analysis for multi-dueling setting, and demonstrate the effectiveness of our algorithm empirically on a wide range of simulation settings.", "target": "Multi-dueling Bandits with Dependent Arms"}
{"id": "task1540-972c681f770a43db80faee8533a36003", "input": "We study propagation of the RegularGcc global constraint. This ensures that each row of a matrix of decision variables satisfies a Regular constraint, and each column satisfies a Gcc constraint. On the negative side, we prove that propagation is NP-hard even under some strong restrictions (e.g. just 3 values, just 4 states in the automaton, or just 5 columns to the matrix). On the positive side, we identify two cases where propagation is fixed parameter tractable. In addition, we show how to improve propagation over a simple decomposition into separate Regular and Gcc constraints by identifying some necessary but insufficient conditions for a solution. We enforce these conditions with some additional weighted row automata. Experimental results demonstrate the potential of these methods on some standard benchmark problems.", "target": "The RegularGcc Matrix Constraint"}
{"id": "task1540-80c09b5ad8584af79fee920e6139b2de", "input": "Several speaker identification systems are giving good performance with clean speech but are affected by the degradations introduced by noisy audio conditions. To deal with this problem, we investigate the use of complementary information at different levels for computing a combined match score for the unknown speaker. In this work, we observe the effect of two supervised machine learning approaches including support vectors machines (SVM) and na\u00efve bayes (NB). We define two feature vector sets based on mel frequency cepstral coefficients (MFCC) and relative spectral perceptual linear predictive coefficients (RASTA-PLP). Each feature is modeled using the Gaussian Mixture Model (GMM). Several ways of combining these information sources give significant improvements in a text-independent speaker identification task using a very large telephone degraded NTIMIT database.", "target": "A Multi Level Data Fusion Approach for Speaker Identification on Telephone Speech"}
{"id": "task1540-1181b2e2d75b4a6db06a2a1d4271a58f", "input": "As artificial agents proliferate, it is becoming increasingly important to ensure that their interactions with one another are well-behaved. In this paper, we formalize a common-sense notion of when algorithms are well-behaved: an algorithm is safe if it does no harm. Motivated by recent progress in deep learning, we focus on the specific case where agents update their actions according to gradient descent. The first result is that gradient descent converges to a Nash equilibrium in safe games. The paper provides sufficient conditions that guarantee safe interactions. The main contribution is to define strongly-typed agents and show they are guaranteed to interact safely. A series of examples show that strong-typing generalizes certain key features of convexity and is closely related to blind source separation. The analysis introduce a new perspective on classical multilinear games based on tensor decomposition.", "target": "Strongly-Typed Agents are Guaranteed to Interact Safely"}
{"id": "task1540-1ef5d6d25c47489b8cd69cee3c17ed87", "input": "This paper investigates the learning of 3rd-order tensors representing the semantics of transitive verbs. The meaning representations are part of a type-driven tensor-based semantic framework, from the newly emerging field of compositional distributional semantics. Standard techniques from the neural networks literature are used to learn the tensors, which are tested on a selectional preference-style task with a simple 2-dimensional sentence space. Promising results are obtained against a competitive corpus-based baseline. We argue that extending this work beyond transitive verbs, and to higher-dimensional sentence spaces, is an interesting and challenging problem for the machine learning community to consider.", "target": "Learning Type-Driven Tensor-Based Meaning Representations"}
{"id": "task1540-4ec136cbc64c46cbba097f4aaadc0c18", "input": "Our experience of the world is multimodal we see objects, hear sounds, feel texture, smell odors, and taste flavors. Modality refers to the way in which something happens or is experienced and a research problem is characterized as multimodal when it includes multiple such modalities. In order for Artificial Intelligence to make progress in understanding the world around us, it needs to be able to interpret such multimodal signals together. Multimodal machine learning aims to build models that can process and relate information from multiple modalities. It is a vibrant multi-disciplinary field of increasing importance and with extraordinary potential. Instead of focusing on specific multimodal applications, this paper surveys the recent advances in multimodal machine learning itself and presents them in a common taxonomy. We go beyond the typical early and late fusion categorization and identify broader challenges that are faced by multimodal machine learning, namely: representation, translation, alignment, fusion, and co-learning. This new taxonomy will enable researchers to better understand the state of the field and identify directions for future research.", "target": "Multimodal Machine Learning: A Survey and Taxonomy"}
{"id": "task1540-af3fe5d254fc45f5a7a05713ab94bcb8", "input": "Question answering (QA) has been the subject of a resurgence over the past years. The said resurgence has led to a multitude of question answering (QA) systems being developed both by companies and research facilities. While a few components of QA systems get reused across implementations, most systems do not leverage the full potential of component reuse. Hence, the development of QA systems is currently still a tedious and time-consuming process. We address the challenge of accelerating the creation of novel or tailored QA systems by presenting a concept for a self-wiring approach to composing QA systems. Our approach will allow the reuse of existing, web-based QA systems or modules while developing new QA platforms. To this end, it will rely on QA modules being described using the Web Ontology Language. Based on these descriptions, our approach will be able to automatically compose QA systems using a data-driven approach automatically.", "target": "Self-Wiring Question Answering Systems"}
{"id": "task1540-7dbd57aa4e804a5e9ffb88c88e532d02", "input": "Inference in general Ising models is difficult, due to high treewidth making treebased algorithms intractable. Moreover, when interactions are strong, Gibbs sampling may take exponential time to converge to the stationary distribution. We present an algorithm to project Ising model parameters onto a parameter set that is guaranteed to be fast mixing, under several divergences. We find that Gibbs sampling using the projected parameters is more accurate than with the original parameters when interaction strengths are strong and when limited time is available for sampling.", "target": "Projecting Ising Model Parameters for Fast Mixing"}
{"id": "task1540-1894857b6ad14bd5aa33d2540dc0bc4f", "input": "Bayesian networks offer great potential for use in automating large scale diagnostic rea\u00ad soning tasks. Gibbs sampling is the main technique used to perform diagnostic reason\u00ad ing in large richly interconnected Bayesian networks. Unfortunately Gibbs sampling can take an excessive time to generate a represen\u00ad tative sample. In this paper we describe and test a number of heuristic strategies for im\u00ad proving sampling in noisy-or Bayesian net\u00ad works. The strategies include Monte Carlo Markov chain sampling techniques other than Gibbs sampling. Emphasis is put on strate\u00ad gies that can be implemented in distributed systems.", "target": "Improved Sampling for Diagnostic Reasoning in Bayesian Networks"}
{"id": "task1540-7f783d62a74e419e92980e9c6257a00f", "input": "A BN20 network is a two level belief net in which parent interactions are modeled using the noisy-or interaction model. In this paper we discuss application of the SPI local expression language [1] to effi\u00ad cient inference in large BN20 networks. In particular, we show that there is sig\u00ad nificant structure which can be exploited to improve over the Quickscore result. We further describe how symbolic tech\u00ad niques can provide information which can significantly reduce the computation required for computing all cause poste\u00ad rior marginals. Finally, we present a novel approximation technique with pre\u00ad liminary experimental results.", "target": "Symbolic Probabilistic Inference in large BN20 networks"}
{"id": "task1540-7a65c4b752a348fd9ea6c2ed3ce8f29c", "input": "In recent years, deep architectures have been used for transfer learning with state-of-the-art performance in many datasets. The properties of their features remain, however, largely unstudied under the transfer perspective. In this work, we present an extensive analysis of the resiliency of feature vectors extracted from deep models, with special focus on the trade-off between performance and compression rate. By introducing perturbations to image descriptions extracted from a deep convolutional neural network, we change their precision and number of dimensions, measuring how it affects the final score. We show that deep features are more robust to these disturbances when compared to classical approaches, achieving a compression rate of 98.4%, while losing only 0.88% of their original score for Pascal VOC 2007.", "target": "DEEP NEURAL NETWORKS UNDER STRESS"}
{"id": "task1540-654a607fc3024494aba37a86e93ff73e", "input": "Federated Learning is a machine learning setting where the goal is to train a highquality centralized model with training data distributed over a large number of clients each with unreliable and relatively slow network connections. We consider learning algorithms for this setting where on each round, each client independently computes an update to the current model based on its local data, and communicates this update to a central server, where the client-side updates are aggregated to compute a new global model. The typical clients in this setting are mobile phones, and communication efficiency is of utmost importance. In this paper, we propose two ways to reduce the uplink communication costs. The proposed methods are evaluated on the application of training a deep neural network to perform image classification. Our best approach reduces the upload communication required to train a reasonable model by two orders of magnitude.", "target": "Federated Learning: Strategies for Improving Communication Efficiency"}
{"id": "task1540-5228a14c579b4303a52415d2881f6b91", "input": "ABSTRACT: The \"easy\" problem of cognitive science is explaining how and why we can do what we can do. The \"hard\" problem is explaining how and why we feel. Turing's methodology for cognitive science (the Turing Test) is based on doing: Design a model that can do anything a human can do, indistinguishably from a human, to a human, and you have explained cognition. Searle has shown that the successful model cannot be solely computational. Sensory-motor robotic capacities are necessary to ground some, at least, of the model's words, in what the robot can do with the things in the world that the words are about. But even grounding is not enough to guarantee that -nor to explain how and why -the model feels (if it does). That problem is much harder to solve (and perhaps insoluble).", "target": "Alan Turing and the \u201cHard\u201d and \u201cEasy\u201d Problem of Cognition: Doing and Feeling"}
{"id": "task1540-29b24f23bb1843d3b2b9777e98e949ea", "input": "Semantic roles play an important role in extracting knowledge from text. Current unsupervised approaches utilize features from grammar structures, to induce semantic roles. The dependence on these grammars, however, makes it difficult to adapt to noisy and new languages. In this paper we develop a data-driven approach to identifying semantic roles, the approach is entirely unsupervised up to the point where rules need to be learned to identify the position the semantic role occurs. Specifically we develop a modified-ADIOS algorithm based on ADIOS Solan et al. (2005) to learn grammar structures, and use these grammar structures to learn the rules for identifying the semantic roles based on the context in which the grammar structures appeared. The results obtained are comparable with the current state-of-art models that are inherently dependent on human annotated data.", "target": "A Data-Driven Approach for Semantic Role Labeling from Induced Grammar Structures in Language"}
{"id": "task1540-6c7385d309e54e0480a829a342951aeb", "input": "Boolean matrix factorisation (BooMF) infers interpretable decompositions of a binary data matrix into a pair of low-rank, binary matrices: One containing meaningful patterns, the other quantifying how the observations can be expressed as a combination of these patterns. We introduce the OrMachine, a probabilistic generative model for BooMF and derive a Metropolised Gibbs sampler that facilitates very efficient parallel posterior inference. Our method outperforms all currently existing approaches for Boolean Matrix factorization and completion, as we show on simulated and real world data. This is the first method to provide full posterior inference for BooMF which is relevant in applications, e.g. for controlling false positive rates in collaborative filtering, and crucially it improves the interpretability of the inferred patterns. The proposed algorithm scales to large datasets as we demonstrate by analysing single cell gene expression data in 1.3 million mouse brain cells across 11,000 genes on commodity hardware.", "target": "Bayesian Boolean Matrix Factorisation"}
{"id": "task1540-2c026db23fa5496583819916911efc21", "input": "In this memory we made the design of an indexing model for Arabic language and adapting standards for describing learning resources used (the LOM and their application profiles) with learning conditions such as levels education of students, their levels of understanding... the pedagogical context with taking into account the representative elements of the text, text's length,... in particular, we highlight the specificity of the Arabic language which is a complex language, characterized by its flexion, its voyellation and its agglutination. Keyword: indexing model, pedagogical indexation, complexity of the Arabic language, standard description of educational resources, pedagogical context, intrinsic and extrinsic properties, indexing text, prism, facet.", "target": "Developing a model for a text database indexed pedagogically for teaching the Arabic language"}
{"id": "task1540-be7c365fd1914aa284b86ca10b0a1195", "input": "Symmetry breaking has been proven to be an efficient preprocessing technique for satisfiability solving (SAT). In this paper, we port the state-of-the-art SAT symmetry breaker BreakID to answer set programming (ASP). The result is a lightweight tool that can be plugged in between the grounding and the solving phases that are common when modelling in ASP. We compare our tool with sbass, the current stateof-the-art symmetry breaker for ASP.", "target": "BreakID: Static Symmetry Breaking for ASP (System Description)"}
{"id": "task1540-bfb4232f06b64b7a800da56222d2b71e", "input": "We have previously reported a Bayesian algorithm for determining the coordinates of points in three\u00ad dimensional space from uncertain constraints. This method is useful in the determination of biological molecular structure. It is limited, however, by the requirement that the uncertainty in the constraints be normally distributed. In this paper, we present an extension of the original algorithm that allows constraint uncertainty to be represented as a mixture of Gaussians, and thereby allows arbitrary constraint distributions. We illustrate the performance of this algorithm on a problem drawn from the domain of molecular structure determination, in which a multicomponent constraint representation produces a much more accurate solution than the old single component mechanism. The new mechanism uses mixture distributions to decompose the problem into a set of independent problems with unimodal constraint uncertainty. The results of the unimodal subproblems are periodically recombined using Bayes' law, to avoid combinatorial explosion. The new algorithm is particularly suited for parallel", "target": "Probabilistic Constraint Satisfaction with Non-Gaussian Noise"}
{"id": "task1540-46138a5e9a7641ae8fc0ec58da0032c3", "input": "Many tasks in AI require the collaboration of multiple agents. Typically, the communication protocol between agents is manually specified and not altered during training. In this paper we explore a simple neural model, called CommNN, that uses continuous communication for fully cooperative tasks. The model consists of multiple agents and the communication between them is learned alongside their policy. We apply this model to a diverse set of tasks, demonstrating the ability of the agents to learn to communicate amongst themselves, yielding improved performance over non-communicative agents and baselines. In some cases, it is possible to interpret the language devised by the agents, revealing simple but effective strategies for solving the task at hand.", "target": "Learning Multiagent Communication with Backpropagation"}
{"id": "task1540-7e026b45ac57400daf9ded7951eb5100", "input": "In any knowledge discovery process the value of extracted knowledge is directly related to the quality of the data used. Big Data problems, generated by massive growth in the scale of data observed in recent years, also follow the same dictate. A common problem affecting data quality is the presence of noise, particularly in classification problems, where label noise refers to the incorrect labeling of training instances, and is known to be a very disruptive feature of data. However, in this Big Data era, the massive growth in the scale of the data poses a challenge to traditional proposals created to tackle noise, as they have difficulties coping with such a large amount of data. New algorithms need to be proposed to treat the noise in Big Data problems, providing high quality and clean data, also known as Smart Data. In this paper, two Big Data preprocessing approaches to remove noisy examples are proposed: an homogeneous ensemble and an heterogeneous ensemble filter, with special emphasis in their scalability and performance traits. The obtained results show that these proposals enable the practitioner to efficiently obtain a Smart Dataset from any Big Data classification problem.", "target": "Enabling Smart Data: Noise filtering in Big Data classification"}
{"id": "task1540-ffc8a5e4d03d424d8ac513818847d465", "input": "We present a new decision rule, maximin safety, that seeks to maintain a large margin from the worst outcome, in much the same way minimax regret seeks to minimize distance from the best. We argue that maximin safety is valuable both descriptively and normatively. Descriptively, maximin safety explains the well-known decoy effect, in which the introduction of a dominated option changes preferences among the other options. Normatively, we provide an axiomatization that characterizes preferences induced by maximin safety, and show that maximin safety shares much of the same behavioral basis with minimax regret.", "target": "Maximin Safety: When Failing to Lose is Preferable to Trying to Win"}
{"id": "task1540-5ded0e21876041379c6eecb97c7950b8", "input": "There are already quite a few tools for solving the Satisfiability Modulo Theories (SMT) problems. In this paper, we present VolCE, a tool for counting the solutions of SMT constraints, or in other words, for computing the volume of the solution space. Its input is essentially a set of Boolean combinations of linear constraints, where the numeric variables are either all integers or all reals, and each variable is bounded. The tool extends SMT solving with integer solution counting and volume computation/estimation for convex polytopes. Effective heuristics are adopted, which enable the tool to deal with high-dimensional problem instances efficiently and accurately.", "target": "A Tool for Computing and Estimating the Volume of the Solution Space of SMT(LA) Constraints"}
{"id": "task1540-1426c1623cb04d38b5c0ade6089a4e38", "input": "Previous studies on Chinese semantic role labeling (SRL) have concentrated on single semantically annotated corpus. But the training data of single corpus is often limited. Meanwhile, there usually exists other semantically annotated corpora for Chinese SRL scattered across different annotation frameworks. Data sparsity remains a bottleneck. This situation calls for larger training datasets, or effective approaches which can take advantage of highly heterogeneous data. In these papers, we focus mainly on the latter, that is, to improve Chinese SRL by using heterogeneous corpora together. We propose a novel progressive learning model which augments the Progressive Neural Network with Gated Recurrent Adapters. The model can accommodate heterogeneous inputs and effectively transfer knowledge between them. We also release a new corpus, Chinese SemBank, for Chinese SRL. Experiments on CPB 1.0 show that ours model outperforms state-of-the-art methods.", "target": "A Progressive Learning Approach to Chinese SRL Using Heterogeneous Data"}
{"id": "task1540-c967db7e3a6646918d3229ddcdd80f1d", "input": "Metric learning has been shown to be highly effective to improve the performance of nearest neighbor classification. In this paper, we address the problem of metric learning for symmetric positive definite (SPD) matrices such as covariance matrices, which arise in many real-world applications. Naively using standard Mahalanobis metric learning methods under the Euclidean geometry for SPD matrices is not appropriate, because the difference of SPD matrices can be a non-SPD matrix and thus the obtained solution can be uninterpretable. To cope with this problem, we propose to use a properly parameterized LogEuclidean distance and optimize the metric with respect to kernel-target alignment, which is a supervised criterion for kernel learning. Then the resulting non-trivial optimization problem is solved by utilizing the Riemannian geometry. Finally, we experimentally demonstrate the usefulness of our LogEuclidean metric learning algorithm on real-world classification tasks for EEG signals and texture patches.", "target": "Supervised LogEuclidean Metric Learning for Symmetric Positive Definite Matrices"}
{"id": "task1540-e9c0746fda5f4d728a01ab4d46f15f0c", "input": "Natural language correction has the potential to help language learners improve their writing skills. While approaches with separate classifiers for different error types have high precision, they do not flexibly handle errors such as redundancy or non-idiomatic phrasing. On the other hand, word and phrase-based machine translation methods are not designed to cope with orthographic errors, and have recently been outpaced by neural models. Motivated by these issues, we present a neural network-based approach to language correction. The core component of our method is an encoder-decoder recurrent neural network with an attention mechanism. By operating at the character level, the network avoids the problem of out-of-vocabulary words. We illustrate the flexibility of our approach on dataset of noisy, user-generated text collected from an English learner forum. When combined with a language model, our method achieves a state-of-the-art F0.5-score on the CoNLL 2014 Shared Task. We further illustrate that training the network on additional data with synthesized errors can improve performance.", "target": "Neural Language Correction with Character-Based Attention"}
{"id": "task1540-d37c0a84a36146c48ce8fdb879700633", "input": "The AGM theory of belief revision has be\u00ad come an important paradigm for investigat\u00ad ing rational belief changes. Unfortunately, researchers working in this paradigm have re\u00ad stricted much of their attention to rather sim\u00ad ple representations of belief states, namely logically closed sets of propositional sen\u00ad tences. In our opinion, this has resulted in a too abstract categorisation of belief change operations: expansion, revision, or contrac\u00ad tion. Occasionally, in the AGM paradigm, also probabilistic belief changes have been considered, and it is widely accepted that the probabilistic version of expansion is con\u00ad ditioning. However, we argue that it may be more correct to view conditioning and expan\u00ad sion as two essentially different kinds of belief change, and that what we call constraining is a better candidate for being considered prob\u00ad abilistic expansion.", "target": "Probabilistic Belief Change: Expansion, Conditioning and Constraining"}
{"id": "task1540-9e192a9d66d74d27a818c8e535a4abe0", "input": "Despite the successes in capturing continuous distributions, the application of generative adversarial networks (GANs) to discrete settings, like natural language tasks, is rather restricted. The fundamental reason is the difficulty of backpropagation through discrete random variables combined with the inherent instability of the GAN training objective. To address these problems, we propose Maximum-Likelihood Augmented Discrete Generative Adversarial Networks. Instead of directly optimizing the GAN objective, we derive a novel and low-variance objective using the discriminator\u2019s output that follows corresponds to the log-likelihood. Compared with the original, the new objective is proved to be consistent in theory and beneficial in practice. The experimental results on various discrete datasets demonstrate the effectiveness of the proposed approach.", "target": "Maximum-Likelihood Augmented Discrete Generative Adversarial Networks"}
{"id": "task1540-298120deaaef46ae814ba65f64a3815d", "input": "An important way to make large training sets is to gather noisy labels from crowds of non experts. We propose a method to aggregate noisy labels collected from a crowd of workers or annotators. Eliciting labels is important in tasks such as judging web search quality and rating products. Our method assumes that labels are generated by a probability distribution over items and labels. We formulate the method by drawing parallels between Gaussian Mixture Models (GMMs) and Restricted Boltzmann Machines (RBMs) and show that the problem of vote aggregation can be viewed as one of clustering. We use K-RBMs to perform clustering. We finally show some empirical evaluations over real datasets.", "target": "Vote Aggregation as a Clustering Problem"}
{"id": "task1540-05116b2dda7a49f1bea2efb4c6f20638", "input": "In this paper, a framework for testing Deep Neural Network (DNN) design in Python is presented. First, big data, machine learning (ML), and Artificial Neural Networks (ANNs) are discussed to familiarize the reader with the importance of such a system. Next, the benefits and detriments of implementing such a system in Python are presented. Lastly, the specifics of the system are explained, and some experimental results are presented to prove the effectiveness of the system.", "target": "A Framework for Distributed Deep Learning Layer Design in Python"}
{"id": "task1540-26c5a8bc59a94730a31502faa0d570ef", "input": "We study a subclass of POMDPs, called Deterministic POMDPs, that is characterized by deterministic actions and observations. These models do not provide the same generality of POMDPs yet they capture a number of interesting and challenging problems, and permit more efficient algorithms. Indeed, some of the recent work in planning is built around such assumptions mainly by the quest of amenable models more expressive than the classical deterministic models. We provide results about the fundamental properties of Deterministic POMDPs, their relation with AND/OR search problems and algorithms, and their computational complexity.", "target": "Deterministic POMDPs Revisited"}
{"id": "task1540-afb4f84c16eb47498d929be76d65c634", "input": "Studies of the overall structure of vocabulary and its dynamics became possible due to creation of diachronic text corpora, especially Google Books Ngram. This article discusses the question of core change rate and the degree to which the core words cover the texts. Different periods of the last three centuries and six main European languages presented in Google Books Ngram are compared. The main result is high stability of core change rate, which is analogous to stability of the Swadesh list.", "target": "Dynamics of core of language vocabulary"}
{"id": "task1540-6886492287ac4d56b75ed6954d841325", "input": "The work presented here involves the design of a Multi Layer Perceptron (MLP) based pattern classifier for recognition of handwritten Bangla digits using a 76 element feature vector. Bangla is the second most popular script and language in the Indian subcontinent and the fifth most popular language in the world. The feature set developed for representing handwritten Bangla numerals here includes 24 shadow features, 16 centroid features and 36 longest-run features. On experimentation with a database of 6000 samples, the technique yields an average recognition rate of 96.67% evaluated after three-fold cross validation of results. It is useful for applications related to OCR of handwritten Bangla Digit and can also be extended to include OCR of handwritten characters of Bangla alphabet.", "target": "An MLP based Approach for Recognition of Handwritten \u2018Bangla\u2019 Numerals"}
{"id": "task1540-9763d30b2535457a86f53d71b4501282", "input": "We study the problem of learning local metrics for nearest neighbor classification. Most previous works on local metric learning learn a number of local unrelated metrics. While this \u201dindependence\u201d approach delivers an increased flexibility its downside is the considerable risk of overfitting. We present a new parametric local metric learning method in which we learn a smooth metric matrix function over the data manifold. Using an approximation error bound of the metric matrix function we learn local metrics as linear combinations of basis metrics defined on anchor points over different regions of the instance space. We constrain the metric matrix function by imposing on the linear combinations manifold regularization which makes the learned metric matrix function vary smoothly along the geodesics of the data manifold. Our metric learning method has excellent performance both in terms of predictive power and scalability. We experimented with several largescale classification problems, tens of thousands of instances, and compared it with several state of the art metric learning methods, both global and local, as well as to SVM with automatic kernel selection, all of which it outperforms in a significant manner.", "target": "Parametric Local Metric Learning for Nearest Neighbor Classification"}
{"id": "task1540-931ffc42d43e46ab889aa89f9406c372", "input": "We propose a new framework for abstractive text summarization based on a sequence-to-sequence oriented encoderdecoder model equipped with a deep recurrent generative decoder (DRGN). Latent structure information implied in the target summaries is learned based on a recurrent latent random model for improving the summarization quality. Neural variational inference is employed to address the intractable posterior inference for the recurrent latent variables. Abstractive summaries are generated based on both the generative latent variables and the discriminative deterministic states. Extensive experiments on some benchmark datasets in different languages show that DRGN achieves improvements over the state-ofthe-art methods.", "target": "Deep Recurrent Generative Decoder for Abstractive Text Summarization\u2217"}
{"id": "task1540-219a6e8b615c466bbe7fe93714ef2ef5", "input": "Assessing uncertainty is an important step towards ensuring the safety and reliability of machine learning systems. Existing uncertainty estimation techniques may fail when their modeling assumptions are not met, e.g. when the data distribution differs from the one seen at training time. Here, we propose techniques that assess a classification algorithm\u2019s uncertainty via calibrated probabilities (i.e. probabilities that match empirical outcome frequencies in the long run) and which are guaranteed to be reliable (i.e. accurate and calibrated) on out-of-distribution input, including input generated by an adversary. This represents an extension of classical online learning that handles uncertainty in addition to guaranteeing accuracy under adversarial assumptions. We establish formal guarantees for our methods, and we validate them on two real-world problems: question answering and medical diagnosis from genomic data.", "target": "Estimating Uncertainty Online Against an Adversary"}
{"id": "task1540-ad6d0ae3396441218265febb85158da4", "input": "Several large cloze-style context-questionanswer datasets have been introduced recently: the CNN and Daily Mail news data and the Children\u2019s Book Test. Thanks to the size of these datasets, the associated text comprehension task is well suited for deep-learning techniques that currently seem to outperform all alternative approaches. We present a new, simple model that uses attention to directly pick the answer from the context as opposed to computing the answer using a blended representation of words in the document as is usual in similar models. This makes the model particularly suitable for question-answering problems where the answer is a single word from the document. Our model outperforms models previously proposed for these tasks by a large margin.", "target": "Text Understanding with the Attention Sum Reader Network"}
{"id": "task1540-90d598abf88843c48841672870daced6", "input": "We simulate the training of a set of state of the art neural networks, the Maxout networks (Goodfellow et al., 2013a), on three benchmark datasets: the MNIST, CIFAR10 and SVHN, with three distinct arithmetics: floating point, fixed point and dynamic fixed point. For each of those datasets and for each of those arithmetics, we assess the impact of the precision of the computations on the final error of the training. We find that very low precision computation is sufficient not just for running trained networks but also for training them. For example, almost state-of-the-art results were obtained on most datasets with around 10 bits for computing activations and gradients, and 12 bits for storing updated parameters.", "target": "LOW PRECISION ARITHMETIC FOR DEEP LEARNING"}
{"id": "task1540-76c68a88f27048c39ecb89c8f40d0a66", "input": "Deliberating on large or continuous state spaces have been long standing challenges in reinforcement learning. Temporal Abstraction have somewhat made this possible, but efficiently planing using temporal abstraction still remains an issue. Moreover using spatial abstractions to learn policies for various situations at once while using temporal abstraction models is an open problem. We propose here an efficient algorithm which is convergent under linear function approximation while planning using temporally abstract actions. We show how this algorithm can be used along with randomly generated option models over multiple time scales to plan agents which need to act real time. Using these randomly generated option models over multiple time scales are shown to reduce number of decision epochs required to solve the given task, hence effectively reducing the time needed for deliberation. ar X iv :1 70 3. 06 47 1v 1 [ cs .A I] 1 9 M ar 2 01 7", "target": "Multi-Timescale, Gradient Descent, Temporal Difference Learning with Linear Options"}
{"id": "task1540-7f2419c4f60040839b12cbc034f178cd", "input": "We present a neural network architecture to predict a point in color space from the sequence of characters in the color\u2019s name. Using large scale color\u2013name pairs obtained from an online color design forum, we evaluate our model on a \u201ccolor Turing test\u201d and find that, given a name, the colors predicted by our model are preferred by annotators to color names created by humans. Our datasets and demo system are available online at http://colorlab.us.", "target": "Character Sequence Models for Colorful Words"}
{"id": "task1540-1e86420250e047849dcc3c8b4742e268", "input": "The main goal of this paper is to describe a new pruning method for solving decision trees and game trees. The pruning method for decision trees suggests a slight variant of decision trees that we call scenario trees. In scenario trees, we do not need a conditional probability for each edge emanating from a chance node. Instead, we require a joint probability for each path from the root node to a leaf node. We compare the pruning method to the traditional rollback method for decision trees and game trees. For problems that require Bayesian revision of probabilities, a scenario tree representation with the pruning method is more efficient than a decision tree representation with the rollback method. For game trees, the pruning method is more efficient than the rollback method.", "target": "A New Pruning Method for Solving Decision Trees and Game Trees"}
{"id": "task1540-3e7e764e5c6b451ca33e40a64f46731d", "input": "Clustering is a useful data exploratory method with its wide applicability in multiple fields. However, data clustering greatly relies on initialization of cluster centers that can result in large intra-cluster variance and dead centers, therefore leading to sub-optimal solutions. This paper proposes a novel variance based version of the conventional Moving K-Means (MKM) algorithm called Variance Based Moving K-Means (VMKM) that can partition data into optimal homogeneous clusters, irrespective of cluster initialization. The algorithm utilizes a novel distance metric and a unique data element selection criteria to transfer the selected elements between clusters to achieve low intra-cluster variance and subsequently avoid dead centers. Quantitative and qualitative comparison with various clustering techniques is performed on four datasets selected from image processing, bioinformatics, remote sensing and the stock market respectively. An extensive analysis highlights the superior performance of the proposed method over other techniques.", "target": "Variance Based Moving K-Means Algorithm"}
{"id": "task1540-1d7e8f10a54d4ca6a907bf71824a1d2c", "input": "Language is a social phenomenon and inherent to its social nature is that it is constantly changing. Recently, a surge of interest can be observed within the computational linguistics (CL) community in the social dimension of language. In this article we present a survey of the emerging field of \u2018Computational Sociolinguistics\u2019 that reflects this increased interest. We aim to provide a comprehensive overview of CL research on sociolinguistic themes, featuring topics such as the relation between language and social identity, language use in social interaction and multilingual communication. Moreover, we demonstrate the potential for synergy between the research communities involved, by showing how the large-scale data-driven methods that are widely used in CL can complement existing sociolinguistic studies, and how sociolinguistics can inform and challenge the methods and assumptions employed in CL studies. We hope to convey the possible benefits of a closer collaboration between the two communities and conclude with a discussion of open challenges.", "target": "Computational Sociolinguistics: A Survey"}
{"id": "task1540-2d8b1c19aa9f4b29923c4f9fea94f98e", "input": "The study of phase transition phenomenon of NP complete problems plays an important role in understanding the nature of hard problems. In this paper, we follow this line of research by considering the problem of counting solutions of Constraint Satisfaction Problems (#CSP). We consider the random model, i.e. RB model. We prove that phase transition of #CSP does exist as the number of variables approaches infinity and the critical values where phase transitions occur are precisely located. Preliminary experimental results also show that the critical point coincides with the theoretical derivation. Moreover, we propose an approximate algorithm to estimate the expectation value of the solutions number of a given CSP instance of RB model.", "target": "Counting Solutions of Constraint Satisfiability Problems: Exact Phase Transitions and Approximate Algorithm"}
{"id": "task1540-ef3fe689762a4169819e5d39b2e5a391", "input": "We consider the problem of sparse variable selection in nonparametric additive models, with the prior knowledge of the structure among the covariates to encourage those variables within a group to be selected jointly. Previous works either study the group sparsity in the parametric setting (e.g., group lasso), or address the problem in the nonparametric setting without exploiting the structural information (e.g., sparse additive models). In this paper, we present a new method, called group sparse additive models (GroupSpAM), which can handle group sparsity in additive models. We generalize the `1/`2 norm to Hilbert spaces as the sparsityinducing penalty in GroupSpAM. Moreover, we derive a novel thresholding condition for identifying the functional sparsity at the group level, and propose an efficient block coordinate descent algorithm for constructing the estimate. We demonstrate by simulation that GroupSpAM substantially outperforms the competing methods in terms of support recovery and prediction accuracy in additive models, and also conduct a comparative experiment on a real breast cancer dataset.", "target": "Group Sparse Additive Models"}
{"id": "task1540-1c751511db0f46729e081e9fe2923d74", "input": "Sample complexity and safety are major challenges when learning policies with reinforcement learning for real-world tasks, especially when the policies are represented using rich function approximators like deep neural networks. Model-based methods where the real-world target domain is approximated using a simulated source domain provide an avenue to tackle the above challenges by augmenting real data with simulated data. However, discrepancies between the simulated source domain and the target domain pose a challenge for simulated training. We introduce the EPOpt algorithm, which uses an ensemble of simulated source domains and a form of adversarial training to learn policies that are robust and generalize to a broad range of possible target domains, including unmodeled effects. Further, the probability distribution over source domains in the ensemble can be adapted using data from target domain and approximate Bayesian methods, to progressively make it a better approximation. Thus, learning on a model ensemble, along with source domain adaptation, provides the benefit of both robustness and learning/adaptation.", "target": "EPOPT: LEARNING ROBUST NEURAL NETWORK POLICIES USING MODEL ENSEMBLES"}
{"id": "task1540-7db6441f6f47476bae213ac7f110f4a8", "input": "This paper presents a powerful genetic algorithm (GA) to solve the traveling salesman problem (TSP). To construct a powerful GA, I use edge swapping(ES) with a local search procedure to determine good combinations of building blocks of parent solutions for generating even better offspring solutions. Experimental results on well studied TSP benchmarks demonstrate that the proposed GA is competitive in finding very high quality solutions on instances with up to 16,862 cities.", "target": "A Powerful Genetic Algorithm for Traveling Salesman Problem"}
{"id": "task1540-287a4c1ea94d48f09c120992ed6a347a", "input": "Learning embeddings of entities and relations is an efficient and versatile method to perform machine learning on relational data such as knowledge graphs. In this work, we propose holographic embeddings (HOLE) to learn compositional vector space representations of entire knowledge graphs. The proposed method is related to holographic models of associative memory in that it employs circular correlation to create compositional representations. By using correlation as the compositional operator HOLE can capture rich interactions but simultaneously remains efficient to compute, easy to train, and scalable to very large datasets. In extensive experiments we show that holographic embeddings are able to outperform state-ofthe-art methods for link prediction in knowledge graphs and relational learning benchmark datasets.", "target": "Holographic Embeddings of Knowledge Graphs"}
{"id": "task1540-648ec5dc46f24e40877c18df0439b50e", "input": "CMA-ES [7, 8] is the state-of-the-art evolutionary optimization method, at least in the area of continuous black-box optimization. Basically, it consists in generating new search points by sampling from a multidimensional normal distribtion, the mean and variance of which are updated from generation to generation. In particular, the population x (g+1) 1 , . . . , x (g+1) \u03bb \u2208 R d of the g + 1-st generation, g \u2265 1, follows the normal distribution with mean m \u2208 R and variance (\u03c3)C \u2208 R resulting from the update in the g-th generation,", "target": "Two Gaussian Approaches to Black-Box Optomization"}
{"id": "task1540-438c1183deb74e049f12e0e5c9489889", "input": "Energy-based models are popular in machine learning due to the elegance of their formulation and their relationship to statistical physics. Among these, the Restricted Boltzmann Machine (RBM) has been the prototype for some recent advancements in the unsupervised training of deep neural networks. However, the contrastive divergence training algorithm, so often used for such models, has a number of drawbacks and ineligancies both in theory and in practice. Here, we investigate the performance of Minimum Probability Flow learning for training RBMs. This approach reconceptualizes the nature of the dynamics defined over a model, rather than thinking about Gibbs sampling, and derives a simple, tractable, and elegant objective function using a Taylor expansion, allowing one to learn the parameters of any distribution over visible states. In the paper, we expound the Minimum Probability Flow learning algorithm under various dynamics. We empirically analyze its performance on these dynamics and demonstrate that MPF algorithms outperform CD on various RBM configurations.", "target": "UNDERSTANDING MINIMUM PROBABILITY FLOW FOR RBMS UNDER VARIOUS KINDS OF DYNAMICS"}
{"id": "task1540-0ed3daa6e0fb4eacb7d6a935dae459c7", "input": "The ICDM Challenge 2013 is to apply machine learning to the problem of hotel ranking, aiming to maximize purchases according to given hotel characteristics, location attractiveness of hotels, users aggregated purchase history and competitive online travel agency (OTA) information for each potential hotel choice. This paper describes the solution of team \u201dbinghsu & MLRush & BrickMover\u201d. We conduct simple feature engineering work and train different models by each individual team member. Afterwards, we use listwise ensemble method to combine each model\u2019s output. Besides describing effective model and features, we will discuss about the lessons we learned while using deep learning in this competition.", "target": "Combination of Diverse Ranking Models for Personalized Expedia Hotel Searches"}
{"id": "task1540-9620d4b3fbb54ed6969781b2f4b8ac07", "input": "In this paper, we introduce a lightweight dynamic epistemic logical framework for automated planning under initial uncertainty. We reduce plan verification and conformant planning to model checking problems of our logic. We show that the model checking problem of the iteration-free fragment is PSPACE-complete. By using two non-standard (but equivalent) semantics, we give novel model checking algorithms to the full language and the iteration-free language.", "target": "A Dynamic Epistemic Framework for Conformant Planning"}
{"id": "task1540-70a79c0b299a46cb86e9b26fc70d40b5", "input": "Recently, there has been a growing interest in modeling planning with information constraints. Accordingly, an agent maximizes a regularized expected utility known as the free energy, where the regularizer is given by the information divergence from a prior to a posterior policy. While this approach can be justified in various ways, including from statistical mechanics and information theory, it is still unclear how it relates to decisionmaking against adversarial environments. This connection has previously been suggested in work relating the free energy to risk-sensitive control and to extensive form games. Here, we show that a single-agent free energy optimization is equivalent to a game between the agent and an imaginary adversary. The adversary can, by paying an exponential penalty, generate costs that diminish the decision maker\u2019s payoffs. It turns out that the optimal strategy of the adversary consists in choosing costs so as to render the decision maker indifferent among its choices, which is a definining property of a Nash equilibrium, thus tightening the connection between free energy optimization and game theory.", "target": "An Adversarial Interpretation of Information-Theoretic Bounded Rationality"}
{"id": "task1540-5790936a49484234a593a04e3a2e443c", "input": "This paper presents a hybrid dialog state tracker that combines a rule based and a machine learning based approach to belief state tracking. Therefore, we call it a hybrid tracker. The machine learning in our tracker is realized by a Long Short Term Memory (LSTM) network. To our knowledge, our hybrid tracker sets a new state-of-the-art result for the Dialog State Tracking Challenge (DSTC) 2 dataset when the system uses only live SLU as its input.", "target": "Hybrid Dialog State Tracker"}
{"id": "task1540-61cebf6c0b754d72806d3ca29e994fb0", "input": "Non-maximum suppression (NMS) is used in virtually all state-of-the-art object detection pipelines. While essential object detection ingredients such as features, classifiers, and proposal methods have been extensively researched surprisingly little work has aimed to systematically address NMS. The de-facto standard for NMS is based on greedy clustering with a fixed distance threshold, which forces to trade-off recall versus precision. We propose a convnet designed to perform NMS of a given set of detections. We report experiments on a synthetic setup, and results on crowded pedestrian detection scenes. Our approach overcomes the intrinsic limitations of greedy NMS, obtaining better recall and precision.", "target": "A CONVNET FOR NON-MAXIMUM SUPPRESSION"}
{"id": "task1540-2c4b1009fa8a482189c1f25d4008af83", "input": "Sentence similarity is considered the basis of many natural language tasks such as information retrieval, question answering and text summarization. The semantic meaning between compared text fragments is based on the words\u2019 semantic features and their relationships. This article reviews a set of word and sentence similarity measures and compares them on benchmark datasets. On the studied datasets, results showed that hybrid semantic measures perform better than both knowledge and corpus based measures. General Terms Semantic Similarity, Natural Language Processing, Computational Linguistics, Text Similarity", "target": "A Comprehensive Comparative Study of Word and Sentence Similarity Measures"}
{"id": "task1540-41b3474fe93b4b24bcc8ec26ff18673f", "input": "To appear in Theory and Practice of Logic Programming (TPLP). GNU Prolog is a general-purpose implementation of the Prolog language, which distinguishes itself from most other systems by being, above all else, a native-code compiler which produces standalone executables which don\u2019t rely on any byte-code emulator or meta-interpreter. Other aspects which stand out include the explicit organization of the Prolog system as a multipass compiler, where intermediate representations are materialized, in Unix compiler tradition. GNU Prolog also includes an extensible and highperformance finite domain constraint solver, integrated with the Prolog language but implemented using independent lower-level mechanisms. This article discusses the main issues involved in designing and implementing GNU Prolog: requirements, system organization, performance and portability issues as well as its position with respect to other Prolog system implementations and the ISO standardization initiative.", "target": "On the Implementation of GNU Prolog"}
{"id": "task1540-81803e5a331e4118ae86c804a11908a6", "input": "In this paper we propose a unified framework for structured prediction with latent variables which includes hidden conditional random fields and latent structured support vector machines as special cases. We describe a local entropy approximation for this general formulation using duality, and derive an efficient message passing algorithm that is guaranteed to converge. We demonstrate its effectiveness in the tasks of image segmentation as well as 3D indoor scene understanding from single images, showing that our approach is superior to latent structured support vector machines and hidden conditional random fields.", "target": "Efficient Structured Prediction with Latent Variables for General Graphical Models"}
{"id": "task1540-22d4d255b34347339e737a8ba817f4a8", "input": "Binary embedding of high-dimensional data requires long codes to preserve the discriminative power of the input space. Traditional binary coding methods often suffer from very high computation and storage costs in such a scenario. To address this problem, we propose Circulant Binary Embedding (CBE) which generates binary codes by projecting the data with a circulant matrix. The circulant structure enables the use of Fast Fourier Transformation to speed up the computation. Compared to methods that use unstructured matrices, the proposed method improves the time complexity from O(d2) to O(d log d), and the space complexity from O(d2) to O(d) where d is the input dimensionality. We also propose a novel time-frequency alternating optimization to learn data-dependent circulant projections, which alternatively minimizes the objective in original and Fourier domains. We show by extensive experiments that the proposed approach gives much better performance than the state-of-the-art approaches for fixed time, and provides much faster computation with no performance degradation for fixed number of bits.", "target": "Circulant Binary Embedding"}
{"id": "task1540-771ec19f3da34513b81519b0b7f55bed", "input": "Markov chain Monte Carlo (MCMC) is one of the main workhorses of probabilistic inference, but it is notoriously hard to measure the quality of approximate posterior samples. This challenge is particularly salient in black box inference methods, which can hide details and obscure inference failures. In this work, we extend the recently introduced bidirectional Monte Carlo [GGA15] technique to evaluate MCMC-based posterior inference algorithms. By running annealed importance sampling (AIS) chains both from prior to posterior and vice versa on simulated data, we upper bound in expectation the symmetrized KL divergence between the true posterior distribution and the distribution of approximate samples. We present Bounding Divergences with REverse Annealing (BREAD), a protocol for validating the relevance of simulated data experiments to real datasets, and integrate it into two probabilistic programming languages: WebPPL [GS] and Stan [CGHL+ p]. As an example of how BREAD can be used to guide the design of inference algorithms, we apply it to study the effectiveness of different model representations in both WebPPL and Stan.", "target": "Measuring the reliability of MCMC inference with bidirectional Monte Carlo"}
{"id": "task1540-b143c3da71f448dca5cfe2f72651374d", "input": "Texts present coherent stories that have a particular theme or overall setting, for example science fiction or western. In this paper, we present a text generation method called rewriting that edits existing human-authored narratives to change their theme without changing the underlying story. We apply the approach to math word problems, where it might help students stay more engaged by quickly transforming all of their homework assignments to the theme of their favorite movie without changing the math concepts that are being taught. Our rewriting method uses a twostage decoding process, which proposes new words from the target theme and scores the resulting stories according to a number of factors defining aspects of syntactic, semantic, and thematic coherence. Experiments demonstrate that the final stories typically represent the new theme well while still testing the original math concepts, outperforming a number of baselines. We also release a new dataset of human-authored rewrites of math word problems in several themes.", "target": "A Theme-Rewriting Approach for Generating Algebra Word Problems"}
{"id": "task1540-e8e446ef2ee444b8a5b58cb9aa912b07", "input": "Real estate appraisal, which is the process of estimating the price for real estate properties, is crucial for both buys and sellers as the basis for negotiation and transaction. Traditionally, the repeat sales model has been widely adopted to estimate real estate price. However, it depends the design and calculation of a complex economic related index, which is challenging to estimate accurately. Today, real estate brokers provide easy access to detailed online information on real estate properties to their clients. We are interested in estimating the real estate price from these large amounts of easily accessed data. In particular, we analyze the prediction power of online house pictures, which is one of the key factors for online users to make a potential visiting decision. The development of robust computer vision algorithms makes the analysis of visual content possible. In this work, we employ a Recurrent Neural Network (RNN) to predict real estate price using the state-of-the-art visual features. The experimental results indicate that our model outperforms several of other state-of-the-art baseline algorithms in terms of both mean absolute error (MAE) and mean absolute percentage error (MAPE).", "target": "Image Based Appraisal of Real Estate Properties"}
{"id": "task1540-040ff6d313f74db2aa32fe250a9d5d2a", "input": "We study optimal conformity measures for various criteria of efficiency of classification in an idealised setting. This leads to an important class of criteria of efficiency that we call probabilistic; it turns out that the most standard criteria of efficiency used in literature on conformal prediction are not probabilistic unless the problem of classification is binary. We consider both unconditional and label-conditional conformal prediction. The conference version of this paper has been published in the Proceedings of COPA 2016.", "target": "Criteria of efficiency for conformal prediction\u2217"}
{"id": "task1540-de243a64a8cb4b2cbe1c69e1e9c6566f", "input": "Probabilistic logic programs are logic programs in which some of the facts are annotated with probabilities. Several classical probabilistic inference tasks (such as MAP and computing marginals) have not yet received a lot of attention for this formalism. The contribution of this paper is that we develop efficient inference algorithms for these tasks. This is based on a conversion of the probabilistic logic program and the query and evidence to a weighted CNF formula. This allows us to reduce the inference tasks to wellstudied tasks such as weighted model counting. To solve such tasks, we employ state-ofthe-art methods. We consider multiple methods for the conversion of the programs as well as for inference on the weighted CNF. The resulting approach is evaluated experimentally and shown to improve upon the state-of-theart in probabilistic logic programming.", "target": "Inference in Probabilistic Logic Programs using Weighted CNF\u2019s"}
{"id": "task1540-ae8691e4b5af4455a3b7cf73bad288ca", "input": "Learning. Abstract: Document categorization is a technique where the category of a document is determined. In this paper three well-known supervised learning techniques which are Support Vector Machine(SVM), Na\u00efve Bayes(NB) and Stochastic Gradient Descent(SGD) compared for Bengali document categorization. Besides classifier, classification also depends on how feature is selected from dataset. For analyzing those classifier performances on predicting a document against twelve categories several feature selection techniques are also applied in this article namely Chi square distribution, normalized TFIDF (term frequency-inverse document frequency) with word analyzer. So, we attempt to explore the efficiency of those three-classification algorithms by using two different feature selection techniques in this article.", "target": "A Comparative Study on Different Types of Approaches to Bengali document Categorization"}
{"id": "task1540-2a71975a766b43f0896ef0058a4eca65", "input": "In this paper, we introduce a new deep convolutional neural network (ConvNet) module that promotes competition among a set of multi-scale convolutional filters. This new module is inspired by the inception module, where we replace the original collaborative pooling stage (consisting of a concatenation of the multi-scale filter outputs) by a competitive pooling represented by a maxout activation unit. This extension has the following two objectives: 1) the selection of the maximum response among the multi-scale filters prevents filter co-adaptation and allows the formation of multiple sub-networks within the same model, which has been shown to facilitate the training of complex learning problems; and 2) the maxout unit reduces the dimensionality of the outputs from the multi-scale filters. We show that the use of our proposed module in typical deep ConvNets produces classification results that are either better than or comparable to the state of the art on the following benchmark datasets: MNIST, CIFAR-10, CIFAR-100 and SVHN.", "target": "Competitive Multi-scale Convolution"}
{"id": "task1540-cdba9dd83c2540ff9bdde7c04b207496", "input": "We define two algorithms for propagating information in classification problems with pairwise relationships. The algorithms are based on contraction maps and are related to non-linear diffusion and random walks on graphs. The approach is also related to message passing algorithms, including belief propagation and mean field methods. The algorithms we describe are guaranteed to converge on graphs with arbitrary topology. Moreover they always converge to a unique fixed point, independent of initialization. We prove that the fixed points of the algorithms under consideration define lower-bounds on the energy function and the max-marginals of a Markov random field. The theoretical results also illustrate a relationship between message passing algorithms and value iteration for an infinite horizon Markov decision process. We illustrate the practical application of the algorithms under study with numerical experiments in image restoration, stereo depth estimation and binary classification on a grid.", "target": "Diffusion Methods for Classification with Pairwise Relationships"}
{"id": "task1540-eb7579d813e14f10a19070317703a9bf", "input": "The current trend in object detection and localization is to learn predictions with high capacity deep neural networks trained on a very large amount of annotated data and using a high amount of processing power. In this work, we propose a new neural model which directly predicts bounding box coordinates. The particularity of our contribution lies in the local computations of predictions with a new form of local parameter sharing which keeps the overall amount of trainable parameters low. Key components of the model are spatial 2D-LSTM recurrent layers which convey contextual information between the regions of the image. We show that this model is more powerful than the state of the art in applications where training data is not as abundant as in the classical configuration of natural images and Imagenet/Pascal VOC tasks. We particularly target the detection of text in document images, but our method is not limited to this setting. The proposed model also facilitates the detection of many objects in a single image and can deal with inputs of variable sizes without resizing.", "target": "Learning to detect and localize many objects from few examples"}
{"id": "task1540-0e3d45b7964d4f86b33e577915aab30f", "input": "Transfer learning is a vital technique that generalizes models trained for one setting or task to other settings or tasks. For example in speech recognition, an acoustic model trained for one language can be used to recognize speech in another language, with little or no re-training data. Transfer learning is closely related to multi-task learning (cross-lingual vs. multilingual), and is traditionally studied in the name of \u2018model adaptation\u2019. Recent advance in deep learning shows that transfer learning becomes much easier and more effective with high-level abstract features learned by deep models, and the \u2018transfer\u2019 can be conducted not only between data distributions and data types, but also between model structures (e.g., shallow nets and deep nets) or even model types (e.g., Bayesian models and neural models). This review paper summarizes some recent prominent research towards this direction, particularly for speech and language processing. We also report some results from our group and highlight the potential of this very interesting research field.", "target": "Transfer Learning for Speech and Language Processing"}
{"id": "task1540-37d466f28bc8460887a0bf7e1c3e5a4c", "input": "This paper is about reducing influence dia\u00ad gram (ID) evaluation into Bayesian network (BN) inference problems. Such reduction is interesting because it enables one to read\u00ad ily use one's favorite BN inference algorithm to efficiently evaluate IDs. Two such reduc\u00ad tion methods have been proposed previously (Cooper 1988, Shachter and Peot 1992). This paper proposes a new method. The BN in\u00ad ference problems induced by the mew method are much easier to solve than those induced by the two previous methods.", "target": "Probabilistic Inference in Influence Diagrams"}
{"id": "task1540-bcb103c3e403487fbf69c0efa6d38aa4", "input": "Seven years on from OWL becoming a W3C recommendation, and two years on from the more recent OWL 2 W3C recommendation, OWL has still experienced only patchy uptake on the Web. Although certain OWL features (like owl:sameAs) are very popular, other features of OWL are largely neglected by publishers in the Linked Data world. This may suggest that despite the promise of easy implementations and the proposal of tractable profiles suggested in OWL\u2019s second version, there is still no \u201cright\u201d standard fragment for the Linked Data community. In this paper, we (1) analyse uptake of OWL on the Web of Data, (2) gain insights into the OWL fragment that is actually used/usable on the Web, where we arrive at the conclusion that this fragment is likely to be a simplified profile based on OWL RL, (3) propose and discuss such a new fragment, which we call OWL LD (for Linked Data).", "target": "OWL: Yet to arrive on the Web of Data?"}
{"id": "task1540-8e541c5135de42a1b31da46fcccbbdb5", "input": "Many genres of natural language text are narratively structured, a testament to our predilection for organizing our experiences as narratives. There is broad consensus that understanding a narrative requires identifying and tracking the goals and desires of the characters and their narrative outcomes. However, to date, there has been limited work on computational models for this problem. We introduce a new dataset, DesireDB, which includes goldstandard labels for identifying statements of desire, textual evidence for desire fulfillment, and annotations for whether the stated desire is fulfilled given the evidence in the narrative context. We report experiments on tracking desire fulfillment using different methods, and show that LSTM Skip-Thought model achieves F-measure of 0.7 on our corpus.", "target": "Modelling Protagonist Goals and Desires in First-Person Narrative"}
{"id": "task1540-6e79e43701bb4dea86151fdff95d305c", "input": "We describe a general framework for online adaptation of optimization hyperparameters by \u2018hot swapping\u2019 their values during learning. We investigate this approach in the context of adaptive learning rate selection using an explore-exploit strategy from the multi-armed bandit literature. Experiments on a benchmark neural network show that the hot swapping approach leads to consistently better solutions compared to well-known alternatives such as AdaDelta and stochastic gradient with exhaustive hyperparameter search.", "target": "OPTIMIZATION HYPERPARAMETERS"}
{"id": "task1540-707717acd759407da880270ae8c2f331", "input": "We describe and analyze a simple and effective algorithm for sequence segmentation applied to speech processing tasks. We propose a neural architecture that is composed of two modules trained jointly: a recurrent neural network (RNN) module and a structured prediction model. The RNN outputs are considered as feature functions to the structured model. The overall model is trained with a structured loss function which can be designed to the given segmentation task. We demonstrate the effectiveness of our method by applying it to two simple tasks commonly used in phonetic studies: word segmentation and voice onset time segmentation. Results suggest the proposed model is superior to previous methods, obtaining state-of-the-art results on the tested datasets.", "target": "SEQUENCE SEGMENTATION USING JOINT RNN AND STRUCTURED PREDICTION MODELS"}
{"id": "task1540-cfdc3756317048749198fd62c2f6e95c", "input": "In this work we use the recent advances in representation learning to propose a neural architecture for the problem of natural language inference. Our approach is aligned to mimic how a human does the natural language inference process given two statements. The model uses variants of Long Short Term Memory (LSTM), attention mechanism and composable neural networks, to carry out the task. Each part of our model can be mapped to a clear functionality humans do for carrying out the overall task of natural language inference. The model is end-to-end differentiable enabling training by stochastic gradient descent. On Stanford Natural Language Inference(SNLI) dataset, the proposed model achieves better accuracy numbers than all published models in literature.", "target": "A Neural Architecture Mimicking Humans End-to-End for Natural Language Inference"}
{"id": "task1540-795f7f3b0aca42bcb2a354ab0031a3dd", "input": "We introduce a new approach to solving path-finding problems under uncertainty by representing them as probabilistic models and applying domain-independent inference algorithms to the models. This approach separates problem representation from the inference algorithm and provides a framework for efficient learning of path-finding policies. We evaluate the new approach on the Canadian Traveller Problem, which we formulate as a probabilistic model, and show how probabilistic inference allows high performance stochastic policies to be obtained for this problem.", "target": "Path Finding under Uncertainty through Probabilistic Inference"}
{"id": "task1540-9ca8eaa6cf814a9eb7feb9681a2bc61c", "input": "We introduce AI rationalization, an approach for generating explanations of autonomous system behavior as if a human had done the behavior. We describe a rationalization technique that uses neural machine translation to translate internal state-action representations of the autonomous agent into natural language. We evaluate our technique in the Frogger game environment. The natural language is collected from human players thinking out loud as they play the game. We motivate the use of rationalization as an approach to explanation generation, show the results of experiments on the accuracy of our rationalization technique, and describe future research agenda.", "target": "Rationalization: A Neural Machine Translation Approach to Generating Natural Language Explanations"}
{"id": "task1540-0e6a27d942194692ba5dd672fd1f7917", "input": "We present a deep layered architecture that generalizes classical convolutional neural networks (ConvNets). The architecture, called SimNets, is driven by two operators, one being a similarity function whose family contains the convolution operator used in ConvNets, and the other is a new soft max-min-mean operator called MEX that realizes classical operators like ReLU and max pooling, but has additional capabilities that make SimNets a powerful generalization of ConvNets. Three interesting properties emerge from the architecture: (i) the basic input to hidden layer to output machinery contains as special cases kernel machines with the Exponential and Generalized Gaussian kernels, the output units being \u201dneurons in feature space\u201d (ii) in its general form, the basic machinery has a higher abstraction level than kernel machines, and (iii) initializing networks using unsupervised learning is natural. Experiments demonstrate the capability of achieving state of the art accuracy with networks that are an order of magnitude smaller than comparable ConvNets.", "target": "SimNets: A Generalization of Convolutional Networks"}
{"id": "task1540-0a987800f0224edead47119c14e696da", "input": "We present a new algorithm for the contextual bandit learning problem, where the learner repeatedly takes an action in response to the observed context, observing the reward only for that action. Our method assumes access to an oracle for solving cost-sensitive classification problems and achieves the statistically optimal regret guarantee with only \u00d5( \u221a T ) oracle calls across all T rounds. By doing so, we obtain the most practical contextual bandit learning algorithm amongst approaches that work for general policy classes. We further conduct a proof-of-concept experiment which demonstrates the excellent computational and prediction performance of (an online variant of) our algorithm relative to several baselines.", "target": "Taming the Monster: A Fast and Simple Algorithm for Contextual Bandits"}
{"id": "task1540-c49006adf99c40ada6dbe97dd2ccd20a", "input": "Attack detection problems in the smart grid are posed as statistical learning problems for different attack scenarios in which the measurements are observed in batch or online settings. In this approach, machine learning algorithms are used to classify measurements as being either secure or attacked. An attack detection framework is provided to exploit any available prior knowledge about the system and surmount constraints arising from the sparse structure of the problem in the proposed approach. Well-known batch and online learning algorithms (supervised and semi-supervised) are employed with decision and feature level fusion to model the attack detection problem. The relationships between statistical and geometric properties of attack vectors employed in the attack scenarios and learning algorithms are analyzed to detect unobservable attacks using statistical learning methods. The proposed algorithms are examined on various IEEE test systems. Experimental analyses show that machine learning algorithms can detect attacks with performances higher than the attack detection algorithms which employ state vector estimation methods in the proposed attack detection framework.", "target": "Machine Learning Methods for Attack Detection in the Smart Grid"}
{"id": "task1540-2146e17c8b914db1a87e5f58b9ba3c0d", "input": "In recent years, adaptive learning systems rely increasingly on learning hierarchy to customize the educational logic developed in their courses. Most approaches do not consider that the relationships of prerequisites between the skills are fuzzy relationships. In this article, we describe a new approach of a practical application of fuzzy logic techniques to the construction of learning hierarchies. For this, we use a learning hierarchy predefined by one or more experts of a specific field. However, the relationships of prerequisites between the skills in the learning hierarchy are not definitive and they are fuzzy relationships. Indeed, we measure relevance degree of all relationships existing in this learning hierarchy and we try to answer to the following question: Is the relationships of prerequisites predefined in initial learning hierarchy are correctly established or not?", "target": "A New Approach of Learning Hierarchy Construction Based on Fuzzy Logic"}
{"id": "task1540-9948a8a3becd4c48804ce136b62127c2", "input": "Many modern multiclass and multilabel problems are characterized by increasingly large output spaces. For these problems, label embeddings have been shown to be a useful primitive that can improve computational and statistical efficiency. In this work we utilize a correspondence between rank constrained estimation and low dimensional label embeddings that uncovers a fast label embedding algorithm which works in both the multiclass and multilabel settings. The result is a randomized algorithm whose running time is exponentially faster than naive algorithms. We demonstrate our techniques on two large-scale public datasets, from the Large Scale Hierarchical Text Challenge and the Open Directory Project, where we obtain state of the art results.", "target": "Fast Label Embeddings via Randomized Linear Algebra"}
{"id": "task1540-4e9bf7929fe548ee8aa46bb30a3ad742", "input": "We train a generator by maximum likelihood and we also train the same generator architecture by Wasserstein GAN. We then compare the generated samples, exact log-probability densities and approximate Wasserstein distances. We show that an independent critic trained to approximate Wasserstein distance between the validation set and the generator distribution helps detect overfitting. Finally, we use ideas from the one-shot learning literature to develop a novel fast learning critic.", "target": "Comparison of Maximum Likelihood and GAN-based training of Real NVPs"}
{"id": "task1540-7464d89a76564b788fe3b53a9923675d", "input": "Generating optimal plans in highly dynamic environments is challenging. Plans are predicated on an assumed initial state, but this state can change unexpectedly during plan generation, potentially invalidating the planning effort. In this paper we make three contributions: (1) We propose a novel algorithm for generating optimal plans in settings where frequent, unexpected events interfere with planning. It is able to quickly distinguish relevant from irrelevant state changes, and to update the existing planning search tree if necessary. (2) We argue for a new criterion for evaluating plan adaptation techniques: the relative running time compared to the \u201csize\u201d of changes. This is significant since during recovery more changes may occur that need to be recovered from subsequently, and in order for this process of repeated recovery to terminate, recovery time has to converge. (3) We show empirically that our approach can converge and find optimal plans in environments that would ordinarily defy planning due to their high dynamics.", "target": "Generating Optimal Plans in Highly-Dynamic Domains"}
{"id": "task1540-279654985e4e46deae21a26561bf713c", "input": "We improve the regret bound of the Upper Confidence Bound (UCB) algorithm of Auer et al. (2002) and show that its regret is with high-probability a problem dependent constant. In the case of linear bandits (Dani et al., 2008), we improve the problem dependent bound in the dimension and number of time steps. Furthermore, as opposed to the previous result, we prove that our bound holds for small sample sizes, and at the same time the worst case bound is improved by a logarithmic factor and the constant is improved.", "target": "Online Least Squares Estimation with Self-Normalized Processes: An Application to Bandit Problems\u2217"}
{"id": "task1540-13d45ec9f9594e04b6da9886f14d7062", "input": "An RNN-based forecasting approach is used to early detect anomalies in industrial multivariate time series data from a simulated Tennessee Eastman Process (TEP) with many cyberattacks. This work continues a previously proposed LSTM-based approach to the fault detection in simpler data. It is considered necessary to adapt the RNN network to deal with data containing stochastic, stationary, transitive and a rich variety of anomalous behaviours. There is particular focus on early detection with special NABmetric. A comparison with the DPCA approach is provided. The generated data set is made publicly available.", "target": "RNN-based Early Cyber-Attack Detection for the Tennessee Eastman Process"}
{"id": "task1540-ddb0c182f92741cdba68a1d08d0230cb", "input": "In this paper, we provide two axiomatizations of algebraic expected utility, which is a particular generalized expected utility, in a von NeumannMorgenstern setting, i.e. uncertainty representation is supposed to be given and here to be described by a plausibility measure valued on a semiring, which could be partially ordered. We show that axioms identical to those for expected utility entail that preferences are represented by an algebraic expected utility. This algebraic approach allows many previous propositions (expected utility, binary possibilistic utility,...) to be unified in a same general framework and proves that the obtained utility enjoys the same nice features as expected utility: linearity, dynamic consistency, autoduality of the underlying uncertainty representation, autoduality of the decision criterion and possibility of modeling decision maker\u2019s attitude toward uncertainty.", "target": "Axiomatic Foundations for a Class of Generalized Expected Utility: Algebraic Expected Utility"}
{"id": "task1540-d94c6bed5a104201b9268d9d5e6ea614", "input": "Faced with continuously increasing scale of data, original back-propagation neural network based machine learning algorithm presents two non-trivial challenges: huge amount of data makes it difficult to maintain both efficiency and accuracy; redundant data aggravates the system workload. This project is mainly focused on the solution to the issues above, combining deep learning algorithm with cloud computing platform to deal with large-scale data. A MapReduce-based handwriting character recognizer will be designed in this project to verify the efficiency improvement this mechanism will achieve on training and practical large-scale data. Careful discussion and experiment will be developed to illustrate how deep learning algorithm works to train handwritten digits data, how MapReduce is implemented on deep learning neural network, and why this combination accelerates computation. Besides performance, the scalability and robustness will be mentioned in this report as well. Our system comes with two demonstration software that visually illustrates our handwritten digit recognition/encoding application. 1", "target": "Large-scale Artificial Neural Network: MapReduce-based Deep Learning"}
{"id": "task1540-f3c7a04f900e40388c67083aa28ace5d", "input": "As robots enter human environments, they will be expected to accomplish a tremendous range of tasks. It is not feasible for robot designers to pre-program these behaviors or know them in advance, so one way to address this is through end-user programming, such as learning from demonstration (LfD). While significant work has been done on the mechanics of enabling robot learning from human teachers, one unexplored aspect is enabling mutual feedback between both the human teacher and robot during the learning process, i.e., implicit learning. In this paper, we explore one aspect of this mutual understanding, grounding sequences, where both a human and robot provide non-verbal feedback to signify their mutual understanding during interaction. We conducted a study where people taught an autonomous humanoid robot a dance, and performed gesture analysis to measure people\u2019s responses to the robot during correct and incorrect demonstrations.", "target": "Exploring Implicit Human Responses to Robot Mistakes in a Learning from Demonstration Task"}
{"id": "task1540-502107fd12dd4605b397d4f2eeed9e37", "input": "Warehouse is one of the important aspects of a company. Therefore, it is necessary to improve Warehouse Management System (WMS) to have a simple function that can determine the layout of the storage goods. In this paper we propose an improved warehouse layout method based on ant colony algorithm and backtracking algorithm. The method works on two steps. First, it generates a solutions parameter tree from backtracking algorithm. Then second, it deducts the solutions parameter by using a combination of ant colony algorithm and backtracking algorithm. This method was tested by measuring the time needed to build the tree and to fill up the space using two scenarios. The method needs 0.294 to 33.15 seconds to construct the tree and 3.23 seconds (best case) to61.41 minutes (worst case) to fill up the warehouse. This method is proved to be an attractive alternative solution for warehouse layout system. Keywords\u2014warehouse layout; block stacking method; ant colony algorithm; backtracking algorithm.", "target": "Warehouse Layout Method Based on Ant Colony and Backtracking Algorithm"}
{"id": "task1540-2b0663c21d6c4b6cbc637c2709254695", "input": "The human visual system can spot an abnormal image, and reason about what makes it strange. This task has not received enough attention in computer vision. In this paper we study various types of atypicalities in images in a more comprehensive way than has been done before. We propose a new dataset of abnormal images showing a wide range of atypicalities. We design human subject experiments to discover a coarse taxonomy of the reasons for abnormality. Our experiments reveal three major categories of abnormality: object-centric, scene-centric, and contextual. Based on this taxonomy, we propose a comprehensive computational model that can predict all different types of abnormality in images and outperform prior arts in abnormality recognition.", "target": "Toward a Taxonomy and Computational Models of Abnormalities in Images"}
{"id": "task1540-839e205a1f6243c985e85d657bc3f075", "input": "Learning to localize objects with minimal supervision is an important problem in computer vision, since large fully annotated datasets are extremely costly to obtain. In this paper, we propose a new method that achieves this goal with only image-level labels of whether the objects are present or not. Our approach combines a discriminative submodular cover problem for automatically discovering a set of positive object windows with a smoothed latent SVM formulation. The latter allows us to leverage efficient quasiNewton optimization techniques. Our experiments demonstrate that the proposed approach provides a 50% relative improvement in mean average precision over the current state-of-the-art on PASCAL VOC 2007 detection.", "target": "On learning to localize objects with minimal supervision"}
{"id": "task1540-ab14921167814f9e9d46b1f7745c91a3", "input": "Surely we want solid foundations. What kind of castle can we build on sand? What is the point of devoting effort to balconies and minarets, if the foundation may be so weak as to allow the structure to collapse of its own weight? We want our foundations set on bedrock, designed to last for generations. Who would want an architect who cannot certify the soundness of the foundations of his buildings?", "target": "Why Do We Need Foundations for Modelling Uncertainties?"}
{"id": "task1540-2d6e83f73060455a98de4c328ef38a8b", "input": "We introduce a simple, general strategy to manipulate the behavior of a neural decoder that enables it to generate outputs that have specific properties of interest (e.g., sequences of a pre-specified length). The model can be thought of as a simple version of the actor-critic model that uses an interpolation of the actor (the MLE-based token generation policy) and the critic (a value function that estimates the future values of the desired property) for decision making. We demonstrate that the approach is able to incorporate a variety of properties that cannot be handled by standard neural sequence decoders, such as sequence length and backward probability (probability of sources given targets), in addition to yielding consistent improvements in abstractive summarization and machine translation when the property to be optimized is BLEU or ROUGE scores.", "target": "Learning to Decode for Future Success"}
{"id": "task1540-e7e6e97015f74f959bdc474d39255f49", "input": "In this paper we propose a simple yet powerful method for learning representations in supervised learning scenarios where an input datapoint is described by a set of feature vectors and its associated output may be given by soft labels indicating, for example, class probabilities. We represent an input datapoint as a K-dimensional vector, where each component is a mixture of probabilities over its corresponding set of feature vectors. Each probability indicates how likely a feature vector is to belong to one-out-of-K unknown prototype patterns. We propose a probabilistic model that parameterizes these prototype patterns in terms of hidden variables and therefore it can be trained with conventional approaches based on likelihood maximization. More importantly, both the model parameters and the prototype patterns can be learned from data in a discriminative way. We show that our model can be seen as a probabilistic generalization of learning vector quantization (LVQ). We apply our method to the problems of shape classification, hyperspectral imaging classification and people\u2019s work class categorization, showing the superior performance of our method compared to the standard prototype-based classification approach and other competitive benchmarks.", "target": "Discriminative Probabilistic Prototype Learning"}
{"id": "task1540-aad47cd34e6f4956b343636a055b2ce3", "input": "In this paper, we propose the distributed tree kernels (DTK) as a novel method to reduce time and space complexity of tree kernels. Using a linear complexity algorithm to compute vectors for trees, we embed feature spaces of tree fragments in low-dimensional spaces where the kernel computation is directly done with dot product. We show that DTKs are faster, correlate with tree kernels, and obtain a statistically similar performance in two natural language processing tasks.", "target": "Distributed Tree Kernels"}
{"id": "task1540-40a55737fd8c4d35bdb3c4c5cc051ed1", "input": "Bayesian Belief Networks (BBNs) are a pow\u00ad erful formalism for reasoning under uncer\u00ad tainty but bear some severe limitations: they require a large amount of information be\u00ad fore any reasoning process can start, they have limited contradiction handling capabil\u00ad ities, and their ability to provide explana\u00ad tions for their conclusion is still controversial. There exists a class of reasoning systems, called 11-uth Maintenance Systems (TMSs), which are able to deal with partially speci\u00ad fied knowledge, to provide well-founded ex\u00ad planation for their conclusions, and to detect and handle contradictions. TMSs incorporat\u00ad ing measure of uncertainty are called Belief Maintenance Systems (BMss). This paper de\u00ad scribes how a BMS based on probabilitistic logic can be applied to BBNs, thus introduc\u00ad ing a new class of BBNs, called Ignorant Be\u00ad lief Networks, able to incrementally deal with partially specified conditional dependencies, to provide explanations, and to detect and handle contradictions.", "target": "Belief Maintenance in Bayesian Networks"}
{"id": "task1540-136371659e5642a98dc3981b32560714", "input": "It has been proved that large-scale realistic Knowledge Based Machine Translation (KBMT) applications require acquisition of huge knowledge about language and about the world. This knowledge is encoded in computational grammars, lexicons and domain models. Another approach \u2013 which avoids the need for collecting and analyzing massive knowledgeis the Example Based approach, which is the topic of this paper. We show through the paper that using Example Based in its native form is not suitable for translating into Arabic. Therefore a modification to the basic approach is presented to improve the accuracy of the translation process. The basic idea of the new approach is to improve the technique by which template-based approaches select the appropriate templates. It relies on extracting, from a parallel Bilingual Corpus, all possible templates that could match parts of the source sentence. These templates are selected as suitable candidate chunks for the source sentence. The corresponding Arabic templates are also extracted and represented by a diredted graph. Each branch represents one possible string of templates candidate to represent the target sentence. The shortest continuous path or the most probable tree branch is selected to represent the target sentence. Finally the Arabic translation of the selected tree branch is generated.", "target": "The Best Templates Match Technique For Example Based Machine Translation"}
{"id": "task1540-9d63545981ea457292baa4d004b65772", "input": "In this paper, we propose a framework for training multiple neural networks simultaneously. The parameters from all models are regularised by the tensor trace norm, so that one neural network is encouraged to reuse others\u2019 parameters if possible \u2013 this is the main motivation behind multi-task learning. In contrast to many deep multi-task learning work, we do not predefine a parameter sharing strategy by tying some (usually bottom) layers\u2019 parameters, instead, our framework allows the sharing for all shareable layers thus the sharing strategy is learned from a pure data-driven way.", "target": "Trace Norm Regularised Deep Multi-Task Learning"}
{"id": "task1540-bf6386f97d9a416cb9bdfe58e25e9d5c", "input": "Deep neural networks (DNNs) are powerful types of artificial neural networks (ANNs) that use several hidden layers. They have recently gained considerable attention in the speech transcription and image recognition community (Krizhevsky et al., 2012) for their superior predictive properties including robustness to overfitting. However their application to algorithmic trading has not been previously researched, partly because of their computational complexity. This paper describes the application of DNNs to predicting financial market movement directions. In particular we describe the configuration and training approach and then demonstrate their application to backtesting a simple trading strategy over 43 different Commodity and FX future mid-prices at 5-minute intervals. All results in this paper are generated using a C++ implementation on the Intel Xeon Phi co-processor which is 11.4x faster than the serial version and a Python strategy backtesting environment both of which are available as open source code written by the authors.", "target": "Classification-based Financial Markets Prediction using Deep Neural Networks"}
{"id": "task1540-b03f69ae9b3e4e13b3db01640405a1e1", "input": "We propose an attention-enabled encoder-decoder model for the problem of grapheme-to-phoneme conversion. Most previous work has tackled the problem via joint sequence models that require explicit alignments for training. In contrast, the attention-enabled encoder-decoder model allows for jointly learning to align and convert characters to phonemes. We explore different types of attention models, including global and local attention, and our best models achieve state-of-the-art results on three standard data sets (CMUDict, Pronlex, and NetTalk).", "target": "JOINTLY LEARNING TO ALIGN AND CONVERT GRAPHEMES TO PHONEMES WITH NEURAL ATTENTION MODELS"}
{"id": "task1540-c09912aef1984f77af50529f61941a4d", "input": "People can recognize scenes across many different modalities beyond natural images. In this paper, we investigate how to learn cross-modal scene representations that transfer across modalities. To study this problem, we introduce a new cross-modal scene dataset. While convolutional neural networks can categorize scenes well, they also learn an intermediate representation not aligned across modalities, which is undesirable for cross-modal transfer applications. We present methods to regularize cross-modal convolutional neural networks so that they have a shared representation that is agnostic of the modality. Our experiments suggest that our scene representation can help transfer representations across modalities for retrieval. Moreover, our visualizations suggest that units emerge in the shared representation that tend to activate on consistent concepts independently of the modality.", "target": "Cross-Modal Scene Networks"}
{"id": "task1540-8c649678a24f479b8a34f2e3c939d722", "input": "This article describes the systems jointly submitted by Institute for Infocomm (IR), the Laboratoire d\u2019Informatique de l\u2019Universit du Maine (LIUM), Nanyang Technology University (NTU) and the University of Eastern Finland (UEF) for 2015 NIST Language Recognition Evaluation (LRE). The submitted system is a fusion of nine sub-systems based on i-vectors [1] extracted from different types of features. Given the i-vectors, several classifiers are adopted for the language detection task including support vector machines (SVM) [2], multi-class logistic regression (MCLR), Probabilistic Linear Discriminant Analysis (PLDA) [3] and Deep Neural Networks (DNN).", "target": "Fantastic 4 system for NIST 2015 Language Recognition Evaluation"}
{"id": "task1540-e1a63f17d2474168bd4720a212397fe8", "input": "We introduce Discriminative BLEU (\u2206BLEU), a novel metric for intrinsic evaluation of generated text in tasks that admit a diverse range of possible outputs. Reference strings are scored for quality by human raters on a scale of [\u22121, +1] to weight multi-reference BLEU. In tasks involving generation of conversational responses, \u2206BLEU correlates reasonably with human judgments and outperforms sentence-level and IBM BLEU in terms of both Spearman\u2019s \u03c1 and Kendall\u2019s \u03c4 .", "target": "\u2206BLEU: A Discriminative Metric for Generation Tasks with Intrinsically Diverse Targets\u2217"}
{"id": "task1540-7dc75037d9a04682b61db865fcea2492", "input": "We introduce an abductive method for a coherent integration of independent datasources. The idea is to compute a list of data-facts that should be inserted to the amalgamated database or retracted from it in order to restore its consistency. This method is implemented by an abductive solver, called Asystem, that applies SLDNFA-resolution on a meta-theory that relates different, possibly contradicting, input databases. We also give a pure model-theoretic analysis of the possible ways to \u2018recover\u2019 consistent data from an inconsistent database in terms of those models of the database that exhibit as minimal inconsistent information as reasonably possible. This allows us to characterize the \u2018recovered databases\u2019 in terms of the \u2018preferred\u2019 (i.e., most consistent) models of the theory. The outcome is an abductive-based application that is sound and complete with respect to a corresponding model-based, preferential semantics, and \u2013 to the best of our knowledge \u2013 is more expressive (thus more general) than any other implementation of coherent integration of databases.", "target": "Coherent Integration of Databases by Abductive Logic Programming"}
{"id": "task1540-f77bb7d258a0483e86f4b5cf4563be8f", "input": "This paper presents Centre for Development of Advanced Computing Mumbai\u2019s (CDACM) submission to NLP Tools Contest on Statistical Machine Translation in Indian Languages (ILSMT) 2015 (collocated with ICON 2015). The aim of the contest was to collectively explore the effectiveness of Statistical Machine Translation (SMT) while translating within Indian languages and between English and Indian languages. In this paper, we report our work on all five language pairs, namely Bengali-Hindi (bn-hi), Marathi-Hindi (mrhi), Tamil-Hindi (ta-hi), Telugu-Hindi (tehi), and English-Hindi (en-hi) for Health, Tourism and General domains. We have used suffix separation, compound splitting and preordering prior to SMT training and testing.", "target": "Statistical Machine Translation for Indian Languages: Mission Hindi 2"}
{"id": "task1540-421f04b2531446188dbe142a345c79b2", "input": "In this paper we explore a symmetry-based search space reduction technique which can speed up optimal pathfinding on undirected uniform-cost grid maps by up to 38 times. Our technique decomposes grid maps into a set of empty rectangles, removing from each rectangle all interior nodes and possibly some from along the perimeter. We then add a series of macro-edges between selected pairs of remaining perimeter nodes to facilitate provably optimal traversal through each rectangle. We also develop a novel online pruning technique to further speed up search. Our algorithm is fast, memory efficient and retains the same optimality and completeness guarantees as searching on an unmodified grid map.", "target": "Symmetry-Based Search Space Reduction For Grid Maps"}
{"id": "task1540-76fffbaec6f84b348e194759911da5f8", "input": "Experimenting with a new dataset of 1.6M user comments from a Greek news portal and existing datasets of English Wikipedia comments, we show that an RNN outperforms the previous state of the art in moderation. A deep, classification-specific attention mechanism improves further the overall performance of the RNN. We also compare against a CNN and a word-list baseline, considering both fully automatic and semi-automatic moderation.", "target": "Deep Learning for User Comment Moderation"}
{"id": "task1540-5dfaaae27d804500abaae8a5321ccea8", "input": "Recently deep neural networks (DNNs) have been used to learn speaker features. However, the quality of the learned features is not sufficiently good, so a complex back-end model, either neural or probabilistic, has to be used to address the residual uncertainty when applied to speaker verification, just as with raw features. This paper presents a convolutional timedelay deep neural network structure (CT-DNN) for speaker feature learning. Our experimental results on the Fisher database demonstrated that this CT-DNN can produce highquality speaker features: even with a single feature (0.3 seconds including the context), the EER can be as low as 7.68%. This effectively confirmed that the speaker trait is largely a deterministic short-time property rather than a long-time distributional pattern, and therefore can be extracted from just dozens of frames.", "target": "Deep Speaker Feature Learning for Text-independent Speaker Verification"}
{"id": "task1540-775e62579ca44fc4ab2da6f46dfed241", "input": "We propose novel model transfer-learning methods that refine a decision forest model M learned within a \u201csource\u201d domain using a training set sampled from a \u201ctarget\u201d domain, assumed to be a variation of the source. We present two random forest transfer algorithms. The first algorithm searches greedily for locally optimal modifications of each tree structure by trying to locally expand or reduce the tree around individual nodes. The second algorithm does not modify structure, but only the parameter (thresholds) associated with decision nodes. We also propose to combine both methods by considering an ensemble that contains the union of the two forests. The proposed methods exhibit impressive experimental results over a range of problems.", "target": "LEARN ON SOURCE, REFINE ON TARGET: A MODEL TRANSFER LEARNING FRAMEWORK WITH RANDOM FORESTS 1 Learn on Source, Refine on Target: A Model Transfer Learning Framework with Random Forests"}
{"id": "task1540-0b8b0b7b937e4576a921790e30602e78", "input": "We are proposing a tool able to gather information on social networks from narrative texts. Its name is CHAPLIN, CHAracters and PLaces Interaction Network, implemented in VB.NET. Characters and places of the narrative works are extracted in a list of raw words. Aided by the interface, the user selects names out of them. After this choice, the tool allows the user to enter some parameters, and, according to them, creates a network where the nodes are the characters and places, and the edges their interactions. Edges are labelled by performances. The output is a GV file, written in the DOT graph scripting language, which is rendered by means of the free open source software Graphviz.", "target": "Extracting Networks of Characters and Places from Written Works with CHAPLIN Extracting Networks of Characters and Places from Written Works with CHAPLIN"}
{"id": "task1540-45a95bd033174b5b93b23742f4c80e63", "input": "The speech feature extraction has been a key focus in robust speech recognition research; it significantly affects the recognition performance. In this paper, we first study a set of different features extraction methods such as linear predictive coding (LPC), mel frequency cepstral coefficient (MFCC) and perceptual linear prediction (PLP) with several features normalization techniques like rasta filtering and cepstral mean subtraction (CMS). Based on this, a comparative evaluation of these features is performed on the task of text independent speaker identification using a combination between gaussian mixture models (GMM) and linear and non-linear kernels based on support vector machine (SVM).", "target": "On the Use of Different Feature Extraction Methods for Linear and Non Linear kernels"}
{"id": "task1540-97f79137ecb0469c8cce6d59baac1e9a", "input": "Our study identifies sentences in Wikipedia articles that are either identical or highly similar by applying techniques for near-duplicate detection of web pages. This is accomplished with a MapReduce implementation of minhash to identify clusters of sentences with high Jaccard similarity. We show that these clusters can be categorized into six different types, two of which are particularly interesting: identical sentences quantify the extent to which content in Wikipedia is copied and pasted, and near-duplicate sentences that state contradictory facts point to quality issues in Wikipedia.", "target": "Identifying Duplicate and Contradictory Information in Wikipedia"}
{"id": "task1540-8e37a1e9f0604b28bf6e7c76deea8489", "input": "We consider the problem of principal component analysis (PCA) in a streaming stochastic setting, where our goal is to find a direction of approximate maximal variance, based on a stream of i.i.d. data points in R. A simple and computationally cheap algorithm for this is stochastic gradient descent (SGD), which incrementally updates its estimate based on each new data point. However, due to the non-convex nature of the problem, analyzing its performance has been a challenge. In particular, existing guarantees rely on a non-trivial eigengap assumption on the covariance matrix, which is intuitively unnecessary. In this paper, we provide (to the best of our knowledge) the first eigengap-free convergence guarantees for SGD in the context of PCA. This also partially resolves an open problem posed in [10]. Moreover, under an eigengap assumption, we show that the same techniques lead to new SGD convergence guarantees with better dependence on the eigengap.", "target": "Convergence of Stochastic Gradient Descent for PCA"}
{"id": "task1540-3544051dd22140cc950e5a7195d57a73", "input": "Recent work in learning vector-space embeddings for multi-relational data has focused on combining relational information derived from knowledge bases with distributional information derived from large text corpora. We propose a simple approach that leverages the descriptions of entities or phrases available in lexical resources, in conjunction with distributional semantics, in order to derive a better initialization for training relational models. Applying this initialization to the TransE model results in significant new stateof-the-art performances on the WordNet dataset, decreasing the mean rank from the previous best of 212 to 51. It also results in faster convergence of the entity representations. We find that there is a tradeoff between improving the mean rank and the hits@10 with this approach. This illustrates that much remains to be understood regarding performance improvements in relational models.", "target": "Leveraging Lexical Resources for Learning Entity Embeddings in Multi-Relational Data"}
{"id": "task1540-a306a974822242fc9bfcb18830380fa2", "input": "Sequence-to-sequence (Seq2Seq) models with attention have excelled at tasks which involve generating natural language sentences such as machine translation, image captioning and speech recognition. Performance has further been improved by leveraging unlabeled data, often in the form of a language model. In this work, we present the Cold Fusion method, which leverages a pre-trained language model during training, and show its effectiveness on the speech recognition task. We show that Seq2Seq models with Cold Fusion are able to better utilize language information enjoying i) faster convergence and better generalization, and ii) almost complete transfer to a new domain while using less than 10% of the labeled training data.", "target": "Cold Fusion: Training Seq2Seq Models Together with Language Models"}
{"id": "task1540-fb3119e4f050432da4cc2297d7be30f7", "input": "Although end-to-end Neural Machine Translation (NMT) has achieved remarkable progress in the past two years, it suffers from a major drawback: translations generated by NMT systems often lack of adequacy. It has been widely observed that NMT tends to repeatedly translate some source words while mistakenly ignoring other words. To alleviate this problem, we propose a novel encoder-decoder-reconstructor framework for NMT. The reconstructor, incorporated into the NMT model, manages to reconstruct the input source sentence from the hidden layer of the output target sentence, to ensure that the information in the source side is transformed to the target side as much as possible. Experiments show that the proposed framework significantly improves the adequacy of NMT output and achieves superior translation result over state-of-theart NMT and statistical MT systems.", "target": "Neural Machine Translation with Reconstruction"}
{"id": "task1540-e2c700b875514b64b2823c41745a1ca6", "input": "A series of monte carlo studies were performed to compare the behavior of some alternative procedures for reasoning under uncertainty. The behavior of several Bayesian, linear model and default reasoning procedures were examined in the context of increasing levels of calibration error. The most interesting result is that Bayesian procedures tended to output more extreme posterior belief values (posterior beliefs near 0.0 or 1.0) than other techniques, but the linear models were relatively less likely to output strong support for an erroneous conclusion. Also, accounting for the probabilistic dependencies between evidence items was important for both Bayesian and linear updating procedures.", "target": "Reasoning under Uncertainty:"}
{"id": "task1540-029eab49ccd04900b0ef751ad2edd860", "input": "Recently, sequence-to-sequence model by using encoder-decoder neural network has gained popularity for automatic speech recognition (ASR). The architecture commonly uses an attentional mechanism which allows the model to learn alignments between source speech sequence and target text sequence. Most attentional mechanisms used today is based on a global attention property which requires a computation of a weighted summarization of the whole input sequence generated by encoder states. However, it is computationally expensive and often produces misalignment on the longer input sequence. Furthermore, it does not fit with monotonous or left-to-right nature in speech recognition task. In this paper, we propose a novel attention mechanism that has local and monotonic properties. Various ways to control those properties are also explored. Experimental results demonstrate that encoder-decoder based ASR with local monotonic attention could achieve significant performance improvements and reduce the computational complexity in comparison with the one that used the standard global attention architecture.", "target": "Local Monotonic Attention Mechanism for End-to-End Speech Recognition"}
{"id": "task1540-8f6c655c043f493aa990fe4c29032d9b", "input": "Artificial intelligence methods have often been applied to perform specific functions or tasks in the cyber\u2013 defense realm. However, as adversary methods become more complex and difficult to divine, piecemeal efforts to understand cyber\u2013attacks, and malware\u2013based attacks in particular, are not providing sufficient means for malware analysts to understand the past, present and future characteristics of malware. In this paper, we present the Malware Analysis and Attributed using Genetic Information (MAAGI) system. The underlying idea behind the MAAGI system is that there are strong similarities between malware behavior and biological organism behavior, and applying biologically inspired methods to corpora of malware can help analysts better understand the ecosystem of malware attacks. Due to the sophistication of the malware and the analysis, the MAAGI system relies heavily on artificial intelligence techniques to provide this capability. It has already yielded promising results over its development life, and will hopefully inspire more integration between the artificial intelligence and cyber\u2013defense communities.", "target": "Artificial Intelligence Based Malware Analysis"}
{"id": "task1540-857ebc553d8a4d1890993ce268c0cbef", "input": "In this paper, we study the problem of learning a monotone DNF with at most s terms of size (number of variables in each term) at most r (s term r-MDNF) from membership queries. This problem is equivalent to the problem of learning a general hypergraph using hyperedge-detecting queries, a problem motivated by applications arising in chemical reactions and genome sequencing. We first present new lower bounds for this problem and then present deterministic and randomized adaptive algorithms with query complexities that are almost optimal. All the algorithms we present in this paper run in time linear in the query complexity and the number of variables n. In addition, all of the algorithms we present in this paper are asymptotically tight for fixed r and/or s.", "target": "On Exact Learning Monotone DNF from Membership Queries"}
{"id": "task1540-59e59f8107a747429708559a7a35bfe5", "input": "We discuss methodological issues related to the evaluation of unsupervised binary code construction methods for nearest neighbor search. These issues have been widely ignored in literature. These coding methods attempt to preserve either Euclidean distance or angular (cosine) distance in the binary embedding space. We explain why when comparing a method whose goal is preserving cosine similarity to one designed for preserving Euclidean distance, the original features should be normalized by mapping them to the unit hypersphere before learning the binary mapping functions. To compare a method whose goal is to preserves Euclidean distance to one that preserves cosine similarity, the original feature data must be mapped to a higher dimension by including a bias term in binary mapping functions. These conditions ensure the fair comparison between different binary code methods for the task of nearest neighbor search. Our experiments show under these conditions the very simple methods (e.g. LSH and ITQ) often outperform recent state-of-the-art methods (e.g. MDSH and OK-means).", "target": "Comparing apples to apples in the evaluation of binary coding methods"}
{"id": "task1540-bd584069de0247d1a48d1660c7ca97db", "input": "This paper examines the \"K2\" network scoring metric of Cooper and Herskovits. It shows counterintuitive results from applying this metric to simple networks. One family of noninformative priors is suggested for assigning equal scores to equivalent networks.", "target": "A Bayesian Method Reexamined"}
{"id": "task1540-4a28bdd6d34d4ebf891673d8c549543f", "input": "We present an asymptotic analysis of Viterbi Training (VT) and contrast it with a more conventional Maximum Likelihood (ML) approach to parameter estimation in Hidden Markov Models. While ML estimator works by (locally) maximizing the likelihood of the observed data, VT seeks to maximize the probability of the most likely hidden state sequence. We develop an analytical framework based on a generating function formalism and illustrate it on an exactly solvable model of HMM with one unambiguous symbol. For this particular model the ML objective function is continuously degenerate. VT objective, in contrast, is shown to have only finite degeneracy. Furthermore, VT converges faster and results in sparser (simpler) models, thus realizing an automatic Occam\u2019s razor for HMM learning. For more general scenario VT can be worse compared to ML but still capable of correctly recovering most of the parameters.", "target": "Comparative Analysis of Viterbi Training and Maximum Likelihood Estimation for HMMs"}
{"id": "task1540-3f8dd5bcbf1a412f9e12a6ded7a05c86", "input": "Randomized matrix compression techniques, such as the Johnson-Lindenstrauss transform, have emerged as an effective and practical way for solving large-scale problems efficiently. With a focus on computational efficiency, however, forsaking solutions quality and accuracy becomes the trade-off. In this paper, we investigate compressed least-squares problems and propose new models and algorithms that address the issue of error and noise introduced by compression. While maintaining computational efficiency, our models provide robust solutions that are more accurate\u2014relative to solutions of uncompressed least-squares\u2014than those of classical compressed variants. We introduce tools from robust optimization together with a form of partial compression to improve the error-time trade-offs of compressed least-squares solvers. We develop an efficient solution algorithm for our Robust Partially-Compressed (RPC) model based on a reduction to a one-dimensional search. We also derive the first approximation error bounds for Partially-Compressed least-squares solutions. Empirical results comparing numerous alternatives suggest that robust and partially compressed solutions are effectively insulated against aggressive randomized transforms.", "target": "Robust Partially-Compressed Least-Squares"}
{"id": "task1540-dc3fbb7aaf664fde9848150df8a1bd04", "input": "With ever-increasing computational demand for deep learning, it is critical to investigate the implications of the numeric representation and precision of DNN model weights and activations on computational efficiency. In this work, we explore unconventional narrow-precision floating-point representations as it relates to inference accuracy and efficiency to steer the improved design of future DNN platforms. We show that inference using these custom numeric representations on production-grade DNNs, including GoogLeNet and VGG, achieves an average speedup of 7.6\u00d7 with less than 1% degradation in inference accuracy relative to a state-of-the-art baseline platform representing the most sophisticated hardware using single-precision floating point. To facilitate the use of such customized precision, we also present a novel technique that drastically reduces the time required to derive the optimal precision configuration.", "target": "DEEP NEURAL NETWORKS"}
{"id": "task1540-476c9c7793454f059041a4963c8ad286", "input": "As statistical classifiers become integrated into realworld applications, it is important to consider not only their accuracy but also their robustness to changes in the data distribution. In this paper, we consider the case where there is an unobserved confounding variable z that influences both the features x and the class variable y. When the influence of z changes from training to testing data, we find that the classifier accuracy can degrade rapidly. In our approach, we assume that we can predict the value of z at training time with some error. The prediction for z is then fed to Pearl\u2019s back-door adjustment to build our model. Because of the attenuation bias caused by measurement error in z, standard approaches to controlling for z are ineffective. In response, we propose a method to properly control for the influence of z by first estimating its relationship with the class variable y, then updating predictions for z to match that estimated relationship. By adjusting the influence of z, we show that we can build a model that exceeds competing baselines on accuracy as well as on robustness over a range of confounding relationships.", "target": "Controlling for Unobserved Confounds in Classification Using Correlational Constraints"}
{"id": "task1540-70a466ed17e649e6a6b321f33fd7d38f", "input": "This paper introduces an elemental building block which combines Dictionary Learning and Dimension Reduction (DRDL). We show how this foundational element can be used to iteratively construct a Hierarchical Sparse Representation (HSR) of a sensory stream. We compare our approach to existing models showing the generality of our simple prescription. We then perform preliminary experiments using this framework, illustrating with the example of an object recognition task using standard datasets. This work introduces the very first steps towards an integrated framework for designing and analyzing various computational tasks from learning to attention to action. The ultimate goal is building a mathematically rigorous, integrated theory of intelligence.", "target": "Learning Hierarchical Sparse Representations using Iterative Dictionary Learning and Dimension Reduction"}
{"id": "task1540-51c269a98d5f4947b926709f1f587716", "input": "Item Response Theory (IRT) allows for measuring ability of Machine Learning models as compared to a human population. However, it is difficult to create a large dataset to train the ability of deep neural network models (DNNs). We propose Crowd-Informed Fine-Tuning (CIFT) as a new training process, where a pre-trained model is fine-tuned with a specialized supplemental training set obtained via IRT modelfitting on a large set of crowdsourced response patterns. With CIFT we can leverage the specialized set of data obtained through IRT to inform parameter tuning in DNNs. We experiment with two loss functions in CIFT to represent (i) memorization of fine-tuning items and (ii) learning a probability distribution over potential labels that is similar to the crowdsourced distribution over labels to simulate crowd knowledge. Our results show that CIFT improves ability for a state-of-theart DNN model for Recognizing Textual Entailment (RTE) tasks and is generalizable to a large-scale RTE test set.", "target": "CIFT: Crowd-Informed Fine-Tuning to Improve Machine Learning Ability"}
{"id": "task1540-a445440089c447fa97388f992d327fd6", "input": "The goal of minimizing misclassification error on a training set is often just one of several real-world goals that might be defined on different datasets. For example, one may require a classifier to also make positive predictions at some specified rate for some subpopulation (fairness), or to achieve a specified empirical recall. Other real-world goals include reducing churn with respect to a previously deployed model, or stabilizing online training. In this paper we propose handling multiple goals on multiple datasets by training with dataset constraints, using the ramp penalty to accurately quantify costs, and present an efficient algorithm to approximately optimize the resulting non-convex constrained optimization problem. Experiments on both benchmark and real-world industry datasets demonstrate the effectiveness of our approach.", "target": "Satisfying Real-world Goals with Dataset Constraints"}
{"id": "task1540-a9538d9604a148018d327deb2c23bb17", "input": "In this work we introduce a conditional accelerated lazy stochastic gradient descent algorithm with optimal number of calls to a stochastic first-order oracle and convergence rate O( 1 \u03b52 ) improving over the projection-free, Online Frank-Wolfe based stochastic gradient descent of Hazan and Kale [2012] with convergence rate O( 1 \u03b54 ).", "target": "Conditional Accelerated Lazy Stochastic Gradient Descent"}
{"id": "task1540-0e0bd165550a4d2ba8bd0b35e5f5166a", "input": "Intrusion detection is so much popular since the last two decades where intrusion is attempted to break into or misuse the system. It is mainly of two types based on the intrusions, first is Misuse or signature based detection and the other is Anomaly detection. In this paper Machine learning based methods which are one of the types of Anomaly detection techniques is discussed.", "target": "A Review of Machine Learning based Anomaly Detection Techniques"}
{"id": "task1540-34f125e15f064521949404bf9d41dba9", "input": "We investigate evaluation metrics for endto-end dialogue systems where supervised labels, such as task completion, are not available. Recent works in end-to-end dialogue systems have adopted metrics from machine translation and text summarization to compare a model\u2019s generated response to a single target response. We show that these metrics correlate very weakly or not at all with human judgements of the response quality in both technical and non-technical domains. We provide quantitative and qualitative results highlighting specific weaknesses in existing metrics, and provide recommendations for future development of better automatic evaluation metrics for dialogue systems.", "target": "How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation"}
{"id": "task1540-491c52031eac41da9ae2fa36219153ae", "input": "This paper investigates the mining of class association rules with rough set approach. In data mining, an association occurs between two set of elements when one element set happen together with another. A class association rule set (CARs) is a subset of association rules with classes specified as their consequences. We present an efficient algorithm for mining the finest class rule set inspired form Apriori algorithm, where the support and confidence are computed based on the elementary set of lower approximation included in the property of rough set theory. Our proposed approach has been shown very effective, where the rough set approach for class association discovery is much simpler than the classic association method. Data Mining, RST, CAR, ARM, NAR, Bitmap, class association rules, Rough Set Theory", "target": "Class Association Rules Mining based Rough Set Method"}
{"id": "task1540-0b2d0d49abfb44f9ab1619c6707966d9", "input": "The use of microblogging platforms such as Twitter during crises has become widespread. More importantly, information disseminated by affected people contains useful information like reports of missing and found people, requests for urgent needs etc. For rapid crisis response, humanitarian organizations look for situational awareness information to understand and assess the severity of the crisis. In this paper, we present a novel framework (i) to generate abstractive summaries useful for situational awareness, and (ii) to capture sub-topics and present a short informative summary for each of these topics. A summary is generated using a two stage framework that first extracts a set of important tweets from the whole set of information through an Integer-linear programming (ILP) based optimization technique and then follows a word graph and concept event based abstractive summarization technique to produce the final summary. High accuracies obtained for all the tasks show the effectiveness of the proposed framework.", "target": "Summarizing Situational and Topical Information During Crises"}
{"id": "task1540-edc20043ee7a44febfd29f20631bf9ea", "input": "In this paper, we propose a multi-kernel classifier learning algorithm to optimize a given nonlinear and nonsmoonth multivariate classifier performance measure. Moreover, to solve the problem of kernel function selection and kernel parameter tuning, we proposed to construct an optimal kernel by weighted linear combination of some candidate kernels. The learning of the classifier parameter and the kernel weight are unified in a single objective function considering to minimize the upper boundary of the given multivariate performance measure. The objective function is optimized with regard to classifier parameter and kernel weight alternately in an iterative algorithm by using cutting plane algorithm. The developed algorithm is evaluated on two different pattern classification methods with regard to various multivariate performance measure optimization problems. The experiment results show the proposed algorithm outperforms the competing methods.", "target": "Multiple kernel multivariate performance learning using cutting plane algorithm"}
{"id": "task1540-cbe9f614479a4c7388b7de2f5fb9275b", "input": "In late 2011, Fado was elevated to the oral and intangible heritage of humanity by UNESCO. This study aims to develop a tool for automatic detection of Fado music based on the audio signal. To do this, frequency spectrum-related characteristics were captured form the audio signal: in addition to the Mel Frequency Cepstral Coefficients (MFCCs) and the energy of the signal, the signal was further analysed in two frequency ranges, providing additional information. Tests were run both in a 10-fold cross-validation setup (97.6% accuracy), and in a traditional train/test setup (95.8% accuracy). The good results reflect the fact that Fado is a very distinctive musical style.", "target": "Automatic Fado Music Classification"}
{"id": "task1540-48f17c238b0a45879d96a5b9e51814a5", "input": "A Support Vector Machine (SVM) has become a very popular machine learning method for text classification. One reason for this relates to the range of existing kernels which allow for classifying data that is not linearly separable. The linear, polynomial and RBF (Gaussian Radial Basis Function) kernel are commonly used and serve as a basis of comparison in our study. We show how to derive the primal form of the quadratic Power Kernel (PK) \u2013 also called the Negative Euclidean Distance Kernel (NDK) \u2013 by means of complex numbers. We exemplify the NDK in the framework of text categorization using the Dewey Document Classification (DDC) as the target scheme. Our evaluation shows that the power kernel produces F-scores that are comparable to the reference kernels, but is \u2013 except for the linear kernel \u2013 faster to compute. Finally, we show how to extend the NDK-approach by including the Mahalanobis distance. Keywords\u2014SVM, kernel function, text categorization", "target": "Complex Decomposition of the Negative Distance Kernel"}
{"id": "task1540-5ddac4af416d447d8e39178cb274f27a", "input": "This paper takes an approach to clustering domestic electricity load profiles that has been successfully used with data from Portugal and applies it to UK data. Clustering techniques are applied and it is found that the preferred technique in the Portuguese work (a two stage process combining Self Organised Maps and Kmeans) is not appropriate for the UK data. The work shows that up to nine clusters of households can be identified with the differences in usage profiles being visually striking. This demonstrates the appropriateness of breaking the electricity usage patterns down to more detail than the two load profiles currently published by the electricity industry. The paper details initial results using data collected in Milton Keynes around 1990. Further work is described and will concentrate on building accurate and meaningful clusters of similar electricity users in order to better direct demand side management initiatives to the most relevant target customers.", "target": "Application of a clustering framework to UK domestic electricity data"}
{"id": "task1540-58977a559317436397921c3b76f351d2", "input": "In this paper, we present the results obtained by our DKP-AOM system within the OAEI 2015 campaign. DKPAOM is an ontology merging tool designed to merge heterogeneous ontologies. In OAEI, we have participated with its ontology mapping component which serves as a basic module capable of matching large scale ontologies before their merging. This is our first successful participation in the Conference, OA4QA and Anatomy track of OAEI. DKP-AOM is participating with two versions (DKP-AOM and DKP-AOM_lite), DKP-AOM performs coherence analysis. In OA4QA track, DKPAOM out-performed in the evaluation and generated accurate alignments allowed to answer all the queries of the evaluation. We can also see its competitive results for the conference track in the evaluation initiative among other reputed systems. In the anatomy track, it has produced alignments within an allocated time and appeared in the list of systems which produce coherent results. Finally, we discuss some future work towards the development of DKP-AOM.", "target": "Initial results for Ontology Matching workshop 2015 DKP-AOM: results for OAEI 2015"}
{"id": "task1540-4fc419c4352e44e2bff5e53132cdedde", "input": "Variational inference provides a powerful tool for approximate probabilistic inference on complex, structured models. Typical variational inference methods, however, require to use inference networks with computationally tractable probability density functions. This largely limits the design and implementation of variational inference methods. We consider wild variational inference methods that do not require tractable density functions on the inference networks, and hence can be applied in more challenging cases. As an example of application, we treat stochastic gradient Langevin dynamics (SGLD) as an inference network, and use our methods to automatically adjust the step sizes of SGLD, yielding significant improvement over the hand-designed step size schemes.", "target": "TWO METHODS FOR WILD VARIATIONAL INFERENCE"}
{"id": "task1540-712456b4707e49d5ac17e23117687c8a", "input": "Suppose we are given the conditional proba\u00ad bility of one variable given some other vari\u00ad ables. Normally the full joint distribution over the conditioning variables is required to determine the probability of the conditioned variable. Under what circumstances are the marginal distributions over the conditioning variables sufficient to determine the probabil\u00ad ity of the conditioned variable? Sufficiency in this sense is equivalent to additive separa\u00ad bility of the conditional probability distribu\u00ad tion. Such separability structure is natural and can be exploited for efficient inference. Separability has a natural generalization to conditional separability. Separability provides a precise notion of hi\u00ad erarchical decomposition in temporal prob\u00ad abilistic models. Given a system that is decomposed into separable subsystems, ex\u00ad act marginal probabilities over subsystems at future points in time can be computed by propagating marginal subsystem probabil\u00ad ities, rather than complete system joint prob\u00ad abilities. Thus, separability can make exact prediction tractable. However, observations can break separability, so exact monitoring of dynamic systems remains hard.", "target": "Sufficiency, Separability and Temporal Probabilistic Models"}
{"id": "task1540-331773d1a0ff40109df555bd94062dfe", "input": "Since its appearance, Generative Adversarial Networks (GANs) [2] have received a lot of interest in the AI community. In image generation several projects showed how GANs are able to generate photorealistic images but the results so far didn\u2019t look adequate for the quality standard of visual media production industry. We present an optimized image generation process based on a Deep Convolutional Generative Adversarial Networks (DCGANs), in order to create photorealistic high-resolution images (up to 1024x1024 pixels). Furthermore, the system was fed with a limited dataset of images, less than two thousand images. All these results give more clue about future exploitation of GANs in Computer Graphics and Visual Effects.", "target": "Megapixel Size Image Creation using Generative Adversarial Networks"}
{"id": "task1540-11bcc33fc43347d7bbc74f6ef2474f3b", "input": "In this work, we present a minimal neural model for constituency parsing based on independent scoring of labels and spans. We show that this model is not only compatible with classical dynamic programming techniques, but also admits a novel greedy top-down inference algorithm based on recursive partitioning of the input. We demonstrate empirically that both prediction schemes are competitive with recent work, and when combined with basic extensions to the scoring model are capable of achieving state-of-the-art single-model performance on the Penn Treebank (91.79 F1) and strong performance on the French Treebank (82.23 F1).", "target": "A Minimal Span-Based Neural Constituency Parser"}
{"id": "task1540-fc07a62f5e61499db32a71c68a29ebf7", "input": "In this paper, we define event expression over sentences of natural language and semantic relations between events. Based on this definition, we formally consider text understanding process having events as basic unit.", "target": "Natural Language Understanding Based on Semantic Relations between Sentences"}
{"id": "task1540-f09e84051a2646a3a4d5f508e8cfb79d", "input": "We consider the problem of search through comparisons, where a user is presented with two candidate objects and reveals which is closer to her intended target. We study adaptive strategies for finding the target, that require knowledge of rank relationships but not actual distances between objects. We propose a new strategy based on rank nets, and show that for target distributions with a bounded doubling constant, it finds the target in a number of comparisons close to the entropy of the target distribution and, hence, of the optimum. We extend these results to the case of noisy oracles, and compare this strategy to prior art over multiple datasets.", "target": "Comparison-Based Learning with Rank Nets"}
{"id": "task1540-4f0290fc291e41469982ba0edaf133dc", "input": "In this article using Cuckoo Optimization Algorithm and simple additive weighting method the hybrid COAW algorithm is presented to solve multi-objective problems. Cuckoo algorithm is an efficient and structured method for solving nonlinear continuous problems. The created Pareto frontiers of the COAW proposed algorithm are exact and have good dispersion. This method has a high speed in finding the Pareto frontiers and identifies the beginning and end points of Pareto frontiers properly. In order to validation the proposed algorithm, several experimental problems were analyzed. The results of which indicate the proper effectiveness of COAW algorithm for solving multi-objective problems.", "target": "THE NEW HYBRID COAW METHOD FOR SOLVING MULTI-OBJECTIVE PROBLEMS"}
{"id": "task1540-8d78163321a748339e4e0a82be310140", "input": "Observable operator models (OOMs) and related models are one of the most important and powerful tools for modeling and analyzing stochastic systems. They can exactly describe dynamics of finite-rank systems, and be efficiently learned from data by moment based algorithms. Almost all OOM learning algorithms are developed based on the assumption of equilibrium data which is very difficult to guarantee in real life, especially for complex processes with large time scales. In this paper, we derive a nonequilibrium learning algorithm for OOMs, which dismisses this assumption and can effectively extract the equilibrium dynamics of a system from nonequilibrium observation data. In addition, we propose binless OOMs for the application of nonequilibrium learning to continuous-valued systems. In comparison with the other OOMs with continuous observations, binless OOMs can achieve consistent estimation from nonequilibrium data with only linear computational complexity.", "target": "Spectral learning of dynamic systems from nonequilibrium data\u2217"}
{"id": "task1540-a879c357c4e04d2ebdc2b37e4b46c7ae", "input": "This paper covers a number of approaches that leverage Artificial Intelligence algorithms and techniques to aid Unmanned Combat Aerial Vehicle (UCAV) autonomy. An analysis of current approaches to autonomous control is provided followed by an exploration of how these techniques can be extended and enriched with AI techniques including Artificial Neural Networks (ANN), Ensembling and Reinforcement Learning (RL) to evolve control strategies for UCAVs.", "target": "Artificial Intelligence Approaches To UCAV Autonomy"}
{"id": "task1540-c45fb0c9d9e2400bba125c95b497d254", "input": "Gaussian Process bandit optimization has emerged as a powerful tool for optimizing noisy black box functions. One example in machine learning is hyper-parameter optimization where each evaluation of the target function requires training a model which may involve days or even weeks of computation. Most methods for this so-called \u201cBayesian optimization\u201d only allow sequential exploration of the parameter space. However, it is often desirable to propose batches or sets of parameter values to explore simultaneously, especially when there are large parallel processing facilities at our disposal. Batch methods require modeling the interaction between the different evaluations in the batch, which can be expensive in complex scenarios. In this paper, we propose a new approach for parallelizing Bayesian optimization by modeling the diversity of a batch via Determinantal point processes (DPPs) whose kernels are learned automatically. This allows us to generalize a previous result as well as prove better regret bounds based on DPP sampling. Our experiments on a variety of synthetic and real-world robotics and hyper-parameter optimization tasks indicate that our DPP-based methods, especially those based on DPP sampling, outperform state-of-the-art methods.", "target": "Batched Gaussian Process Bandit Optimization via Determinantal Point Processes"}
{"id": "task1540-7a5f137302ae4023bb1b8b8e7ff5991d", "input": "Kimmo Kettunen, Eetu M\u00e4kel\u00e4, Teemu Ruokolainen, Juha Kuokkala and Laura L\u00f6fberg 1 National Library of Finland, Centre for Preservation and Digitization, Mikkeli, Finland kimmo.kettunen@helsinki.fi 2 Aalto University, Semantic Computing Research Group, Espoo, Finland eetu.makela@aalto.fi 3 National Library of Finland, Centre for Preservation and Digitization, Mikkeli, Finland teemu.ruokolainen@helsinki.fi 4 University of Helsinki, Department of Modern Languages, Helsinki, Finland juha.kuokkala@helsinki.fi 5 Department of Linguistics and English Language, Lancaster University, UK l.lofberg@lancaster.ac.uk", "target": "Old Content and Modern Tools \u2013 Searching Named Entities in a Finnish OCRed Historical Newspaper Collection 1771\u20131910"}
{"id": "task1540-9095b39e92ab47629f9911f26990715c", "input": "A pseudo independent (PI) model is a proba\u00ad bilistic domain model (PDM) where proper subsets of a set of collectively dependent variables display marginal independence. PI models cannot be learned correctly by many algorithms that rely on a single link search. Earlier work on learning PI models has sug\u00ad gested a straightforward multi-link search al\u00ad gorithm. However, when a domain contains recursively embedded PI submodels, it may escape the detection of such an algorithm. In this paper, we propose an improved al\u00ad gorithm that ensures the learning of all em\u00ad bedded PI submodels whose sizes are upper bounded by a predetermined parameter. We show that this improved learning capability only increases the complexity slightly beyond that of the previous algorithm. The perfor\u00ad mance of the new algorithm is demonstrated through experiment.", "target": "Learning Belief Networks in Domains with Recursively Embedded Pseudo Independent Submodels"}
{"id": "task1540-de2f6f8d2f834264bb1310fbc16ce996", "input": "In Computer Vision, problem of identifying or classifying the objects present in an image is called Object Categorization. It is a challenging problem, especially when the images have clutter background, occlusions or different lighting conditions. Many vision features have been proposed which aid object categorization even in such adverse conditions. Past research has shown that, employing multiple features rather than any single features leads to better recognition. Multiple Kernel Learning (MKL) framework has been developed for learning an optimal combination of features for object categorization. Existing MKL methods use linear combination of base kernels which may not be optimal for object categorization. Real-world object categorization may need to consider complex combination of kernels(non-linear) and not only linear combination. Evolving non-linear functions of base kernels using Genetic Programming is proposed in this report. Experiment results show that non-kernel generated using genetic programming gives good accuracy as compared to linear combination of kernels.", "target": "Finding Optimal Combination of Kernels using Genetic Programming"}
{"id": "task1540-49bbea4673a74c889842b1bd5578c269", "input": "L\u2019articolo presenta un\u2019introduzione all\u2019Intelligenza Artificiale (IA) in forma divulgativa e informale ma precisa. L\u2019articolo affronta prevalentemente gli aspetti informatici della disciplina, presentando varie tecniche usate nei sistemi di IA e dividendole in simboliche e subsimboliche. L\u2019articolo si conclude presentando il dibattito in corso sull\u2019IA e in particolare sui vantaggi e i pericoli che sono stati individuati, terminando con l\u2019opinione dell\u2019autore al riguardo.", "target": "Introduzione all\u2019Intelligenza Artificiale\u2217"}
{"id": "task1540-a7c9c482e64e45c4a31b348bdb509dda", "input": "Protein subcellular localization prediction is an important and challenging problem. The traditional biology experiments are expensive and time-consuming, so more and more research interests tend to a series of machine learning approaches for predicting protein subcellular location. There are two main difficult problems among the existing state-of-the-art methods. First, most of the existing techniques are designed to deal with the multi-class but not the multi-label classification, which ignores the connection between the multiple labels. In reality, multiple location proteins implicate that there are vital and unique biological significances worthy of special focus, which cannot be ignored. Second, the techniques for handling imbalanced data in multi-label classification problem is significant but less. For solving the two issues, we have developed an ensemble multi-label classifier called HPSLPred which can be applied for the multi-label classification with imbalanced protein source. For the conveniences of users, a user-friendly webserver for HPSLPred was established at http://server.malab.cn/HPSLPred.", "target": "HPSLPred: An Ensemble Multi-label Classifier for Human Protein Subcellular Location Prediction with Imbalanced Source"}
{"id": "task1540-d081da5ec9bc465cbda3d7dfa6678f20", "input": "We exhibit a strong link between frequentist PAC-Bayesian bounds and the Bayesian marginal likelihood. That is, for the negative log-likelihood loss function, we show that the minimization of PAC-Bayesian generalization bounds maximizes the Bayesian marginal likelihood. This provides an alternative explanation to the Bayesian Occam\u2019s razor criteria, under the assumption that the data is generated by a i.i.d. distribution. Moreover, as the negative log-likelihood is an unbounded loss function, we motivate and propose a PAC-Bayesian theorem tailored for the sub-Gamma loss family, and we show that our approach is sound on classical Bayesian linear regression tasks.", "target": "PAC-Bayesian Theory Meets Bayesian Inference"}
{"id": "task1540-bb33a5a9bdab437faebb8b154991e54b", "input": "This paper addresses the issue of model selection for hidden Markov models (HMMs). We generalize factorized asymptotic Bayesian inference (FAB), which has been recently developed for model selection on independent hidden variables (i.e., mixture models), for time-dependent hidden variables. As with FAB in mixture models, FAB for HMMs is derived as an iterative lower bound maximization algorithm of a factorized information criterion (FIC). It inherits, from FAB for mixture models, several desirable properties for learning HMMs, such as asymptotic consistency of FIC with marginal log-likelihood, a shrinkage effect for hidden state selection, monotonic increase of the lower FIC bound through the iterative optimization. Further, it does not have a tunable hyper-parameter, and thus its model selection process can be fully automated. Experimental results shows that FAB outperforms states-of-the-art variational Bayesian HMM and non-parametric Bayesian HMM in terms of model selection accuracy and computational efficiency.", "target": "Factorized Asymptotic Bayesian Hidden Markov Models"}
{"id": "task1540-78d85d67b29d471fb8296dbb69dc5f43", "input": "P lanning problems where effects of actions are non-deterministic can be modeled a8 Markov decision processes. Planning prob\u00ad lems are usually goal-directed. This paper proposes several techniques for exploiting the goal-directedness to accelerate value itera\u00ad tion, a standard algorithm for solving Markov decision processes. Empirical studies have shown that the techniques can bring about significant speedups.", "target": "Fast Value Iteration for Goal-Directed Markov Decision Processes"}
{"id": "task1540-31b23df662e44769a2bb3f93c9918515", "input": "Statistics pedagogy values using a variety of examples. Thanks to text resources on the Web, and since statistical packages have the ability to analyze string data, it is now easy to use language-based examples in a statistics class. Three such examples are discussed here. First, many types of wordplay (e.g., crosswords and hangman) involve finding words with letters that satisfy a certain pattern. Second, linguistics has shown that idiomatic pairs of words often appear together more frequently than chance. For example, in the Brown Corpus, this is true of the phrasal verb to throw up (p-value=7.92E10.) Third, a pangram contains all the letters of the alphabet at least once. These are searched for in Charles Dickens' A Christmas Carol, and their lengths are compared to the expected value given by the unequal probability coupon collector's problem as well as simulations.", "target": "Language-based Examples in the Statistics Classroom"}
{"id": "task1540-59c7852e8bcf42bf9a3a7eb9bdb4fa5f", "input": "We describe a graphical representation of probabilistic relationships-an alternative to the Bayesian network-called a dependency network. Like a Bayesian network, a depen\u00ad dency network has a graph and a probabil\u00ad ity component. The graph component is a (cyclic) directed graph such that a node's parents render that node independent of all other nodes in the network. The probabil\u00ad ity component consists of the probability of a node given its parents for each node (as in a Bayesian network) . We identify several ba\u00ad sic properties of this representation, and de\u00ad scribe its use in collaborative filtering (the task of predicting preferences) and the visu\u00ad alization of predictive relationships.", "target": "Dependency Networks for Collaborative Filtering and Data Visualization"}
{"id": "task1540-a954d6827a4e49aca9a9177ca9760786", "input": "Building large models with parameter sharing accounts for most of the success of deep convolutional neural networks (CNNs). In this paper, we propose doubly convolutional neural networks (DCNNs), which significantly improve the performance of CNNs by further exploring this idea. In stead of allocating a set of convolutional filters that are independently learned, a DCNN maintains groups of filters where filters within each group are translated versions of each other. Practically, a DCNN can be easily implemented by a two-step convolution procedure, which is supported by most modern deep learning libraries. We perform extensive experiments on three image classification benchmarks: CIFAR-10, CIFAR-100 and ImageNet, and show that DCNNs consistently outperform other competing architectures. We have also verified that replacing a convolutional layer with a doubly convolutional layer at any depth of a CNN can improve its performance. Moreover, various design choices of DCNNs are demonstrated, which shows that DCNN can serve the dual purpose of building more accurate models and/or reducing the memory footprint without sacrificing the accuracy.", "target": "Doubly Convolutional Neural Networks"}
{"id": "task1540-d38c7ec9f215480083d287e57ebd88d5", "input": "Variational autoencoders (VAE) are directed generative models that learn factorial latent variables. As noted by Burda et al. (2015), these models exhibit the problem of factor overpruning where a significant number of stochastic factors fail to learn anything and become inactive. This can limit their modeling power and their ability to learn diverse and meaningful latent representations. In this paper, we evaluate several methods to address this problem and propose a more effective model-based approach called the epitomic variational autoencoder (eVAE). The so-called epitomes of this model are groups of mutually exclusive latent factors that compete to explain the data. This approach helps prevent inactive units since each group is pressured to explain the data. We compare the approaches with qualitative and quantitative results on MNIST and TFD datasets. Our results show that eVAE makes efficient use of model capacity and generalizes better than VAE.", "target": "Tackling Over-pruning in Variational Autoencoders"}
{"id": "task1540-5da0c50136ab48869cacc32d3e91ddf4", "input": "Given a graphical model, one essential problem is MAP inference, that is, finding the most likely configuration of states according to the model. Although this problem is NP-hard, large instances can be solved in practice. A major open question is to explain why this is true. We give a natural condition under which we can provably perform MAP inference in polynomial time. We require that the number of fractional vertices in the LP relaxation exceeding the optimal solution is bounded by a polynomial in the problem size. This resolves an open question by Dimakis, Gohari, and Wainwright. In contrast, for general LP relaxations of integer programs, known techniques can only handle a constant number of fractional vertices whose value exceeds the optimal solution. We experimentally verify this condition and demonstrate how efficient various integer programming methods are at removing fractional solutions.", "target": "Exact MAP Inference by Avoiding Fractional Vertices"}
{"id": "task1540-72580d448f3b4de5beea5e365b71eea1", "input": "Recently emerged intelligent assistants on smartphones and home electronics (e.g., Siri and Alexa) can be seen as novel hybrids of domain-specific taskoriented spoken dialogue systems and open-domain non-task-oriented ones. To realize such hybrid dialogue systems, this paper investigates determining whether or not a user is going to have a chat with the system. To address the lack of benchmark datasets for this task, we construct a new dataset consisting of 15, 160 utterances collected from the real log data of a commercial intelligent assistant (and will release the dataset to facilitate future research activity). In addition, we investigate using tweets and Web search queries for handling open-domain user utterances, which characterize the task of chat detection. Experiments demonstrated that, while simple supervised methods are effective, the use of the tweets and search queries further improves the F1-score from 86.21 to 87.53.", "target": "Chat Detection in an Intelligent Assistant: Combining Task-oriented and Non-task-oriented Spoken Dialogue Systems"}
{"id": "task1540-4debc447ce1b4f40ae1fd045329d5c53", "input": "In this paper, we propose a context-aware keyword spotting model employing a character-level recurrent neural network (RNN) for spoken term detection in continuous speech. The RNN is end-toend trained with connectionist temporal classification (CTC) to generate the probabilities of character and word-boundary labels. There is no need for the phonetic transcription, senone modeling, or system dictionary in training and testing. Also, keywords can easily be added and modified by editing the text based keyword list without retraining the RNN. Moreover, the unidirectional RNN processes an infinitely long input audio streams without pre-segmentation and keywords are detected with low-latency before the utterance is finished. Experimental results show that the proposed keyword spotter significantly outperforms the deep neural network (DNN) and hidden Markov model (HMM) based keyword-filler model even with less computations.", "target": "Online Keyword Spotting with a Character-Level Recurrent Neural Network"}
{"id": "task1540-2df08e9bf29f479591c1ab567316771b", "input": "Square grids are commonly used in robotics and game development to model an agent\u2019s environment, and well known in Artificial Intelligence heuristic search algorithms (A*, JPS, Theta* etc.) are utilized for grid path planning. A lot of research in this area has been focused so far on finding the shortest paths while in many applications producing smooth paths is preferable. In our work, we study the problem of generating smooth grid paths and concentrate on angle constrained path planning. We put angle constrained path planning problem formally and present a new algorithm of solving it \u2013 LIAN. We examine LIAN both theoretically and empirically. On the theoretical side, we prove that LIAN is sound and complete (under well-defined restrictions). On the experimental side, we show that LIAN significantly outperforms competitors in ability to find solutions under tough resource constraints and in computational efficiency.", "target": "Grid-based angle-constrained path planning"}
{"id": "task1540-c8be7cf1e8914b85ab857156e4eb7ef3", "input": "We propose a novel framework for the analysis of learning algorithms that allows us to say when such algorithms can and cannot generalize certain patterns from training data to test data. In particular we focus on situations where the rule that must be learned concerns two components of a stimulus being identical. We call such a basis for discrimination an identitybased rule. Identity-based rules have proven to be difficult or impossible for certain types of learning algorithms to acquire from limited datasets. This is in contrast to human behaviour on similar tasks. Here we provide a framework for rigorously establishing which learning algorithms will fail at generalizing identity-based rules to novel stimuli. We use this framework to show that such algorithms are unable to generalize identitybased rules to novel inputs unless trained on virtually all possible inputs. We demonstrate these results computationally with a multilayer feedforward neural network.", "target": "Which Learning Algorithms Can Generalize Identity-Based Rules to Novel Inputs?"}
{"id": "task1540-897edcc5b256414291231e96b3979eba", "input": "In action domains where agents may have erroneous beliefs, reasoning about the effects of actions involves reasoning about belief change. In this paper, we use a transition system approach to reason about the evolution of an agent\u2019s beliefs as actions are executed. Some actions cause an agent to perform belief revision while others cause an agent to perform belief update, but the interaction between revision and update can be nonelementary. We present a set of rationality properties describing the interaction between revision and update, and we introduce a new class of belief change operators for reasoning about alternating sequences of revisions and updates. Our belief change operators can be characterized in terms of a natural shifting operation on total pre-orderings over interpretations. We compare our approach with related work on iterated belief change due to action, and we conclude with some directions for future research.", "target": "Iterated Belief Change Due to Actions and Observations"}
{"id": "task1540-c2222454f0794c2ea2ace638642da20f", "input": "Sparse coding is a common approach to learning local features for object recognition. Recently, there has been an increasing interest in learning features from spatio-temporal, binocular, or other multi-observation data, where the goal is to encode the relationship between images rather than the content of a single image. We provide an analysis of multi-view feature learning, which shows that hidden variables encode transformations by detecting rotation angles in the eigenspaces shared among multiple image warps. Our analysis helps explain recent experimental results showing that transformation-specific features emerge when training complex cell models on videos. Our analysis also shows that transformation-invariant features can emerge as a by-product of learning representations of transformations.", "target": "On multi-view feature learning"}
{"id": "task1540-3cf3b6ff1e824caea849084a71f83a35", "input": "The paper analyzes dynamic epistemic logic from a topological perspective. The main contribution consists of a framework in which dynamic epistemic logic satisfies the requirements for being a topological dynamical system thus interfacing discrete dynamic logics with continuous mappings of dynamical systems. The setting is based on a notion of logical convergence, demonstratively equivalent with convergence in Stone topology. Presented is a flexible, parametrized family of metrics inducing the latter, used as an analytical aid. We show maps induced by action model transformations continuous with respect to the Stone topology and present results on the recurrent behavior of said maps.", "target": "Convergence, Continuity and Recurrence in Dynamic Epistemic Logic"}
{"id": "task1540-a1d9591ac91e453a80db6219b13bb9e7", "input": "Sound events often occur in unstructured environments where they exhibit wide variations in their frequency content and temporal structure. Convolutional neural networks (CNN) are able to extract higher level features that are invariant to local spectral and temporal variations. Recurrent neural networks (RNNs) are powerful in learning the longer term temporal context in the audio signals. CNNs and RNNs as classifiers have recently shown improved performances over established methods in various sound recognition tasks. We combine these two approaches in a Convolutional Recurrent Neural Network (CRNN) and apply it on a polyphonic sound event detection task. We compare the performance of the proposed CRNN method with CNN, RNN, and other established methods, and observe a considerable improvement for four different datasets consisting of everyday sound events.", "target": "Convolutional Recurrent Neural Networks for Polyphonic Sound Event Detection"}
{"id": "task1540-9b43f0e947194bc1a5fc3fc39964f045", "input": "In this study we address the problem of training a neuralnetwork for language identification using both labeled and unlabeled speech samples in the form of i-vectors. We propose a neural network architecture that can also handle out-of-set languages. We utilize a modified version of the recently proposed Ladder Network semisupervised training procedure that optimizes the reconstruction costs of a stack of denoising autoencoders. We show that this approach can be successfully applied to the case where the training dataset is composed of both labeled and unlabeled acoustic data. The results show enhanced language identification on the NIST 2015 language identification dataset.", "target": "A Semisupervised Approach for Language Identification based on Ladder Networks"}
{"id": "task1540-58f8ce7c39944b4d988455d99de10622", "input": "Unsupervised models of dependency parsing typically require large amounts of clean, unlabeled data plus gold-standard part-of-speech tags. Adding indirect supervision (e.g. language universals and rules) can help, but we show that obtaining small amounts of direct supervision\u2014here, partial dependency annotations\u2014provides a strong balance between zero and full supervision. We adapt the unsupervised ConvexMST dependency parser to learn from partial dependencies expressed in the Graph Fragment Language. With less than 24 hours of total annotation, we obtain 7% and 17% absolute improvement in unlabeled dependency scores for English and Spanish, respectively, compared to the same parser using only universal grammar constraints.", "target": "Fill it up: Exploiting partial dependency annotations in a minimum spanning tree parser"}
{"id": "task1540-d0de5041b24446768cff747c68e8b80e", "input": "We develop new representations for the L\u00e9vy measures of the beta and gamma processes. These representations are manifested in terms of an infinite sum of well-behaved (proper) beta and gamma distributions. Further, we demonstrate how these infinite sums may be truncated in practice, and explicitly characterize truncation errors. We also perform an analysis of the characteristics of posterior distributions, based on the proposed decompositions. The decompositions provide new insights into the beta and gamma processes (and their generalizations), and we demonstrate how the proposed representation unifies some properties of the two. This paper is meant to provide a rigorous foundation for and new perspectives on L\u00e9vy processes, as these are of increasing importance in machine learning.", "target": "Le\u0301vy Measure Decompositions for the Beta and Gamma Processes"}
{"id": "task1540-07016232358b4b7aa267738e054c15f2", "input": "This paper describes a new method for reducing the error in a classifier. It uses a weight adjustment update, but includes the very simple rule of either adding or subtracting the adjustment, based on whether the data point is currently larger or smaller than the desired value, and on a pointby-point basis. This gives added flexibility to the convergence procedure, where through a series of transpositions, values far away can continue towards the desired value, whereas values that are originally much closer can oscillate from one side to the other. Tests show that the method can successfully classify some known datasets. It can also work in a batch mode, with reduced training times and can be used as part of a neural network, or classifiers in general. There are also some updates on an earlier wave shape paper.", "target": "A New Oscillating-Error Technique for Classifiers"}
{"id": "task1540-9e6a278e2f024853bbd9c857fb510462", "input": "As probabilistic systems gain popularity and are coming into wider use, the need for a mechanism that explains the system's findings and recom\u00ad mendations becomes more critical. The system will also need a mechanism for ordering compet\u00ad ing explanations. We examine two representa\u00ad tive approaches to explanation in the literature\u00ad one due to Gardenfors and one due to Pearl-and show that both suffer from significant problems. We propose an approach to defining a notion of \"better explanation\" that combines some of the features of both together with more recent work by Pearl and others on causality.", "target": "Defining Explanation in Probabilistic Systems"}
{"id": "task1540-fb2ec5c938a840f891408fcec9f8a3de", "input": "This paper takes an information visualization perspective to visual representations in the general SOM paradigm. This involves viewing SOM-based visualizations through the eyes of Bertin's and Tufte's theories on data graphics. The regular grid shape of the Self-Organizing Map (SOM), while being a virtue for linking visualizations to it, restricts representation of cluster structures. From the viewpoint of information visualization, this paper provides a general, yet simple, solution to projection-based coloring of the SOM that reveals structures. First, the proposed color space is easy to construct and customize to the purpose of use, while aiming at being perceptually correct and informative through two separable dimensions. Second, the coloring method is not dependent on any specific method of projection, but is rather modular to fit any objective function suitable for the task at hand. The cluster coloring is illustrated on two datasets: the iris data, and welfare and poverty indicators. KeywordsSelf-Organizing Maps (SOMs); cluster coloring;", "target": "Cluster coloring of the Self-Organizing Map: An information visualization perspective"}
{"id": "task1540-07f4642ac9994035a2e17fb66407011c", "input": "Word embedding, specially with its recent developments, promises a quantification of the similarity between terms. However, it is not clear to which extent this similarity value can be genuinely meaningful and useful for subsequent tasks. We explore how the similarity score obtained from the models is really indicative of term relatedness. We first observe and quantify the uncertainty factor of the word embedding models regarding to the similarity value. Based on this factor, we introduce a general threshold on various dimensions which effectively filters the highly related terms. Our evaluation on four information retrieval collections supports the effectiveness of our approach as the results of the introduced threshold are significantly better than the baseline while being equal to or statistically indistinguishable from the optimal results.", "target": "Uncertainty in Neural Network Word Embedding Exploration of Threshold for Similarity"}
{"id": "task1540-949a4960246742169197d2d9e418a818", "input": "Anomaly detection is an important task in many real world applications such as fraud detection, suspicious activity detection, health care monitoring etc. In this paper, we tackle this problem from supervised learning perspective in online learning setting. We maximize well known Gmean metric for classimbalance learning in online learning framework. Specifically, we show that maximizing Gmean is equivalent to minimizing a convex surrogate loss function and based on that we propose novel online learning algorithm for anomaly detection. We then show, by extensive experiments, that the performance of the proposed algorithm with respect to sum metric is as good as a recently proposed Cost-Sensitive Online Classification(CSOC) algorithm for class-imbalance learning over various benchmarked data sets while keeping running time close to the perception algorithm. Our another conclusion is that other competitive online algorithms do not perform consistently over data sets of varying size. This shows the potential applicability of our proposed approach.", "target": "Online Anomaly Detection via Class-Imbalance Learning"}
{"id": "task1540-4a661474c1c6404eb019f76c5f1e989d", "input": "The redundant features existing in high dimensional datasets always affect the performance of learning and mining algorithms. How to detect and remove them is an important research topic in machine learning and data mining research. In this paper, we propose a graph based approach to find and remove those redundant features automatically for high dimensional data. Based on sparse learning based unsupervised feature selection framework, Sparse Feature Graph (SFG) is introduced not only to model the redundancy between two features, but also to disclose the group redundancy between two groups of features. With SFG, we can divide the whole features into different groups, and improve the intrinsic structure of data by removing detected redundant features. With accurate data structure, quality indicator vectors can be obtained to improve the learning performance of existing unsupervised feature selection algorithms such as multi-cluster feature selection (MCFS). Our experimental results on benchmark datasets show that the proposed SFG and feature redundancy remove algorithm can improve the performance of unsupervised feature selection algorithms consistently.", "target": "Automatically Redundant Features Removal for Unsupervised Feature Selection via Sparse Feature Graph"}
{"id": "task1540-6543fa794fe84368bdc1a309765b153b", "input": "Artificial neural networks are powerful pattern classifiers; however, they have been surpassed in accuracy by methods such as support vector machines and random forests that are also easier to use and faster to train. Backpropagation, which is used to train artificial neural networks, suffers from the herd effect problem which leads to long training times and limit classification accuracy. We use the disjunctive normal form and approximate the boolean conjunction operations with products to construct a novel network architecture. The proposed model can be trained by minimizing an error function and it allows an effective and intuitive initialization which solves the herdeffect problem associated with backpropagation. This leads to state-of-the art classification accuracy and fast training times. In addition, our model can be jointly optimized with convolutional features in an unified structure leading to state-of-the-art results on computer vision problems with fast convergence rates. A GPU implementation of LDNN with optional convolutional features is also available", "target": "Disjunctive Normal Networks"}
{"id": "task1540-bb32e2fc0bce46b3bbf0afdf41bac42b", "input": "We propose multi-way, multilingual neural machine translation. The proposed approach enables a single neural translation model to translate between multiple languages, with a number of parameters that grows only linearly with the number of languages. This is made possible by having a single attention mechanism that is shared across all language pairs. We train the proposed multiway, multilingual model on ten language pairs from WMT\u201915 simultaneously and observe clear performance improvements over models trained on only one language pair. In particular, we observe that the proposed model significantly improves the translation quality of low-resource language pairs.", "target": "Multi-Way, Multilingual Neural Machine Translation with a Shared Attention Mechanism"}
{"id": "task1540-dde06b69e89243c2bc37293de2b4d706", "input": "For high level path planning, environments are usually modeled as distance graphs, and path planning problems are reduced to com\u00ad puting the shortest path in distance graphs. One major drawback of this modeling is the inability to model uncertainties, which are of\u00ad ten encountered in practice. In this paper, a new tool, called U-graph, is proposed for environment modeling. A U-graph is an ex\u00ad tension of distance graphs with the ability to handle a kind of uncertainty. By model\u00ad ing an uncertain environment as a U-graph, and a navigation problem as a Markovian decision process, we can precisely define a new optimality criterion for navigation plans, and more importantly, we can come up with a general algorithm for computing optimal plans for navigation tasks.", "target": "High Level Path Planning with Uncertainty"}
{"id": "task1540-6c564840a1bb4f788546a92bb2575788", "input": "We propose a model to learn visually grounded word embeddings (vis-w2v) to capture visual notions of semantic relatedness. While word embeddings trained using text have been extremely successful, they cannot uncover notions of semantic relatedness implicit in our visual world. For instance, visual grounding can help us realize that concepts like eating and staring at are related, since when people are eating something, they also tend to stare at the food. Grounding a rich variety of relations like eating and stare at in vision is a challenging task, despite recent progress in vision. We realize the visual grounding for words depends on the semantics of our visual world, and not the literal pixels. We thus use abstract scenes created from clipart to provide the visual grounding. We find that the embeddings we learn capture fine-grained visually grounded notions of semantic relatedness. We show improvements over text only word embeddings (word2vec) on three tasks: common-sense assertion classification, visual paraphrasing and text-based image retrieval. Our code and datasets will be available online.", "target": "Visual Word2Vec (vis-w2v): Learning Visually Grounded Word Embeddings Using Abstract Scenes"}
{"id": "task1540-4f94cdfb177347fdae09d628f972b653", "input": "This work focuses on the rapid development of linguistic annotation tools for resource-poor languages. We experiment several cross-lingual annotation projection methods using Recurrent Neural Networks (RNN) models. The distinctive feature of our approach is that our multilingual word representation requires only a parallel corpus between the source and target language. More precisely, our method has the following characteristics: (a) it does not use word alignment information, (b) it does not assume any knowledge about foreign languages, which makes it applicable to a wide range of resource-poor languages, (c) it provides truly multilingual taggers. We investigate both uniand bi-directional RNN models and propose a method to include external information (for instance low level information from POS) in the RNN to train higher level taggers (for instance, super sense taggers). We demonstrate the validity and genericity of our model by using parallel corpora (obtained by manual or automatic translation). Our experiments are conducted to induce cross-lingual POS and super sense taggers.", "target": "Inducing Multilingual Text Analysis Tools Using Bidirectional Recurrent Neural Networks"}
{"id": "task1540-d4da71c18d5e40df83bcb8c122a5254e", "input": "In this paper, we propose a method which uses semi-supervised convolutional neural networks (CNNs) to select in-domain training data for statistical machine translation. This approach is particularly effective when only tiny amounts of in-domain data are available. The in-domain data and randomly sampled general-domain data are used to train a data selection model with semi-supervised CNN, then this model computes domain relevance scores for all the sentences in the generaldomain data set. The sentence pairs with top scores are selected to train the system. We carry out experiments on 4 language directions with three test domains. Compared with strong baseline systems trained with large amount of data, this method can improve the performance up to 3.1 BLEU. Its performances are significant better than three state-of-the-art language model based data selection methods. We also show that the in-domain data used to train the selection model could be as few as 100 sentences, which makes finegrained topic-dependent translation adaptation possible.", "target": "Semi-supervised Convolutional Networks for Translation Adaptation with Tiny Amount of In-domain Data"}
{"id": "task1540-6f7ff285d1f3451fa4977e84061bf44f", "input": "The theory of actual causality, defined by Halpern and Pearl, and its quantitative measure \u2013 the degree of responsibility \u2013 was shown to be extremely useful in various areas of computer science due to a good match between the results it produces and our intuition. In this paper, I describe the applications of causality to formal verification, namely, explanation of counter-examples, refinement of coverage metrics, and symbolic trajectory evaluation. I also briefly discuss recent applications of causality to legal reasoning.", "target": "Causality and Responsibility for Formal Verification and Beyond"}
{"id": "task1540-8357ec923c8f4b6aa9bb2c3e237d6bed", "input": "We present Grid Beam Search (GBS), an algorithm which extends beam search to allow the inclusion of pre-specified lexical constraints. The algorithm can be used with any model that generates a sequence \u0177 = {y0 . . . yT }, by maximizing p(y|x) = \u220f t p(yt|x; {y0 . . . yt\u22121}). Lexical constraints take the form of phrases or words that must be present in the output sequence. This is a very general way to incorporate additional knowledge into a model\u2019s output without requiring any modification of the model parameters or training data. We demonstrate the feasibility and flexibility of Lexically Constrained Decoding by conducting experiments on Neural Interactive-Predictive Translation, as well as Domain Adaptation for Neural Machine Translation. Experiments show that GBS can provide large improvements in translation quality in interactive scenarios, and that, even without any user input, GBS can be used to achieve significant gains in performance in domain adaptation scenarios.", "target": "Lexically Constrained Decoding for Sequence Generation Using Grid Beam Search"}
{"id": "task1540-7cdbb4ace1af47a5b3da8b10d83e9007", "input": "For agents and robots to become more useful, they must be able to quickly learn from non-technical users. This paper investigates the problem of interactively learning behaviors communicated by a human teacher using positive and negative feedback. Much previous work on this problem has made the assumption that people provide feedback for decisions that is dependent on the behavior they are teaching and is independent from the learner\u2019s current policy. We present empirical results that show this assumption to be false\u2014whether human trainers give a positive or negative feedback for a decision is influenced by the learner\u2019s current policy. We argue that policy-dependent feedback, in addition to being commonplace, enables useful training strategies from which agents should benefit. Based on this insight, we introduce Convergent Actor-Critic by Humans (COACH), an algorithm for learning from policy-dependent feedback that converges to a local optimum. Finally, we demonstrate that COACH can successfully learn multiple behaviors on a physical robot, even with noisy image features.", "target": "Interactive Learning from Policy-Dependent Human Feedback"}
{"id": "task1540-ab2cdace8adf4f8686fef878f44a4f36", "input": "Data mining practitioners are facing challenges from data with network structure. In this paper, we address a specific class of global-state networks which comprises of a set of network instances sharing a similar structure yet having different values at local nodes. Each instance is associated with a global state which indicates the occurrence of an event. The objective is to uncover a small set of discriminative subnetworks that can optimally classify global network values. Unlike most existing studies which explore an exponential subnetwork space, we address this difficult problem by adopting a space transformation approach. Specifically, we present an algorithm that optimizes a constrained dualobjective function to learn a low-dimensional subspace that is capable of discriminating networks labelled by different global states, while reconciling with common network topology sharing across instances. Our algorithm takes an appealing approach from spectral graph learning and we show that the globally optimum solution can be achieved via matrix eigen-decomposition.", "target": "Discriminative Subnetworks with Regularized Spectral Learning for Global-state Network Data"}
{"id": "task1540-59769ce69f3d4f349219be4e85f1072f", "input": "We propose an effective technique to solving review-level sentiment classification problem by using sentence-level polarity correction. Our polarity correction technique takes into account the consistency of the polarities (positive and negative) of sentences within each product review before performing the actual machine learning task. While sentences with inconsistent polarities are removed, sentences with consistent polarities are used to learn state-of-the-art classifiers. The technique achieved better results on different types of products reviews and outperforms baseline models without the correction technique. Experimental results show an average of 82% F-measure on four different product review domains.", "target": "Review-Level Sentiment Classification with Sentence-Level Polarity Correction"}
{"id": "task1540-f95ba1cf431e49ae9d7c6fc4919a93c3", "input": "Extracting time expressions from free text is a fundamental task for many applications. We analyze the time expressions from four datasets and observe that only a small group of words are used to express time information, and the words in time expressions demonstrate similar syntactic behaviour. Based on the observations, we propose a type-based approach, named SynTime, to recognize time expressions. Specifically, we define three main syntactic types, namely time token, modifier, and numeral, to group time-related regular expressions over tokens. On the types we design simple heuristic rules to recognize time expressions. In recognition, SynTime first identifies the time tokens from raw text, then searches their surroundings for modifiers and numerals to form time segments, and finally merges the time segments to time expressions. As a light-weight rule-based tagger, SynTime runs in real time, and can be easily expanded by simply adding keywords for the text of different types and of different domains. Experiment results show that SynTime outperforms state-of-the-art methods on benchmark datasets and tweets data.", "target": "Time Expression Analysis and Recognition Using Syntactic Types and Simple Heuristic Rules"}
{"id": "task1540-8770c24e3b8b4b63ad29ff37d0b30cd1", "input": "Recommendation system is a common demand in daily life and matrix completion is a widely adopted technique for this task. However, most matrix completion methods lack semantic interpretation and usually result in weak-semantic recommendations. To this end, this paper proposes a Semantic Analysis approach for Recommendation systems (SAR), which applies a two-level hierarchical generative process that assigns semantic properties and categories for user and item. SAR learns semantic representations of users/items merely from user ratings on items, which offers a new path to recommendation by semantic matching with the learned representations. Extensive experiments demonstrate SAR outperforms other state-of-the-art baselines substantially.", "target": "SAR: A Semantic Analysis Approach for Recommendation"}
{"id": "task1540-16bbd4f4682640d39bda6b1926548f90", "input": "This paper proposes a novel approach for relation extraction from free text which is trained to jointly use information from the text and from existing knowledge. Our model is based on two scoring functions that operate by learning low-dimensional embeddings of words and of entities and relationships from a knowledge base. We empirically show on New York Times articles aligned with Freebase relations that our approach is able to efficiently use the extra information provided by a large subset of Freebase data (4M entities, 23k relationships) to improve over existing methods that rely on text features alone.", "target": "Connecting Language and Knowledge Bases with Embedding Models for Relation Extraction"}
{"id": "task1540-62ffdc104a164f2cb0966d7a2d99efc0", "input": "Matrix factorization (MF) and Autoencoder (AE) are among the most successful approaches of unsupervised learning. While MF based models have been extensively exploited in the graph modeling and link prediction literature, the AE family has not gained much attention. In this paper we investigate both MF and AE\u2019s application to the link prediction problem in sparse graphs. We show the connection between AE and MF from the perspective of multiview learning, and further propose MF+AE: a model training MF and AE jointly with shared parameters. We apply dropout to training both the MF and AE parts, and show that it can significantly prevent overfitting by acting as an adaptive regularization. We conduct experiments on six real world sparse graph datasets, and show that MF+AE consistently outperforms the competing methods, especially on datasets that demonstrate strong non-cohesive structures.", "target": "Dropout Training of Matrix Factorization and Autoencoder for Link Prediction in Sparse Graphs"}
{"id": "task1540-a6f48ad58d064892b29b2aa975245840", "input": "The purpose of this paper is to serve as a reference guide for the development of chatterbots implemented with the AIML language. In order to achieve this, the main concepts in Pattern Recognition area are described because the AIML uses such theoretical framework in their syntactic and semantic structures. After that, AIML language is described and each AIML command/tag is followed by an application example. Also, the usage of AIML embedded tags for the handling of sequence dialogue limitations between humans and machines is shown. Finally, computer systems that assist in the design of chatterbots with the AIML language are classified and described.", "target": "ARTIFICIAL INTELLIGENCE MARKUP LANGUAGE: A BRIEF TUTORIAL"}
{"id": "task1540-7254dbcfa26c4d83b1d6e8fdcc9f5956", "input": "We investigate the use of temporally abstract actions, or macro-actions, in the solution of Markov decision processes. Unlike current mod\u00ad els that combine both primitive actions and macro-actions and leave the state space un\u00ad changed, we propose a hierarchical model (using an abstract MDP) that works with macro-actions only, and that significantly reduces the size of the state space. This is achieved by treating macro\u00ad actions as local policies that act in certain regions of state space, and by restricting states in the ab\u00ad stract MDP to those at the boundaries of regions. The abstract MDP approximates the original and can be solved more efficiently. We discuss sev\u00ad eral ways in which macro-actions can be gen\u00ad erated to ensure good solution quality. Finally, we consider ways in which macro-actions can be reused to solve multiple, related MDPs; and we show that this can justify the computational over\u00ad head of macro-action generation.", "target": "Hierarchical Solution of Markov Decision Processes using Macro-actions"}
{"id": "task1540-e4a95afd41414b3ea969feeb58e511ee", "input": "Influence diagrams are decision theoretic extensions of Bayesian networks. They are applied to diverse decision problems. In this paper we apply influence diagrams to the optimization of a vehicle speed profile. We present results of computational experiments in which an influence diagram was used to optimize the speed profile of a Formula 1 race car at the Silverstone F1 circuit. The computed lap time and speed profiles correspond well to those achieved by test pilots. An extended version of our model that considers a more complex optimization function and diverse traffic constraints is currently being tested onboard a testing car by a major car manufacturer. This paper opens doors for new applications of influence diagrams.", "target": "Influence diagrams for the optimization of a vehicle speed profile"}
{"id": "task1540-b306122e2b9d420a9cca25370d180606", "input": "Automated Text Scoring (ATS) provides a cost-effective and consistent alternative to human marking. However, in order to achieve good performance, the predictive features of the system need to be manually engineered by human experts. We introduce a model that forms word representations by learning the extent to which specific words contribute to the text\u2019s score. Using Long-Short Term Memory networks to represent the meaning of texts, we demonstrate that a fully automated framework is able to achieve excellent results over similar approaches. In an attempt to make our results more interpretable, and inspired by recent advances in visualizing neural networks, we introduce a novel method for identifying the regions of the text that the model has found more discriminative.", "target": "Automatic Text Scoring Using Neural Networks"}
{"id": "task1540-efd04e09199e4d08b48ef7f682b50775", "input": "The comparison of heterogeneous samples extensively exists in many applications, especially in the task of image classification. In this paper, we propose a simple but effective coupled neural network, called Deeply Coupled Autoencoder Networks (DCAN), which seeks to build two deep neural networks, coupled with each other in every corresponding layers. In DCAN, each deep structure is developed via stacking multiple discriminative coupled auto-encoders, a denoising auto-encoder trained with maximum margin criterion consisting of intra-class compactness and inter-class penalty. This single layer component makes our model simultaneously preserve the local consistency and enhance its discriminative capability. With increasing number of layers, the coupled networks can gradually narrow the gap between the two views. Extensive experiments on cross-view image classification tasks demonstrate the superiority of our method over state-of-the-art methods.", "target": "Deeply Coupled Auto-encoder Networks for Cross-view Classification"}
{"id": "task1540-5774120f216b48e398f7897fd8b16cc3", "input": "Fine-grained entity type classification (FETC) is the task of classifying an entity mention to a broad set of types. Distant supervision paradigm is extensively used to generate training data for this task. However, generated training data assigns same set of labels to every mention of an entity without considering its local context. Existing FETC systems have two major drawbacks: assuming training data to be noise free and use of hand crafted features. Our work overcomes both drawbacks. We propose a neural network model that jointly learns entity mentions and their context representation to eliminate use of hand crafted features. Our model treats training data as noisy and uses non-parametric variant of hinge loss function. Experiments show that the proposed model outperforms previous stateof-the-art methods on two publicly available datasets, namely FIGER(GOLD) and BBN with an average relative improvement of 2.69% in micro-F1 score. Knowledge learnt by our model on one dataset can be transferred to other datasets while using same model or other FETC systems. These approaches of transferring knowledge further improve the performance of respective models.", "target": "Fine-Grained Entity Type Classification by Jointly Learning Representations and Label Embeddings"}
{"id": "task1540-4f1125478e8c4f1caf7a9500ff813e14", "input": "An efficient algorithm for recurrent neural network training is presented. The approach increases the training speed for tasks where a length of the input sequence may vary significantly. The proposed approach is based on the optimal batch bucketing by input sequence length and data parallelization on multiple graphical processing units. The baseline training performance without sequence bucketing is compared with the proposed solution for a different number of buckets. An example is given for the online handwriting recognition task using an LSTM recurrent neural network. The evaluation is performed in terms of the wall clock time, number of epochs, and validation loss value.", "target": "Accelerating Recurrent Neural Network Training"}
{"id": "task1540-23eb7c983589412e971bb994c498a976", "input": "Applying deep reinforcement learning (RL) on real systems suffers from slow data sampling. We propose an enhanced generative adversarial network (EGAN) to initialize an RL agent in order to achieve faster learning. The EGAN utilizes the relation between states and actions to enhance the quality of data samples generated by a GAN. Pre-training the agent with the EGAN shows a steeper learning curve with a 20% improvement of training time in the beginning of learning, compared to no pre-training, and an improvement compared to training with GAN by about 5% with smaller variations. For real time systems with sparse and slow data sampling the EGAN could be used to speed up the early phases of the training process.", "target": "Enhanced Experience Replay Generation for Efficient Reinforcement Learning"}
{"id": "task1540-c33871bef392411e930e7aa9479d8d0e", "input": "In many machine learning applications, labeled data is scarce and obtaining more labels is expensive. We introduce a new approach to supervising neural networks by specifying constraints that should hold over the output space, rather than direct examples of input-output pairs. These constraints are derived from prior domain knowledge, e.g., from known laws of physics. We demonstrate the effectiveness of this approach on real world and simulated computer vision tasks. We are able to train a convolutional neural network to detect and track objects without any labeled examples. Our approach can significantly reduce the need for labeled training data, but introduces new challenges for encoding prior knowledge into appropriate loss functions.", "target": "Label-Free Supervision of Neural Networks with Physics and Domain Knowledge"}
{"id": "task1540-4dfaac1b3ff74cbebb6f1aa7f9fd0045", "input": "Financial fraud detection is an important problem with a number of design aspects to consider. Issues such as algorithm selection and performance analysis will affect the perceived ability of proposed solutions, so for auditors and researchers to be able to sufficiently detect financial fraud it is necessary that these issues be thoroughly explored. In this paper we will revisit the key performance metrics used for financial fraud detection with a focus on credit card fraud, critiquing the prevailing ideas and offering our own understandings. There are many different performance metrics that have been employed in prior financial fraud detection research. We will analyse several of the popular metrics and compare their effectiveness at measuring the ability of detection mechanisms. We further investigated the performance of a range of computational intelligence techniques when applied to this problem domain, and explored the efficacy of several binary classification methods. Keywords\u2014Financial fraud detection, credit card fraud; data mining; computational intelligence; performance metric", "target": "Some Experimental Issues in Financial Fraud Detection: An Investigation"}
{"id": "task1540-74be15d16a1f407cb3581d9c882e24cf", "input": "Global optimization of the energy consumption of dual power source vehicles such as hybrid electric vehicles, plugin hybrid electric vehicles, and plug in fuel cell electric vehicles requires knowledge of the complete route characteristics at the beginning of the trip. One of the main characteristics is the vehicle speed profile across the route. The profile will translate directly into energy requirements for a given vehicle. However, the vehicle speed that a given driver chooses will vary from driver to driver and from time to time, and may be slower, equal to, or faster than the average traffic flow. If the specific driver speed profile can be predicted, the energy usage can be optimized across the route chosen. The purpose of this paper is to research the application of Deep Learning techniques to this problem to identify at the beginning of a drive cycle the driver specific vehicle speed profile for an individual driver repeated drive cycle, which can be used in an optimization algorithm to minimize the amount of fossil fuel energy used during the trip. Keywords\u2014Deep Learning, Stacked Auto Encoders, Neural Networks, Traffic Prediction", "target": "Vehicle Speed Prediction using Deep Learning"}
{"id": "task1540-e828e94105e440ef8f2a7df8ae5b16cf", "input": "We consider Markov decision processes under parameter uncertainty. Previous studies all restrict to the case that uncertainties among different states are uncoupled, which leads to conservative solutions. In contrast, we introduce an intuitive concept, termed \u201cLightning Does not Strike Twice,\u201d to model coupled uncertain parameters. Specifically, we require that the system can deviate from its nominal parameters only a bounded number of times. We give probabilistic guarantees indicating that this model represents real life situations and devise tractable algorithms for computing optimal control policies.", "target": "Lightning Does Not Strike Twice:  Robust MDPs with Coupled Uncertainty"}
{"id": "task1540-74c839aa0fef4e9890dd30235bf90bd8", "input": "Marginal MAP inference involves making MAP predictions in systems defined<lb>with latent variables or missing information. It is significantly more difficult than<lb>pure marginalization and MAP tasks, for which a large class of efficient and con-<lb>vergent variational algorithms, such as dual decomposition, exist. In this work, we<lb>generalize dual decomposition to a generic power sum inference task, which in-<lb>cludes marginal MAP, along with pure marginalization and MAP, as special cases.<lb>Our method is based on a block coordinate descent algorithm on a new convex<lb>decomposition bound, that is guaranteed to converge monotonically, and can be<lb>parallelized efficiently. We demonstrate our approach on marginal MAP queries<lb>defined on real-world problems from the UAI approximate inference challenge,<lb>showing that our framework is faster and more reliable than previous methods.", "target": "Decomposition Bounds for Marginal MAP"}
{"id": "task1540-181666bb4e6143c3bee51c3c2942b92e", "input": "We present a new perspective on graph-based methods for collaborative ranking for recommender systems. Unlike user-based or item-based methods that compute a weighted average of ratings given by the nearest neighbors, or low-rank approximation methods using convex optimization and the nuclear norm, we formulate matrix completion as a series of semi-supervised learning problems, and propagate the known ratings to the missing ones on the user-user or item-item graph globally. The semi-supervised learning problems are expressed as LaplaceBeltrami equations on a manifold, or namely, harmonic extension, and can be discretized by a point integral method. We show that our approach does not impose a low-rank Euclidean subspace on the data points, but instead minimizes the dimension of the underlying manifold. Our method, named LDM (low dimensional manifold), turns out to be particularly effective in generating rankings of items, showing decent computational efficiency and robust ranking quality compared to state-of-the-art methods.", "target": "A Harmonic Extension Approach for Collaborative Ranking"}
{"id": "task1540-596a744c074c4af594ca9c281a56f20c", "input": "Contemporary research on computational processing of linguistic metaphors is divided into two main branches: metaphor recognition and metaphor interpretation. We take a different line of research and present an automated method for generating conceptual metaphors from linguistic data. Given the generated conceptual metaphors, we find corresponding linguistic metaphors in corpora. In this paper, we describe our approach and its evaluation using English and Russian data.", "target": "Generating Conceptual Metaphors from Proposition Stores"}
{"id": "task1540-32f865bfe7694f25954f14579bc515db", "input": "While influence diagrams have many ad\u00ad vantages as a representation framework for Bayesian decision problems, they have a se\u00ad rious drawback in handling asymmetric de\u00ad cision problems. To be represented in an influence diagram, an asymmetric decision problem must be symmetrized. A consid\u00ad erable amount of unnecessary computation may be involved when a symmetrized influ\u00ad ence diagram is evaluated by conventional al\u00ad gorithms. In this paper we present an ap\u00ad proach for avoiding such unnecessary compu\u00ad tation in influence diagram evaluation.", "target": "Solving Asymmetric Decision Problems with Influence Diagrams"}
{"id": "task1540-d9d4e20829734dde80d650caf8237987", "input": "Multivariate time series forecasting is an important machine learning problem across many domains, including predictions of solar plant energy output, electricity consumption, and traffic jam situation. Temporal data arise in these realworld applications often involves a mixture of long-term and short-term patterns, for which traditional approaches such as Autoregressive models and Gaussian Process may fail. In this paper, we proposed a novel deep learning framework, namely Longand Short-term Time-series network (LSTNet), to address this open challenge. LSTNet uses the Convolution Neural Network (CNN) to extract short-term local dependency patterns among variables, and the Recurrent Neural Network (RNN) to discover long-term patterns and trends. In our evaluation on real-world data with complex mixtures of repetitive patterns, LSTNet achieved significant performance improvements over that of several state-of-the-art baseline methods. The dataset and experiment code both are uploaded to Github.", "target": "Modeling Long- and Short-Term Temporal Patterns with Deep Neural Networks"}
{"id": "task1540-4f5b114cabd643ce806d9a8a80eb768a", "input": "When you need to enable deep learning on low-cost embedded SoCs, is it better to port an existing deep learning framework or should you build one from scratch? In this paper, we share our practical experiences of building an embedded inference engine using ARM Compute Library (ACL). The results show that, contradictory to conventional wisdoms, for simple models, it takes much less development time to build an inference engine from scratch compared to porting existing frameworks. In addition, by utilizing ACL, we managed to build an inference engine that outperforms TensorFlow by 25%. Our conclusion is that, on embedded devices, we most likely will use very simple deep learning models for inference, and with well-developed building blocks such as ACL, it may be better in both performance and development time to build the engine from scratch.", "target": "Enabling Embedded Inference Engine with ARM Compute Library"}
{"id": "task1540-b16d4da1396f43dc9aa59e8466c72cc4", "input": "We present an OWL 2 ontology representing the Saint Gall plan, one of the most ancient documents arrived intact to us, that describes the ideal model of a Benedictine monastic complex, and that inspired the design of many European", "target": "The Shape of a Benedictine Monastery: The SaintGall Ontology"}
{"id": "task1540-2c2401e97fe841818edb136988e8dbf3", "input": "Normalized graph cut (NGC) has become a popular research topic due to its wide applications in a large variety of areas like machine learning and very large scale integration (VLSI) circuit design. Most of traditional NGC methods are based on pairwise relationships (similarities). However, in real-world applications relationships among the vertices (objects) may be more complex than pairwise, which are typically represented as hyperedges in hypergraphs. Thus, normalized hypergraph cut (NHC) has attracted more and more attention. Existing NHC methods cannot achieve satisfactory performance in real applications. In this paper, we propose a novel relaxation approach, which is called relaxed NHC (RNHC), to solve the NHC problem. Our model is defined as an optimization problem on the Stiefel manifold. To solve this problem, we resort to the Cayley transformation to devise a feasible learning algorithm. Experimental results on a set of large hypergraph benchmarks for clustering and partitioning in VLSI domain show that RNHC can outperform the state-of-the-art methods.", "target": "A New Relaxation Approach to Normalized Hypergraph Cut"}
{"id": "task1540-707d120900bf4b528dfac0e241d70023", "input": "The distinction between strong negation and default negation has been useful in answer set programming. We present an alternative account of strong negation, which lets us view strong negation in terms of the functional stable model semantics by Bartholomew and Lee. More specifically, we show that, under complete interpretations, minimizing both positive and negative literals in the traditional answer set semantics is essentially the same as ensuring the uniqueness of Boolean function values under the functional stable model semantics. The same account lets us view Lifschitz\u2019s two-valued logic programs as a special case of the functional stable model semantics. In addition, we show how non-Boolean intensional functions can be eliminated in favor of Boolean intensional functions, and furthermore can be represented using strong negation, which provides a way to compute the functional stable model semantics using existing ASP solvers. We also note that similar results hold with the functional stable model semantics by Cabalar.", "target": "A Functional View of Strong Negation in Answer Set Programming"}
{"id": "task1540-f66ac35e629b4e1d87c342ee26d156cb", "input": "Given a set of documents from a specific domain (e.g., medical research journals), how do we automatically build a Knowledge Graph (KG) for that domain? Automatic identification of relations and their schemas, i.e., type signature of arguments of relations (e.g., undergo(Patient, Surgery)), is an important first step towards this goal. We refer to this problem as Relation Schema Induction (RSI). In this paper, we propose Schema Induction using Coupled Tensor Factorization (SICTF), a novel tensor factorization method for relation schema induction. SICTF factorizes Open Information Extraction (OpenIE) triples extracted from a domain corpus along with additional side information in a principled way to induce relation schemas. To the best of our knowledge, this is the first application of tensor factorization for the RSI problem. Through extensive experiments on multiple real-world datasets, we find that SICTF is not only more accurate than state-of-the-art baselines, but also significantly faster (about 14x faster).", "target": "Relation Schema Induction using Tensor Factorization with Side Information"}
{"id": "task1540-023fd03f9ad641498898501a0f9b61ee", "input": "The downfall of many supervised learning algorithms, such as neural networks, is the inherent need for a large amount of training data (Benediktsson et al., 1993). Although there is a lot of buzz about big data, there is still the problem of doing classification from a small data-set. Other methods such as support vector machines, although capable of dealing with few samples, are inherently binary classifiers (Cortes and Vapnik, 1995), and are in need of learning strategies such as One vs All in the case of multi-classification. In the presence of a large number of classes this can become problematic. In this paper we present, a novel approach to supervised learning through the method of clustering. Unlike traditional methods such as K-Means (MacQueen, 1967), Gravitational Clustering does not require the initial number of clusters, and automatically builds the clusters, individual samples can be arbitrarily weighted and it requires only few samples while staying resilient to over-fitting. Keywords\u2014Machine Learning, Classification, Clustering.", "target": "Introduction to Gravitational Clustering"}
{"id": "task1540-4fb50d9ebfe1425b92d613ff85fd97ad", "input": "Chinese characters have a complex and hierarchical graphical structure carrying both semantic and phonetic information. We use this structure to enhance the text model and obtain better results in standard NLP operations. First of all, to tackle the problem of graphical variation we define allographic classes of characters. Next, the relation of inclusion of a subcharacter in a characters, provides us with a directed graph of allographic classes. We provide this graph with two weights: semanticity (semantic relation between subcharacter and character) and phoneticity (phonetic relation) and calculate \u201cmost semantic subcharacter paths\u201d for each character. Finally, adding the information contained in these paths to unigrams we claim to increase the efficiency of text mining methods. We evaluate our method on a text classification task on two corpora (Chinese and Japanese) of a total of 18 million characters and get an improvement of 3% on an already high baseline of 89.6% precision, obtained by a linear SVM classifier. Other possible applications and perspectives of the system are discussed.", "target": "New Perspectives in Sinographic Language Processing Through the Use of Character Structure"}
{"id": "task1540-2023aff5c1194f21a0756f5305b69032", "input": "Our world can be succinctly and compactly described as structured scenes of objects and relations. A typical room, for example, contains salient objects such as tables, chairs and books, and these objects typically relate to each other by their underlying causes and semantics. This gives rise to correlated features, such as position, function and shape. Humans exploit knowledge of objects and their relations for learning a wide spectrum of tasks, and more generally when learning the structure underlying observed data. In this work, we introduce relation networks (RNs) a general purpose neural network architecture for object-relation reasoning. We show that RNs are capable of learning object relations from scene description data. Furthermore, we show that RNs can act as a bottleneck that induces the factorization of objects from entangled scene description inputs, and from distributed deep representations of scene images provided by a variational autoencoder. The model can also be used in conjunction with differentiable memory mechanisms for implicit relation discovery in one-shot learning tasks. Our results suggest that relation networks are a potentially powerful architecture for solving a variety of problems that require object relation reasoning.", "target": "DISCOVERING OBJECTS AND THEIR RELATIONS FROM ENTANGLED SCENE REPRESENTATIONS"}
{"id": "task1540-73a9eda755074f239e7393e374a261c2", "input": "Pattern classification systems are commonly used in adversarial applications, like biometric authentication, network intrusion detection, and spam filtering, in which data can be purposely manipulated by humans to undermine their operation. As this adversarial scenario is not taken into account by classical design methods, pattern classification systems may exhibit vulnerabilities, whose exploitation may severely affect their performance, and consequently limit their practical utility. Extending pattern classification theory and design methods to adversarial settings is thus a novel and very relevant research direction, which has not yet been pursued in a systematic way. In this paper, we address one of the main open issues: evaluating at design phase the security of pattern classifiers, namely, the performance degradation under potential attacks they may incur during operation. We propose a framework for empirical evaluation of classifier security that formalizes and generalizes the main ideas proposed in the literature, and give examples of its use in three real applications. Reported results show that security evaluation can provide a more complete understanding of the classifier\u2019s behavior in adversarial environments, and lead to better design choices.", "target": "Security Evaluation of Pattern Classifiers under Attack"}
{"id": "task1540-24e4652415cf46f98768850aa6034679", "input": "This research applies ideas from argumentation theory in the context of semantic wikis, aiming to provide support for structured-large scale argumentation between human agents. The implemented prototype is exemplified by modelling the MMR vaccine controversy.", "target": "Using Semantic Wikis for Structured Argument in Medical Domain"}
{"id": "task1540-5d816755bb3d45f9b9f47869d876c35d", "input": "We present a novel framework for generating pop music. Our model is a hierarchical Recurrent Neural Network, where the layers and the structure of the hierarchy encode our prior knowledge about how pop music is composed. In particular, the bottom layers generate the melody, while the higher levels produce the drums and chords. We conduct several human studies that show strong preference of our generated music over that produced by the recent method by Google. We additionally show two applications of our framework: neural dancing and karaoke, as well as neural story singing.", "target": "SONG FROM PI: A MUSICALLY PLAUSIBLE NETWORK FOR POP MUSIC GENERATION"}
{"id": "task1540-5262709cf899441d87aee7f56be0bc31", "input": "Optimization by stochastic gradient descent is an important component of many large-scale machine learning algorithms. A wide variety of such optimization algorithms have been devised; however, it is unclear whether these algorithms are robust and widely applicable across many different optimization landscapes. In this paper we develop a collection of unit tests for stochastic optimization. Each unit test rapidly evaluates an optimization algorithm on a small-scale, isolated, and well-understood difficulty, rather than in real-world scenarios where many such issues are entangled. Passing these unit tests is not sufficient, but absolutely necessary for any algorithms with claims to generality or robustness. We give initial quantitative and qualitative results on a dozen established algorithms. The testing framework is open-source, extensible, and easy to apply to new algorithms.", "target": "Unit Tests for Stochastic Optimization"}
{"id": "task1540-c194f4f836a54546b6c9e3acbc78ed98", "input": "Transcription of broadcast news is an interesting and challenging application for large-vocabulary continuous speech recognition (LVCSR). We present in detail the structure of a manually segmented and annotated corpus including over 160 hours of German broadcast news, and propose it as an evaluation framework of LVCSR systems. We show our own experimental results on the corpus, achieved with a state-of-the-art LVCSR decoder, measuring the effect of different feature sets and decoding parameters, and thereby demonstrate that real-time decoding of our test set is feasible on a desktop PC at 9.2 % word error rate.", "target": "A Broadcast News Corpus for Evaluation and Tuning of German LVCSR Systems"}
{"id": "task1540-ff4f7a672d6543e297a5253f7825e653", "input": "This paper presents an adaptation of the harmony search algorithm to solve the storage allocation problem for inbound and outbound containers. This problem is studied considering multiple container type (regular, open side, open top, tank, empty and refrigerated) which lets the situation more complicated, as various storage constraints appeared. The objective is to find an optimal container arrangement which respects their departure dates, and minimize the re-handle operations of containers. The performance of the proposed approach is verified comparing to the results generated by genetic algorithm and LIFO algorithm. General Terms Container storage problem, metaheuristics.", "target": "Harmony search to solve the container storage problem with different container types"}
{"id": "task1540-173244dff343471c940d214d53e04777", "input": "This paper investigates two feature-scoring criteria that make use of estimated class probabilities: one method proposed by Shen et al. (2008) and a complementary approach proposed below. We develop a theoretical framework to analyze each criterion and show that both estimate the spread (across all values of a given feature) of the probability that an example belongs to the positive class. Based on our analysis, we predict when each scoring technique will be advantageous over the other and give empirical results validating our predictions.", "target": "Feature Selection via Probabilistic Outputs"}
{"id": "task1540-68776346b0174350941f6e4dd849c5c1", "input": "The relationship between belief networks and relational databases is examined. Based on this analysis, a method to construct belief networks automatically from statistical rela\u00ad tional data is proposed. A comparison be\u00ad tween our method and other methods shows that our method has several advantages when generalization or prediction is deeded.", "target": "From Relational Databases to Belief Networks"}
{"id": "task1540-1c0bf0d471444593888716c425a3cbe7", "input": "Online media offers opportunities to marketers to deliver brand messages to a large audience. Advertising technology platforms enables the advertisers to find the proper group of audiences and deliver ad impressions to them in real time. The recent growth of the real time bidding has posed a significant challenge on monitoring such a complicated system. With so many components we need a reliable system that detects the possible changes in the system and alerts the engineering team. In this paper we describe the mechanism that we invented for recovering the representative metrics and detecting the change in their behavior. We show that this mechanism is able to detect the possible problems in time by describing some incident cases.", "target": "Finding Needle in a Million Metrics: Anomaly Detection in a Large-scale Computational Advertising Platform"}
{"id": "task1540-a81913cf161f4c079b43cccc301130e1", "input": "We present a case study of artificial intelligence techniques applied to the control of production printing equipment. Like many other real-world applications, this complex domain requires high-speed autonomous decision-making and robust continual operation. To our knowledge, this work represents the first successful industrial application of embedded domain-independent temporal planning. Our system handles execution failures and multiobjective preferences. At its heart is an on-line algorithm that combines techniques from state-space planning and partial-order scheduling. We suggest that this general architecture may prove useful in other applications as more intelligent systems operate in continual, on-line settings. Our system has been used to drive several commercial prototypes and has enabled a new product architecture for our industrial partner. When compared with state-of-the-art off-line planners, our system is hundreds of times faster and often finds better plans. Our experience demonstrates that domain-independent AI planning based on heuristic search can flexibly handle time, resources, replanning, and multiple objectives in a high-speed practical application without requiring hand-coded control knowledge.", "target": "On-line Planning and Scheduling: An Application to Controlling Modular Printers"}
{"id": "task1540-2b95055baf1b49809b83d962f3e6203a", "input": "Class imbalance is one of the challenging problems for machine learning in many real-world applications, such as coal and gas burst accident monitoring: the burst premonition data is extreme smaller than the normal data, however, which is the highlight we truly focus on. Cost-sensitive adjustment approach is a typical algorithm-level method resisting the data set imbalance. For SVMs classifier, which is modified to incorporate varying penalty parameter(C) for each of considered groups of examples. However, the C value is determined empirically, or is calculated according to the evaluation metric, which need to be computed iteratively and time consuming. This paper presents a novel cost-sensitive SVM method whose penalty parameter C optimized on the basis of cluster probability density function(PDF) and the cluster PDF is estimated only according to similarity matrix and some predefined hyper-parameters. Experimental results on various standard benchmark data sets and real-world data with different ratios of imbalance show that the proposed method is effective in comparison with commonly used cost-sensitive techniques.", "target": "Optimizing Cost-Sensitive SVM for Imbalanced Data :Connecting Cluster to Classification"}
{"id": "task1540-c742c611b7a54441aa1498697334f7d9", "input": "We present a novel approach to learning an HMM whose outputs are distributed according to a parametric family. This is done by decoupling the learning task into two steps: first estimating the output parameters, and then estimating the hidden states transition probabilities. The first step is accomplished by fitting a mixture model to the output stationary distribution. Given the parameters of this mixture model, the second step is formulated as the solution of an easily solvable convex quadratic program. We provide an error analysis for the estimated transition probabilities and show they are robust to small perturbations in the estimates of the mixture parameters. Finally, we support our analysis with some encouraging empirical results.", "target": "On learning parametric-output HMMs"}
{"id": "task1540-f4d9c5735c7e40f19286f693023c0b7c", "input": "In Distributional Semantic Models (DSMs), Vector Cosine is widely used to estimate similarity between word vectors, although this measure was noticed to suffer from several shortcomings. The recent literature has proposed other methods which attempt to mitigate such biases. In this paper, we intend to investigate APSyn, a measure that computes the extent of the intersection between the most associated contexts of two target words, weighting it by context relevance. We evaluated this metric in a similarity estimation task on several popular test sets, and our results show that APSyn is in fact highly competitive, even with respect to the results reported in the literature for word embeddings. On top of it, APSyn addresses some of the weaknesses of Vector Cosine, performing well also on genuine similarity estimation.", "target": "Testing APSyn against Vector Cosine on Similarity Estimation"}
{"id": "task1540-45a0e7a9142c44ef9f5b3e88a9198654", "input": "We present the first differentially private algorithms for reinforcement learning, which apply to the task of evaluating a fixed policy. We establish two approaches for achieving differential privacy, provide a theoretical analysis of the privacy and utility of the two algorithms, and show promising results on simple empirical examples.", "target": "Differentially Private Policy Evaluation\u2217"}
{"id": "task1540-27b49b4469a8480f8cc8d877a5dbcde9", "input": "This paper presents a simple end-to-end model for speech recognition, combining a convolutional network based acoustic model and a graph decoding. It is trained to output letters, with transcribed speech, without the need for force alignment of phonemes. We introduce an automatic segmentation criterion for training from sequence annotation without alignment that is on par with CTC [6] while being simpler. We show competitive results in word error rate on the Librispeech corpus [18] with MFCC features, and promising results from raw waveform.", "target": "Wav2Letter: an End-to-End ConvNet-based Speech Recognition System"}
{"id": "task1540-1695dec2a13241639d931c5505dd44ac", "input": "We present a new method for estimating the expected return of a POMDP from experi\u00ad ence. The estimator does not assume any knowledge of the POMDP, can estimate the returns for finite state controllers, allows ex\u00ad perience to be gathered from arbitrary se\u00ad quences of policies, and estimates the return for any new policy. We motivate the estima\u00ad tor from function-approximation and impor\u00ad tance sampling points-of-view and derive its bias and variance. Although the estimator is biased, it has low variance and the bias is of\u00ad ten irrelevant when the estimator is used for pair-wise comparisons. We conclude by ex\u00ad tending the estimator to policies with mem\u00ad ory and compare its performance in a greedy search algorithm to the REINFORCE algo\u00ad rithm showing an order of magnitude reduc\u00ad tion in the number of trials required.", "target": "Policy Improvement for POMDPs using Normalized Importance Sampling"}
{"id": "task1540-8d8044676b64429e8b51c957682fd39b", "input": "This paper proposes a deep denoising auto-encoder technique to extract better acoustic features for speech synthesis. The technique allows us to automatically extract low-dimensional features from high dimensional spectral features in a non-linear, data-driven, unsupervised way. We compared the new stochastic feature extractor with conventional mel-cepstral analysis in analysis-by-synthesis and text-to-speech experiments. Our results confirm that the proposed method increases the quality of synthetic speech in both experiments.", "target": "DEEP DENOISING AUTO-ENCODER FOR STATISTICAL SPEECH SYNTHESIS"}
{"id": "task1540-a5cec44af7444430b20e900e08eca397", "input": "In the Bayesian Reinforcement Learning (BRL) setting, agents try to maximise the collected rewards while interacting with their environment while using some prior knowledge that is accessed beforehand. Many BRL algorithms have already been proposed, but even though a few toy examples exist in the literature, there are still no extensive or rigorous benchmarks to compare them. The paper addresses this problem, and provides a new BRL comparison methodology along with the corresponding open source library. In this methodology, a comparison criterion that measures the performance of algorithms on large sets of Markov Decision Processes (MDPs) drawn from some probability distributions is defined. In order to enable the comparison of non-anytime algorithms, our methodology also includes a detailed analysis of the computation time requirement of each algorithm. Our library is released with all source code and documentation: it includes three test problems, each of which has two different prior distributions, and seven state-of-the-art RL algorithms. Finally, our library is illustrated by comparing all the available algorithms and the results are discussed.", "target": "Benchmarking for Bayesian Reinforcement Learning Benchmarking for Bayesian Reinforcement Learning"}
{"id": "task1540-d5d3990554604442bf9e0d552a36f291", "input": "Constructing an accurate system model for formal model verification can be both resource demanding and time-consuming. To alleviate this shortcoming, algorithms have been proposed for automatically learning system models based on observed system behaviors. In this paper we extend the algorithm on learning probabilistic automata to reactive systems, where the observed system behavior is in the form of alternating sequences of inputs and outputs. We propose an algorithm for automatically learning a deterministic labeled Markov decision process model from the observed behavior of a reactive system. The proposed learning algorithm is adapted from algorithms for learning deterministic probabilistic finite automata, and extended to include both probabilistic and nondeterministic transitions. The algorithm is empirically analyzed and evaluated by learning system models of slot machines. The evaluation is performed by analyzing the probabilistic linear temporal logic properties of the system as well as by analyzing the schedulers, in particular the optimal schedulers, induced by the learned models.", "target": "Learning Markov Decision Processes for Model Checking"}
{"id": "task1540-6a9c9c9eeea740e1837b1138da16902d", "input": "We investigate the usage of convolutional neural networks (CNNs) for the slot filling task in spoken language understanding. We propose a novel CNN architecture for sequence labeling which takes into account the previous context words with preserved order information and pays special attention to the current word with its surrounding context. Moreover, it combines the information from the past and the future words for classification. Our proposed CNN architecture outperforms even the previously best ensembling recurrent neural network model and achieves state-of-the-art results with an F1-score of 95.61% on the ATIS benchmark dataset without using any additional linguistic knowledge and resources.", "target": "Sequential Convolutional Neural Networks for Slot Filling in Spoken Language Understanding"}
{"id": "task1540-1326bd7530624e1896ff3247cafb98e0", "input": "Drugs are frequently prescribed to patients with the aim of improving each patient\u2019s medical state, but an unfortunate consequence of most prescription drugs is the occurrence of undesirable side effects. Side effects that occur in more than one in a thousand patients are likely to be signalled efficiently by current drug surveillance methods, however, these same methods may take decades before generating signals for rarer side effects, risking medical morbidity or mortality in patients prescribed the drug while the rare side effect is undiscovered. In this paper we propose a novel computational meta-analysis framework for signalling rare side effects that integrates existing methods, knowledge from the web, metric learning and semi-supervised clustering. The novel framework was able to signal many known rare and serious side effects for the selection of drugs investigated, such as tendon rupture when prescribed Ciprofloxacin or Levofloxacin, renal failure with Naproxen and depression associated with Rimonabant. Furthermore, for the majority of the drug investigated it generated signals for rare side effects at a more stringent signalling threshold than existing methods and shows the potential to become a fundamental part of post marketing surveillance to detect rare side effects.", "target": "A Novel Semi-Supervised Algorithm for Rare Prescription Side Effect Discovery"}
{"id": "task1540-318d464f559340ddb44bcd0a813c4be6", "input": "Nonnegative matrix factorization (NMF) has become a ubiquitous tool for data analysis. An important variant is the sparse NMF problem which arises when we explicitly require the learnt features to be sparse. A natural measure of sparsity is the L0 norm, however its optimization is NP-hard. Mixed norms, such as L1/L2 measure, have been shown to model sparsity robustly, based on intuitive attributes that such measures need to satisfy. This is in contrast to computationally cheaper alternatives such as the plain L1 norm. However, present algorithms designed for optimizing the mixed norm L1/L2 are slow and other formulations for sparse NMF have been proposed such as those based on L1 and L0 norms. Our proposed algorithm allows us to solve the mixed norm sparsity constraints while not sacrificing computation time. We present experimental evidence on real-world datasets that shows our new algorithm performs an order of magnitude faster compared to the current state-of-the-art solvers optimizing the mixed norm and is suitable for large-scale datasets.", "target": "Block Coordinate Descent for Sparse NMF"}
{"id": "task1540-4f8e3c6d231e4b079a988057384d06da", "input": "We present a tool, simplify-defun, that transforms the definition of a given function into a simplified definition of a new function, providing a proof checked by ACL2 that the old and new functions are equivalent. When appropriate it also generates termination and guard proofs for the new function. We explain how the tool is engineered so that these proofs will succeed. Examples illustrate its utility, in particular for program transformation in synthesis and verification.", "target": "A Versatile, Sound Tool for Simplifying Definitions"}
{"id": "task1540-f9b485e6cc014d04ab2297d3ba9ddd78", "input": "Recently, we see a new type of interfaces for programmers based on web technology. For example, JSFiddle, IPython Notebook and R-studio. Web technology enables cloud-based solutions, embedding in tutorial web pages, attractive rendering of results, web-scale cooperative development, etc. This article describes SWISH, a web front-end for Prolog. A public website exposes SWIProlog using SWISH, which is used to run small Prolog programs for demonstration, experimentation and education. We connected SWISH to the ClioPatria semantic web toolkit, where it allows for collaborative development of programs and queries related to a dataset as well as performing maintenance tasks on the running server and we embedded SWISH in the Learn Prolog Now! online Prolog book.", "target": "SWISH: SWI-Prolog for Sharing"}
{"id": "task1540-2a606b01f05b468e9ccbec8ce0aed68e", "input": "In Machine Learning, the parent set identification problem is to find a set of random variables that best explain selected variable given the data and some predefined scoring function. This problem is a critical component to structure learning of Bayesian networks and Markov blankets discovery, and thus has many practical applications ranging from fraud detection to clinical decision support. In this paper, we introduce a new distributed memory approach to the exact parent sets assignment problem. To achieve scalability, we derive theoretical bounds to constraint the search space when MDL scoring function is used, and we reorganize the underlying dynamic programming such that the computational density is increased and fine-grain synchronization is eliminated. We then design efficient realization of our approach in the Apache Spark platform. Through experimental results, we demonstrate that the method maintains strong scalability on a 500-core standalone Spark cluster, and it can be used to efficiently process data sets with 70 variables, far beyond the reach of the currently available solutions.", "target": "Scalable Exact Parent Sets Identification in Bayesian Networks Learning with Apache Spark"}
{"id": "task1540-5b3e5442b78f497dbee9dbbe2e0ebf5a", "input": "Verbs play a critical role in the meaning of sentences, but these ubiquitous words have received little attention in recent distributional semantics research. We introduce SimVerb-3500, an evaluation resource that provides human ratings for the similarity of 3,500 verb pairs. SimVerb-3500 covers all normed verb types from the USF free-association database, providing at least three examples for every VerbNet class. This broad coverage facilitates detailed analyses of how syntactic and semantic phenomena together influence human understanding of verb meaning. Further, with significantly larger development and test sets than existing benchmarks, SimVerb-3500 enables more robust evaluation of representation learning architectures and promotes the development of methods tailored to verbs. We hope that SimVerb-3500 will enable a richer understanding of the diversity and complexity of verb semantics and guide the development of systems that can effectively represent and interpret this meaning.", "target": "SimVerb-3500: A Large-Scale Evaluation Set of Verb Similarity"}
{"id": "task1540-00009716d646442ba381a411bcbc163e", "input": "The recent adaptation of deep neural networkbased methods to reinforcement learning and planning domains has yielded remarkable progress on individual tasks. Nonetheless, progress on task-to-task transfer remains limited. In pursuit of efficient and robust generalization, we introduce the Schema Network, an objectoriented generative physics simulator capable of disentangling multiple causes of events and reasoning backward through causes to achieve goals. The richly structured architecture of the Schema Network can learn the dynamics of an environment directly from data. We compare Schema Networks with Asynchronous Advantage Actor-Critic and Progressive Networks on a suite of Breakout variations, reporting results on training efficiency and zero-shot generalization, consistently demonstrating faster, more robust learning and better transfer. We argue that generalizing from limited data and learning causal relationships are essential abilities on the path toward generally intelligent systems.", "target": "Schema Networks: Zero-shot Transfer with a Generative Causal Model of Intuitive Physics"}
{"id": "task1540-59afc18ddec74f309c8a4dacb86d0b86", "input": "We introduce a batched lazy algorithm for supervised classification using decision trees. It avoids unnecessary visits to irrelevant nodes when it is used to make predictions with either eagerly or lazily trained decision trees. A set of experiments demonstrate that the proposed algorithm can outperform both the conventional and lazy decision tree algorithms in terms of computation time as well as memory consumption, without compromising accuracy.", "target": "Batched Lazy Decision Trees"}
{"id": "task1540-c24ab63bbff345308af6532cafff687b", "input": "The steadily growing use of license-free frequency bands requires reliable coexistence management for deterministic medium utilization. For interference mitigation, proper wireless interference identification (WII) is essential. In this work we propose the first WII approach based upon deep convolutional neural networks (CNNs). The CNN naively learns its features through self-optimization during an extensive data-driven GPU-based training process. We propose a CNN example which is based upon sensing snapshots with a limited duration of 12.8 \u03bcs and an acquisition bandwidth of 10 MHz. The CNN differs between 15 classes. They represent packet transmissions of IEEE 802.11 b/g, IEEE 802.15.4 and IEEE 802.15.1 with overlapping frequency channels within the 2.4 GHz ISM band. We show that the CNN outperforms state-ofthe-art WII approaches and has a classification accuracy greater than 95 % for signal-to-noise ratio of at least -5 dB.", "target": "Wireless Interference Identification with Convolutional Neural Networks"}
{"id": "task1540-6138e221047e4ca7ba5364954edd06a0", "input": "We consider the problem of learning a causal graph over a set of variables with interventions. We study the cost-optimal causal graph learning problem: For a given skeleton (undirected version of the causal graph), design the set of interventions with minimum total cost, that can uniquely identify any causal graph with the given skeleton. We show that this problem is solvable in polynomial time. Later, we consider the case when the number of interventions is limited. For this case, we provide polynomial time algorithms when the skeleton is a tree or a clique tree. For a general chordal skeleton, we develop an efficient greedy algorithm, which can be improved when the causal graph skeleton is an interval graph.", "target": "Cost-Optimal Learning of Causal Graphs"}
{"id": "task1540-6e76462cf49a4c429a3a40ddb443c3a0", "input": "Summarization of large texts is still an open problem in language processing. In this work we develop a full fledged pipeline to generate summaries of news articles using the Abstract Meaning Representation(AMR). We first generate the AMR graphs of stories then extract summary graphs from the story graphs and finally generate sentences from the summary graph. For extracting summary AMRs from the story AMRs we use a two step process. First, we find important sentences from the text and then extract the summary AMRs from those selected sentences. We outperform the previous methods using AMR for summarization by more that 3 ROGUE-1 points. On the CNN-Dailymail corpus we achieve results competitive with the strong lead-3 baseline till summary graph extraction step.", "target": "Text Summarization using Abstract Meaning Representation"}
{"id": "task1540-01f34d9ee6cd4c6b8baa29bd8fc1cd7e", "input": "We present a new model for prediction markets, in which we use risk measures to model agents and introduce a market maker to describe the trading process. This specific choice on modelling tools brings us mathematical convenience. The analysis shows that the whole market effectively approaches a global objective, despite that the market is designed such that each agent only cares about its own goal. Additionally, the market dynamics provides a sensible algorithm for optimising the global objective. An intimate connection between machine learning and our markets is thus established, such that we could 1) analyse a market by applying machine learning methods to the global objective, and 2) solve machine learning problems by setting up and running certain markets.", "target": "Multi-period Trading Prediction Markets with Connections to Machine Learning"}
{"id": "task1540-931fe73cdf3e428dae953d550f473555", "input": "Multivariate time series naturally exist in many fields, like energy, bioinformatics, signal processing, and finance. Most of these applications need to be able to compare these structured data. In this context, dynamic time warping (DTW) is probably the most common comparison measure. However, not much research effort has been put into improving it by learning. In this paper, we propose a novel method for learning similarities based on DTW, in order to improve time series classification. Making use of the uniform stability framework, we provide the first theoretical guarantees in the form of a generalization bound for linear classification. The experimental study shows that the proposed approach is efficient, while yielding sparse classifiers.", "target": "Similarity Learning for Time Series Classification"}
{"id": "task1540-d7d1c71851f149928a0d60bf3a01b35f", "input": "Many real-world problems involving constraints can be regarded as instances of the Max-SAT problem, which is the optimization variant of the classic satisfiability problem. In this paper, we propose a novel probabilistic approach for Max-SAT called ProMS. Our algorithm relies on a stochastic local search strategy using a novel probability distribution function with two strategies for picking variables, one based on available information and another purely random one. Moreover, while most previous algorithms based on WalkSAT choose unsatisfied clauses randomly, we introduce a novel clause selection strategy to improve our algorithm. Experimental results illustrate that ProMS outperforms many state-of-the-art stochastic local search solvers on hard unweighted random Max-SAT benchmarks.", "target": "A Probability Distribution Strategy with Efficient Clause Selection for Hard Max-SAT Formulas"}
{"id": "task1540-fbf56aff8ce546c292b903e413a7a1c7", "input": "Concept maps can be used to concisely represent important information and bring structure into large document collections. Therefore, we study a variant of multidocument summarization that produces summaries in the form of concept maps. However, suitable evaluation datasets for this task are currently missing. To close this gap, we present a newly created corpus of concept maps that summarize heterogeneous collections of web documents on educational topics. It was created using a novel crowdsourcing approach that allows us to efficiently determine important elements in large document collections. We release the corpus along with a baseline system and proposed evaluation protocol to enable further research on this variant of summarization.", "target": "Connecting the dots: Summarizing and Structuring Large Document Collections Using Concept Maps"}
{"id": "task1540-e583e1adceb448e1a3eb796404eab86c", "input": "This paper develops the idea of membership function assignment for OWL (Web Ontology Language) ontology elements in order to subsequently generate fuzzy rules from this ontology. The task of membership function assignment for OWL ontology elements had already been partially described, but this concerned the case , when several OWL ontologies of the same domain were available, and they were merged into a single ontology. The purpose of this paper is to present the way of membership function assignment for OWL ontology elements in the case, when there is the only one available ontology. Fuzzy rules, generated from the OWL ontology, are necessary for supplement of the SWES (Semantic Web Expert System) knowledge base. SWES is an expert system, which will be able to extract knowledge from OWL ontologies , found in the Web, and will serve as a universal expert for the user.", "target": "Membership Function Assignment for Elements of Single OWL Ontology"}
{"id": "task1540-4ca3c0a3060645b9ba34a0c46ab676f6", "input": "In current perception systems applied to the rebuilding of the environment for intelligent vehicles, the part reserved to object association for the tracking is increasingly significant. This allows firstly to follow the objects temporal evolution and secondly to increase the reliability of environment perception. We propose in this communication the development of a multi\u00ad objects association algorithm with ambiguity removal entering into the design of such a dynamic perception system for intelligent vehicles. This algorithm uses the belief theory and data modelling with fuzzy mathematics in order to be able to handle inaccurate as well as uncertain information due to imperfect sensors. These theories also allow the fusion of numerical as well as symbolic data. We develop in this article the problem of matching between known and perceived objects. This makes it possible to update a dynamic environment map for a vehicle. The belief theory will enable us to quantify the belief in the association of each perceived object with each known object. Conflicts can appear in the case of object appearance or disappearance, or in the case of a confused situation or bad perception. These conflicts are removed or solved using an assignment algorithm, giving a solution called the \u00ab best \u00bb and so ensuring the tracking of some objects present in our environment.", "target": "Multi-objects association in perception of dynamical situation"}
{"id": "task1540-f35712fd50014aa0a838b712c84f3387", "input": "Recent research shows that deep neural networks (DNNs) can be used to extract deep speaker vectors (d-vectors) that preserve speaker characteristics and can be used in speaker verification. This new method has been tested on text-dependent speaker verification tasks, and improvement was reported when combined with the conventional i-vector method. This paper extends the d-vector approach to semi textindependent speaker verification tasks, i.e., the text of the speech is in a limited set of short phrases. We explore various settings of the DNN structure used for d-vector extraction, and present a phone-dependent training which employs the posterior features obtained from an ASR system. The experimental results show that it is possible to apply d-vectors on semi text-independent speaker recognition, and the phone-dependent training improves system performance.", "target": "Deep Speaker Vectors for Semi Text-independent Speaker Verification"}
{"id": "task1540-8f5eb39997324b678835cd1d6f72e1db", "input": "Many online communities present user-contributed responses such as reviews of products and answers to questions. User-provided helpfulness votes can highlight the most useful responses, but voting is a social process that can gain momentum based on the popularity of responses and the polarity of existing votes. We propose the Chinese Voting Process (CVP) which models the evolution of helpfulness votes as a self-reinforcing process dependent on position and presentation biases. We evaluate this model on Amazon product reviews and more than 80 StackExchange forums, measuring the intrinsic quality of individual responses and behavioral coefficients of different communities.", "target": "Beyond Exchangeability: The Chinese Voting Process"}
{"id": "task1540-89ec796a24d045ff91f15bc45e4e2a1c", "input": "Machine-learning techniques are widely used in security-related applications, like spam and malware detection. However, in such settings, they have been shown to be vulnerable to adversarial attacks, including the deliberate manipulation of data at test time to evade detection. In this work, we focus on the vulnerability of linear classifiers to evasion attacks. This can be considered a relevant problem, as linear classifiers have been increasingly used in embedded systems and mobile devices for their low processing time and memory requirements. We exploit recent findings in robust optimization to investigate the link between regularization and security of linear classifiers, depending on the type of attack. We also analyze the relationship between the sparsity of feature weights, which is desirable for reducing processing cost, and the security of linear classifiers. We further propose a novel octagonal regularizer that allows us to achieve a proper trade-off between them. Finally, we empirically show how this regularizer can improve classifier security and sparsity in real-world application examples including spam and malware detection.", "target": "On Security and Sparsity of Linear Classifiers for Adversarial Settings"}
{"id": "task1540-ff956737d80642918b70c4702e1bdaa8", "input": "Recurrent neural networks (RNN) are capable of learning to encode and exploit activation history over an arbitrary timescale. However, in practice, state of the art gradient descent based training methods are known to suffer from difficulties in learning long term dependencies. Here, we describe a novel training method that involves concurrent parallel cloned networks, each sharing the same weights, each trained at different stimulus phase and each maintaining independent activation histories. Training proceeds by recursively performing batch-updates over the parallel clones as activation history is progressively increased. This allows conflicts to propagate hierarchically from short-term contexts towards longer-term contexts until they are resolved. We illustrate the parallel clones method and hierarchical conflict propagation with a character-level deep RNN tasked with memorizing a paragraph of Moby Dick (by Herman Melville).", "target": "Hierarchical Conflict Propagation: Sequence Learning in a Recurrent Deep Neural Network"}
{"id": "task1540-3bb53b6c282849d88748787e8f3a1a29", "input": "Can one parallelize complex exploration\u2013 exploitation tradeoffs? As an example, consider the problem of optimal highthroughput experimental design, where we wish to sequentially design batches of experiments in order to simultaneously learn a surrogate function mapping stimulus to response and identify the maximum of the function. We formalize the task as a multiarmed bandit problem, where the unknown payoff function is sampled from a Gaussian process (GP), and instead of a single arm, in each round we pull a batch of several arms in parallel. We develop GP-BUCB, a principled algorithm for choosing batches, based on the GP-UCB algorithm for sequential GP optimization. We prove a surprising result; as compared to the sequential approach, the cumulative regret of the parallel algorithm only increases by a constant factor independent of the batch size B. Our results provide rigorous theoretical support for exploiting parallelism in Bayesian global optimization. We demonstrate the effectiveness of our approach on two real-world applications.", "target": "Parallelizing Exploration\u2013Exploitation Tradeoffs with Gaussian Process Bandit Optimization"}
{"id": "task1540-743f6f9617084b8eae5aa42f223b40b0", "input": "Most of the existing image-to-image translation frameworks\u2014mapping an image in one domain to a corresponding image in another\u2014are based on supervised learning, i.e., pairs of corresponding images in two domains are required for learning the translation function. This largely limits their applications, because capturing corresponding images in two different domains is often a difficult task. To address the issue, we propose the UNsupervised Image-to-image Translation (UNIT) framework, which is based on variational autoencoders and generative adversarial networks. The proposed framework can learn the translation function without any corresponding images in two domains. We enable this learning capability by combining a weight-sharing constraint and an adversarial training objective. Through visualization results from various unsupervised image translation tasks, we verify the effectiveness of the proposed framework. An ablation study further reveals the critical design choices. Moreover, we apply the UNIT framework to the unsupervised domain adaptation task and achieve better results than competing algorithms do in benchmark datasets.", "target": "Unsupervised Image-to-Image Translation Networks"}
{"id": "task1540-f3f3d48492f741f4be3a301d2f7f9dca", "input": "Deep neural networks can be obscenely wasteful. When processing video, a convolutional network expends a fixed amount of computation for each frame with no regard to the similarity between neighbouring frames. As a result, it ends up repeatedly doing very similar computations. To put an end to such waste, we introduce SigmaDelta networks. With each new input, each layer in this network sends a discretized form of its change in activation to the next layer. Thus the amount of computation that the network does scales with the amount of change in the input and layer activations, rather than the size of the network. We introduce an optimization method for converting any pre-trained deep network into an optimally efficient Sigma-Delta network, and show that our algorithm, if run on the appropriate hardware, could cut at least an order of magnitude from the computational cost of processing video data.", "target": "SIGMA-DELTA QUANTIZED NETWORKS"}
{"id": "task1540-41fb234bff624c4493c381cefd39683a", "input": "One of the most important aims of the fields of robotics, artificial intelligence and artificial life is the design and construction of systems and machines as versatile and as reliable as living organisms at performing high level human-like tasks. But how are we to evaluate artificial systems if we are not certain how to measure these capacities in living systems, let alone how to define life or intelligence? Here I survey a concrete metric towards measuring abstract properties of natural and artificial systems, such as the ability to react to the environment and to control one\u2019s own behaviour.", "target": "Quantifying Natural and Artificial Intelligence in Robots and Natural Systems with an Algorithmic Behavioural Test"}
{"id": "task1540-ddb571fcfed74277b06676d2284565a1", "input": "With the rise of big data sets, the popularity of kernel methods declined and neural networks took over again. The main problem with kernel methods is that the kernel matrix grows quadratically with the number of data points. Most attempts to scale up kernel methods solve this problem by discarding data points or basis functions of some approximation of the kernel map. Here we present a simple yet effective alternative for scaling up kernel methods that takes into account the entire data set via doubly stochastic optimization of the emprical kernel map. The algorithm is straightforward to implement, in particular in parallel execution settings; it leverages the full power and versatility of classical kernel functions without the need to explicitly formulate a kernel map approximation. We provide empirical evidence that the algorithm works on large data sets.", "target": "Doubly stochastic large scale kernel learning with the empirical kernel map"}
{"id": "task1540-43207c34415f40d3ad2e0a5aa65d4636", "input": "Driven by the multi-level structure of human intracranial electroencephalogram (iEEG) recordings of epileptic seizures, we introduce a new variant of a hierarchical Dirichlet Process\u2014the multi-level clustering hierarchical Dirichlet Process (MLC-HDP)\u2014that simultaneously clusters datasets on multiple levels. Our seizure dataset contains brain activity recorded in typically more than a hundred individual channels for each seizure of each patient. The MLC-HDP model clusters over channels-types, seizure-types, and patient-types simultaneously. We describe this model and its implementation in detail. We also present the results of a simulation study comparing the MLC-HDP to a similar model, the Nested Dirichlet Process and finally demonstrate the MLC-HDP\u2019s use in modeling seizures across multiple patients. We find the MLC-HDP\u2019s clustering to be comparable to independent human physician clusterings. To our knowledge, the MLCHDP model is the first in the epilepsy literature capable of clustering seizures within and between patients.", "target": "A Hierarchical Dirichlet Process Model with Multiple Levels of Clustering for Human EEG Seizure Modeling"}
{"id": "task1540-2acd20017a2d4e08a548ad0cded147f2", "input": "Understanding user instructions in natural language is an active research topic in AI and robotics. Typically, natural user instructions are high-level and can be reduced into low-level tasks expressed in common verbs (e.g., \u2018take\u2019, \u2018get\u2019, \u2018put\u2019). For robots understanding such instructions, one of the key challenges is to process high-level user instructions and achieve the specified tasks with robots\u2019 primitive actions. To address this, we propose novel algorithms by utilizing semantic roles of common verbs defined in semantic dictionaries and integrating multiple open knowledge to generate task plans. Specifically, we present a new method for matching and recovering semantics of user instructions and a novel task planner that exploits functional knowledge of robot\u2019s action model. To verify and evaluate our approach, we implemented a prototype system using knowledge from several open resources. Experiments on our system confirmed the correctness and efficiency of our algorithms. Notably, our system has been deployed in the KeJia robot, which participated the annual RoboCup@Home competitions in the past three years and achieved encouragingly high scores in the benchmark tests.", "target": "Understanding User Instructions by Utilizing Open Knowledge for Service Robots"}
{"id": "task1540-6efbcd3f295e4fd5ad2611958678e029", "input": "Deep learning is a branch of artificial intelligence employing deep neural network architectures that has significantly advanced the state-of-the-art in computer vision, speech recognition, natural language processing and other domains. In November 2015, Google released TensorFlow, an open source deep learning software library for defining, training and deploying machine learning models. In this paper, we review TensorFlow and put it in context of modern deep learning concepts and software. We discuss its basic computational paradigms and distributed execution model, its programming interface as well as accompanying visualization toolkits. We then compare TensorFlow to alternative libraries such as Theano, Torch or Caffe on a qualitative as well as quantitative basis and finally comment on observed use-cases of TensorFlow in academia and industry.", "target": "A Tour of TensorFlow"}
{"id": "task1540-e6349a740239409e9cd07d21e165cc28", "input": "There are few knowledge representation (KR) techniques available for efficiently representing knowledge. However, with the increase in complexity, better methods are needed. Some researchers came up with hybrid mechanisms by combining two or more methods. In an effort to construct an intelligent computer system, a primary consideration is to represent large amounts of knowledge in a way that allows effective use and efficiently organizing information to facilitate making the recommended inferences. There are merits and demerits of combinations, and standardized method of KR is needed. In this paper, various hybrid schemes of KR were explored at length and details presented. KeywordsKnowledge representation; hybrid system; hybrid schema structure.", "target": "Hybrid Systems for Knowledge Representation in Artificial Intelligence"}
{"id": "task1540-75a1634615454f8280fc7ddc919df5a5", "input": "In this paper, we propose an extremely simple deep model for the unsupervised nonlinear dimensionality reduction \u2013 deep distributed random samplings. First, its network structure is novel: each layer of the network is a group of mutually independent k-centers clusterings. Second, its learning method is extremely simple: the k centers of each clustering are only k randomly selected examples from the training data; for small-scale data sets, the k centers are further randomly reconstructed by a simple cyclic-shift operation. Experimental results on nonlinear dimensionality reduction show that the proposed method can learn abstract representations on both large-scale and small-scale problems, and meanwhile is much faster than deep neural networks on large-scale problems.", "target": "Learning Deep Representations By Distributed Random Samplings"}
{"id": "task1540-a405c782040c4d9fa6f13418fcfbeea4", "input": "We study the problem of prediction with expert advice when the number of experts in question may be extremely large or even infinite. We devise an algorithm that obtains a tight regret bound of r Op\u01ebT `N ` ? NT q, where N is the empirical \u01eb-covering number of the sequence of loss functions generated by the environment. In addition, we present a hedging procedure that allows us to find the optimal \u01eb in hindsight. Finally, we discuss a few interesting applications of our algorithm. We show how our algorithm is applicable in the approximately low rank experts model of Hazan et al., 2016, and discuss the case of experts with bounded variation, in which there is a surprisingly large gap between the regret bounds obtained in the statistical and online settings.", "target": "Online Learning with Many Experts"}
{"id": "task1540-d442a1f3ff27485f89ecbc5281f89df2", "input": "The paper presents a new script classification method for the discrimination of the South Slavic medieval labels. It consists in the textural analysis of the script types. In the first step, each letter is coded by the equivalent script type, which is defined by its typographical features. Obtained coded text is subjected to the run-length statistical analysis and to the adjacent local binary pattern analysis in order to extract the features. The result shows a diversity between the extracted features of the scripts, which makes the feature classification more effective. It is the basis for the classification process of the script identification by using an extension of a state-of-the-art approach for document clustering. The proposed method is evaluated on an example of hand-engraved in stone and handprinted in paper labels in old Cyrillic, angular and round Glagolitic. Experiments demonstrate very positive results, which prove the effectiveness of the proposed method.", "target": "An Approach to the Analysis of the South Slavic Medieval Labels Using Image Texture"}
{"id": "task1540-319e02c086b945df9503b8046c61a204", "input": "Arabic language and writing are now facing a resurgence of international normative solutions that challenge most of their local or network based operating principles. Even if the multilingual digital coding solutions, especially those proposed by Unicode, have solved many difficulties of Arabic writing, the linguistic aspect is still in search of more adapted solutions. Terminology is one of the sectors in which the Arabic language requires a deep modernization of its classical productivity models. The normative approach, in particular that of the ISO TC37, is proposed as one of the solutions that would allow it to combine with international standards to better integrate the knowledge society under construction.", "target": "Normalisation de la langue et de l\u2019e\u0301criture arabe : enjeux culturels re\u0301gionaux et mondiaux"}
{"id": "task1540-40c860633eb04f199bd3054f021e8a30", "input": "Based on a new atomic norm, we propose a new convex formulation for sparse matrix factorization problems in which the number of nonzero elements of the factors is assumed fixed and known. The formulation counts sparse PCA with multiple factors, subspace clustering and low-rank sparse bilinear regression as potential applications. We compute slow rates and an upper bound on the statistical dimension Amelunxen et al. (2013) of the suggested norm for rank 1 matrices, showing that its statistical dimension is an order of magnitude smaller than the usual l1-norm, trace norm and their combinations. Even though our convex formulation is in theory hard and does not lead to provably polynomial time algorithmic schemes, we propose an active set algorithm leveraging the structure of the convex problem to solve it and show promising numerical results.", "target": "Tight convex relaxations for sparse matrix factorization"}
{"id": "task1540-aea7d096df984357870044e753754d6a", "input": "As a step towards developing zero-shot task generalization capabilities in reinforcement learning (RL), we introduce a new RL problem where the agent should learn to execute sequences of instructions after learning useful skills that solve subtasks. In this problem, we consider two types of generalizations: to previously unseen instructions and to longer sequences of instructions. For generalization over unseen instructions, we propose a new objective which encourages learning correspondences between similar subtasks by making analogies. For generalization over sequential instructions, we present a hierarchical architecture where a meta controller learns to use the acquired skills for executing the instructions. To deal with delayed reward, we propose a new neural architecture in the meta controller that learns when to update the subtask, which makes learning more efficient. Experimental results on a stochastic 3D domain show that the proposed ideas are crucial for generalization to longer instructions as well as unseen instructions.", "target": "Zero-Shot Task Generalization with Multi-Task Deep Reinforcement Learning"}
{"id": "task1540-5759bfe072f64cd8aa9211f24aae0a11", "input": "In recent years, Deep Learning has become the go-to solution for a broad range of applications, often outperforming state-of-the-art. However, it is important, for both theoreticians and practitioners, to gain a deeper understanding of the difficulties and limitations associated with common approaches and algorithms. We describe four families of problems for which some of the commonly used existing algorithms fail or suffer significant difficulty. We illustrate the failures through practical experiments, and provide theoretical insights explaining their source, and how they might be remedied.", "target": "Failures of Deep Learning"}
{"id": "task1540-5831b5b39bc84a80ade8bc4d47ecdff3", "input": "We describe recurrent neural networks (RNNs), which have attracted great attention on sequential tasks, such as handwriting recognition, speech recognition and image to text. However, compared to general feedforward neural networks, RNNs have feedback loops, which makes it a little hard to understand the backpropagation step. Thus, we focus on basics, especially the error backpropagation to compute gradients with respect to model parameters. Further, we go into detail on how error backpropagation algorithm is applied on long short-term memory (LSTM) by unfolding the memory unit.", "target": "A Gentle Tutorial of Recurrent Neural Network with Error Backpropagation"}
{"id": "task1540-7117175e9d524a83a092300a1bd28387", "input": "Motivated by concerns for user privacy, we design a steganographic system (\u201cstegosystem\u201d) that enables two users to exchange encrypted messages without an adversary detecting that such an exchange is taking place. We propose a new linguistic stegosystem based on a Long ShortTerm Memory (LSTM) neural network. We demonstrate our approach on the Twitter and Enron email datasets and show that it yields high-quality steganographic text while significantly improving capacity (encrypted bits per word) relative to the state-of-the-art.", "target": "Generating Steganographic Text with LSTMs"}
{"id": "task1540-58f00837e55242db992d6b1ce0334b1d", "input": "Rapid crisis response requires real-time analysis of messages. After a disaster happens, volunteers attempt to classify tweets to determine needs, e.g., supplies, infrastructure damage, etc. Given labeled data, supervised machine learning can help classify these messages. Scarcity of labeled data causes poor performance in machine training. Can we reuse old tweets to train classifiers? How can we choose labeled tweets for training? Specifically, we study the usefulness of labeled data of past events. Do labeled tweets in different language help? We observe the performance of our classifiers trained using different combinations of training sets obtained from past disasters. We perform extensive experimentation on real crisis datasets and show that the past labels are useful when both source and target events are of the same type (e.g. both earthquakes). For similar languages (e.g., Italian and Spanish), cross-language domain adaptation was useful, however, when for different languages (e.g., Italian and English), the performance decreased.", "target": "Cross-Language Domain Adaptation for Classifying Crisis-Related Short Messages"}
{"id": "task1540-4a5933ddd5b641659fd5d6fcadb8b152", "input": "Recommender systems play an increasingly important role in online applications to help users find what they need or prefer. Collaborative filtering algorithms that generate predictions by analyzing the user-item rating matrix perform poorly when the matrix is sparse. To alleviate this problem, this paper proposes a simple recommendation algorithm that fully exploits the similarity information among users and items and intrinsic structural information of the user-item matrix. The proposed method constructs a new representation which preserves affinity and structure information in the user-item rating matrix and then performs recommendation task. To capture proximity information about users and items, two graphs are constructed. Manifold learning idea is used to constrain the new representation to be smooth on these graphs, so as to enforce users and item proximities. Our model is formulated as a convex optimization problem, for which we need to solve the well known Sylvester equation only. We carry out extensive empirical evaluations on six benchmark datasets to show the effectiveness of this approach.", "target": "Top-N Recommendation on Graphs"}
{"id": "task1540-45e11824963948f39fe552d39fdc2cf5", "input": "State-of-the-art sequence labeling systems traditionally require large amounts of taskspecific knowledge in the form of handcrafted features and data pre-processing. In this paper, we introduce a novel neutral network architecture that benefits from both wordand character-level representations automatically, by using combination of bidirectional LSTM, CNN and CRF. Our system is truly end-to-end, requiring no feature engineering or data preprocessing, thus making it applicable to a wide range of sequence labeling tasks on different languages. We evaluate our system on two data sets for two sequence labeling tasks \u2014 Penn Treebank WSJ corpus for part-of-speech (POS) tagging and CoNLL 2003 corpus for named entity recognition (NER). We obtain stateof-the-art performance on both the two data \u2014 97.55% accuracy for POS tagging and 91.21% F1 for NER.", "target": "End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF"}
{"id": "task1540-d9a5612250e0425f80652f7d68803fa3", "input": "We report efforts to interpret a Long-Short Term Memory neural network trained to recognize gender and writing instructions on a set of essays from a psychological educational intervention known as a values affirmation. Adjusting the model at test time to output sequential probabilities as each new token is encountered, rather than predicting the class holistically, we query the model with carefully constructed sentences designed to test a theoretically informed hypothesis: male versus female students write in a way that reflects a greater emphasis on independence versus interdependence, respectively. The LSTM model outperforms the baseline, and the model\u2019s predictions to our constructed test sentences suggest modest support for these hypotheses.", "target": "Interpreting Neural Networks to Understand Written Justifications in Values-Affirmation Essays"}
{"id": "task1540-bb394c3f4d4c4c15acfcaae347dd7344", "input": "In this paper we present a short history of logics: from particular cases of 2-symbol or numerical valued logic to the general case of n-symbol or numerical valued logic. We show generalizations of 2-valued Boolean logic to fuzzy logic, also from the Kleene\u2019s and Lukasiewicz\u2019 3-symbol valued logics or Belnap\u2019s 4-symbol valued logic to the most general n-symbol or numerical valued refined neutrosophic logic. Two classes of neutrosophic norm (n-norm) and neutrosophic conorm (n-conorm) are defined. Examples of applications of neutrosophic logic to physics are listed in the last section. Similar generalizations can be done for n-Valued Refined Neutrosophic Set, and respectively nValued Refined Neutrosopjhic Probability.", "target": "n-Valued Refined Neutrosophic Logic and Its Applications to Physics"}
{"id": "task1540-d4e6dedd6c894a9e8a90c5a708c603f3", "input": "LSTMs have become a basic building block for many deep NLP models. In recent years, many improvements and variations have been proposed for deep sequence models in general, and LSTMs in particular. We propose and analyze a series of augmentations and modifications to LSTM networks resulting in improved performance for text classification datasets. We observe compounding improvements on traditional LSTMs using Monte Carlo test-time model averaging, average pooling, and residual connections, along with four other suggested modifications. Our analysis provides a simple, reliable, and high quality baseline model.", "target": "BINING RECENT INSIGHTS FOR LSTMS"}
{"id": "task1540-fe54fef6f3984532b1b39e57d7f2c471", "input": "For a finite state automaton, a synchronizing sequence is an input sequence that takes all the states to the same state. Checking the existence of a synchronizing sequence and finding a synchronizing sequence, if one exists, can be performed in polynomial time. However, the problem of finding a shortest synchronizing sequence is known to be NP-hard. In this work, the usefulness of Answer Set Programming to solve this optimization problem is investigated, in comparison with brute-force algorithms and SAT-based approaches.", "target": "Generating Shortest Synchronizing Sequences using Answer Set Programming"}
{"id": "task1540-f85c741113da43f7836d6458ecfff5a2", "input": "We derive bounds on the sample complexity of empirical risk minimization (ERM) in the context of minimizing non-convex risks that admit the strict saddle property. Recent progress in non-convex optimization has yielded efficient algorithms for minimizing such functions. Our results imply that these efficient algorithms are statistically stable and also generalize well. In particular, we derive fast rates which resemble the bounds that are often attained in the strongly convex setting. We specify our bounds to Principal Component Analysis and Independent Component Analysis. Our results and techniquesmay pave the way for statistical analyses of additional strict saddle problems.", "target": "Fast Rates for Empirical Risk Minimization of Strict Saddle Problems"}
{"id": "task1540-c85c2152c3ed48ed9306e7f83b89bf91", "input": "Inverse optimal control, also known as inverse reinforcement learning, is the problem of recovering an unknown reward function in a Markov decision process from expert demonstrations of the optimal policy. We introduce a probabilistic inverse optimal control algorithm that scales gracefully with task dimensionality, and is suitable for large, continuous domains where even computing a full policy is impractical. By using a local approximation of the reward function, our method can also drop the assumption that the demonstrations are globally optimal, requiring only local optimality. This allows it to learn from examples that are unsuitable for prior methods.", "target": "Continuous Inverse Optimal Control with Locally Optimal Examples"}
{"id": "task1540-669ceed5aa8c44b9963f9047c57263f4", "input": "Lifting attempts to speedup probabilistic inference by exploiting symmetries in the model. Exact lifted inference methods, like their propositional counterparts, work by recursively decomposing the model and the problem. In the propositional case, there exist formal structures, such as decomposition trees (dtrees), that represent such a decomposition and allow us to determine the complexity of inference a priori. However, there is currently no equivalent structure nor analogous complexity results for lifted inference. In this paper, we introduce FO-dtrees, which upgrade propositional dtrees to the first-order level. We show how these trees can characterize a lifted inference solution for a probabilistic logical model (in terms of a sequence of lifted operations), and make a theoretical analysis of the complexity of lifted inference in terms of the novel notion of lifted width for the tree.", "target": "First-Order Decomposition Trees"}
{"id": "task1540-dbfc13519543422098b60aeb6c2b89de", "input": "The paper steps outside the comfort-zone of the traditional NLP tasks like automatic speech recognition (ASR) and machine translation (MT) to addresses two novel problems arising in the automated multilingual news monitoring: segmentation of the TV and radio program ASR transcripts into individual stories, and clustering of the individual stories coming from various sources and languages into storylines. Storyline clustering of stories covering the same events is an essential task for inquisitorial media monitoring. We address these two problems jointly by engaging the low-dimensional semantic representation capabilities of the sequence to sequence neural translation models. To enable joint multi-task learning for multilingual neural translation of morphologically rich languages we replace the attention mechanism with the sliding-window mechanism and operate the sequence to sequence neural translation model on the character-level rather than on the word-level. The story segmentation and storyline clustering problem is tackled by examining the low-dimensional vectors produced as a side-product of the neural translation process. The results of this paper describe a novel approach to the automatic story segmentation and storyline clustering problem.", "target": "Character-Level Neural Translation for Multilingual Media Monitoring in the SUMMA Project"}
{"id": "task1540-e9461ce95cbb4eb788e47775aa55f00c", "input": "Even for common NLP tasks, sufficient supervision is not available in many languages\u2014morphological tagging is no exception. In the work presented here, we explore a transfer learning scheme, whereby we train character-level recurrent neural taggers to predict morphological taggings for high-resource languages and low-resource languages together. Learning joint character representations among multiple related languages successfully enables knowledge transfer from the high-resource languages to the low-resource ones, improving accuracy by up to 30%.", "target": "Cross-lingual, Character-Level Neural Morphological Tagging"}
{"id": "task1540-f266720ef1ab4cd2a76b14133fdbbc15", "input": "Finite chase, or alternatively chase termination, is an important condition to ensure the decidability of existential rule languages. In the past few years, a number of rule languages with finite chase have been studied. In this work, we propose a novel approach for classifying the rule languages with finite chase. Using this approach, a family of decidable rule languages, which extend the existing languages with the finite chase property, are naturally defined. We then study the complexity of these languages. Although all of them are tractable for data complexity, we show that their combined complexity can be arbitrarily high. Furthermore, we prove that all the rule languages with finite chase that extend the weakly acyclic language are of the same expressiveness as the weakly acyclic one, while rule languages with higher combined complexity are in general more succinct than those with lower combined complexity.", "target": "Existential Rule Languages with Finite Chase: Complexity and Expressiveness"}
{"id": "task1540-e9f0a382b5704ebd94e1d6a6dc5cf14e", "input": "The efficiency of algorithms using sec\u00ad ondary structures for probabilistic inference in Bayesian networks can be improved by ex\u00ad ploiting independence relations induced by evidence and the direction of the links in the original network. In this paper we present an algorithm that on-line exploits indepen\u00ad dence relations induced by evidence and the direction of the links in the original network to reduce both time and space costs. In\u00ad stead of multiplying the conditional proba\u00ad bility distributions for the various cliques, we determine on-line which potentials to multi\u00ad ply when a message is to be produced. The performance improvement of the algorithm is emphasized through empirical evaluations in\u00ad volving large real world Bayesian networks, and we compare the method with the HUGIN and Shafer-Shenoy inference algorithms.", "target": "Lazy Propagation in Junction Trees"}
{"id": "task1540-dc960104922a4c698e799ef155259800", "input": "We propose Novel Object Captioner (NOC), a deep visual semantic captioning model that can describe a large number of object categories not present in existing image-caption datasets. Recent captioning models are limited in their ability to scale and describe concepts outside of paired image-text corpora. Our model takes advantage of external sources labeled images from object recognition datasets, and semantic knowledge extracted from unannotated text and combines them to generate descriptions about novel objects. We propose minimizing a joint objective which can learn from diverse data sources and leverage distributional semantic embeddings, enabling the model to generalize and describe novel objects outside of imagecaption datasets. We demonstrate that our model exploits semantic information to generate captions for hundreds of object categories in the ImageNet object recognition dataset that are not observed in imagecaption training data, as well as many categories that are observed very rarely.", "target": "Captioning Images with Diverse Objects"}
{"id": "task1540-16aacbbf94514f389b9afff8a19afe84", "input": "With the acceptance of Western culture and science, Traditional Chinese Medicine (TCM) has become a controversial issue in China. So, it\u2019s important to study the public\u2019s sentiment and opinion on TCM. The rapid development of online social network, such as twitter, make it convenient and efficient to sample hundreds of millions of people for the aforementioned sentiment study. To the best of our knowledge, the present work is the first attempt that applies sentiment analysis to the domain of TCM on Sina Weibo (a twitter-like microblogging service in China). In our work, firstly we collect tweets topic about TCM from Sina Weibo, and label the tweets as supporting TCM and opposing TCM automatically based on user tag. Then, a support vector machine classifier has been built to predict the sentiment of TCM tweets without labels. Finally, we present a method to adjust the classifier result. The performance of F-measure attained with our method is 97%.", "target": "Sentiment Analysis based on User Tag for Traditional Chinese Medicine in Weibo"}
{"id": "task1540-b835180669914a539e1138c2c6cdf02f", "input": "We present two algorithms for exact and ap\u00ad proximate inference in causal networks. The first algorithm, dynamic conditioning, is a re\u00ad finement of cutset conditioning that has lin\u00ad ear complexity on some networks for which cutset conditioning is exponential. The sec\u00ad ond algorithm, B-conditioning, is an algo\u00ad rithm for approximate inference that allows one to trade-off the quality of approxima\u00ad tions with the computation time. We also present some experimental results illustrating the properties of the proposed algorithms.", "target": "Conditioning Algorithms for Exact and Approximate Inference in Causal Networks"}
{"id": "task1540-bf52a2a7cdbd48afb2af1299035f193c", "input": "This paper is concerned with algorithms for prediction of discrete sequences over a finite alphabet, using variable order Markov models. The class of such algorithms is large and in principle includes any lossless compression algorithm. We focus on six prominent prediction algorithms, including Context Tree Weighting (CTW), Prediction by Partial Match (PPM) and Probabilistic Suffix Trees (PSTs). We discuss the properties of these algorithms and compare their performance using real life sequences from three domains: proteins, English text and music pieces. The comparison is made with respect to prediction quality as measured by the average log-loss. We also compare classification algorithms based on these predictors with respect to a number of large protein classification tasks. Our results indicate that a \u201cdecomposed\u201d CTW (a variant of the CTW algorithm) and PPM outperform all other algorithms in sequence prediction tasks. Somewhat surprisingly, a different algorithm, which is a modification of the Lempel-Ziv compression algorithm, significantly outperforms all algorithms on the protein classification problems.", "target": "On Prediction Using Variable Order Markov Models"}
{"id": "task1540-372aa334ac134d819ecc4b576908316f", "input": "Generating a novel textual description of an image is an interesting problem that connects computer vision and natural language processing. In this paper, we present a simple model that is able to generate descriptive sentences given a sample image. This model has a strong focus on the syntax of the descriptions. We train a purely bilinear model that learns a metric between an image representation (generated from a previously trained Convolutional Neural Network) and phrases that are used to described them. The system is then able to infer phrases from a given image sample. Based on caption syntax statistics, we propose a simple language model that can produce relevant descriptions for a given test image using the phrases inferred. Our approach, which is considerably simpler than state-of-the-art models, achieves comparable results in two popular datasets for the task: Flickr30k and the recently proposed Microsoft COCO.", "target": "Phrase-based Image Captioning"}
{"id": "task1540-437adabda45a4105b5416a3fe2e99afb", "input": "In practical situations, interval-valued fuzzy sets are frequently encountered. In this paper, firstly, we present shadowed sets for interpreting and understanding interval fuzzy sets. We also provide an analytic solution to computing the pair of thresholds by searching for a balance of uncertainty in the framework of shadowed sets. Secondly, we construct errorsbased three-way approximations of interval-valued fuzzy sets. We also provide an alternative decision-theoretic formulation for calculating the pair of thresholds by transforming intervalvalued loss functions into single-valued loss functions, in which the required thresholds are computed by minimizing decision costs. Thirdly, we compute errors-based three-way approximations of interval-valued fuzzy sets by using interval-valued loss functions. Finally, we employ several examples to illustrate that how to take an action for an object with intervalvalued membership grade by using interval-valued loss functions.", "target": "Decision-theoretic rough sets-based three-way approximations of interval-valued fuzzy sets"}
{"id": "task1540-0bc1856ae330485ba403450a719d5010", "input": "We consider the problem of learning good trajectories for manipulation tasks. This is challenging because the criterion defining a good trajectory varies with users, tasks and environments. In this paper, we propose a co-active online learning framework for teaching robots the preferences of its users for object manipulation tasks. The key novelty of our approach lies in the type of feedback expected from the user: the human user does not need to demonstrate optimal trajectories as training data, but merely needs to iteratively provide trajectories that slightly improve over the trajectory currently proposed by the system. We argue that this co-active preference feedback can be more easily elicited from the user than demonstrations of optimal trajectories, which are often challenging and non-intuitive to provide on high degrees of freedom manipulators. Nevertheless, theoretical regret bounds of our algorithm match the asymptotic rates of optimal trajectory algorithms. We demonstrate the generalizability of our algorithm on a variety of grocery checkout tasks, for whom, the preferences were not only influenced by the object being manipulated but also by the surrounding environment.1", "target": "Learning Trajectory Preferences for Manipulators via Iterative Improvement"}
{"id": "task1540-3800588ab60e4836b41c919c5bca44db", "input": "This paper presents an empirical study of two widely-used sequence prediction models, Conditional Random Fields (CRFs) and Long Short-Term Memory Networks (LSTMs), on two fundamental tasks for Vietnamese text processing, including part-of-speech tagging and named entity recognition. We show that a strong lower bound for labeling accuracy can be obtained by relying only on simple word-based features with minimal handcrafted feature engineering, of 90.65% and 86.03% performance scores on the standard test sets for the two tasks respectively. In particular, we demonstrate empirically the surprising efficiency of word embeddings in both of the two tasks, with both of the two models. We point out that the state-of-the-art LSTMs model does not always outperform significantly the traditional CRFs model, especially on moderate-sized data sets. Finally, we give some suggestions and discussions for efficient use of sequence labeling models in practical applications.", "target": "An Empirical Study of Discriminative Sequence Labeling Models for Vietnamese Text Processing"}
{"id": "task1540-e317fc5b0e194bb09302de0d034903b0", "input": "Given the incessant growth of documents describing the opinions of different people circulating on the web, including Web 2.0 has made it possible to give an opinion on any product in the net. In this paper, we examine the various opinions expressed in the tweets and classify them (positive, negative or neutral) by using the emoticons for the Bayesian method and adjectives and adverbs for the Turney\u2019s method.", "target": "Detecting Opinions in Tweets"}
{"id": "task1540-b3905604550040b692afc43376a98903", "input": "Today data mining techniques are exploited in medical science for diagnosing, overcoming and treating diseases. Neural network is one of the techniques which are widely used for diagnosis in medical field. In this article efficiency of nine algorithms, which are basis of neural network learning in diagnosing cardiovascular diseases, will be assessed. Algorithms are assessed in terms of accuracy, sensitivity, transparency, AROC and convergence rate by means of 10 fold cross validation. The results suggest that in training phase, Lonberg-M algorithm has the best efficiency in terms of all metrics, algorithm OSS has maximum accuracy in testing phase, algorithm SCG has the maximum transparency and algorithm CGB has the maximum sensitivity. Keywords\u2014 cardiovascular disease; neural network; learning algorithms.", "target": "Comparing learning algorithms in neural network for diagnosing cardiovascular disease"}
{"id": "task1540-b3d5c02b9a9a48eaa918a6c44212c628", "input": "How do news sources tackle controversial issues? In this work, we take a data-driven approach to understand how controversy interplays with emotional expression and biased language in the news. We begin by introducing a new dataset of controversial and noncontroversial terms collected using crowdsourcing. Then, focusing on 15 major U.S. news outlets, we compare millions of articles discussing controversial and non-controversial issues over a span of 7 months. We find that in general, when it comes to controversial issues, the use of negative affect and biased language is prevalent, while the use of strong emotion is tempered. We also observe many differences across news sources. Using these findings, we show that we can indicate to what extent an issue is controversial, by comparing it with other issues in terms of how they are portrayed across different media.", "target": "Controversy and Sentiment in Online News"}
{"id": "task1540-30be581ad42046bba3efe3fc2b25dd30", "input": "The skip-thought model has been proven to be effective at learning sentence representations and capturing sentence semantics. In this paper, we propose a suite of techniques to trim and improve it. First, we validate a hypothesis that, given a current sentence, inferring the previous and inferring the next sentence provide similar supervision power, therefore only one decoder for predicting the next sentence is preserved in our trimmed skip-thought model. Second, we present a connection layer between encoder and decoder to help the model to generalize better on semantic relatedness tasks. Third, we found that a good word embedding initialization is also essential for learning better sentence representations. We train our model unsupervised on a large corpus with contiguous sentences, and then evaluate the trained model on 7 supervised tasks, which includes semantic relatedness, paraphrase detection, and text classification benchmarks. We empirically show that, our proposed model is a faster, lighter-weight and equally powerful alternative to the original skip-thought model.", "target": "Trimming and Improving Skip-thought Vectors"}
{"id": "task1540-b5eda0ffc4ac4406841896f0f30df967", "input": "We describe Swapout, a new stochastic training method, that outperforms ResNets of identical network structure yielding impressive results on CIFAR-10 and CIFAR100. Swapout samples from a rich set of architectures including dropout [17], stochastic depth [6] and residual architectures [4, 5] as special cases. When viewed as a regularization method swapout not only inhibits co-adaptation of units in a layer, similar to dropout, but also across network layers. We conjecture that swapout achieves strong regularization by implicitly tying the parameters across layers. When viewed as an ensemble training method, it samples a much richer set of architectures than existing methods such as dropout or stochastic depth. We propose a parameterization that reveals connections to exiting architectures and suggests a much richer set of architectures to be explored. We show that our formulation suggests an efficient training method and validate our conclusions on CIFAR-10 and CIFAR-100 matching state of the art accuracy. Remarkably, our 32 layer wider model performs similar to a 1001 layer ResNet model.", "target": "Swapout: Learning an ensemble of deep architectures"}
{"id": "task1540-63490108ef9f436aa24e0ceef0e64277", "input": "Parsing accuracy using efficient greedy transition systems has improved dramatically in recent years thanks to neural networks. Despite striking results in dependency parsing, however, neural models have not surpassed stateof-the-art approaches in constituency parsing. To remedy this, we introduce a new shiftreduce system whose stack contains merely sentence spans, represented by a bare minimum of LSTM features. We also design the first provably optimal dynamic oracle for constituency parsing, which runs in amortized O(1) time, compared to O(n) oracles for standard dependency parsing. Training with this oracle, we achieve the best F1 scores on both English and French of any parser that does not use reranking or external data.", "target": "Span-Based Constituency Parsing with a Structure-Label System and Provably Optimal Dynamic Oracles"}
{"id": "task1540-94d569e6630b49a78fb57256faf00c63", "input": "Policies for complex visual tasks have been successfully learned with deep reinforcement learning, using an approach called deep Q-networks (DQN), but relatively large (task-specific) networks and extensive training are needed to achieve good performance. In this work, we present a novel method called policy distillation that can be used to extract the policy of a reinforcement learning agent and train a new network that performs at the expert level while being dramatically smaller and more efficient. Furthermore, the same method can be used to consolidate multiple task-specific policies into a single policy. We demonstrate these claims using the Atari domain and show that the multi-task distilled agent outperforms the single-task teachers as well as a jointly-trained DQN agent.", "target": "POLICY DISTILLATION"}
{"id": "task1540-2357b84c9e534caea841eb83bd6f6e2a", "input": "Theano is a linear algebra compiler that optimizes a user\u2019s symbolically-specified mathematical computations to produce efficient low-level implementations. In this paper, we present new features and efficiency improvements to Theano, and benchmarks demonstrating Theano\u2019s performance relative to Torch7, a recently introduced machine learning library, and to RNNLM, a C++ library targeted at recurrent neural networks.", "target": "Theano: new features and speed improvements"}
{"id": "task1540-6b134c916bdf4e318074c78c144691c2", "input": "Modeling the purposeful behavior of imperfect agents from a small number of observations is a challenging task. When restricted to the single-agent decision-theoretic setting, inverse optimal control techniques assume that observed behavior is an approximately optimal solution to an unknown decision problem. These techniques learn a utility function that explains the example behavior and can then be used to accurately predict or imitate future behavior in similar observed or unobserved situations. In this work, we consider similar tasks in competitive and cooperative multi-agent domains. Here, unlike single-agent settings, a player cannot myopically maximize its reward; it must speculate on how the other agents may act to influence the game\u2019s outcome. Employing the 1 ar X iv :1 30 8. 35 06 v1 [ cs .G T ] 1 5 A ug 2 01 3 game-theoretic notion of regret and the principle of maximum entropy, we introduce a technique for predicting and generalizing behavior.", "target": "Computational Rationalization: The Inverse Equilibrium Problem"}
{"id": "task1540-fda476d165cb458c86fc3556bcff4568", "input": "In this paper we present a hybrid approach for automatic composition of Web services that generates semantic inputoutput based compositions with optimal end-to-end QoS, minimizing the number of services of the resulting composition. The proposed approach has four main steps: 1) generation of the composition graph for a request; 2) computation of the optimal composition that minimizes a single objective QoS function; 3) multi-step optimizations to reduce the search space by identifying equivalent and dominated services; and 4) hybrid local-global search to extract the optimal QoS with the minimum number of services. An extensive validation with the datasets of the Web Service Challenge 2009-2010 and randomly generated datasets shows that: 1) the combination of local and global optimization is a general and powerful technique to extract optimal compositions in diverse scenarios; and 2) the hybrid strategy performs better than the state-of-the-art, obtaining solutions with less services and optimal QoS. Keywords\u2014Service Composition; Service Optimization; Hybrid Algorithm; QoS-aware; Semantic Web Services.", "target": "Hybrid Optimization Algorithm for Large-Scale QoS-Aware Service Composition"}
{"id": "task1540-89b844de77d141139f4960e8045529e5", "input": "This paper proposes CF-NADE, a neural autoregressive architecture for collaborative filtering (CF) tasks, which is inspired by the Restricted Boltzmann Machine (RBM) based CF model and the Neural Autoregressive Distribution Estimator (NADE). We first describe the basic CF-NADE model for CF tasks. Then we propose to improve the model by sharing parameters between different ratings. A factored version of CF-NADE is also proposed for better scalability. Furthermore, we take the ordinal nature of the preferences into consideration and propose an ordinal cost to optimize CF-NADE, which shows superior performance. Finally, CF-NADE can be extended to a deep model, with only moderately increased computational complexity. Experimental results show that CF-NADE with a single hidden layer beats all previous state-of-the-art methods on MovieLens 1M, MovieLens 10M, and Netflix datasets, and adding more hidden layers can further improve the performance.", "target": "A Neural Autoregressive Approach to Collaborative Filtering"}
{"id": "task1540-d9543de07c0e49a6a17cf9e5b656dc47", "input": "We describe an approach for unsupervised learning of a generic, distributed sentence encoder. Using the continuity of text from books, we train an encoderdecoder model that tries to reconstruct the surrounding sentences of an encoded passage. Sentences that share semantic and syntactic properties are thus mapped to similar vector representations. We next introduce a simple vocabulary expansion method to encode words that were not seen as part of training, allowing us to expand our vocabulary to a million words. After training our model, we extract and evaluate our vectors with linear models on 8 tasks: semantic relatedness, paraphrase detection, image-sentence ranking, question-type classification and 4 benchmark sentiment and subjectivity datasets. The end result is an off-the-shelf encoder that can produce highly generic sentence representations that are robust and perform well in practice. We will make our encoder publicly available.", "target": "Skip-Thought Vectors"}
{"id": "task1540-fb2056a239514997a462550215f4e0c7", "input": "In this study, the problem of shallow parsing of Hindi-English code-mixed social media text (CSMT) has been addressed. We have annotated the data, developed a language identifier, a normalizer, a part-of-speech tagger and a shallow parser. To the best of our knowledge, we are the first to attempt shallow parsing on CSMT. The pipeline developed has been made available to the research community with the goal of enabling better text analysis of Hindi English CSMT. The pipeline is accessible at 1.", "target": "Shallow Parsing Pipeline for Hindi-English Code-Mixed Social Media Text"}
{"id": "task1540-10deba8b096e44b092ba1a6f5a54bf2b", "input": "In this paper, we propose epitomic variational autoencoder (eVAE), a probabilistic generative model of high dimensional data. eVAE is composed of a number of sparse variational autoencoders called \u2018epitome\u2019 such that each epitome partially shares its encoder-decoder architecture with other epitomes in the composition. We show that the proposed model greatly overcomes the common problem in variational autoencoders (VAE) of model over-pruning. We substantiate that eVAE is efficient in using its model capacity and generalizes better than VAE, by presenting qualitative and quantitative results on MNIST and TFD datasets.", "target": "EPITOMIC VARIATIONAL AUTOENCODER"}
{"id": "task1540-23e3b738817d44b0b22b48be5ded4209", "input": "We present a factorized compositional distributional semantics model for the representation of transitive verb constructions. Our model first produces (subject, verb) and (verb, object) vector representations based on the similarity of the nouns in the construction to each of the nouns in the vocabulary and the tendency of these nouns to take the subject and object roles of the verb. These vectors are then combined into a final (subject,verb,object) representation through simple vector operations. On two established tasks for the transitive verb construction our model outperforms recent previous work.", "target": "A Factorized Model for Transitive Verbs in Compositional Distributional Semantics"}
{"id": "task1540-5b2025d6b0f84387aa58c49a14acda3e", "input": "We propose a new, socially-impactful task for natural language processing: from a news corpus, extract names of persons who have been killed by police. We present a newly collected police fatality corpus, which we release publicly, and present a model to solve this problem that uses EM-based distant supervision with logistic regression and convolutional neural network classifiers. Our model outperforms two off-the-shelf event extractor systems, and it can suggest candidate victim names in some cases faster than one of the major manually-collected police fatality databases. Appendix, software, and data are available online at: http://slanglab.cs.umass. edu/PoliceKillingsExtraction/ [This paper appears in Proceedings of EMNLP 2017. This version includes the appendix.]", "target": "Identifying civilians killed by police with distantly supervised entity-event extraction"}
{"id": "task1540-e8e4c951ac214b84bbf280381f7ba205", "input": "The quality of a Neural Machine Translation system depends substantially on the availability of sizable parallel corpora. For low-resource language pairs this is not the case, resulting in poor translation quality. Inspired by work in computer vision, we propose a novel data augmentation approach that targets low-frequency words by generating new sentence pairs containing rare words in new, synthetically created contexts. Experimental results on simulated low-resource settings show that our method improves translation quality by up to 2.9 BLEU points over the baseline and up to 3.2 BLEU over back-translation.", "target": "Data Augmentation for Low-Resource Neural Machine Translation"}
{"id": "task1540-286305d3b6a74eba9d963f7128f1a55a", "input": "Electronic health records capture patient information using structured controlled vocabularies and unstructured narrative text. While structured data typically encodes lab values, encounters and medication lists, unstructured data captures the physician\u2019s interpretation of the patient\u2019s condition, prognosis, and response to therapeutic intervention. In this paper, we demonstrate that information extraction from unstructured clinical narratives is essential to most clinical applications. We perform an empirical study to validate the argument and show that structured data alone is insufficient in resolving eligibility criteria for recruiting patients onto clinical trials for chronic lymphocytic leukemia (CLL) and prostate cancer. Unstructured data is essential to solving 59% of the CLL trial criteria and 77% of the prostate cancer trial criteria. More specifically, for resolving eligibility criteria with temporal constraints, we show the need for temporal reasoning and information integration with medical events within and across unstructured clinical narratives and structured data.", "target": "How essential are unstructured clinical narratives and information fusion to clinical trial recruitment?"}
{"id": "task1540-69370b10a3a04170ba18474cab4888c3", "input": "This paper focuses on style transfer on the basis of non-parallel text. This is an instance of a broader family of problems including machine translation, decipherment, and sentiment modification. The key technical challenge is to separate the content from desired text characteristics such as sentiment. We leverage refined alignment of latent representations across mono-lingual text corpora with different characteristics. We deliberately modify encoded examples according to their characteristics, requiring the reproduced instances to match available examples with the altered characteristics as a population. We demonstrate the effectiveness of this cross-alignment method on three tasks: sentiment modification, decipherment of word substitution ciphers, and recovery of word order.", "target": "Style Transfer from Non-Parallel Text by Cross-Alignment"}
{"id": "task1540-9c3c817dba9347648d6578b60adedb71", "input": "amLite is a framework developed to map ASCII transliterated Amharic texts back to the original Amharic letter texts. The aim of such a framework is to make existing Amharic linguistic data consistent and interoperable among researchers. For achieving the objective, a key map dictionary is constructed using the possible ASCII combinations actively in use for transliterating Amharic letters; and a mapping of the combinations to the corresponding Amharic letters is done. The mapping is then used to replace the Amharic linguistic text back to form the original Amharic letters text. The framework indicated 97.7, 99.7 and 98.4 percentage accuracy on converting the three sample random test data. It is; however, possible to improve the accuracy of the framework by adding an exception to the implementation of the algorithm, or by preprocessing the input text prior to conversion. This paper outlined the rationales behind the need for developing the framework and the processes undertaken in the development.", "target": "amLite: Amharic Transliteration Using Key\r Map Dictionary"}
{"id": "task1540-1f3cee89eb0f4ebea67ac6a4b8197954", "input": "Retrieval tasks typically require a ranking of items given a query. Collaborative filtering tasks, on the other hand, learn to model user\u2019s preferences over items. In this paper we study the joint problem of recommending items to a user with respect to a given query, which is a surprisingly common task. This setup differs from the standard collaborative filtering one in that we are given a query \u00d7 user \u00d7 item tensor for training instead of the more traditional user \u00d7 item matrix. Compared to document retrieval we do have a query, but we may or may not have content features (we will consider both cases) and we can also take account of the user\u2019s profile. We introduce a factorized model for this new task that optimizes the top-ranked items returned for the given query and user. We report empirical results where it outperforms several baselines.", "target": "Latent Collaborative Retrieval"}
{"id": "task1540-cf532ea49b64402d841c9cd25e20a7e4", "input": "We consider the problem of using sentence compression techniques to facilitate queryfocused multi-document summarization. We present a sentence-compression-based framework for the task, and design a series of learning-based compression models built on parse trees. An innovative beam search decoder is proposed to efficiently find highly probable compressions. Under this framework, we show how to integrate various indicative metrics such as linguistic motivation and query relevance into the compression process by deriving a novel formulation of a compression scoring function. Our best model achieves statistically significant improvement over the state-of-the-art systems on several metrics (e.g. 8.0% and 5.4% improvements in ROUGE-2 respectively) for the DUC 2006 and 2007 summarization task.", "target": "A Sentence Compression Based Framework to Query-Focused Multi-Document Summarization"}
{"id": "task1540-4e825e7f409049f298e0bf4c1529584d", "input": "Recent work exhibited that distributed word representations are good at capturing linguistic regularities in language. This allows vector-oriented reasoning based on simple linear algebra between words. Since many different methods have been proposed for learning document representations, it is natural to ask whether there is also linear structure in these learned representations to allow similar reasoning at document level. To answer this question, we design a new document analogy task for testing the semantic regularities in document representations, and conduct empirical evaluations over several state-of-theart document representation models. The results reveal that neural embedding based document representations work better on this analogy task than conventional methods, and we provide some preliminary explanations over these observations.", "target": "Semantic Regularities in Document Representations"}
{"id": "task1540-48f89123f7a2482c9f6df1d943f781c1", "input": "The Internet and online forums such as Reddit have become an increasingly popular medium for citizens to engage in political conversations. However, the online disinhibition effect resulting from the ability to use pseudonymous identities may manifest in the form of offensive speech, consequently making political discussions more aggressive and polarizing than they already are. Such environments may result in harassment and self-censorship from its targets. In this paper, we present preliminary results from a large-scale temporal measurement aimed at quantifying offensiveness in online political discussions. To enable our measurements, we develop and evaluate an offensive speech classifier. We then use this classifier to quantify and compare offensiveness in the political and general contexts. We perform our study using a database of over 168M Reddit comments made by over 7M pseudonyms between January 2015 and January 2017 \u2013 a period covering several divisive political events including the 2016 US presidential elections.", "target": "Measuring Offensive Speech in Online Political Discourse"}
{"id": "task1540-43b02fb21dab4d65918c9b81894ebbae", "input": "This paper has two parts. In the first part we discuss word embeddings. We discuss the need for them, some of the methods to create them, and some of their interesting properties. We also compare them to image embeddings and see how word embedding and image embedding can be combined to perform different tasks. In the second part we implement a convolutional neural network trained on top of pre-trained word vectors. The network is used for several sentence-level classification tasks, and achieves state-of-art (or comparable) results, demonstrating the great power of pre-trainted word embeddings over random ones.", "target": "Word Embeddings and Their Use In Sentence Classification Tasks"}
{"id": "task1540-2fffbec47c244330a18be55aa49b4813", "input": "Discriminative latent-variable models are typically learned using EM or gradient-based optimization, which suffer from local optima. In this paper, we develop a new computationally efficient and provably consistent estimator for a mixture of linear regressions, a simple instance of a discriminative latentvariable model. Our approach relies on a lowrank linear regression to recover a symmetric tensor, which can be factorized into the parameters using a tensor power method. We prove rates of convergence for our estimator and provide an empirical evaluation illustrating its strengths relative to local optimization (EM). Last Modified: June 18, 2013", "target": "Spectral Experts for Estimating Mixtures of Linear Regressions"}
{"id": "task1540-33d54828d4c24fe796c251f125681ae7", "input": "We develop novel firstand second-order features for dependency parsing based on the Google Syntactic Ngrams corpus, a collection of subtree counts of parsed sentences from scanned books. We also extend previous work on surface n-gram features from Web1T to the Google Books corpus and from first-order to second-order, comparing and analysing performance over newswire and web treebanks. Surface and syntactic n-grams both produce substantial and complementary gains in parsing accuracy across domains. Our best system combines the two feature sets, achieving up to 0.8% absolute UAS improvements on newswire and 1.4% on web text.", "target": "Web-scale Surface and Syntactic n-gram Features for Dependency Parsing"}
{"id": "task1540-8c7378ed97974f66bc714ce476865e90", "input": "The Minimum Weight Dominating Set (MWDS) problem is an important generalization of the Minimum Dominating Set (MDS) problem with extensive applications. This paper proposes a new local search algorithm for the MWDS problem, which is based on two new ideas. The first idea is a heuristic called two-level configuration checking (CC), which is a new variant of a recent powerful configuration checking strategy (CC) for effectively avoiding the recent search paths. The second idea is a novel scoring function based on the frequency of being uncovered of vertices. Our algorithm is called CCFS, according to the names of the two ideas. The experimental results show that, CCFS performs much better than some state-of-the-art algorithms in terms of solution quality on a broad range of MWDS benchmarks.", "target": "Local Search for Minimum Weight Dominating Set with Two-Level Configuration Checking and Frequency Based Scoring Function"}
{"id": "task1540-77e0f04773ac43dd806cf3ba8e515ea9", "input": "We consider the problem of online active learning to collect data for regression modeling. Specifically, we consider a decision maker with a limited experimentation budget who must efficiently learn an underlying linear population model. Our main contribution is a novel threshold-based algorithm for selection of most informative observations; we characterize its performance and fundamental lower bounds. We extend the algorithm and its guarantees to sparse linear regression in high-dimensional settings. Simulations suggest the algorithm is remarkably robust: it provides significant benefits over passive random sampling in real-world datasets that exhibit high nonlinearity and high dimensionality \u2014 significantly reducing both the mean and variance of the squared error.", "target": "Online Active Linear Regression via Thresholding"}
{"id": "task1540-098ac218d70d49b7885d9ffea090806d", "input": "Automatic headline generation is an important research area within text summarization and sentence compression. Recently, neural headline generation models have been proposed to take advantage of well-trained neural networks in learning sentence representations and mapping sequence to sequence. Nevertheless, traditional neural network encoder utilizes maximum likelihood estimation for parameter optimization, which essentially constraints the expected training objective within word level instead of sentence level. Moreover, the performance of model prediction significantly relies on training data distribution. To overcome these drawbacks, we employ minimum risk training strategy in this paper, which directly optimizes model parameters with respect to evaluation metrics and statistically leads to significant improvements for headline generation. Experiment results show that our approach outperforms state-of-the-art systems on both English and Chinese headline generation tasks.", "target": "Neural Headline Generation with Minimum Risk Training"}
{"id": "task1540-93ea1e1e77bb46a493b872c5ddb38c9d", "input": "We discuss relations between Residual Networks (ResNet), Recurrent Neural Networks (RNNs) and the primate visual cortex. We begin with the observation that a shallow RNN is exactly equivalent to a very deep ResNet with weight sharing among the layers. A direct implementation of such a RNN, although having orders of magnitude fewer parameters, leads to a performance similar to the corresponding ResNet. We propose 1) a generalization of both RNN and ResNet architectures and 2) the conjecture that a class of moderately deep RNNs is a biologically-plausible model of the ventral stream in visual cortex. We demonstrate the effectiveness of the architectures by testing them on the CIFAR-10 dataset. This work was supported by the Center for Brains, Minds and Machines (CBMM), funded by NSF STC award CCF 1231216. 1 ar X iv :1 60 4. 03 64 0v 1 [ cs .L G ] 1 3 A pr 2 01 6", "target": "Bridging the Gaps Between Residual Learning, Recurrent Neural Networks and Visual Cortex"}
{"id": "task1540-21c6e1cb394d40f3adf9e7dd09eba407", "input": "The rise of smart applications has drawn interest to logical reasoning over data streams. Recently, different query languages and stream processing/reasoning engines were proposed in different communities. However, due to a lack of theoretical foundations, the expressivity and semantics of these diverse approaches are given only informally. Towards clear specifications and means for analytic study, a formal framework is needed to define their semantics in precise terms. To this end, we present a first step towards an ideal semantics that allows for exact descriptions and comparisons of stream reasoning systems.", "target": "Towards Ideal Semantics for Analyzing Stream Reasoning"}
{"id": "task1540-97f06ae5ec1c465d98b51240b4cc1ca5", "input": "We present a work-in-progress snapshot of learning with a 15 billion parameter deep learning network on HPC architectures applied to the largest publicly available natural image and video dataset released to-date. Recent advancements in unsupervised deep neural networks suggest that scaling up such networks in both model and training dataset size can yield significant improvements in the learning of concepts at the highest layers. We train our three-layer deep neural network on the Yahoo! Flickr Creative Commons 100M dataset. The dataset comprises approximately 99.2 million images and 800, 000 user-created videos from Yahoo\u2019s Flickr image and video sharing platform. Training of our network takes eight days on 98 GPU nodes at the High Performance Computing Center at Lawrence Livermore National Laboratory. Encouraging preliminary results and future research directions are presented and discussed.", "target": "LARGE-SCALE DEEP LEARNING ON THE YFCC100M DATASET"}
{"id": "task1540-ad21ecd1d1a34ead915e06a3ccdd9ca9", "input": "The task of computing approximate Nash equilibria in large zero-sum extensive-form games has received a tremendous amount of attention due mainly to the Annual Computer Poker Competition. Immediately after its inception, two competing and seemingly different approaches emerged\u2014one an application of noregret online learning, the other a sophisticated gradient method applied to a convex-concave saddle-point formulation. Since then, both approaches have grown in relative isolation with advancements on one side not effecting the other. In this paper, we rectify this by dissecting and, in a sense, unify the two views.", "target": "A Unified View of Large-scale Zero-sum Equilibrium Computation"}
{"id": "task1540-359c0bb99f294a42b2a4c01c7739d43d", "input": "A rhetorical structure tree (RS tree) is a representation of discourse relations among elementary discourse units (EDUs). A RS tree is very useful to many text processing tasks employing relationships among EDUs such as text understanding, summarization, and question-answering. Thai language with its unique linguistic characteristics requires a unique RS tree construction technique. This paper proposes an approach for Thai RS tree construction which consists of three major steps: EDU segmentation, Thai RS tree construction, and discourse relation (DR) identification. Two hidden markov models derived from grammatical rules are used to segment EDUs, a clustering technique with its similarity measure derived from Thai semantic rules is used to construct a Thai RS tree, and a decision tree whose features extracted from the rules is used to determine the DR between EDUs. The proposed technique is evaluated using three Thai corpora. The results show the Thai RS tree construction and the DR identification effectiveness of 94.90% and 82.81%, respectively. KeywordsThai Language, Element Discourse Unit, Rhetorical Structure Tree, Discourse Relation.", "target": "THAI RHETORICAL STRUCTURE ANALYSIS"}
{"id": "task1540-49fc4532d29347d9b97d960f8be1a0dc", "input": "The goal of this paper is to investigate the connection between the performance gain that can be obtained by selftraining and the similarity between the corpora used in this approach. Self-training is a semi-supervised technique designed to increase the performance of machine learning algorithms by automatically classifying instances of a task and adding these as additional training material to the same classifier. In the context of language processing tasks, this training material is mostly an (annotated) corpus. Unfortunately self-training does not always lead to a performance increase and whether it will is largely unpredictable. We show that the similarity between corpora can be used to identify those setups for which self-training can be beneficial. We consider this research as a step in the process of developing a classifier that is able to adapt itself to each new test corpus that it is presented with.", "target": "Predicting the Effectiveness of Self-Training: Application to Sentiment Classification"}
{"id": "task1540-2b86cfc08fe14299b632bd2199aac1cb", "input": "Numerous data mining techniques have been developed to extract information and identify patterns and predict trends from large data sets. In this study, two classification techniques, the J48 implementation of the C4.5 algorithm and a Naive Bayes classifier are applied to predict lung cancer survivability from an extensive data set with fifteen years of patient records. The purpose of the project is to verify the predictive effectiveness of the two techniques on real, historical data. Besides the performance outcome that renders J48 marginally better than the Naive Bayes technique, there is a detailed description of the data and the required pre-processing activities. The performance results confirm expectations while some of the issues that appeared during experimentation, underscore the value of having domain-specific understanding to leverage any domain-specific characteristics inherent in the data.", "target": "Comparison of the C4.5 and a Naive Bayes Classifier for the Prediction of Lung Cancer Survivability"}
{"id": "task1540-8051db58aaab46d78acaeae82bbe0f24", "input": "A number of representation schemes have been presented for use within Learning Classifier Systems, ranging from binary encodings to Neural Networks, and more recently Dynamical Genetic Programming (DGP). This paper presents results from an investigation into using a fuzzy DGP representation within the XCSF Learning Classifier System. In particular, asynchronous Fuzzy Logic Networks are used to represent the traditional condition-action production system rules. It is shown possible to use self-adaptive, open-ended evolution to design an ensemble of such fuzzy dynamical systems within XCSF to solve several well-known continuousvalued test problems.", "target": "Fuzzy Dynamical Genetic Programming in XCSF"}
{"id": "task1540-1d596f6fb46c416e8d3ae6c98f18790f", "input": "Discourse structure is the hidden link between surface features and document-level properties, such as sentiment polarity. We show that the discourse analyses produced by Rhetorical Structure Theory (RST) parsers can improve document-level sentiment analysis, via composition of local information up the discourse tree. First, we show that reweighting discourse units according to their position in a dependency representation of the rhetorical structure can yield substantial improvements on lexicon-based sentiment analysis. Next, we present a recursive neural network over the RST structure, which offers significant improvements over classificationbased methods.", "target": "Better Document-level Sentiment Analysis from RST Discourse Parsing\u2217"}
{"id": "task1540-610c4c86194f4a5588d25ef9aeb729a4", "input": "Optimal control of thermostatically controlled loads connected to a district heating network is considered a sequential decisionmaking problem under uncertainty. The practicality of a direct model-based approach is compromised by two challenges, namely scalability due to the large dimensionality of the problem and the system identification required to identify an accurate model. To help in mitigating these problems, this paper leverages on recent developments in reinforcement learning in combination with a market-based multi-agent system to obtain a scalable solution that obtains a significant performance improvement in a practical learning time. The control approach is applied on a scenario comprising 100 thermostatically controlled loads connected to a radial district heating network supplied by a central combined heat and power plant. Both for an energy arbitrage and a peak shaving objective, the control approach requires 60 days to obtain a performance within 65% of a theoretical lower bound on the cost.", "target": "Model-Free Control of Thermostatically Controlled Loads Connected to a District Heating Network"}
{"id": "task1540-4fa5bbaef5ed4416ac51b1a61fd34d66", "input": "Many of the existing methods for learning joint embedding of images and text use only supervised information from paired images and its textual attributes. Taking advantage of the recent success of unsupervised learning in deep neural networks, we propose an end-to-end learning framework that is able to extract more robust multi-modal representations across domains. The proposed method combines representation learning models (i.e., auto-encoders) together with cross-domain learning criteria (i.e., Maximum Mean Discrepancy loss) to learn joint embeddings for semantic and visual features. A novel technique of unsupervised-data adaptation inference is introduced to construct more comprehensive embeddings for both labeled and unlabeled data. We evaluate our method on Animals with Attributes and Caltech-UCSD Birds 200-2011 dataset with a wide range of applications, including zero and fewshot image recognition and retrieval, from inductive to transductive settings. Empirically, we show that our framework improves over the current state of the art on many of the considered tasks.", "target": "Learning Robust Visual-Semantic Embeddings"}
{"id": "task1540-6846ae521f7a45ccbc25474744373305", "input": "Weather affects our mood and behaviors, and many aspects of our life. When it is sunny, most people become happier; but when it rains, some people get depressed. Despite this evidence and the abundance of data, weather has mostly been overlooked in the machine learning and data science research. This work presents a causal analysis of how weather affects TV watching patterns. We show that some weather attributes, such as pressure and precipitation, cause major changes in TV watching patterns. To the best of our knowledge, this is the first large-scale causal study of the impact of weather on TV watching patterns.", "target": "Does Weather Matter? Causal Analysis of TV Logs"}
{"id": "task1540-c703b90efb484578b54b3913192ffea9", "input": "We present decentralized rollout sampling policy iteration (DecRSPI) \u2014 a new algorithm for multi-agent decision problems formalized as DEC-POMDPs. DecRSPI is designed to improve scalability and tackle problems that lack an explicit model. The algorithm uses MonteCarlo methods to generate a sample of reachable belief states. Then it computes a joint policy for each belief state based on the rollout estimations. A new policy representation allows us to represent solutions compactly. The key benefits of the algorithm are its linear time complexity over the number of agents, its bounded memory usage and good solution quality. It can solve larger problems that are intractable for existing planning algorithms. Experimental results confirm the effectiveness and scalability of the approach.", "target": "Rollout Sampling Policy Iteration for Decentralized POMDPs"}
{"id": "task1540-22623baf7f1340de886d4eeac76ef48e", "input": "We describe TweeTIME, a temporal tagger for recognizing and normalizing time expressions in Twitter. Most previous work in social media analysis has to rely on temporal resolvers that are designed for well-edited text, and therefore suffer from the reduced performance due to domain mismatch. We present a minimally supervised method that learns from large quantities of unlabeled data and requires no hand-engineered rules or hand-annotated training corpora. TweeTIME achieves 0.68 F1 score on the end-to-end task of resolving date expressions, outperforming a broad range of state-of-the-art systems.1", "target": "A Minimally Supervised Method for Recognizing and Normalizing Time Expressions in Twitter"}
{"id": "task1540-51f3323487bd47f09066e56461fbe998", "input": "In this paper, we introduce the task of targeted aspect-based sentiment analysis. The goal is to extract fine-grained information with respect to entities mentioned in user comments. This work extends both aspect-based sentiment analysis that assumes a single entity per document and targeted sentiment analysis that assumes a single sentiment towards a target entity. In particular, we identify the sentiment towards each aspect of one or more entities. As a testbed for this task, we introduce the SentiHood dataset, extracted from a question answering (QA) platform where urban neighbourhoods are discussed by users. In this context units of text often mention several aspects of one or more neighbourhoods. This is the first time that a generic social media platform in this case a QA platform, is used for fine-grained opinion mining. Text coming from QA platforms is far less constrained compared to text from review specific platforms which current datasets are based on. We develop several strong baselines, relying on logistic regression and state-of-the-art recurrent neural networks.", "target": "SentiHood: Targeted Aspect Based Sentiment Analysis Dataset for Urban Neighbourhoods"}
{"id": "task1540-40f122b87c7e402098700f4c7d73f4a8", "input": "Colorization of grayscale images has been a hot topic in computer vision. Previous research mainly focuses on producing a colored image to match the original one. However, since many colors share the same gray value, an input grayscale image could be diversely colored while maintaining its reality. In this paper, we design a novel solution for unsupervised diverse colorization. Specifically, we leverage conditional generative adversarial networks to model the distribution of real-world item colors, in which we develop a fully convolutional generator with multi-layer noise to enhance diversity, with multi-layer condition concatenation to maintain reality, and with stride 1 to keep spatial information. With such a novel network architecture, the model yields highly competitive performance on the open LSUN bedroom dataset. The Turing test of 80 humans further indicates our generated color schemes are highly convincible.", "target": "Unsupervised Diverse Colorization via Generative Adversarial Networks"}
{"id": "task1540-60bf9601191748198ac9755f5b138466", "input": "Abstract The nearest neighbor rule is one of the most widely used models for classification, and selecting a compact set of prototype instances is a primary challenges for its applications. Many existing approaches for prototype selection exploit instance-based analyses and locally-defined criteria on the class distribution, which are intractable for numerical optimization techniques. In this paper, we explore a parametric framework with an adjusted nearest neighbor rule, in which the selection of the neighboring prototypes is modified by their respective parameters. The framework allows us to formulate a minimization problem of the violation of the adjusted nearest neighbor rule over the training set with regards to numerical parameters. We show that the problem reduces to a large-margin principled learning and demonstrate its advantage by empirical comparisons with recent state-ofthe-art methods using public benchmark data.", "target": "Discriminative Learning of the Prototype Set for Nearest Neighbor Classification"}
{"id": "task1540-f55fddc273664cf78e18bdc6c39d8dfe", "input": "Stochastic variational inference (SVI) lets us scale up Bayesian computation to massive data. It uses stochastic optimization to fit a variational distribution, following easy-to-compute noisy natural gradients. As with most traditional stochastic optimization methods, SVI takes precautions to use unbiased stochastic gradients whose expectations are equal to the true gradients. In this paper, we explore the idea of following biased stochastic gradients in SVI. Our method replaces the natural gradient with a similarly constructed vector that uses a fixed-window moving average of some of its previous terms. We will demonstrate the many advantages of this technique. First, its computational cost is the same as for SVI and storage requirements only multiply by a constant factor. Second, it enjoys significant variance reduction over the unbiased estimates, smaller bias than averaged gradients, and leads to smaller mean-squared error against the full gradient. We test our method on latent Dirichlet allocation with three large corpora.", "target": "Smoothed Gradients for Stochastic Variational Inference"}
{"id": "task1540-149bc602d3084e259923c07599a36557", "input": "The user equilibrium in traffic assignment problem is based on the fact that travelers choose the minimum-cost path between every origin-destination pair and on the assumption that such a behavior will lead to an equilibrium of the traffic network. In this paper, we consider this problem when the traffic network links are fuzzy cost. Therefore, a Physarum-type algorithm is developed to unify the Physarum network and the traffic network for taking full of advantage of Physarum Polycephalum\u2019s adaptivity in network design to solve the user equilibrium problem. Eventually, some experiments are used to test the performance of this method. The results demonstrate that our approach is competitive when compared with other existing algorithms.", "target": "A bio-inspired algorithm for fuzzy user equilibrium problem by aid of Physarum Polycephalum"}
{"id": "task1540-67435538f83d4adda62308b619f792a6", "input": "This paper proposes a new method for the K-armed dueling bandit problem, a variation on the regular K-armed bandit problem that offers only relative feedback about pairs of arms. Our approach extends the Upper Confidence Bound algorithm to the relative setting by using estimates of the pairwise probabilities to select a promising arm and applying Upper Confidence Bound with the winner as a benchmark. We prove a finite-time regret bound of order O(log t). In addition, our empirical results using real data from an information retrieval application show that it greatly outperforms the state of the art.", "target": "Relative Upper Confidence Bound for the K-Armed Dueling Bandit Problem"}
{"id": "task1540-f8be7b1130ee46b59a4d7811b264cdb8", "input": "A word\u2019s sentiment depends on the domain in which it is used. Computational social science research thus requires sentiment lexicons that are specific to the domains being studied. We combine domain-specific word embeddings with a label propagation framework to induce accurate domain-specific sentiment lexicons using small sets of seed words. We show that our approach achieves state-of-the-art performance on inducing sentiment lexicons from domain-specific corpora and that our purely corpus-based approach outperforms methods that rely on hand-curated resources (e.g., WordNet). Using our framework, we induce and release historical sentiment lexicons for 150 years of English and community-specific sentiment lexicons for 250 online communities from the social media forum Reddit. The historical lexicons we induce show that more than 5% of sentiment-bearing (nonneutral) English words completely switched polarity during the last 150 years, and the community-specific lexicons highlight how sentiment varies drastically between different communities.", "target": "Inducing Domain-Specific Sentiment Lexicons from Unlabeled Corpora"}
{"id": "task1540-21db2a00cf774d3189599cc3c3b4d27b", "input": "Joint extraction of entities and relations is an important task in information extraction. To tackle this problem, we firstly propose a novel tagging scheme that can convert the joint extraction task to a tagging problem. Then, based on our tagging scheme, we study different end-toend models to extract entities and their relations directly, without identifying entities and relations separately. We conduct experiments on a public dataset produced by distant supervision method and the experimental results show that the tagging based methods are better than most of the existing pipelined and joint learning methods. What\u2019s more, the end-to-end model proposed in this paper, achieves the best results on the public dataset.", "target": "Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme"}
{"id": "task1540-8e8e2ea1bf9d4d2da5a90434766d8260", "input": "Learning commonsense knowledge from natural language text is nontrivial due to reporting bias: people rarely state the obvious, e.g., \u201cMy house is bigger than me.\u201d However, while rarely stated explicitly, this trivial everyday knowledge does influence the way people talk about the world, which provides indirect clues to reason about the world. For example, a statement like, \u201cTyler entered his house\u201d implies that his house is bigger than Tyler. In this paper, we present an approach to infer relative physical knowledge of actions and objects along five dimensions (e.g., size, weight, and strength) from unstructured natural language text. We frame knowledge acquisition as joint inference over two closely related problems: learning (1) relative physical knowledge of object pairs and (2) physical implications of actions when applied to those object pairs. Empirical results demonstrate that it is possible to extract knowledge of actions and objects from language and that joint inference over different types of knowledge improves performance.", "target": "VERB PHYSICS: Relative Physical Knowledge of Actions and Objects"}
{"id": "task1540-609dee87333d4540ae0226dec2585401", "input": "The ability to accurately represent sentences is central to language understanding. We describe a convolutional architecture dubbed the Dynamic Convolutional Neural Network (DCNN) that we adopt for the semantic modelling of sentences. The network uses Dynamic k-Max Pooling, a global pooling operation over linear sequences. The network handles input sentences of varying length and induces a feature graph over the sentence that is capable of explicitly capturing short and long-range relations. The network does not rely on a parse tree and is easily applicable to any language. We test the DCNN in four experiments: small scale binary and multi-class sentiment prediction, six-way question classification and Twitter sentiment prediction by distant supervision. The network achieves excellent performance in the first three tasks and a greater than 25% error reduction in the last task with respect to the strongest baseline.", "target": "A Convolutional Neural Network for Modelling Sentences"}
{"id": "task1540-df0f63fcc68a4c49b6672a9df984719a", "input": "<lb>Many canonical machine learning problems boil down to a convex optimization problem with a finite<lb>sum structure. However, whereas much progress has been made in developing faster algorithms for<lb>this setting, the inherent limitations of these problems are not satisfactorily addressed by existing lower<lb>bounds. Indeed, current bounds focus on first-order optimization algorithms, and only apply in the often<lb>unrealistic regime where the number of iterations is less than O(d/n) (where d is the dimension and n<lb>is the number of samples). In this work, we extend the framework of Arjevani et al. [3, 5] to provide<lb>new lower bounds, which are dimension-free, and go beyond the assumptions of current bounds, thereby<lb>covering standard finite sum optimization methods, e.g., SAG, SAGA, SVRG, SDCA without duality, as<lb>well as stochastic coordinate-descent methods, such as SDCA and accelerated proximal SDCA.", "target": "Dimension-Free Iteration Complexity of Finite Sum Optimization Problems"}
{"id": "task1540-08c5ce1b13974f5db134fc27c5d6280e", "input": "In this paper one presents new similarity, cardinality and entropy measures for bipolar fuzzy set and for its particular forms like intuitionistic, paraconsistent and fuzzy set. All these are constructed in the framework of multi-valued representations and are based on a penta-valued logic that uses the following logical values: true, false, unknown, contradictory and ambiguous. Also a new distance for bounded real interval was defined.", "target": "Similarity, Cardinality and Entropy for Bipolar Fuzzy Set in the Framework of Penta-valued Representation"}
{"id": "task1540-fd3d0d9cee5641faa80687e1ec5261e6", "input": "Contextual bandit learning is an increasingly popular approach to optimizing recommender systems via user feedback, but can be slow to converge in practice due to the need for exploring a large feature space. In this paper, we propose a coarse-to-fine hierarchical approach for encoding prior knowledge that drastically reduces the amount of exploration required. Intuitively, user preferences can be reasonably embedded in a coarse low-dimensional feature space that can be explored efficiently, requiring exploration in the high-dimensional space only as necessary. We introduce a bandit algorithm that explores within this coarse-to-fine spectrum, and prove performance guarantees that depend on how well the coarse space captures the user\u2019s preferences. We demonstrate substantial improvement over conventional bandit algorithms through extensive simulation as well as a live user study in the setting of personalized news recommendation.", "target": "Hierarchical Exploration for Accelerating Contextual Bandits"}
{"id": "task1540-a93bde414a6f4d72a89cf9905a29f8e6", "input": "Several authors have explained that the like\u00ad lihood ratio measures the strength of the evi\u00ad dence represented by observations in statisti\u00ad cal problems. This idea works fine when the goal is to evaluate the strength of the avail\u00ad able evidence for a simple hypothesis versus another simple hypothesis. However, the ap\u00ad plicability of this idea is limited to simple hypotheses because the likelihood function is primarily defined on points simple hy\u00ad potheses of the parameter space. In this paper we define a general weight of evidence that is applicable to both simple and compos\u00ad ite hypotheses. It is based on the Dempster\u00ad Shafer concept of plausibility and is shown to be a generalization of the likelihood ratio. Functional models are of a fundamental im\u00ad portance for the general weight of evidence proposed in this paper. The relevant con\u00ad cepts and ideas are explained by means of a familiar urn problem and the general analysis of a real-world medical problem is presented.", "target": "From Likelihood to Plausibility"}
{"id": "task1540-e6f93704aed44197bd13ef04b6f4db64", "input": "The CSA-ES is an Evolution Strategy with Cumulative Step size Adaptation, where the step size is adapted measuring the length of a so-called cumulative path. The cumulative path is a combination of the previous steps realized by the algorithm, where the importance of each step decreases with time. This article studies the CSA-ES on composites of strictly increasing with affine linear functions through the investigation of its underlying Markov chains. Rigorous results on the change and the variation of the step size are derived with and without cumulation. The step-size diverges geometrically fast in most cases. Furthermore, the influence of the cumulation parameter is studied.", "target": "Cumulative Step-size Adaptation on Linear Functions"}
{"id": "task1540-0e581ae6d613485394f0fcd0586b2e84", "input": "We present new algorithms for learning Bayesian networks from data with missing values without the assumption that data are missing at random (MAR). An exact Bayesian network learning algorithm is obtained by recasting the problem into a standard Bayesian network learning problem without missing data. To the best of our knowledge, this is the first exact algorithm for this problem. As expected, the exact algorithm does not scale to large domains. We build on the exact method to create a new approximate algorithm using a hill-climbing technique. This algorithm scales to large domains so long as a suitable standard structure learning method for complete data is available. We perform a wide range of experiments to demonstrate the benefits of learning Bayesian networks without assuming MAR.", "target": "Learning Bayesian Networks without Assuming Missing at Random"}
{"id": "task1540-d3a40733635540648e1e7c0f140a16f4", "input": "Network Intrusion Detection Systems (NIDS) monitor a network with the aim of discerning malicious from benign activity on that network. While a wide range of approaches have met varying levels of success, most IDS\u2019s rely on having access to a database of known attack signatures which are written by security experts. Nowadays, in order to solve problems with false positive alerts, correlation algorithms are used to add additional structure to sequences of IDS alerts. However, such techniques are of no help in discovering novel attacks or variations of known attacks, something the human immune system (HIS) is capable of doing in its own specialised domain. This paper presents a novel immune algorithm for application to an intrusion detection problem. The goal is to discover packets containing novel variations of attacks covered by an existing signature base.", "target": "Integrating Innate and Adaptive Immunity for Intrusion Detection"}
{"id": "task1540-d35b621dfa17463b89fb8c0d7d7beb7f", "input": "Deep neural networks trained on large supervised datasets have led to impressive results in recent years. However, since well-annotated datasets can be prohibitively expensive and time-consuming to collect, recent work has explored the use of larger but noisy datasets that can be more easily obtained. In this paper, we investigate the behavior of deep neural networks on training sets with massively noisy labels. We show that successful learning is possible even with an essentially arbitrary amount of noise. For example, on MNIST we find that accuracy of above 90 percent is still attainable even when the dataset has been diluted with 100 noisy examples for each clean example. Such behavior holds across multiple patterns of label noise, even when noisy labels are biased towards confusing classes. Further, we show how the required dataset size for successful training increases with higher label noise. Finally, we present simple actionable techniques for improving learning in the regime of high label noise.", "target": "Deep Learning is Robust to Massive Label Noise"}
{"id": "task1540-619696e14ce54c26a6aba2b8ad329ac1", "input": "In this paper, we extend the deep long short-term memory (DLSTM) recurrent neural networks by introducing gated direct connections between memory cells in adjacent layers. These direct links, called highway connections, enable unimpeded information flow across different layers and thus alleviate the gradient vanishing problem when building deeper LSTMs. We further introduce the latency-controlled bidirectional LSTMs (BLSTMs) which can exploit the whole history while keeping the latency under control. Efficient algorithms are proposed to train these novel networks using both frame and sequence discriminative criteria. Experiments on the AMI distant speech recognition (DSR) task indicate that we can train deeper LSTMs and achieve better improvement from sequence training with highway LSTMs (HLSTMs). Our novel model obtains 43.9/47.7% WER on AMI (SDM) dev and eval sets, outperforming all previous works. It beats the strong DNN and DLSTM baselines with 15.7% and 5.3% relative improvement respectively.", "target": "HIGHWAY LONG SHORT-TERM MEMORY RNNS FOR DISTANT SPEECH RECOGNITION"}
{"id": "task1540-0e698b417518461d831267c5a900286d", "input": "As the type and the number of such venues increase, automated analysis of sentiment on textual resources has become an essential data mining task. In this paper, we investigate the problem of mining opinions on the collection of informal short texts. Both positive and negative sentiment strength of texts are detected. We focus on a non-English language that has few resources for text mining. This approach would help enhance the sentiment analysis in languages where a list of opinionated words does not exist. We propose a new method projects the text into dense and low dimensional feature vectors according to the sentiment strength of the words. We detect the mixture of positive and negative sentiments on a multi-variant scale. Empirical evaluation of the proposed framework on Turkish tweets shows that our approach gets good results for opinion mining.", "target": "Opinion Mining on Non-English Short Text"}
{"id": "task1540-4acb781c42884f66a5b4ff2f7d324493", "input": "We introduce a simple new regularizer for auto-encoders whose hidden-unit activation functions contain at least one zero-gradient (saturated) region. This regularizer explicitly encourages activations in the saturated region(s) of the corresponding activation function. We call these Saturating Auto-Encoders (SATAE). We show that the saturation regularizer explicitly limits the SATAE\u2019s ability to reconstruct inputs which are not near the data manifold. Furthermore, we show that a wide variety of features can be learned when different activation functions are used. Finally, connections are established with the Contractive and Sparse Auto-Encoders.", "target": "Saturating Auto-Encoder"}
{"id": "task1540-93913c1765ab40ffaaf732f9c44ed6b4", "input": "The attention model has become a standard component in neural machine translation (NMT) and it guides translation process by selectively focusing on parts of the source sentence when predicting each target word. However, we find that the generation of a target word does not only depend on the source sentence, but also rely heavily on the previous generated target words, especially the distant words which are difficult to model by using recurrent neural networks. To solve this problem, we propose in this paper a novel look-ahead attention mechanism for generation in NMT, which aims at directly capturing the dependency relationship between target words. We further design three patterns to integrate our look-ahead attention into the conventional attention model. Experiments on NIST Chinese-to-English and WMT English-to-German translation tasks show that our proposed look-ahead attention mechanism achieves substantial improvements over state-of-the-art baselines.", "target": "Look-ahead Attention for Generation in Neural Machine Translation"}
{"id": "task1540-0785fd660ef64689931a08d9b739fd21", "input": "We introduce a simple recurrent variational autoencoder architecture that significantly improves image modeling. The system represents the stateof-the-art in latent variable models for both the ImageNet and Omniglot datasets. We show that it naturally separates global conceptual information from lower level details, thus addressing one of the fundamentally desired properties of unsupervised learning. Furthermore, the possibility of restricting ourselves to storing only global information about an image allows us to achieve high quality \u2018conceptual compression\u2019.", "target": "Towards Conceptual Compression"}
{"id": "task1540-5ec21e271d174d2ba237e1decfa2236f", "input": "Deep learning and reinforcement learning methods have recently been used to solve a variety of problems in continuous control domains. An obvious application of these techniques is dexterous manipulation tasks in robotics which are difficult to solve using traditional control theory or hand-engineered approaches. One example of such a task is to grasp an object and precisely stack it on another. Solving this difficult and practically relevant problem in the real world is an important long-term goal for the field of robotics. Here we take a step towards this goal by examining the problem in simulation and providing models and techniques aimed at solving it. We introduce two extensions to the Deep Deterministic Policy Gradient algorithm (DDPG), a model-free Q-learning based method, which make it significantly more data-efficient and scalable. Our results show that by making extensive use of off-policy data and replay, it is possible to find control policies that robustly grasp objects and stack them. Further, our results hint that it may soon be feasible to train successful stacking policies by collecting interactions on real robots.", "target": "Data-efficient Deep Reinforcement Learning for Dexterous Manipulation"}
{"id": "task1540-3288c9c305c64f06816b6d8df76fdac9", "input": "Latent force models (LFMs) are flexible models that combine mechanistic modelling principles (i.e., physical models) with nonparametric data-driven components. Several key applications of LFMs need nonlinearities, which results in analytically intractable inference. In this work we show how non-linear LFMs can be represented as nonlinear white noise driven state-space models and present an efficient non-linear Kalman filtering and smoothing based method for approximate state and parameter inference. We illustrate the performance of the proposed methodology via two simulated examples, and apply it to a real-world problem of long-term prediction of GPS satellite orbits.", "target": "State-Space Inference for Non-Linear Latent Force Models with Application to Satellite Orbit Prediction"}
{"id": "task1540-6f69b82510e04d0a9ebfeef23c1f4cf4", "input": "The paper focuses on the problem of learning saccades enabling visual object search. The developed system combines reinforcement learning with a neural network for learning to predict the possible outcomes of its actions. We validated the solution in three types of environment consisting of (pseudo)-randomly generated matrices of digits. The experimental verification is followed by the discussion regarding elements required by systems mimicking the fovea movement and possible further research directions.", "target": "Utilization of Deep Reinforcement Learning for saccadic-based object visual search"}
{"id": "task1540-e35c42c5c4274d2a91f43091256158ab", "input": "In this paper, the continuity and strong continuity in domain-free information algebras and labeled information algebras are introduced respectively. A more general concept of continuous function which is defined between two domain-free continuous information algebras is presented. It is shown that, with the operations combination and focusing, the set of all continuous functions between two domain-free s-continuous information algebras forms a new s-continuous information algebra. By studying the relationship between domain-free information algebras and labeled information algebras, it is demonstrated that they do correspond to each other on s-compactness.", "target": "CONTINUITY IN INFORMATION ALGEBRAS: A SURVEY ON THE RELATIONSHIP BETWEEN TWO TYPES OF INFORMATION ALGEBRAS"}
{"id": "task1540-fa07e332ec8245aca23944f1610009a5", "input": "One of the limitations of semantic parsing approaches to open-domain question answering is the lexicosyntactic gap between natural language questions and knowledge base entries \u2013 there are many ways to ask a question, all with the same answer. In this paper we propose to bridge this gap by generating paraphrases of the input question with the goal that at least one of them will be correctly mapped to a knowledge-base query. We introduce a novel grammar model for paraphrase generation that does not require any sentence-aligned paraphrase corpus. Our key idea is to leverage the flexibility and scalability of latent-variable probabilistic context-free grammars to sample paraphrases. We do an extrinsic evaluation of our paraphrases by plugging them into a semantic parser for Freebase. Our evaluation experiments on the WebQuestions benchmark dataset show that the performance of the semantic parser improves over strong baselines.", "target": "Paraphrase Generation from Latent-Variable PCFGs for Semantic Parsing"}
{"id": "task1540-f9b919e89aba41debac58653b2dd29d7", "input": "Determining semantic textual similarity is a core research subject in natural language processing. Since vector-based models for sentence representation often use shallow information, capturing accurate semantics is difficult. By contrast, logical semantic representations capture deeper levels of sentence semantics, but their symbolic nature does not offer graded notions of textual similarity. We propose a method for determining semantic textual similarity by combining shallow features with features extracted from natural deduction proofs of bidirectional entailment relations between sentence pairs. For the natural deduction proofs, we use ccg2lambda, a higherorder automatic inference system, which converts Combinatory Categorial Grammar (CCG) derivation trees into semantic representations and conducts natural deduction proofs. Experiments show that our system was able to outperform other logicbased systems and that features derived from the proofs are effective for learning textual similarity.", "target": "Determining Semantic Textual Similarity using Natural Deduction Proofs"}
{"id": "task1540-cb71fdd1c45e445a89d3804dcea78b83", "input": "We present the EpiReader, a novel model for machine comprehension of text. Machine comprehension of unstructured, real-world text is a major research goal for natural language processing. Current tests of machine comprehension pose questions whose answers can be inferred from some supporting text, and evaluate a model\u2019s response to the questions. The EpiReader is an end-to-end neural model comprising two components: the first component proposes a small set of candidate answers after comparing a question to its supporting text, and the second component formulates hypotheses using the proposed candidates and the question, then reranks the hypotheses based on their estimated concordance with the supporting text. We present experiments demonstrating that the EpiReader sets a new state-of-the-art on the CNN and Children\u2019s Book Test machine comprehension benchmarks, outperforming previous neural models by a significant margin.", "target": "Natural Language Comprehension with the EpiReader"}
{"id": "task1540-938718b0eaf0418da0d04a9f38c41bce", "input": "Abstract: We often encounter situations in which an experimenter wants to find, by sequential experimentation, xmax = arg maxx f(x), where f(x) is a (possibly unknown) function of a well controllable variable x. Taking inspiration from physics and engineering, we have designed a new method to address this problem. In this paper, we first introduce the method in continuous time, and then present two algorithms for use in sequential experiments. Through a series of simulation studies, we show that the method is effective for finding maxima of unknown functions by experimentation, even when the maximum of the functions drifts or when the signal to noise ratio is low.", "target": "Thompson sampling with the online bootstrap"}
{"id": "task1540-011f924a19304fce95a5855d161a66f2", "input": "We present an image-conditional image generation model. The model transfers an input domain to a target domain in semantic level, and generates the target image in pixel level. To generate realistic target images, we employ the real/fake-discriminator in Generative Adversarial Nets [1], but also introduce a novel domain-discriminator to make the generated image relevant to the input image. We verify our model through a challenging task of generating a piece of clothing from an input image of a dressed person. We present a high quality clothing dataset containing the two domains, and succeed in demonstrating decent results.", "target": "Pixel-Level Domain Transfer"}
{"id": "task1540-c0bc391131254f0398dee4ca6ede38b4", "input": "In this paper we initiate the study of whether or not sparse estimation tasks can be performed efficiently in high dimensions, in the robust setting where an \u03b5-fraction of samples are corrupted adversarially. We study the natural robust version of two classical sparse estimation problems, namely, sparse mean estimation and sparse PCA in the spiked covariance model. For both of these problems, we provide the first efficient algorithms that provide non-trivial error guarantees in the presence of noise, using only a number of samples which is similar to the number required for these problems without noise. In particular, our sample complexities are sublinear in the ambient dimension d. Our work also suggests evidence for new computational-vs-statistical gaps for these problems (similar to those for sparse PCA without noise) which only arise in the presence of noise.", "target": "Robust Sparse Estimation Tasks in High Dimensions"}
{"id": "task1540-174d871c048842f4a26a1343dda2364e", "input": "In this paper, a novel approach is proposed to automatically construct parallel discourse corpus for dialogue machine translation. Firstly, the parallel subtitle data and its corresponding monolingual movie script data are crawled and collected from Internet. Then tags such as speaker and discourse boundary from the script data are projected to its subtitle data via an information retrieval approach in order to map monolingual discourse to bilingual texts. We not only evaluate the mapping results, but also integrate speaker information into the translation. Experiments show our proposed method can achieve 81.79% and 98.64% accuracy on speaker and dialogue boundary annotation, and speaker-based language model adaptation can obtain around 0.5 BLEU points improvement in translation qualities. Finally, we publicly release around 100K parallel discourse data with manual speaker and dialogue boundary annotation.", "target": "Automatic Construction of Discourse Corpora for Dialogue Translation"}
{"id": "task1540-973ac3de5ede4dc5b3f4e2865df48a46", "input": "Recurrent neural networks have achieved remarkable success at generating sequences with complex structures, thanks to advances that include richer embeddings of input and cures for vanishing gradients. Trained only on sequences from a known grammar, though, they can still struggle to learn rules and constraints of the grammar. Neural Attribute Machines (NAMs) are equipped with a logical machine that represents the underlying grammar, which is used to teach the constraints to the neural machine by (i) augmenting the input sequence, and (ii) optimizing a custom loss function. Unlike traditional RNNs, NAMs are exposed to the grammar, as well as samples from the language of the grammar. During generation, NAMs make significantly fewer violations of the constraints of the underlying grammar than RNNs trained only on samples from the language of the grammar.", "target": "Neural Attribute Machines for Program Generation\u2217"}
{"id": "task1540-bc8a2797a9c24ef3a255201cd64d044b", "input": "We propose Diverse Embedding Neural Network (DENN), a novel architecture for language models (LMs). A DENNLM projects the input word history vector onto multiple diverse low-dimensional sub-spaces instead of a single higherdimensional sub-space as in conventional feed-forward neural network LMs. We encourage these sub-spaces to be diverse during network training through an augmented loss function. Our language modeling experiments on the Penn Treebank data set show the performance benefit of using a DENNLM.", "target": "DIVERSE EMBEDDING NEURAL NETWORK LANGUAGE MODELS"}
{"id": "task1540-b023c1a36bfd40098ec563ef72398b1e", "input": "The bootstrap provides a simple and powerful means of assessing the quality of estimators. However, in settings involving large datasets, the computation of bootstrap-based quantities can be prohibitively demanding. As an alternative, we present the Bag of Little Bootstraps (BLB), a new procedure which incorporates features of both the bootstrap and subsampling to obtain a robust, computationally efficient means of assessing estimator quality. BLB is well suited to modern parallel and distributed computing architectures and retains the generic applicability, statistical efficiency, and favorable theoretical properties of the bootstrap. We provide the results of an extensive empirical and theoretical investigation of BLB\u2019s behavior, including a study of its statistical correctness, its largescale implementation and performance, selection of hyperparameters, and performance on real data.", "target": "The Big Data Bootstrap"}
{"id": "task1540-774a099de40d4e57a845a84ed14a5859", "input": "We introduce a novel validation framework to measure the true robustness of learning models for real-world applications by creating sourceinclusive and source-exclusive partitions in a dataset via clustering. We develop a robustness metric derived from source-aware lower and upper bounds of model accuracy even when data source labels are not readily available. We clearly demonstrate that even on a well-explored dataset like MNIST, challenging training scenarios can be constructed under the proposed assessment framework for two separate yet equally important applications: i) more rigorous learning model comparison and ii) dataset adequacy evaluation. In addition, our findings not only promise a more complete identification of trade-offs between model complexity, accuracy and robustness but can also help researchers optimize their efforts in data collection by identifying the less robust and more challenging class labels.", "target": "Clustering-based Source-aware Assessment of True Robustness for Learning Models"}
{"id": "task1540-6434e58a39734f92b45b9ab7ced0399d", "input": "Multimodal machine translation is the task of translating sentences in a visual context. We decompose this problem into two sub-tasks: learning to translate and learning visually grounded representations. In a multitask learning framework, translations are learned in an attention-based encoderdecoder, and grounded representations are learned through image representation prediction. Our approach improves translation performance compared to the state of the art on the Multi30K dataset. Furthermore, it is equally effective if we train the image prediction task on the external MS COCO dataset, and we find improvements if we train the translation model on the external News Commentary parallel text.", "target": "Imagination improves Multimodal Translation"}
{"id": "task1540-7ad7038fe240409b9b5a41a202bb15fd", "input": "The 2002 Trading Agent Competition (TAC) presented a challenging market game in the domain of travel shopping. One of the pivotal issues in this domain is uncertainty about hotel prices, which have a significant influence on the relative cost of alternative trip schedules. Thus, virtually all participants employ some method for predicting hotel prices. We survey approaches employed in the tournament, finding that agents apply an interesting diversity of techniques, taking into account differing sources of evidence bearing on prices. Based on data provided by entrants on their agents\u2019 actual predictions in the TAC-02 finals and semifinals, we analyze the relative efficacy of these approaches. The results show that taking into account game-specific information about flight prices is a major distinguishing factor. Machine learning methods effectively induce the relationship between flight and hotel prices from game data, and a purely analytical approach based on competitive equilibrium analysis achieves equal accuracy with no historical data. Employing a new measure of prediction quality, we relate absolute accuracy to bottom-line performance in the game.", "target": "Price Prediction in a Trading Agent Competition"}
{"id": "task1540-52d1a147dfd844419b5c11bfa8f28404", "input": "In the context of contemporary monophonic music, expression can be seen as the difference between a musical performance and its symbolic representation, i.e. a musical score. In this paper, we show how Maximum Entropy (MaxEnt) models can be used to generate musical expression in order to mimic a human performance. As a training corpus, we had a professional pianist play about 150 melodies of jazz, pop, and latin jazz. The results show a good predictive power, validating the choice of our model. Additionally, we set up a listening test whose results reveal that on average, people significantly prefer the melodies generated by the MaxEnt model than the ones without any expression, or with fully random expression. Furthermore, in some cases, MaxEnt melodies are almost as popular as the human performed ones.", "target": "Maximum entropy models for generation of expressive music"}
{"id": "task1540-87a35380ce494e19a218ac0628e693c7", "input": "In this paper the behavior of various be\u00ad lief network learning algorithms is stud\u00ad ied. Selecting belief networks with cer\u00ad tain minimallity properties turns out to be NP-hard, which justifies the use of search heuristics. Search heuristics based on the Bayesian measure of Cooper and Her\u00ad skovits and a minimum description length (MDL) measure are compared with re\u00ad spect to their properties for both limit\u00ad ing and finite database sizes. It is shown that the MDL measure has more desir\u00ad able properties than the Bayesian mea\u00ad sure. Experimental results suggest that for learning probabilities of belief net\u00ad works smoothing is helpful.", "target": "Properties of Bayesian Belief Network Learning Algorithms"}
{"id": "task1540-25436e02ec2842d4bf67be1956489514", "input": "In this work we describe and evaluate methods to learn musical embeddings. Each embedding is a vector that represents four contiguous beats of music and is derived from a symbolic representation. We consider autoencoding-based methods including denoising autoencoders, and context reconstruction, and evaluate the resulting embeddings on a forward prediction and a classification task.", "target": "A Unit Selection Methodology for Music Generation Using Deep Neural Networks"}
{"id": "task1540-7573cdf0dbf44d82bef139cc40619787", "input": "This paper reviews related work and state-of-the-art publications for recognizing motor symptoms of Parkinson's Disease (PD). It presents research efforts that were undertaken to inform on how well traditional machine learning algorithms can handle this task. In particular, four PD related motor symptoms are highlighted (i.e. tremor, bradykinesia, freezing of gait and dyskinesia) and their details summarized. Thus the primary objective of this research is to provide a literary foundation for development and improvement of algorithms for detecting PD related motor symptoms.", "target": "PARKINSON'S DISEASE MOTOR SYMPTOMS IN MACHINE LEARNING: A REVIEW"}
{"id": "task1540-1244ade57d6c4933afd15da7e5994844", "input": "We present local discriminative Gaussian (LDG) dimensionality reduction, a supervised dimensionality reduction technique for classification. The LDG objective function is an approximation to the leave-one-out training error of a local quadratic discriminant analysis classifier, and thus acts locally to each training point in order to find a mapping where similar data can be discriminated from dissimilar data. While other state-ofthe-art linear dimensionality reduction methods require gradient descent or iterative solution approaches, LDG is solved with a single eigen-decomposition. Thus, it scales better for datasets with a large number of feature dimensions or training examples. We also adapt LDG to the transfer learning setting, and show that it achieves good performance when the test data distribution differs from that of the training data.", "target": "Dimensionality Reduction by Local Discriminative Gaussians"}
{"id": "task1540-9e1ec38334d146aaa7fe5159485536ef", "input": "Recently deeplearning models have been shown to be capable of making remarkable performance in sentences and documents classification tasks. In this work, we propose a novel framework called AC-BLSTM for modeling sentences and documents, which combines the asymmetric convolution neural network (ACNN) with the Bidirectional Long ShortTerm Memory network (BLSTM). Experiment results demonstrate that our model achieves state-ofthe-art results on five tasks, including sentiment analysis, question type classification, and subjectivity classification. In order to further improve the performance of AC-BLSTM, we propose a semi-supervised learning framework called G-AC-BLSTM for text classification by combining the generative model with AC-BLSTM.", "target": "AC-BLSTM: Asymmetric Convolutional Bidirectional LSTM Networks for Text Classification"}
{"id": "task1540-3519a685880d42bc9399c0c4698be355", "input": "The large and growing amounts of online scholarly data present both challenges and opportunities to enhance knowledge discovery. One such challenge is to automatically extract a small set of keyphrases from a document that can accurately describe the document\u2019s content and can facilitate fast information processing. In this paper, we propose PositionRank, an unsupervised model for keyphrase extraction from scholarly documents that incorporates information from all positions of a word\u2019s occurrences into a biased PageRank. Our model obtains remarkable improvements in performance over PageRank models that do not take into account word positions as well as over strong baselines for this task. Specifically, on several datasets of research papers, PositionRank achieves improvements as high as 29.09%.", "target": "PositionRank: An Unsupervised Approach to Keyphrase Extraction from Scholarly Documents"}
{"id": "task1540-035de7a8fdef4cfab7829e1527af7208", "input": "We present a general framework for classification of sparse and irregularly-sampled time series. The properties of such time series can result in substantial uncertainty about the values of the underlying temporal processes, while making the data difficult to deal with using standard classification methods that assume fixeddimensional feature spaces. To address these challenges, we propose an uncertaintyaware classification framework based on a special computational layer we refer to as the Gaussian process adapter that can connect irregularly sampled time series data to any black-box classifier learnable using gradient descent. We show how to scale up the required computations based on combining the structured kernel interpolation framework and the Lanczos approximation method, and how to discriminatively train the Gaussian process adapter in combination with a number of classifiers end-to-end using backpropagation.", "target": "A scalable end-to-end Gaussian process adapter for irregularly sampled time series classification"}
{"id": "task1540-f9667a20eda04875a9ad3aaa30123ae2", "input": "We present a novel diffusion scheme for online kernel-based learning over networks. So far, a major drawback of any online learning algorithm, operating in a reproducing kernel Hilbert space (RKHS), is the need for updating a growing number of parameters as time iterations evolve. Besides complexity, this leads to an increased need of communication resources, in a distributed setting. In contrast, the proposed method approximates the solution as a fixed-size vector (of larger dimension than the input space) using Random Fourier Features. This paves the way to use standard linear combine-then-adapt techniques. To the best of our knowledge, this is the first time that a complete protocol for distributed online learning in RKHS is presented. Conditions for asymptotic convergence and boundness of the networkwise regret are also provided. The simulated tests illustrate the performance of the proposed scheme.", "target": "Online Distributed Learning Over Networks in RKH Spaces Using Random Fourier Features"}
{"id": "task1540-8deee8083ec645bfb0bd43636185eb86", "input": "This paper presents an analysis of data from a gift-exchange-game experiment. The experiment was described in \u2018The Impact of Social Comparisons on Reciprocity\u2019 by G\u00e4chter et al. 2012. Since this paper uses state-of-art data science techniques, the results provide a different point of view on the problem. As already shown in relevant literature from experimental economics, human decisions deviate from rational payoff maximization. The average gift rate was 31%. Gift rate was under no conditions zero. Further, we derive some special findings and calculate their significance.", "target": "Reciprocity in Gift-Exchange-Games"}
{"id": "task1540-464cb6c27f7040e5ada67176b17fd7f4", "input": "We present Deep Speaker, a neural speaker embedding system that maps utterances to a hypersphere where speaker similarity is measured by cosine similarity. The embeddings generated by Deep Speaker can be used for many tasks, including speaker identification, verification, and clustering. We experiment with ResCNN and GRU architectures to extract the acoustic features, then mean pool to produce utterance-level speaker embeddings, and train using triplet loss based on cosine similarity. Experiments on three distinct datasets suggest that Deep Speaker outperforms a DNN-based i-vector baseline. For example, Deep Speaker reduces the verification equal error rate by 50% (relatively) and improves the identification accuracy by 60% (relatively) on a text-independent dataset. We also present results that suggest adapting from a model trained with Mandarin can improve accuracy for English speaker recognition.", "target": "Deep Speaker: an End-to-End Neural Speaker Embedding System"}
{"id": "task1540-cbddcd8a3f454139836afb1f559bd1c5", "input": "We study the problem of estimating, in the sense of optimal transport metrics, a measure which is assumed supported on a manifold embedded in a Hilbert space. By establishing a precise connection between optimal transport metrics, optimal quantization, and learning theory, we derive new probabilistic bounds for the performance of a classic algorithm in unsupervised learning (k-means), when used to produce a probability measure derived from the data. In the course of the analysis, we arrive at new lower bounds, as well as probabilistic upper bounds on the convergence rate of the empirical law of large numbers, which, unlike existing bounds, are applicable to a wide class of measures.", "target": "Learning Probability Measures with respect to Optimal Transport Metrics"}
{"id": "task1540-fcdfbc66516948d4ba1648464e609387", "input": "Automatically generated political event data is an important part of the social science data ecosystem. The approaches for generating this data, though, have remained largely the same for two decades. During this time, the field of computational linguistics has progressed tremendously. This paper presents an overview of political event data, including methods and ontologies, and a set of experiments to determine the applicability of deep neural networks to the extraction of political events from news text.", "target": "Generating Politically-Relevant Event Data"}
{"id": "task1540-1b2ef36b2c2e4a5b80c75b5c1e99bf2f", "input": "In this paper, we develop a novel paradigm, namely hypergraph shift, to find robust graph modes by probabilistic voting strategy, which are semantically sound besides the self-cohesiveness requirement in forming graph modes. Unlike the existing techniques to seek graph modes by shifting vertices based on pair-wise edges (i.e, an edge with 2 ends), our paradigm is based on shifting high-order edges (hyperedges) to deliver graph modes. Specifically, we convert the problem of seeking graph modes as the problem of seeking maximizers of a novel objective function with the aim to generate good graph modes based on sifting edges in hypergraphs. As a result, the generated graph modes based on dense subhypergraphs may more accurately capture the object semantics besides the self-cohesiveness requirement. We also formally prove that our technique is always convergent. Extensive empirical studies on synthetic and real world data sets are conducted on clustering and graph matching. They demonstrate that our techniques significantly outperform the existing techniques.", "target": "Finding Modes by Probabilistic Hypergraphs Shifting"}
{"id": "task1540-1620a7b6fa774e2a94b535bcf9f34d29", "input": "We present a Bayesian approach to adapting parameters of a well-trained context-dependent, deep-neural-network, hidden Markov model (CD-DNN-HMM) to improve automatic speech recognition performance. Given an abundance of DNN parameters but with only a limited amount of data, the effectiveness of the adapted DNN model can often be compromised. We formulate maximum a posteriori (MAP) adaptation of parameters of a specially designed CD-DNN-HMM with an augmented linear hidden networks connected to the output tied states, or senones, and compare it to feature space MAP linear regression previously proposed. Experimental evidences on the 20,000-word open vocabulary Wall Street Journal task demonstrate the feasibility of the proposed framework. In supervised adaptation, the proposed MAP adaptation approach provides more than 10% relative error reduction and consistently outperforms the conventional transformation based methods. Furthermore, we present an initial attempt to generate hierarchical priors to improve adaptation efficiency and effectiveness with limited adaptation data by exploiting similarities among senones.", "target": "Maximum a Posteriori Adaptation of Network Parameters in Deep Models"}
{"id": "task1540-ce01f0db90ac4f958e405854501f1038", "input": "Present day machine learning is computationally intensive and processes large amounts of data. It is implemented in a distributed fashion in order to address these scalability issues. The work is parallelized across a number of computing nodes. It is usually hard to estimate in advance how many nodes to use for a particular workload. We propose a simple framework for estimating the scalability of distributed machine learning algorithms. We measure the scalability by means of the speedup an algorithm achieves with more nodes. We propose time complexity models for gradient descent and graphical model inference. We validate our models with experiments on deep learning training and belief propagation. This framework was used to study the scalability of machine learning algorithms in Apache Spark.", "target": "Modeling Scalability of Distributed Machine Learning"}
{"id": "task1540-a446befd81474f64aabd96d2d52d2d85", "input": "The utilization of statistical machine translation (SMT) has grown enormously over the last decade, many using open-source software developed by the NLP community. As commercial use has increased, there is need for software that is optimized for commercial requirements, in particular, fast phrase-based decoding and more efficient utilization of modern multicore servers. In this paper we re-examine the major components of phrase-based decoding and decoder implementation with particular emphasis on speed and scalability on multicore machines. The result is a drop-in replacement for the Moses decoder which is up to fifteen times faster and scales monotonically with the number of cores.", "target": "Fast, Scalable Phrase-Based SMT Decoding"}
{"id": "task1540-685ee749a5d7459482ff8e95a855a266", "input": "In situ hybridisation gene expression information helps biologists identify where a gene is expressed. However, the databases that republish the experimental information are often both incomplete and inconsistent. This paper examines a system, Argudas, designed to help tackle these issues. Argudas is an evolution of an existing system, and so that system is reviewed as a means of both explaining and justifying the behaviour of Argudas. Throughout the discussion of Argudas a number of issues will be raised including the appropriateness of argumentation in biology and the challenges faced when integrating apparently similar online biological databases.", "target": "Argudas: arguing with gene expression information"}
{"id": "task1540-361d8e77433842a6bced5e6a72533415", "input": "We study online boosting, the task of converting any weak online learner into a strong online learner. Based on a novel and natural definition of weak online learnability, we develop two online boosting algorithms. The first algorithm is an online version of boost-by-majority. By proving a matching lower bound, we show that this algorithm is essentially optimal in terms of the number of weak learners and the sample complexity needed to achieve a specified accuracy. This optimal algorithm is not adaptive, however. Using tools from online loss minimization, we derive an adaptive online boosting algorithm that is also parameter-free, but not optimal. Both algorithms work with base learners that can handle example importance weights directly, as well as by rejection sampling examples with probability defined by the booster. Results are complemented with an experimental study.", "target": "Optimal and Adaptive Algorithms for Online Boosting"}
{"id": "task1540-8f3b66965f444729b911584b94f87e2b", "input": "We propose a new learning method for heterogeneous domain adaptation (HDA), in which the data from the source domain and the target domain are represented by heterogeneous features with different dimensions. Using two different projection matrices, we first transform the data from two domains into a common subspace in order to measure the similarity between the data from two domains. We then propose two new feature mapping functions to augment the transformed data with their original features and zeros. The existing learning methods (e.g., SVM and SVR) can be readily incorporated with our newly proposed augmented feature representations to effectively utilize the data from both domains for HDA. Using the hinge loss function in SVM as an example, we introduce the detailed objective function in our method called Heterogeneous Feature Augmentation (HFA) for a linear case and also describe its kernelization in order to efficiently cope with the data with very high dimensions. Moreover, we also develop an alternating optimization algorithm to effectively solve the nontrivial optimization problem in our HFA method. Comprehensive experiments on two benchmark datasets clearly demonstrate that HFA outperforms the existing HDA methods.", "target": "Learning with Augmented Features for Heterogeneous Domain Adaptation"}
{"id": "task1540-9c7ee691bd764cf892e2b75bfab642c4", "input": "The goal of semi-supervised learning methods is to effectively combine labeled and unlabeled data to arrive at a better model. Many methods rely on graph-based approaches, where labels are propagated through a graph over the input examples. In most current methods, the propagation mechanism underlying the learning objective is based on random walks. While theoretically elegant, random walks suffer from several drawbacks which can hurt predictive performance. In this work, we explore dynamic infection processes as an alternative propagation mechanism. In these, unlabeled nodes can be \u201cinfected\u201d with the label of their already infected neighbors. We provide an efficient, scalable, and parallelizable algorithm for estimating the expected infection outcomes. We also describe an optimization view of the method, relating it to Laplacian approaches. Finally, experiments demonstrate that the method is highly competitive across multiple benchmarks and for various learning settings.", "target": "Semi-Supervised Learning with Competitive Infection Models"}
{"id": "task1540-43bd4fbe2d314f98801214954ab8c6cc", "input": "We describe an adaptation and application of a search-based structured prediction algorithm \u201cSearn\u201d to unsupervised learning problems. We show that it is possible to reduce unsupervised learning to supervised learning and demonstrate a high-quality unsupervised shift-reduce parsing model. We additionally show a close connection between unsupervised Searn and expectation maximization. Finally, we demonstrate the efficacy of a semi-supervised extension. The key idea that enables this is an application of the predict-self idea for unsupervised learning.", "target": "Unsupervised Search-based Structured Prediction"}
{"id": "task1540-520416eb622e4ee6959ce7499f112d02", "input": "The article deals with the issue of modification of metric classification algorithms. In particular, it studies the algorithm k-Nearest Neighbours for its application to sequential data. A method of generalization of metric classification algorithms is proposed. As a part of it, there has been developed an algorithm for solving the problem of classification and labelling of sequential data. The advantages of the developed algorithm of classification in comparison with the existing one are also discussed in the article. There is a comparison of the effectiveness of the proposed algorithm with the algorithm of CRF in the task of chunking in the open data set CoNLL2000.", "target": "Generalization of metric classification algorithms for sequences classification and labelling"}
{"id": "task1540-f6707d9aa22549debbf77c937bbca2fd", "input": "We present Net2Vec, a flexible high-performance platform that allows the execution of deep learning algorithms in the communication network. Net2Vec is able to capture data from the network at more than 60Gbps, transform it into meaningful tuples and apply predictions over the tuples in real time. This platform can be used for different purposes ranging from traffic classification to network performance analysis. Finally, we showcase the use of Net2Vec by implementing and testing a solution able to profile network users at line rate using traces coming from a real network. We show that the use of deep learning for this case outperforms the baseline method both in terms of accuracy and performance.", "target": "Net2Vec: Deep Learning for the Network"}
{"id": "task1540-dc3ab99fe8e946dd990a208f35a59519", "input": "We measured entropy and symbolic diversity for English and Spanish texts including literature Nobel laureates and other famous authors. Entropy, symbol diversity and symbol frequency profiles were compared for these four groups. We also built a scale sensitive to the quality of writing and evaluated its relationship with the Flesch \u0301s readability index for English and the Szigriszt \u0301s perspicuity index for Spanish. Results suggest a correlation between entropy and word diversity with quality of writing. Text genre also influences the resulting entropy and diversity of the text. Results suggest the plausibility of automated quality assessment of texts.", "target": "Quantifying literature quality using complexity criteria"}
{"id": "task1540-47bb7477919a47bdb157956f7091fdf9", "input": "Deep learning has been shown as a successful machine learning method for a variety of tasks, and its popularity results in numerous open-source deep learning software tools coming to public. Training a deep network is usually a very time-consuming process. To address the huge computational challenge in deep learning, many tools exploit hardware features such as multi-core CPUs and many-core GPUs to shorten the training time. However, different tools exhibit different features and running performance when training different types of deep networks on different hardware platforms, which makes it difficult for end users to select an appropriate pair of software and hardware. In this paper, we aim to make a comparative study of the state-of-the-art GPU-accelerated deep learning software tools, including Caffe, CNTK, TensorFlow, and Torch. We benchmark the running performance of these tools with three popular types of neural networks on two CPU platforms and three GPU platforms. Our contribution is twofold. First, for deep learning end users, our benchmarking results can serve as a guide to selecting appropriate software tool and hardware platform. Second, for deep learning software developers, our in-depth analysis points out possible future directions to further optimize the training performance.", "target": "Benchmarking State-of-the-Art Deep Learning Software Tools"}
{"id": "task1540-2ab7c608b2644cbe94409ee94e55b4b8", "input": "Visual Question Answering (VQA) has received a lot of attention over the past couple of years. A number of deep learning models have been proposed for this task. However, it has been shown [1\u20134] that these models are heavily driven by superficial correlations in the training data and lack compositionality \u2013 the ability to answer questions about unseen compositions of seen concepts. This compositionality is desirable and central to intelligence. In this paper, we propose a new setting for Visual Question Answering where the test question-answer pairs are compositionally novel compared to training question-answer pairs. To facilitate developing models under this setting, we present a new compositional split of the VQA v1.0 [5] dataset, which we call Compositional VQA (C-VQA). We analyze the distribution of questions and answers in the C-VQA splits. Finally, we evaluate several existing VQA models under this new setting and show that the performances of these models degrade by a significant amount compared to the original VQA setting.", "target": "C-VQA: A Compositional Split of the Visual Question Answering (VQA) v1.0 Dataset"}
{"id": "task1540-4ca363d1fb6748dab749e2d5fc56e99b", "input": "We present a probabilistic model that simultaneously learns alignments and distributed representations for bilingual data. By marginalizing over word alignments the model captures a larger semantic context than prior work relying on hard alignments. The advantage of this approach is demonstrated in a cross-lingual classification task, where we outperform the prior published state of the art.", "target": "Learning Bilingual Word Representations by Marginalizing Alignments"}
{"id": "task1540-f89f1af175f049a8aa95390d0383493a", "input": "We propose an original particle-based implementation of the Loopy Belief Propagation (LPB) algorithm for pairwise Markov Random Fields (MRF) on a continuous state space. The algorithm constructs adaptively efficient proposal distributions approximating the local beliefs at each note of the MRF. This is achieved by considering proposal distributions in the exponential family whose parameters are updated iterately in an Expectation Propagation (EP) framework. The proposed particle scheme provides consistent estimation of the LBP marginals as the number of particles increases. We demonstrate that it provides more accurate results than the Particle Belief Propagation (PBP) algorithm of [1] at a fraction of the computational cost and is additionally more robust empirically. The computational complexity of our algorithm at each iteration is quadratic in the number of particles. We also propose an accelerated implementation with sub-quadratic computational complexity which still provides consistent estimates of the loopy BP marginal distributions and performs almost as well as the original procedure.", "target": "Expectation Particle Belief Propagation"}
{"id": "task1540-5f412f1324ab47e6a64370f6434a959c", "input": "Efficient and robust algorithms for decentralized estimation in networks are essential to many distributed systems. Whereas distributed estimation of sample mean statistics has been the subject of a good deal of attention, computation of U statistics, relying on more expensive averaging over pairs of observations, is a less investigated area. Yet, such data functionals are essential to describe global properties of a statistical population, with important examples including Area Under the Curve, empirical variance, Gini mean difference and within-cluster point scatter. This paper proposes new synchronous and asynchronous randomized gossip algorithms which simultaneously propagate data across the network and maintain local estimates of the U -statistic of interest. We establish convergence rate bounds of O(1/t) and O(log t/t) for the synchronous and asynchronous cases respectively, where t is the number of iterations, with explicit data and network dependent terms. Beyond favorable comparisons in terms of rate analysis, numerical experiments provide empirical evidence the proposed algorithms surpasses the previously introduced approach.", "target": "Extending Gossip Algorithms to Distributed Estimation of U -Statistics"}
{"id": "task1540-6c88a011c5c54ce2b80a988f4c6d9a60", "input": "Domain adaptation is important in sentiment analysis as sentiment-indicating words vary between domains. Recently, multi-domain adaptation has become more pervasive, but existing approaches train on all available source domains including dissimilar ones. However, the selection of appropriate training data is as important as the choice of algorithm. We undertake \u2013 to our knowledge for the first time \u2013 an extensive study of domain similarity metrics in the context of sentiment analysis and propose novel representations, metrics, and a new scope for data selection. We evaluate the proposed methods on two largescale multi-domain adaptation settings on tweets and reviews and demonstrate that they consistently outperform strong random and balanced baselines, while our proposed selection strategy outperforms instance-level selection and yields the best score on a large reviews corpus. All experiments are available at url_redacted1", "target": "Data Selection Strategies for Multi-Domain Sentiment Analysis"}
{"id": "task1540-0efd0abf465549f48ee93e114abe7dbf", "input": "Algorithmic composition is the partial or total automation of the process of music composition by using computers. Since the 1950s, different computational techniques related to Artificial Intelligence have been used for algorithmic composition, including grammatical representations, probabilistic methods, neural networks, symbolic rule-based systems, constraint programming and evolutionary algorithms. This survey aims to be a comprehensive account of research on algorithmic composition, presenting a thorough view of the field for researchers in Artificial Intelligence.", "target": "AI Methods in Algorithmic Composition: A Comprehensive Survey"}
{"id": "task1540-be9792bad6784d6ea82a5c9295e730de", "input": "Deep learning has become a ubiquitous technology to improve machine intelligence. However, most of the existing deep models are structurally very complex, making them difficult to be deployed on the mobile platforms with limited computational power. In this paper, we propose a novel network compression method called dynamic network surgery, which can remarkably reduce the network complexity by making on-the-fly connection pruning. Unlike the previous methods which accomplish this task in a greedy way, we properly incorporate connection splicing into the whole process to avoid incorrect pruning and make it as a continual network maintenance. The effectiveness of our method is proved with experiments. Without any accuracy loss, our method can efficiently compress the number of parameters in LeNet-5 and AlexNet by a factor of 108\u00d7 and 17.7\u00d7 respectively, proving that it outperforms the recent pruning method by considerable margins. The code will be made publicly available.", "target": "Dynamic Network Surgery for Efficient DNNs"}
{"id": "task1540-28150498f9f047f29f4838357a564b66", "input": "Fixed-vocabulary language models fail to account for one of the most characteristic statistical facts of natural language: the frequent creation and reuse of new word types. Although character-level language models offer a partial solution in that they can create word types not attested in the training corpus, they do not capture the \u201cbursty\u201d distribution of such words. In this paper, we augment a hierarchical LSTM language model that generates sequences of word tokens character by character with a caching mechanism that learns to reuse previously generated words. To validate our model we construct a new open-vocabulary language modeling corpus (the Multilingual Wikipedia Corpus; MWC) from comparable Wikipedia articles in 7 typologically diverse languages and demonstrate the effectiveness of our model across this range of languages.", "target": "Learning to Create and Reuse Words in Open-Vocabulary Neural Language Modeling"}
{"id": "task1540-4664ee9c79504d5a8cb3e7bbc8ded7b7", "input": "In this paper we introduce Latent Tree Language Model (LTLM), a novel approach to language modeling that encodes syntax and semantics of a given sentence as a tree of word roles. The learning phase iteratively updates the trees by moving nodes according to Gibbs sampling. We introduce two algorithms to infer a tree for a given sentence. The first one is based on Gibbs sampling. It is fast, but does not guarantee to find the most probable tree. The second one is based on dynamic programming. It is slower, but guarantees to find the most probable tree. We provide comparison of both algorithms. We combine LTLM with 4-gram Modified Kneser-Ney language model via linear interpolation. Our experiments with English and Czech corpora show significant perplexity reductions (up to 46% for English and 49% for Czech) compared with standalone 4-gram Modified Kneser-Ney language model.", "target": "Latent Tree Language Model"}
{"id": "task1540-7360b8536fa04d439434f8af7a15dcb2", "input": "In this article we present two ways of struc\u00ad turing bodies of evidence, which allow us to reduce the complexity of the operations usu\u00ad ally performed in the framework of evidence theory. The first structure just partitions the focal elements in a body of evidence by their cardinality. With this structure we are able to reduce the complexity on the calculation of the belief functions Bel, PI, and Q. The other structure proposed here, the Hierarchi\u00ad cal Trees, permits us to reduce the complex\u00ad ity of the calculation of Bel, PI, and Q, as well as of the Dempster's rule of combination in relation to the brute-force algorithm. Both these structures do not require the generation of all the subsets of the reference domain.", "target": "Structuring Bodies of Evidence"}
{"id": "task1540-79240c6c277c48cd8240fb8d8f36f592", "input": "Denoising autoencoders (DAs) are typically applied to relatively large datasets for unsupervised learning of representative data encodings; they rely on the idea of making the learned representations robust to partial corruption of the input pattern, and perform learning using stochastic gradient descent with relatively large datasets. In this paper, we present a fully Bayesian DA architecture that allows for the application of DAs even when data is scarce. Our novel approach formulates the signal encoding problem under a nonparametric Bayesian regard, considering a Gaussian process prior over the latent input encodings generated given the (corrupt) input observations. Subsequently, the decoder modules of our model are formulated as large-margin regression models, treated under the Bayesian inference paradigm, by exploiting the maximum entropy discrimination (MED) framework. We exhibit the effectiveness of our approach using several datasets, dealing with both classification and transfer learning applications.", "target": "Maximum Entropy Discrimination Denoising Autoencoders"}
{"id": "task1540-99cf258b04a642c3b4a5e61d1de6b7ef", "input": "Neural machine translation (NMT) models are able to partially learn syntactic information from sequential lexical information. Still, some complex syntactic phenomena such as prepositional phrase attachment are poorly modeled. This work aims to answer two questions: 1) Does explicitly modeling source or target language syntax help NMT? 2) Is tight integration of words and syntax better than multitask training? We introduce syntactic information in the form of CCG supertags either in the source as an extra feature in the embedding, or in the target, by interleaving the target supertags with the word sequence. Our results on WMT data show that explicitly modeling syntax improves machine translation quality for English\u2194German, a high-resource pair, and for English\u2194Romanian, a lowresource pair and also several syntactic phenomena including prepositional phrase attachment. Furthermore, a tight coupling of words and syntax improves translation quality more than multitask training.", "target": "Syntax-aware Neural Machine Translation Using CCG"}
{"id": "task1540-878ce6f6ca1344e7a131c1ebfb0a9833", "input": "Language models for agglutinative languages have always been hindered in past due to myriad of agglutinations possible to any given word through various affixes.We propose a method to diminish the problem of out-of-vocabulary words by introducing an embedding derived from syllables and morphemes which leverages the agglutinative property. Our model outperforms character-level embedding in perplexity by 16.87 with 9.50M parameters. Proposed method achieves state of the art performance over existing input prediction methods in terms of Key Stroke Saving and has been commercialized.", "target": "Syllable-level Neural Language Model for Agglutinative Language"}
{"id": "task1540-af2951272f23498ca304c2cb6ddf12ea", "input": "We propose a technique for learning representations of parser states in transitionbased dependency parsers. Our primary innovation is a new control structure for sequence-to-sequence neural networks\u2014 the stack LSTM. Like the conventional stack data structures used in transitionbased parsing, elements can be pushed to or popped from the top of the stack in constant time, but, in addition, an LSTM maintains a continuous space embedding of the stack contents. This lets us formulate an efficient parsing model that captures three facets of a parser\u2019s state: (i) unbounded look-ahead into the buffer of incoming words, (ii) the complete history of actions taken by the parser, and (iii) the complete contents of the stack of partially built tree fragments, including their internal structures. Standard backpropagation techniques are used for training and yield state-of-the-art parsing performance.", "target": "Transition-Based Dependency Parsing with Stack Long Short-Term Memory"}
{"id": "task1540-3768b81ed07946eb9fc9b8afc0e87106", "input": "Modified policy iteration (MPI) is a dynamic programming (DP) algorithm that contains the two celebrated policy and value iteration methods. Despite its generality, MPI has not been thoroughly studied, especially its approximation form which is used when the state and/or action spaces are large or infinite. In this paper, we propose three implementations of approximate MPI (AMPI) that are extensions of well-known approximate DP algorithms: fitted-value iteration, fitted-Q iteration, and classification-based policy iteration. We provide error propagation analyses that unify those for approximate policy and value iteration. On the last classification-based implementation, we develop a finite-sample analysis that shows that MPI\u2019s main parameter allows to control the balance between the estimation error of the classifier and the overall value function approximation.", "target": "Approximate Modified Policy Iteration"}
{"id": "task1540-a0d013cd69e94843b64622ce0177858b", "input": "Emotional content is a key element in user-generated videos. However, it is difficult to understand emotions conveyed in such videos due to the complex and unstructured nature of user-generated content and the sparsity of video frames that express emotion. In this paper, for the first time, we study the problem of transferring knowledge from heterogeneous external sources, including image and textual data, to facilitate three related tasks in video emotion understanding: emotion recognition, emotion attribution and emotion-oriented summarization. Specifically, our framework (1) learns a video encoding from an auxiliary emotional image dataset in order to improve supervised video emotion recognition, and (2) transfers knowledge from an auxiliary textual corpus for zero-shot recognition of emotion classes unseen during training. The proposed technique for knowledge transfer facilitates novel applications of emotion attribution and emotion-oriented summarization. A comprehensive set of experiments on multiple datasets demonstrate the effectiveness of our framework.", "target": "Heterogeneous Knowledge Transfer in Video Emotion Recognition, Attribution and Summarization"}
{"id": "task1540-6220b21842814665a67d37c8ee137556", "input": "The deep Boltzmann machine (DBM) has been an important development in the quest for powerful \u201cdeep\u201d probabilistic models. To date, simultaneous or joint training of all layers of the DBM has been largely unsuccessful with existing training methods. We introduce a simple regularization scheme that encourages the weight vectors associated with each hidden unit to have similar norms. We demonstrate that this regularization can be easily combined with standard stochastic maximum likelihood to yield an effective training strategy for the simultaneous training of all layers of the deep Boltzmann machine.", "target": "On Training Deep Boltzmann Machines"}
{"id": "task1540-51c313b60a804127859bb7a2173c9153", "input": "Breast cancer is one of the leading causes of cancer death among women worldwide. In clinical routine, automatic breast ultrasound (BUS) image segmentation is very challenging and essential for cancer diagnosis and treatment planning. Many BUS segmentation approaches have been studied in the last two decades, and have been proved to be effective on private datasets. Currently, the advancement of BUS image segmentation seems to meet its bottleneck. The improvement of the performance is increasingly challenging, and only few new approaches were published in the last several years. It is the time to look at the field by reviewing previous approaches comprehensively and to investigate the future directions. In this paper, we study the basic ideas, theories,pros and cons of the approaches, group them into categories, and extensively review each category in depth by discussing the principles, application issues, and advantages/disadvantages. Keyword: breast ultrasound (BUS) images, breast cancer, segmentation, benchmark, early detection, computer-aided diagnosis (CAD)", "target": "Automatic Breast Ultrasound Image Segmentation: A Survey"}
{"id": "task1540-be617c91255e43fa9ff90e7b5b01caef", "input": "Plan recognition aims to discover target plans (i.e., sequences of actions) behind observed actions, with history plan libraries or domain models in hand. Previous approaches either discover plans by maximally \u201cmatching\u201d observed actions to plan libraries, assuming target plans are from plan libraries, or infer plans by executing domain models to best explain the observed actions, assuming complete domain models are available. In real world applications, however, target plans are often not from plan libraries and complete domain models are often not available, since building complete sets of plans and complete domain models are often difficult or expensive. In this paper we view plan libraries as corpora and learn vector representations of actions using the corpora; we then discover target plans based on the vector representations. Our approach is capable of discovering underlying plans that are not from plan libraries, without requiring domain models provided. We empirically demonstrate the effectiveness of our approach by comparing its performance to traditional plan recognition approaches in three planning domains.", "target": "Discovering Underlying Plans Based on Distributed Representations of Actions"}
{"id": "task1540-de884ae9d2f642eea2512138c6548453", "input": "The zero-shot paradigm exploits vector-based word representations extracted from text corpora with unsupervised methods to learn general mapping functions from other feature spaces onto word space, where the words associated to the nearest neighbours of the mapped vectors are used as their linguistic labels. We show that the neighbourhoods of the mapped elements are strongly polluted by hubs, vectors that tend to be near a high proportion of items, pushing their correct labels down the neighbour list. After illustrating the problem empirically, we propose a simple method to correct it by taking the proximity distribution of potential neighbours across many mapped vectors into account. We show that this correction leads to consistent improvements in realistic zero-shot experiments in the cross-lingual, image labeling and image retrieval domains.", "target": "IMPROVING ZERO-SHOT LEARNING BY MITIGATING THE HUBNESS PROBLEM"}
{"id": "task1540-33f0d3d8838d4f6cab56592d84768302", "input": "We investigate attention as the active pursuit of useful information. This contrasts with attention as a mechanism for the attenuation of irrelevant information. We also consider the role of short-term memory, whose use is critical to any model incapable of simultaneously perceiving all information on which its output depends. We present several simple synthetic tasks, which become considerably more interesting when we impose strong constraints on how a model can interact with its input, and on how long it can take to produce its output. We develop a model with a different structure from those seen in previous work, and we train it using stochastic variational inference with a learned proposal distribution.", "target": "Testing Visual Attention in Dynamic Environments"}
{"id": "task1540-ba35066d206c457bb526b7383e8fd8d3", "input": "Constraint-based causal discovery from limited data is a notoriously difficult challenge due to the many borderline independence test decisions. Several approaches to improve the reliability of the predictions by exploiting redundancy in the independence information have been proposed recently. Though promising, existing approaches can still be greatly improved in terms of accuracy and scalability. We present a novel method that reduces the combinatorial explosion of the search space by using a more coarse-grained representation of causal information, drastically reducing computation time. Additionally, we propose a method to score causal predictions based on their confidence. Crucially, our implementation also allows one to easily combine observational and interventional data and to incorporate various types of available background knowledge. We prove soundness and asymptotic consistency of our method and demonstrate that it can outperform the state-ofthe-art on synthetic data, achieving a speedup of several orders of magnitude. We illustrate its practical feasibility by applying it on a challenging protein data set.", "target": "Ancestral Causal Inference"}
{"id": "task1540-f1c760d4bc354caf9d636abd48e34c5c", "input": "The commonly used Q-learning algorithm combined with function approximation induces systematic overestimations of state-action values. These systematic errors might cause instability, poor performance and sometimes divergence of learning. In this work, we present the AVERAGED TARGET DQN (ADQN) algorithm, an adaptation to the DQN class of algorithms which uses a weighted average over past learned networks to reduce generalization noise variance. As a consequence, this leads to reduced overestimations, more stable learning process and improved performance. Additionally, we analyze ADQN variance reduction along trajectories and demonstrate the performance of ADQN on a toy Gridworld problem, as well as on several of the Atari 2600 games from the Arcade Learning Environment.", "target": "Deep Reinforcement Learning with Averaged Target DQN"}
{"id": "task1540-76672374ff054cd1a4383bab12fad5e2", "input": "In cognitive sciences it is not uncommon to use various games effectively. For example, in artificial intelligence, the RoboCup [14] initiative was to set up to catalyse research on the field of autonomous agent technology. In this paper, we introduce a similar soccer simulation initiative to try to investigate a model of human consciousness and a notion of reality in the form of a cognitive problem. In addition, for example, the home pitch advantage and the objective role of the supporters could be naturally described and discussed in terms of this new soccer simulation model.", "target": "Quantum Consciousness Soccer Simulator"}
{"id": "task1540-10c63994fab04fa8818776c084743a9c", "input": "Recent developments show that Multiply Sectioned Bayesian Networks (MSBNs) can be used for diagnosis of natural systems as well as for model-based diagnosis of artificial systems. They can be applied to single-agent oriented reasoning systems as well as multi\u00ad agent distributed reasoning systems. Belief propagation between a pair of subnets plays a central role in maintenance of global consistency in a MSBN. This paper studies the operation UpdateBelief, presented orig\u00ad inally with MSBNs, for inter-subnet propaga\u00ad tion. We analyze how the operation achieves its intended functionality, which provides hints for improving its efficiency. New versions of UpdateBelief are then de\u00ad fined that reduce the computation time for inter-subnet propagation. One of them is optimal in the sense that the minimum amount of computation for coordinating multi-linkage belief propagation is required. The optimization problem is solved through the solution of a graph-theoretic problem: the minimum weight open tour in a tree.", "target": "Optimization of Inter-Subnet Belief Updating in Multiply Sectioned Bayesian Networks"}
{"id": "task1540-17029dc2c7bf4823a9c7e93e3ebf90d9", "input": "Short text messages such as tweets are very noisy and sparse in their use of vocabulary. Traditional textual representations, such as tf-idf, have difficulty grasping the semantic meaning of such texts, which is important in applications such as event detection, opinion mining, news recommendation, etc. We constructed a method based on semantic word embeddings and frequency information to arrive at low-dimensional representations for short texts designed to capture semantic similarity. For this purpose we designed a weight-based model and a learning procedure based on a novel median-based loss function. This paper discusses the details of our model and the optimization methods, together with the experimental results on both Wikipedia and Twitter data. We find that our method outperforms the baseline approaches in the experiments, and that it generalizes well on different word embeddings without retraining. Our method is therefore capable of retaining most of the semantic information in the text, and is applicable out-of-the-box. c \u00a9 2016 Elsevier Ltd. All rights reserved.", "target": "Representation learning for very short texts using weighted word embedding aggregation"}
{"id": "task1540-f4c8462acb604468a5eefad3cbd9cc4b", "input": "We introduce Natural Neural Networks, a novel family of algorithms that speed up convergence by adapting their internal representation during training to improve conditioning of the Fisher matrix. In particular, we show a specific example that employs a simple and efficient reparametrization of the neural network weights by implicitly whitening the representation obtained at each layer, while preserving the feed-forward computation of the network. Such networks can be trained efficiently via the proposed Projected Natural Gradient Descent algorithm (PRONG), which amortizes the cost of these reparametrizations over many parameter updates and is closely related to the Mirror Descent online learning algorithm. We highlight the benefits of our method on both unsupervised and supervised learning tasks, and showcase its scalability by training on the large-scale ImageNet Challenge dataset.", "target": "Natural Neural Networks"}
{"id": "task1540-22c4d7c9162c4bc6985201d011cdda70", "input": "In this era of digitization, knowing the user\u2019s sociolect aspects have become essential features to build the user specific recommendation systems. These sociolect aspects could be found by mining the user\u2019s language sharing in the form of text in social media and reviews. This paper describes about the experiment that was performed in PAN Author Profiling 2017 shared task. The objective of the task is to find the sociolect aspects of the users from their tweets. The sociolect aspects considered in this experiment are user\u2019s gender and native language information. Here user\u2019s tweets written in a different language from their native language are represented as Document Term Matrix with document frequency as the constraint. Further classification is done using the Support Vector Machine by taking gender and native language as target classes. This experiment attains the average accuracy of 73.42% in gender prediction and 76.26% in the native language identification task.", "target": "Vector Space Model as Cognitive Space for Text Classification"}
{"id": "task1540-ce5181a4e45e4759aa2017187673233f", "input": "We prove in this paper that the expected value of the objective function of the k-means++ algorithm for samples converges to population expected value. As k-means++, for samples, provides with constant factor approximation for k-means objectives, such an approximation can be achieved for the population with increase of the sample size. This result is of potential practical relevance when one is considering using subsampling when clustering large data sets (large data bases).", "target": "On the Consistency of k-means++ algorithm"}
{"id": "task1540-acc4cc327b8a491b8b2dfd0bee37c675", "input": "Neural machine translation has become a major alternative to widely used phrase-based statistical machine translation. We notice however that much of research on neural machine translation has focused on European languages despite its language agnostic nature. In this paper, we apply neural machine translation to the task of Arabic translation (Ar\u2194En) and compare it against a standard phrase-based translation system. We run extensive comparison using various configurations in preprocessing Arabic script and show that the phrase-based and neural translation systems perform comparably to each other and that proper preprocessing of Arabic script has a similar effect on both of the systems. We however observe that the neural machine translation significantly outperform the phrase-based system on an out-of-domain test set, making it attractive for real-world deployment.", "target": "First Result on Arabic Neural Machine Translation"}
{"id": "task1540-866aa386f0c9404aa67e10bf1af67def", "input": "Decision theoretical troubleshooting is about minimizing the expected cost of solving a certain problem like repairing a complicated man-made device. In this paper we consider situations where you have to take apart some of the device to get access to certain clusters and actions. Specifically, we investigate troubleshooting with independent actions in a tree of clusters where actions inside a cluster cannot be performed before the cluster is opened. The problem is non-trivial because there is a cost associated with opening and closing a cluster. Troubleshooting with independent actions and no clusters can be solved in O(n \u00b7 lg n) time (n being the number of actions) by the well-known \u201dP-over-C\u201d algorithm due to Kadane and Simon, but an efficient and optimal algorithm for a tree cluster model has not yet been found. In this paper we describe a \u201dbottom-up P-over-C\u201d O(n \u00b7 lg n) time algorithm and show that it is optimal when the clusters do not need to be closed to test whether the actions solved the problem.", "target": "The Cost of Troubleshooting Cost Clusters with Inside Information"}
{"id": "task1540-23f0a4953b1348769a062307a4cd1a64", "input": "Distortion of the underlying speech is a common problem for single-channel speech enhancement algorithms, and hinders such methods from being used more extensively. A dictionary based speech enhancement method that emphasizes preserving the underlying speech is proposed. Spectral patches of clean speech are sampled and clustered to train a dictionary. Given a noisy speech spectral patch, the best matching dictionary entry is selected and used to estimate the noise power at each time-frequency bin. The noise estimation step is formulated as an outlier detection problem, where the noise at each bin is assumed present only if it is an outlier to the corresponding bin of the best matching dictionary entry. This framework assigns higher priority in removing spectral elements that strongly deviate from a typical spoken unit stored in the trained dictionary. Even without the aid of a separate noise model, this method can achieve significant noise reduction for various non-stationary noises, while effectively preserving the underlying speech in more challenging noisy environments.", "target": "SINGLE CHANNEL SPEECH ENHANCEMENT USING OUTLIER DETECTION"}
{"id": "task1540-8c91735f4d0141638e36314bbb5c72f2", "input": "This paper describes the realization of the Ontology Web Search Engine. The Ontology Web Search Engine is realizable as independent project and as a part of other projects. The main purpose of this paper is to present the Ontology Web Search Engine realization details as the part of the Semantic Web Expert System and to present the results of the Ontology Web Search Engine functioning. It is expected that the Semantic Web Expert System will be able to process ontologies from the Web, generate rules from these ontologies and develop its knowledge base.", "target": "REALIZATION OF ONTOLOGY WEB SEARCH ENGINE"}
{"id": "task1540-8023d60327424e6987e21417c9cc3049", "input": "The ability to know in advance the trend of running process instances, with respect to different features, such as the expected completion time, would allow business managers to timely counteract to undesired situations, in order to prevent losses. Therefore, the ability to accurately predict future features of running business process instances would be a very helpful aid when managing processes, especially under service level agreement constraints. However, making such accurate forecasts is not easy: many factors may influence the predicted features. Many approaches have been proposed to cope with this problem but all of them assume that the underling process is stationary. However, in real cases this assumption is not always true. In this work we present new methods for predicting the remaining time of running cases. In particular we propose a method, assuming process stationarity, which outperforms the state-of-the-art and two other methods which are able to make predictions even with nonstationary processes. We also describe an approach able to predict the full sequence of activities that a running case is going to take. All these methods are extensively evaluated on two real case studies.", "target": "Time and Activity Sequence Prediction of Business Process Instances"}
{"id": "task1540-ddccfafa83b149b1b0603aa5956f382a", "input": "The local computation technique (Shafer et a!. 1987, Shafer and Shenoy 1988, Shenoy and Shafer 1986) is used for propagating belief functions in so\u00ad called a Markov Tree. In this paper, we describe an efficient implementation of belief function propagation on the basis of the local computation technique. The presented method avoids all the redundant computations in the propagation process, and so makes the computational complexity decrease with respect to other existing implementations (Hsia and Shenoy 1989, Zarley et a!. 1988). We also give a combined algorithm for both propagation and re-propagation which makes the re-propagation process more efficient when one or more of the prior belief functions is changed.", "target": "An Efficient Implementation of Belief Function Propagation"}
{"id": "task1540-5b7b369018e8437f8e019c2089059ae0", "input": "Learning commonsense knowledge from natural language text is nontrivial due to reporting bias: people rarely state the obvious, e.g., \u201cmy house is bigger than me\u201d. However, while rarely stated explicitly, this trivial everyday knowledge does influence the way people talk about the world, which provides indirect clues to reason about the world. For example, a statement like \u201cJohn entered his house\u201d implies that his house is bigger than John. In this paper, we present an approach to infer relative physical knowledge of actions and objects along six dimensions (e.g., size, weight, and strength) from unstructured natural language text. We frame knowledge acquisition as joint inference over two closely related problems: learning (1) relative physical knowledge of object pairs and (2) physical implications of actions when applied to those object pairs. Empirical results demonstrate that it is possible to extract knowledge of actions and objects from language and that joint inference over different knowledge types improves performance.", "target": "Verb Physics: Relative Physical Knowledge of Actions and Objects"}
{"id": "task1540-66bb10181f3641bab8f03e6bbddbcd06", "input": "vii", "target": "RESOURCE SHARING FOR MULTI-TENANT NOSQL DATA STORE IN CLOUD"}
{"id": "task1540-7488c483258440298aa4ca7a8fca8483", "input": "Studying characters plays a vital role in computationally representing and interpreting narratives. Unlike previous work, which has focused on inferring character roles, we focus on the problem of modeling their relationships. Rather than assuming a fixed relationship for a character pair, we hypothesize that relationships are dynamic and temporally evolve with the progress of the narrative, and formulate the problem of relationship modeling as a structured prediction problem. We propose a semisupervised framework to learn relationship sequences from fully as well as partially labeled data. We present a Markovian model capable of accumulating historical beliefs about the relationship and status changes. We use a set of rich linguistic and semantically motivated features that incorporate world knowledge to investigate the textual content of narrative. We empirically demonstrate that such a framework outperforms competitive baselines.", "target": "Modeling Dynamic Relationships Between Characters in Literary Novels"}
{"id": "task1540-85b19a9b735648058e0bdbc936f39352", "input": "We study a stochastic and distributed algorithm for nonconvex problems whose objective consists of a sum of N nonconvex Li/N -smooth functions, plus a nonsmooth regularizer. The proposed NonconvEx primal-dual SpliTTing (NESTT) algorithm splits the problem into N subproblems, and utilizes an augmented Lagrangian based primal-dual scheme to solve it in a distributed and stochastic manner. With a special non-uniform sampling, a version of NESTT achieves -stationary solution using O(( \u2211N i=1 \u221a Li/N) / ) gradient evaluations, which can be up to O(N) times better than the (proximal) gradient descent methods. It also achieves Q-linear convergence rate for nonconvex `1 penalized quadratic problems with polyhedral constraints. Further, we reveal a fundamental connection between primal-dual based methods and a few primal only methods such as IAG/SAG/SAGA.", "target": "NESTT: A Nonconvex Primal-Dual Splitting Method for Distributed and Stochastic Optimization"}
{"id": "task1540-54ff6cab8abd46539b5d5474810f168f", "input": "Metric learning seeks a transformation of the feature space that enhances prediction quality for the given task at hand. In this work we provide PAC-style sample complexity rates for supervised metric learning. We give matching lowerand upper-bounds showing that the sample complexity scales with the representation dimension when no assumptions are made about the underlying data distribution. However, by leveraging the structure of the data distribution, we show that one can achieve rates that are fine-tuned to a specific notion of intrinsic complexity for a given dataset. Our analysis reveals that augmenting the metric learning optimization criterion with a simple norm-based regularization can help adapt to a dataset\u2019s intrinsic complexity, yielding better generalization. Experiments on benchmark datasets validate our analysis and show that regularizing the metric can help discern the signal even when the data contains high amounts of noise.", "target": "Sample Complexity of Learning Mahalanobis Distance Metrics"}
{"id": "task1540-a57f578932ba4b0997c57a744a9ae080", "input": "Sepsis is a leading cause of mortality in intensive care units (ICUs) and costs hospitals billions annually. Treating a septic patient is highly challenging, because individual patients respond very differently to medical interventions and there is no universally agreed-upon treatment for sepsis. Understanding more about a patient\u2019s physiological state at a given time could hold the key to effective treatment policies. In this work, we propose a new approach to deduce optimal treatment policies for septic patients by using continuous state-space models and deep reinforcement learning. Learning treatment policies over continuous spaces is important, because we retain more of the patient\u2019s physiological information. Our model is able to learn clinically interpretable treatment policies, similar in important aspects to the treatment policies of physicians. Evaluating our algorithm on past ICU patient data, we find that our model could reduce patient mortality in the hospital by up to 3.6% over observed clinical policies, from a baseline mortality of 13.7%. The learned treatment policies could be used to aid intensive care clinicians in medical decision making and improve the likelihood of patient survival.", "target": "Continuous State-Space Models for Optimal Sepsis Treatment - a Deep Reinforcement Learning Approach"}
{"id": "task1540-aa91485d6b7547b1837676e29243cf6c", "input": "This paper describes a class of probabilistic approximation algorithms based on bucket elimination which offer adjustable levels of accuracy and efficiency. We analyze the ap\u00ad proximation for several tasks: finding the most probable explanation, belief updat\u00ad ing and finding the maximum a posteriori hypothesis. We identify regions of com\u00ad pleteness and provide preliminary empiri\u00ad cal evaluation on randomly generated net\u00ad", "target": "A scheme for approximating probabilistic inference"}
{"id": "task1540-fad4e8a5f17d4c889e129015f4574630", "input": "In the existing evidential networks with belief functions, the relations among the variables are always represented by joint belief functions on the product space of the involved variables. In this paper, we use conditional belief functions to represent such relations in the network and show some relations of these two kinds of representations. We also present a propagation algorithm for such networks. By analyzing the properties of some special evidential networks with conditional belief functions, we show that the reasoning process can be simplified in such", "target": "Evidential Reasoning with Conditional Belief Functions"}
{"id": "task1540-877e8d6f0bdb4dafb4a368d99af51bc0", "input": "<lb>This paper introduces AdaSDCA: an adap-<lb>tive variant of stochastic dual coordinate as-<lb>cent (SDCA) for solving the regularized empir-<lb>ical risk minimization problems. Our modifica-<lb>tion consists in allowing the method adaptively<lb>change the probability distribution over the dual<lb>variables throughout the iterative process. AdaS-<lb>DCA achieves provably better complexity bound<lb>than SDCA with the best fixed probability dis-<lb>tribution, known as importance sampling. How-<lb>ever, it is of a theoretical character as it is expen-<lb>sive to implement. We also propose AdaSDCA+:<lb>a practical variant which in our experiments out-<lb>performs existing non-adaptive methods.", "target": "Stochastic Dual Coordinate Ascent with Adaptive Probabilities"}
{"id": "task1540-f10b41bf93f4430b86fe6aea851c35e0", "input": "Multi-Agent Path Finding (MAPF) is an NP-hard problem well studied in artificial intelligence and robotics. It has many real-world applications for which existing MAPF solvers use various heuristics. However, these solvers are deterministic and perform poorly on \u201chard\u201d instances typically characterized by many agents interfering with each other in a small region. In this paper, we enhance MAPF solvers with randomization and observe that they exhibit heavy-tailed distributions of runtimes on hard instances. This leads us to develop simple rapid randomized restart (RRR) strategies with the intuition that, given a hard instance, multiple short runs have a better chance of solving it compared to one long run. We validate this intuition through experiments and show that our RRR strategies indeed boost the performance of state-ofthe-art MAPF solvers such as iECBS and M*.", "target": "Rapid Randomized Restarts for Multi-Agent Path Finding Solvers"}
{"id": "task1540-9809ba5a32794a9e844afc4b6b204c23", "input": "Due to imprecision and uncertainties in predicting real world problems, artificial neural network (ANN) techniques have become increasingly useful for modeling and optimization. This paper presents an artificial neural network approach for forecasting electric energy consumption. For effective planning and operation of power systems, optimal forecasting tools are needed for energy operators to maximize profit and also to provide maximum satisfaction to energy consumers. Monthly data for electric energy consumed in the Gaza strip was collected from year 1994 to 2013. Data was trained and the proposed model was validated using 2-Fold and K-Fold cross validation techniques. The model has been tested with actual energy consumption data and yields satisfactory performance.", "target": "Using Artificial Neural Network Techniques for Prediction of Electric Energy Consumption"}
