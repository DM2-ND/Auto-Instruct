{"id": "task1540-c9643e4c45b449ac847b8dc399bae933", "input": "Sentiment prediction of contemporary music can have a wide-range of applications in modern society, for instance, selecting music for public institutions such as hospitals or restaurants to potentially improve the emotional well-being of personnel, patients, and customers, respectively. In this project, music recommendation system built upon on a naive Bayes classifier, trained to predict the sentiment of songs based on song lyrics alone. The experimental results show that music corresponding to a happy mood can be detected with high precision based on text features obtained from song lyrics.", "target": "MusicMood: Predicting the mood of music from song lyrics using machine learning"}
{"id": "task1540-9b2db386825e479aa263566f26bf762c", "input": "The fastest known exact algorithms for scorebased structure discovery in Bayesian networks on n nodes run in time and space 2n. The usage of these algorithms is limited to networks on at most around 25 nodes mainly due to the space requirement. Here, we study space\u2013time tradeoffs for finding an optimal network structure. When little space is available, we apply the Gurevich\u2013 Shelah recurrence\u2014originally proposed for the Hamiltonian path problem\u2014and obtain time 2n in space 2n for any s = n/2, n/4, n/8, . . .; we assume the indegree of each node is bounded by a constant. For the more practical setting with moderate amounts of space, we present a novel scheme. It yields running time 2(3/2)n in space 2(3/4)n for any p = 0, 1, . . . , n/2; these bounds hold as long as the indegrees are at most 0.238n. Furthermore, the latter scheme allows easy and efficient parallelization beyond previous algorithms. We also explore empirically the potential of the presented techniques.", "target": "Exact Structure Discovery in Bayesian Networks with Less Space"}
{"id": "task1540-82d257b3f09c46059757522b0277400f", "input": "We investigate adversarial attacks for autoencoders. We propose a procedure that distorts the input image to mislead the autoencoder in reconstructing a completely different target image. We attack the internal latent representations, attempting to make the adversarial input produce an internal representation as similar as possible as the target\u2019s. We find that autoencoders are much more robust to the attack than classifiers: while some examples have tolerably small input distortion, and reasonable similarity to the target image, there is a quasi-linear trade-off between those aims. We report results on MNIST and SVHN datasets, and also test regular deterministic autoencoders, reaching similar conclusions in all cases. Finally, we show that the usual adversarial attack for classifiers, while being much easier, also presents a direct proportion between distortion on the input, and misdirection on the output. That proportionality however is hidden by the normalization of the output, which maps a linear layer into non-linear probabilities.", "target": "Adversarial Images for Variational Autoencoders"}
