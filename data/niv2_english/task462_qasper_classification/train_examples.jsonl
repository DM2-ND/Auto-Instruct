{"id": "task462-8d8fcf6d6b814657ad136701e7bc4161", "input": "Through our experiments, we make subtle points related to: (a) the performance of our features, (b) how our approach compares against human ability to detect drunk-texting, (c) most discriminative stylistic features, and (d) an error analysis that points to future work. \n Question: Do the authors mention any confounds to their study?", "target": "Yes-no"}
{"id": "task462-1646d9b313e34f1894eb4bd62715c886", "input": "Our model introduces a multi-turns inference mechanism to process multi-perspective matching features.  \n Question: Which matching features do they employ?", "target": "Abstractive"}
{"id": "task462-d7426cf3a5344d81ab0414fe603aa919", "input": "Results reveal that, contrary to the common belief, correlations between Rouge and the Pyramid scores are weak, which challenges its effectiveness for scientific summarization. Furthermore, we show a large variance of correlations between different Rouge variants and the manual evaluations which further makes the reliability of Rouge for evaluating scientific summaries less clear. \n Question: What is the common belief that this paper refutes? (c.f. 'contrary to the common belief, ROUGE is not much [sic] reliable'", "target": "Extractive"}
{"id": "task462-33fbe7d270b8456384df70bc0748eba8", "input": "the basic model yields good performance for recognizing explicit discourse relations as well, which is comparable with previous best result (92.05% macro F1-score and 93.09% accuracy as reported in BIBREF11 ). In summary, our full paragraph-level neural network model achieves the best macro-average F1-score of 48.82% in predicting implicit discourse relations, which outperforms previous neural tensor network models (e.g., BIBREF18 ) by more than 2 percents and outperforms the best previous system BIBREF19 by 1 percent. Then we also created ensemble models by applying majority voting to combine results of ten runs. From table 5 , each ensemble model obtains performance improvements compared with single model. The full model achieves performance boosting of (51.84 - 48.82 = 3.02) and (94.17 - 93.21 = 0.96) in macro F1-scores for predicting implicit and explicit discourse relations respectively.  \n Question: How much does this model improve state-of-the-art?", "target": "Extractive"}
{"id": "task462-3ef2f9d890814cd1b14c71eacd9126f0", "input": "Ablation Study\nTo further investigate the efficacy of the key components in our framework, namely, THA and STN, we perform ablation study as shown in the second block of Table TABREF39 . The results show that each of THA and STN is helpful for improving the performance, and the contribution of STN is slightly larger than THA. \u201cOURS w/o THA & STN\u201d only keeps the basic bi-linear attention. Although it performs not bad, it is still less competitive compared with the strongest baseline (i.e., CMLA), suggesting that only using attention mechanism to distill opinion summary is not enough. After inserting the STN component before the bi-linear attention, i.e. \u201cOURS w/o THA\u201d, we get about 1% absolute gains on each dataset, and then the performance is comparable to CMLA. By adding THA, i.e. \u201cOURS\u201d, the performance is further improved, and all state-of-the-art methods are surpassed. \n Question: Do they explore how useful is the detection history and opinion summary?", "target": "Yes-no"}
{"id": "task462-0dda27a4c07f4acbbf673ecbbdf5c0f7", "input": "Furthermore, the gains in WER over the baseline are significantly larger for the Density Ratio method than for Shallow Fusion, with up to 28% relative reduction in WER (17.5% $\\rightarrow $ 12.5%) compared to up to 17% relative reduction (17.5% $\\rightarrow $ 14.5%) for Shallow Fusion, in the no fine-tuning scenario. \n Question: What metrics are used for evaluation?", "target": "Abstractive"}
{"id": "task462-90e623a8031d46e4a0d97db4ea0eac8e", "input": "MTMSN BIBREF4 is the first, and only model so far, that specifically tried to tackle the multi-span questions of DROP. Their approach consisted of two parts. The first was to train a dedicated categorical variable to predict the number of spans to extract. The second was to generalize the single-span head method of extracting a span, by utilizing the non-maximum suppression (NMS) algorithm BIBREF7 to find the most probable set of non-overlapping spans. The number of spans to extract was determined by the aforementioned categorical variable \n Question: What approach did previous models use for multi-span questions?", "target": "Abstractive"}
{"id": "task462-422db75baf7246b6a0ea4e162a617e82", "input": "While the dataset was collected with simulated environments, no structure was imposed on the navigation instructions while crowd-sourcing data. Thus, many instructions in our dataset are ambiguous. Moreover, the order of the behaviors in the instructions is not always the same. For instance, a person said \u201cturn right and advance\u201d to describe part of a route, while another person said \u201cgo straight after turning right\u201d in a similar situation. The high variability present in the natural language descriptions of our dataset makes the problem of decoding instructions into behaviors not trivial. See Appendix A of the supplementary material for additional details on our data collection effort. \n Question: What language is the experiment done in?", "target": "Abstractive"}
{"id": "task462-1b6bbe7673ba45ef8c91e9d52e048f49", "input": "Although our approach strongly outperforms random baselines, the relatively low F1 scores indicate that predicting which word is echoed in explanations is a very challenging task. It follows that we are only able to derive a limited understanding of how people choose to echo words in explanations. The extent to which explanation construction is fundamentally random BIBREF47, or whether there exist other unidentified patterns, is of course an open question. We hope that our study and the resources that we release encourage further work in understanding the pragmatics of explanations. \n Question: Do authors provide any explanation for intriguing patterns of word being echoed?", "target": "Yes-no"}
{"id": "task462-9e1ef07f7bac49039e2dd75a93f7a8c4", "input": "As it is reported that conservatives and liberals exhibit different behaviors on online social platforms BIBREF19BIBREF20BIBREF21, we further assigned a political bias label to different US outlets (and therefore news articles) following the procedure described in BIBREF2. In order to assess the robustness of our method, we performed classification experiments by training only on left-biased (or right-biased) outlets of both disinformation and mainstream domains and testing on the entire set of sources, as well as excluding particular sources that outweigh the others in terms of samples to avoid over-fitting. \n Question: How is the political bias of different sources included in the model?", "target": "Abstractive"}
{"id": "task462-489ec21ef45f47d98c1c61cca0415c74", "input": "Our model encodes the information from audio and text sequences using dual RNNs and then combines the information from these sources using a feed-forward neural model to predict the emotion class. \n Question: How do they combine audio and text sequences in their RNN?", "target": "Extractive"}
{"id": "task462-1661c357879d4133b4b5f9b33cddd238", "input": "To identify the most suitable classifier for classifying the scalars associated with each text, we perform evaluations using the stochastic gradient descent, naive bayes, decision tree, and random forest classifiers. \n Question: What was the baseline?", "target": "Extractive"}
{"id": "task462-aa7dba9fb862435e8d419f7c94f91d53", "input": "The regressor and classifier used Support Vector Regression (SVR) and Support Vector Classification (SVC) respectively. The bottom section of Table TABREF26 shows the results of several variants of the neural architecture. The table includes a neural regressor (NNR) and a neural classifier (NNC).  \n Question: What approaches without reinforcement learning have been tried?", "target": "Abstractive"}
{"id": "task462-6da9690afd8745198b3ae1530da2db30", "input": "We plan to apply semantic slot scaffold to news summarization. Specifically, we can annotate the critical entities such as person names or location names to ensure that they are captured correctly in the generated summary. We also plan to collect a human-human dialog dataset with more diverse human-written summaries. \n Question: Is it expected to have speaker role, semantic slot and dialog domain annotations in real world datasets?", "target": "Abstractive"}
{"id": "task462-32f1f19b824e4c2fbb431b0d314a541a", "input": "We compare the results of our model to a baseline multi-task architecture inspired by yang2016multi. In our baseline model there are no explicit connections between tasks - the only shared parameters are in the hidden layer. \n Question: What is the baseline?", "target": "Abstractive"}
{"id": "task462-67d454e10c8544d4bd9bed92fa2f1a22", "input": "On Twitter we can see results that are consistent with the RCV results for the left-to-center political spectrum. The exception, which clearly stands out, is the right-wing groups ENL and EFDD that seem to be the most cohesive ones. This is the direct opposite of what was observed in the RCV data. We speculate that this phenomenon can be attributed to the fact that European right-wing groups, on a European but also on a national level, rely to a large degree on social media to spread their narratives critical of European integration. \n Question: Do the authors mention any possible confounds in their study?", "target": "Yes-no"}
{"id": "task462-99f01dc4af2d47d18b23ac099c1fd257", "input": " Since we are interested in the zero-shot capabilities of our representation, we trained our sentiment analysis model only on the english IMDB Large Movie Review dataset and tested it on the chinese ChnSentiCorp dataset and german SB-10K BIBREF24 , BIBREF25 . A natural language inference task consists of two sentences; a premise and a hypothesis which are either contradictions, entailments or neutral. Learning a NLI task takes a certain nuanced understanding of language. Therefore it is of interest whether or not UG-WGAN captures the necessary linguistic features.  \n Question: Did they experiment with tasks other than word problems in math?", "target": "Abstractive"}
{"id": "task462-b94a5e3d08ff4a339a564bc6f544de60", "input": "We obtained 1260 unique utterances having commands to AMIE from our in-cabin data-set. We expanded this data-set via Amazon Mechanical Turk and ended up with 3347 utterances having intents. The annotations for intents and slots are obtained on the transcribed utterances by majority voting of 3 annotators. \n Question: What is the size of their collected dataset?", "target": "Abstractive"}
{"id": "task462-c516661c3065489c9f96c95296b2ee83", "input": "Our submissions ranked second (EI-Reg), second (EI-Oc), fourth (V-Reg) and fifth (V-Oc), demonstrating that the proposed method is accurate in automatically determining the intensity of emotions and sentiment of Spanish tweets. \n Question: What subtasks did they participate in?", "target": "Abstractive"}
{"id": "task462-207cc81ce6f742d386ad739eb698dd3f", "input": "machine comprehension  Nelufar  \n Question: What MC abbreviate for?", "target": "Abstractive"}
{"id": "task462-caee8653263d4f7abac5245202a89fdc", "input": "Bengali, Gujarati, Marathi, Malayalam and Tamil are the primary source languages, and translation from these to Hindi constitute the child tasks.  \n Question: On how many language pairs do they show that preordering assisting language sentences helps translation quality?", "target": "Abstractive"}
{"id": "task462-49524567f74f457c8af6c73be167e0dd", "input": "N-GrAM ranked first in all cases except for the language variety task. In this case, the baseline was the top-ranked system, and ours was second by a small margin. Our system significantly out-performed the baseline on the joint task, as the baseline scored significantly lower for the gender task than for the variety task.\n\n \n Question: On which task does do model do worst?", "target": "Abstractive"}
{"id": "task462-b45c8de5eb764948912860a06322320f", "input": "Our full dataset consists of all subreddits on Reddit from January 2013 to December 2014, for which there are at least 500 words in the vocabulary used to estimate our measures, in at least 4 months of the subreddit's history. We compute our measures over the comments written by users in a community in time windows of months, for each sufficiently active month, and manually remove communities where the bulk of the contributions are in a foreign language.  \n Question: How did the select the 300 Reddit communities for comparison?", "target": "Abstractive"}
{"id": "task462-dcb0f6b0dbe0452ca45cdadd992f4a87", "input": "The encoder is a bidirectional Long-Short Term Memory(LSTM) cell BIBREF14 and the decoder a single LSTM cell with attention mechanism.  \n Question: How many attention layers are there in their model?", "target": "Abstractive"}
{"id": "task462-84b02ae45bef48a8987c7934dcc2c44f", "input": "We choose the following three models as the baselines:\n\nK-means is a well known data clustering algorithm, we implement the algorithm using sklearn toolbox, and represent documents using bag-of-words weighted by TF-IDF.\n\nLEM BIBREF13 is a Bayesian modeling approach for open-domain event extraction. It treats an event as a latent variable and models the generation of an event as a joint distribution of its individual event elements. We implement the algorithm with the default configuration.\n\nDPEMM BIBREF14 is a non-parametric mixture model for event extraction. It addresses the limitation of LEM that the number of events should be known beforehand. We implement the model with the default configuration. \n Question: What baseline approaches does this approach out-perform?", "target": "Extractive"}
{"id": "task462-2e60701c7cbd46c6ba5e1684439fab58", "input": "Thus, for this work, we build upon Hybrid Code Networks (HCN) BIBREF4 since HCNs achieve state-of-the-art performance in a data-efficient way for task-oriented dialogs, and propose AE-HCNs which extend HCNs with an autoencoder (Figure FIGREF8 ).  AE-HCN(-CNN) outperforms HCN on Test-OOD by a large margin about 17(20) points on average while keeping the minimum performance trade-off compared to Test.  \n Question: By how much does their method outperform state-of-the-art OOD detection?", "target": "Abstractive"}
{"id": "task462-0a422636cd55401789637d93b4e24d54", "input": "Results of the DQN-based agent are presented in fig: scenario comparison. Each plot depicts the average reward (across 5 seeds) of all representations methods. It can be seen that the NLP representation outperforms the other methods.  NLP representations remain robust to changes in the environment as well as task-nuisances in the state.  \n Question: What result from experiments suggest that natural language based agents are more robust?", "target": "Abstractive"}
{"id": "task462-6b35a30060e844bc93ce58a39e8cf9cb", "input": "Besides, we report the sentiment accuracy (Senti ACC) which measures whether the output sentence has the same sentiment polarity as the input sentence based on our standardized sentiment classifiers. The BLEU score BIBREF25 between the input sentences and the output sentences is calculated to evaluate the content preservation performance. In order to evaluate the overall performance of different models, we also report the geometric mean (G2) and harmonic mean (H2) of the sentiment accuracy and the BLEU score. As for the irony accuracy, we only report it in human evaluation results because it is more accurate for the human to evaluate the quality of irony as it is very complicated. \n Question: Who judged the irony accuracy, sentiment preservation and content preservation?", "target": "Abstractive"}
{"id": "task462-a19108ccf9fa46858cde34dcf4dcdc0c", "input": "Depechemood is a lexicon-based emotion detection method gathered from crowd-annotated news BIBREF24. Drawing on approximately 23.5K documents with average of 500 words per document from rappler.com, researchers asked subjects to report their emotions after reading each article. They then multiplied the document-emotion matrix and word-document matrix to derive emotion-word matrix for these words.  \n Question: How id Depechemood trained?", "target": "Abstractive"}
{"id": "task462-d64746b7851c4429a49f245be95ba39c", "input": "We identified some limitations during the process, which we describe in this section.\n\nWhen deciding publisher partisanship, the number of people from whom we computed the score was small. For example, de Stentor is estimated to reach 275K readers each day on its official website. Deciding the audience leaning from 55 samples was subject to sampling bias. Besides, the scores differ very little between publishers. None of the publishers had an absolute score higher than 1, meaning that even the most partisan publisher was only slightly partisan. Deciding which publishers we consider as partisan and which not is thus not very reliable.\n\nThe article-level annotation task was not as well-defined as on a crowdsourcing platform. We included the questions as part of an existing survey and didn't want to create much burden to the annotators. Therefore, we did not provide long descriptive text that explained how a person should annotate an article. We thus run under the risk of annotator bias. This is one of the reasons for a low inter-rater agreement. \n Question: What limitations are mentioned?", "target": "Abstractive"}
{"id": "task462-8c3c880206de4f52876e595548459ce2", "input": "We assessed the proposed models on four different NLG domains: finding a restaurant, finding a hotel, buying a laptop, and buying a television. The Restaurant and Hotel were collected in BIBREF4 , while the Laptop and TV datasets have been released by BIBREF22 with a much larger input space but only one training example for each DA so that the system must learn partial realization of concepts and be able to recombine and apply them to unseen DAs.  \n Question: Does the model evaluated on NLG datasets or dialog datasets?", "target": "Abstractive"}
{"id": "task462-ded7f3b4b5404519ae07e332c2e2ab66", "input": "After Korektor, there are 4262 unique sentences (including 150 seed sentences) that form the COSTRA 1.0 dataset. \n Question: How many sentence transformations on average are available per unique sentence in dataset?", "target": "Abstractive"}
{"id": "task462-79b328fc6819404f99623e9b4f5ed0b2", "input": "Dataset Quality Analysis ::: Inter-Annotator Agreement (IAA)\nTo estimate dataset consistency across different annotations, we measure F1 using our UA metric with 5 generators per predicate. Dataset Quality Analysis ::: Dataset Assessment and Comparison\nWe assess both our gold standard set and the recent Dense set against an integrated expert annotated sample of 100 predicates.  Dataset Quality Analysis ::: Agreement with PropBank Data\nIt is illuminating to observe the agreement between QA-SRL and PropBank (CoNLL-2009) annotations BIBREF7.  \n Question: How was quality measured?", "target": "Abstractive"}
{"id": "task462-9264c9ec73524997a57137ba42809394", "input": "Specifically, we firstly extract multiple relations with an off-the-shelf Open Information Extraction (OpenIE) toolbox BIBREF7, then we select the relation that is most relevant to the answer with carefully designed heuristic rules. \n Question: How they extract \"structured answer-relevant relation\"?", "target": "Abstractive"}
{"id": "task462-4c196e499ba840f781145945e6bd715d", "input": "A representation that is more in line with observed user behavior is a concept map BIBREF5 , a labeled graph showing concepts as nodes and relationships between them as edges (Figure FIGREF2 ). \n Question: How do the authors define a concept map?", "target": "Extractive"}
{"id": "task462-832be5cd842b40f8a38fbe2e4bbfe440", "input": "Thus, to limit the number of different inputs to the classifier, we wish to reduce the number of distinct word recognition outputs that an attacker can induce, not just the number of words on which the model is \u201cfooled\u201d. We denote this property of a model as its sensitivity. \n Question: What does the \"sensitivity\" quantity denote?", "target": "Extractive"}
{"id": "task462-449bb91a9ff14c6eb13fc8413dcf574b", "input": "With this challenge in mind, we introduce Torch-Struct with three specific contributions:\n\nModularity: models are represented as distributions with a standard flexible API integrated into a deep learning framework.\n\nCompleteness: a broad array of classical algorithms are implemented and new models can easily be added in Python.\n\nEfficiency: implementations target computational/memory efficiency for GPUs and the backend includes extensions for optimization. \n Question: Is this library implemented into Torch or is framework agnostic?", "target": "Abstractive"}
{"id": "task462-fa592661e5e447f3b0726a0c706c26e8", "input": "We use the Bayesian model of garg2012unsupervised as our base monolingual model. The semantic roles are predicate-specific. \n Question: What does an individual model consist of?", "target": "Extractive"}
{"id": "task462-74420a0281f24991914295b062ca06a6", "input": "Very recently, the success of deep neural networks in many natural language processing tasks ( BIBREF20 ) has inspired new work in abstractive summarization . BIBREF2 propose a neural attention model with a convolutional encoder to solve this task.  More recently, BIBREF4 extended BIBREF2 's work with an RNN decoder, and BIBREF8 proposed an RNN encoder-decoder architecture for summarization. Both techniques are currently the state-of-the-art on the DUC competition. \n Question: What is the state-of-the art?", "target": "Abstractive"}
{"id": "task462-822d8cd3ba7e41ef971943f02e257acc", "input": "Compared with 2015, our QA-SRL gold yielded 1593 annotations, with 989 core and 604 adjuncts, while theirs yielded 1315 annotations, 979 core and 336 adjuncts.  \n Question: How much more coverage is in the new dataset?", "target": "Abstractive"}
{"id": "task462-2c65f3c84f43497d97844314692f6f88", "input": "Secondly, a well-known problem of uneven yet unknown distribution of word senses is alleviated by modifying a na\u00efve Bayesian classifier. Thanks to this correction, the classifier is no longer biased towards senses that have more training data. Finally, the aggregated feature values corresponding to target word senses are used to build a na\u00efve Bayesian classifier adjusted to a situation of unknown a priori probabilities. \n Question: How do they deal with unknown distribution senses?", "target": "Abstractive"}
{"id": "task462-db5c9ddd30694ba2b760d6629a5fae74", "input": "Our unsupervised ranking model outperforms the supervised IMS system by 1.02% on the CoNLL F1 score, and achieves competitive performance with the latent tree model. Moreover, our approach considerably narrows the gap to other supervised systems listed in Table 3 . \n Question: Is the model presented in the paper state of the art?", "target": "Abstractive"}
{"id": "task462-f276fe26eba04903a30eb29e17868047", "input": "We evaluate our newly proposed models and related baselines in several low-resource settings across different languages with real, distantly supervised data with non-synthetic noise.  \n Question: How they evaluate their approach?", "target": "Abstractive"}
{"id": "task462-c4b9e80c4fb5465cb468a4199f27efa1", "input": "For each variable in encoding set, learn the new embeddings using the embeddings train set . Since there is stochasticity in the embeddings training model, we will repeat the above multiple times, for the different experiments in the paper (and report the respective mean and standard deviation statistics). \n Question: How do their train their embeddings?", "target": "Abstractive"}
{"id": "task462-f609a35795cb43e4815018cc63aedb3c", "input": "The MP framework is based on the core idea of recursive neighborhood aggregation. That is, at every iteration, the representation of each vertex is updated based on messages received from its neighbors. All spectral GNNs can be described in terms of the MP framework.\n\nGNNs have been applied with great success to bioinformatics and social network data, for node classification, link prediction, and graph classification. However, a few studies only have focused on the application of the MP framework to representation learning on text. This paper proposes one such application. The concept of message passing over graphs has been around for many years BIBREF0, BIBREF1, as well as that of graph neural networks \n Question: What is the message passing framework?", "target": "Abstractive"}
{"id": "task462-016e9c78f0e64145933d9ba07a4dadb4", "input": "We recruited 100 crowdworkers on Amazon Mechanical Turk (AMT) and measured completion times and accuracies for typing randomly sampled sentences from the Yelp corpus.  \n Question: What user variations have been tested?", "target": "Extractive"}
{"id": "task462-139c0d19246c4268a794d8e749d87c3d", "input": "The second series of tests consisted of using all the documents of the subcorpus \u201cspecialist\u201d $E$, because the documents of the subcorpus of Annodis are not identical.  As far as system evaluations are concerned, we use the 78 $E$ documents as reference.  We calculate the precision $P$, the recall $R$ and the $F$-score on the text corpus used in our tests, as follow: \n Question: How is segmentation quality evaluated?", "target": "Abstractive"}
{"id": "task462-9fc7ab7be3b34b269bdc884b31f1c23f", "input": "With a single text, we were already able to predict the electricity consumption with a relative error of less than 5% for both data sets. \n Question: How accurate is model trained on text exclusively?", "target": "Abstractive"}
{"id": "task462-500c49a3a07b47ea9b75c7d114a97bef", "input": "Drawing on the concept of variance in mathematics, local variance loss is defined as the reciprocal of its variance expecting the attention model to be able to focus on more salient parts. The standard variance calculation is based on the mean of the distribution. However, as previous work BIBREF15, BIBREF16 mentioned that the median value is more robust to outliers than the mean value, we use the median value to calculate the variance of the attention distribution. Thus, local variance loss can be calculated as:\n\nwhere $\\hat{\\cdot }$ is a median operator and $\\epsilon $ is utilized to avoid zero in the denominator. \n Question: How do they define local variance?", "target": "Abstractive"}
{"id": "task462-b408c166bea94d21b3625a587f32b63b", "input": "Davidson et al. BIBREF2 showed that there were systematic and substantial racial biases in five benchmark Twitter datasets annotated for offensive language detection. By examining more samples and with respect to recently studies BIBREF2, BIBREF24, BIBREF19, it is clear that many errors are due to biases from data collection BIBREF19 and rules of annotation BIBREF24 and not the classifier itself.  \n Question: What are the existing biases?", "target": "Extractive"}
{"id": "task462-28765f93035e45b8b4f0b8d3f08b02ed", "input": "In this experiment, we compute the correlation of the proposed NED measure with the patient-perceived emotional bond ratings. Since the proposed measure is asymmetric in nature, we compute the measures for both patient-to-therapist and therapist-to-patient entrainment. We report Pearson's correlation coefficients ( INLINEFORM0 ) for this experiment in Table TABREF26 along with their INLINEFORM1 -values.  \n Question: How do they correlate NED with emotional bond levels?", "target": "Abstractive"}
{"id": "task462-cce415fa4a984d64aabd679d831cd7c2", "input": "Our system significantly out-performed the baseline on the joint task, as the baseline scored significantly lower for the gender task than for the variety task. \n Question: On which task does do model do best?", "target": "Abstractive"}
{"id": "task462-385a5c5c746e481cac92b0e682333d18", "input": "The baseline approach is based on BIBREF20 . It divides the task of interpreting commands for behavioral navigation into two steps: path generation, and path verification. For path generation, this baseline uses a standard sequence-to-sequence model augmented with an attention mechanism, similar to BIBREF23 , BIBREF6 . For path verification, the baseline uses depth-first search to find a route in the graph that matches the sequence of predicted behaviors. If no route matches perfectly, the baseline changes up to three behaviors in the predicted sequence to try to turn it into a valid path. \n Question: What baselines did they compare their model with?", "target": "Abstractive"}
{"id": "task462-84b384bbfd5145158325970376b297c4", "input": "For the image-aided model (W+C+V; upper row in Figure FIGREF19 ), we confirm that the modality attention successfully attenuates irrelevant signals (selfies, etc.) and amplifies relevant modality-based contexts in prediction of a given token. \n Question: Do they inspect their model to see if their model learned to associate image parts with words related to entities?", "target": "Yes-no"}
{"id": "task462-27378a4cf9624f35a95a6437dae286bb", "input": "We collected a simplified dataset from Simple English Wikipedia that are freely available, which has been previously used for many text simplification methods BIBREF0 , BIBREF10 , BIBREF3 . \n Question: what language does this paper focus on?", "target": "Extractive"}
{"id": "task462-da36d5f8c99f4afaa5ef5aa9bfeb76f2", "input": "We evaluate our model and baseline models on three versions of the dataset. The first one (Inc) only considers the original data, containing naturally incorrect tweets, and achieves accuracy of 80$\\%$ against BERT's 72$\\%$. The second version (Corr) considers the corrected tweets, and shows higher accuracy given that it is less noisy.  \n Question: Should their approach be applied only when dealing with incomplete data?", "target": "Yes-no"}
{"id": "task462-0b4fbd595ac14219896c119704dbacca", "input": "We compare our best F1 score with other state-of-the-art approaches in table TABREF39 , which shows our model has competitive advantage in dealing with drug-drug interaction extraction. \n Question: What is the performance of their model?", "target": "Abstractive"}
{"id": "task462-6e18e91867164439b6d4147998272566", "input": "Also, we compared our models with other existing works on this dataset including OpATT BIBREF6 and Neural Content Planning with conditional copy (NCP+CC) BIBREF4. \n Question: What is the state-of-the-art model for the task?", "target": "Extractive"}
{"id": "task462-e062bb75845442ea8bd64b9fa87f7b94", "input": "The SG model achieved a high average similarity score of 0.650 followed by CBoW with a 0.632 average similarity score. The GloVe also achieved a considerable average score of 0.591 respectively. However, the average similarity score of SdfastText is 0.388 and the word pair Microsoft-Bill Gates is not available in the vocabulary of SdfastText. Moreover, the average semantic relatedness similarity score between countries and their capitals is shown in Table TABREF78 with English translation, where SG also yields the best average score of 0.663 followed by CBoW with 0.611 similarity score. The GloVe also yields better semantic relatedness of 0.576 and the SdfastText yield an average score of 0.391. \n Question: How does proposed word embeddings compare to Sindhi fastText word representations?", "target": "Abstractive"}
{"id": "task462-4c831be9d566400894b0dcce4a1161e4", "input": "Therefore, we implement reinforcement learning and elaborately design two rewards to describe the irony accuracy and sentiment preservation, respectively. \n Question: What is the combination of rewards for reinforcement learning?", "target": "Extractive"}
{"id": "task462-3b21f75e6fa6453b977611a3079793f9", "input": "The primary feed for the analysis collected INLINEFORM0 million tweets containing the keywords `breast' AND `cancer'.  \n Question: How were breast cancer related posts compiled from the Twitter streaming API?", "target": "Abstractive"}
{"id": "task462-e7cd528ebd5544279ea4baf81ba243cc", "input": "Besides, we also use two of its adversarial sets, namely AddSent and AddOneSent BIBREF6 , to evaluate the robustness to noise of MRC models. The passages in the adversarial sets contain misleading sentences, which are aimed at distracting MRC models.  Specifically, we not only evaluate the performance of KAR on the development set and the test set, but also do this on the adversarial sets.  \n Question: How do the authors examine whether a model is robust to noise or not?", "target": "Abstractive"}
{"id": "task462-1c8a1f6afe074b97b3e1820160d0535a", "input": "Therefore, there are 44 tasks (22 INLINEFORM10 2) in total. In addition, there are 2 human summaries for each task. We selected three competitive systems (SumBasic, ILP, and ILP+MC) and therefore we have 3 system-system pairs (ILP+MC vs. ILP, ILP+MC vs. SumBasic, and ILP vs. SumBasic) for each task and each human summary. \n Question: Do they build one model per topic or on all topics?", "target": "Abstractive"}
{"id": "task462-1184a0528d9e48b98ec578dda44de15b", "input": "We embedded COSTRA sentences with LASER BIBREF15, the method that performed very well in revealing linear relations in BaBo2019. \n Question: Are some baseline models trained on this dataset?", "target": "Yes-no"}
{"id": "task462-cbed0fd21fb14aef93543d31f4ee0b41", "input": "We experimented with four different classifiers, namely, support vector machine BIBREF18 , random forest, extra tree and naive bayes classifier BIBREF19 . Chi square feature selection algorithm is applied to reduces the size of our feature vector. For training our system classifier, we used Scikit-learn BIBREF19 . \n Question: What type of system does the baseline classification use?", "target": "Extractive"}
{"id": "task462-6cdcc63723b649b1836824991348e643", "input": "We defined the intents with guidance from queries collected using a scoping crowdsourcing task, which prompted crowd workers to provide questions and commands related to topic domains in the manner they would interact with an artificially intelligent assistant. We manually grouped data generated by scoping tasks into intents.  \n Question: How was the dataset annotated?", "target": "Abstractive"}
{"id": "task462-e9b6f5e57dc04b5ab6f3b8ef8dc7f9c1", "input": "As a document classifier we employ a word-based CNN similar to Kim consisting of the following sequence of layers: $ \\texttt {Conv} \\xrightarrow{} \\texttt {ReLU} \\xrightarrow{} \\texttt {1-Max-Pool} \\xrightarrow{} \\texttt {FC} \\\\ $ Future work would include applying LRP to other neural network architectures (e.g. character-based or recurrent models) on further NLP tasks, as well as exploring how relevance information could be taken into account to improve the classifier's training procedure or prediction performance. \n Question: Do the experiments explore how various architectures and layers contribute towards certain decisions?", "target": "Yes-no"}
{"id": "task462-0b536d6510e34ec893c59907cd710ec4", "input": "The 10-fold cross validation with this setting gave a token-level accuracy of roughly 71%.  \n Question: Does the paper report translation accuracy for an automatic translation model for Tunisian to Arabish words?", "target": "Yes-no"}
{"id": "task462-7bddf643ce9c49f2975c324dba78bfe6", "input": "Experimental Setup \n Question: what english datasets were used?", "target": "Abstractive"}
{"id": "task462-b5898289b3da4e7494da9caf6dd3afd2", "input": "Hashtag prediction for social media has been addressed earlier, for example in BIBREF15 , BIBREF16 . BIBREF15 also use a neural architecture, but compose text embeddings from a lookup table of words. \n Question: Is this hashtag prediction task an established task, or something new?", "target": "Abstractive"}
{"id": "task462-eb29e8c976614b1991a60b9cb159788b", "input": "Our approach consists of structuring input to the transformer network to use and guide the self-attention of the transformers, conditioning it on the entity. Our main mode of encoding the input, the entity-first method, is shown in Figure FIGREF4.  We also have an entity-last variant where the entity is primarily observed just before the classification token to condition the [CLS] token's self-attention accordingly.  As an additional variation, we can either run the transformer once per document with multiple [CLS] tokens (a document-level model as shown in Figure FIGREF4) or specialize the prediction to a single timestep (a sentence-level model). \n Question: In what way is the input restructured?", "target": "Abstractive"}
{"id": "task462-348dd5e2cda642bea9d2bee480b6b0f3", "input": "We now seek to know if a pre-trained multi-BERT has ability to solve RC tasks in the zero-shot setting. \n Question: What model is used as a baseline?  ", "target": "Extractive"}
{"id": "task462-30f4d50b2c1e43c7bddf47cb83b1006d", "input": "We compare the predictions of logistic regression models based on unigram bag-of-words features (BOW), sentiment signals (SENT), the linguistic features from our earlier analyses (LING), and combinations of these features. \n Question: What predictive model do they build?", "target": "Extractive"}
{"id": "task462-27813cd1bc2848078ab70b446b0043ec", "input": " Here a small portion of the large parallel corpus for English-German is used as a simulation for the scenario where we do not have much parallel data: Translating texts in English to German.  \n Question: Which languages do they test on for the under-resourced scenario?", "target": "Extractive"}
{"id": "task462-5f0bb594e009487599a5cd9c79db86b2", "input": "We describe our rules for WikiSQL here. Our rule for KBQA is simple without using a curated mapping dictionary. The pipeline of rules in SequentialQA is similar to that of WikiSQL. \n Question: Are the rules dataset specific?", "target": "Yes-no"}
{"id": "task462-30dbd85a4dd647e4881c6e66ea2882b0", "input": "Since our objective is to compare character-based and word-based approaches, we have also implemented a simple word-level encoder for tweets. The input tweet is first split into tokens along white-spaces. A more sophisticated tokenizer may be used, but for a fair comparison we wanted to keep language specific preprocessing to a minimum. The encoder is essentially the same as tweet2vec, with the input as words instead of characters. A lookup table stores word vectors for the $V$ (20K here) most common words, and the rest are grouped together under the `UNK' token. \n Question: what is the word level baseline they compare to?", "target": "Extractive"}
{"id": "task462-e5aad250a2964750bd24bd34f551d4e8", "input": "The final section of the paper, then, discusses how insights gained from technologically observing opinion dynamics can inform conceptual modelling efforts and approaches to on-line opinion facilitation. As such, the paper brings into view and critically evaluates the fundamental conceptual leap from machine-guided observation to debate facilitation and intervention. \n Question: Does the paper report the results of previous models applied to the same tasks?", "target": "Yes-no"}
{"id": "task462-318730f2b33145fa988f7553104143bd", "input": "Another insight gained from these charts is that a random summarizer resulted in scores more than 50% in all measures, and without using document-aware features, the model achieves a small improvement over a random summarizer. \n Question: Is new approach tested against state of the art?", "target": "Yes-no"}
{"id": "task462-7503bed6fb4f4682ba456a2a6ebef091", "input": "The real-time tweets scores were calculated in the same way as the historical data and summed up for a minute and sent to the machine learning model with the Bitcoin price in the previous minute and the rolling average price. It predicted the next minute's Bitcoin price from the given data. After the actual price arrived, the RMS value was calculated and the machine learning model updated itself to predict with better understanding the next value. \n Question: What experimental evaluation is used?", "target": "Abstractive"}
{"id": "task462-94da89ebe4e74b32a2f76a4aec87a191", "input": "Consequently, we investigate ways to detect suspicious accounts by considering their tweets in groups (chunks). Our hypothesis is that suspicious accounts have a unique pattern in posting tweet sequences. Since their intention is to mislead, the way they transition from one set of tweets to the next has a hidden signature, biased by their intentions. Given a news Twitter account, we read its tweets from the account's timeline. Then we sort the tweets by the posting date in ascending way and we split them into $N$ chunks. Each chunk consists of a sorted sequence of tweets labeled by the label of its corresponding account. \n Question: How is a \"chunk of posts\" defined in this work?", "target": "Extractive"}
{"id": "task462-df82378c1d3a4f77bf2570a3752540e3", "input": "In this section, we evaluate the proposed method intrinsically in terms of whether the co-occurrence matrix after the low-rank approximation is able to capture similar concepts on student response data sets, and also extrinsically in terms of the end task of summarization on all corpora. In the following experiments, summary length is set to be the average number of words in human summaries or less. An alternative way to evaluate the hypothesis is to let humans judge whether two bigrams are similar or not, which we leave for future work. \n Question: Do they quantitavely or qualitatively evalute the output of their low-rank approximation to verify the grouping of lexical items?", "target": "Abstractive"}
{"id": "task462-6f01ae8869ad4a9b881cdbe3ec7d3527", "input": "The resulting sequence of vectors is encoded using an LSTM encoder.  \n Question: What architecture does the encoder have?", "target": "Extractive"}
{"id": "task462-ae5f3c0898d94e8887191f5251f3d101", "input": "To control the quality, we ensured that a single annotator annotates maximum 120 headlines (this protects the annotators from reading too many news headlines and from dominating the annotations). Secondly, we let only annotators who geographically reside in the U.S. contribute to the task.\n\nWe test the annotators on a set of $1,100$ test questions for the first phase (about 10% of the data) and 500 for the second phase. Annotators were required to pass 95%. \n Question: How is quality of annotation measured?", "target": "Abstractive"}
{"id": "task462-aa8632b9c482406797eaa549d9df9312", "input": "We would expect that explanation performance should correlate with prediction performance. Since Possible-answer knowledge is primarily needed to decide if the net has enough information to answer the challenge question without guessing and relevant-variable knowledge is needed for the net to know what to query, we analyzed the network's performance on querying and answering separately. The memory network has particular difficulty learning to query relevant variables, reaching only about .5 accuracy when querying. At the same time, it learns to answer very well, reaching over .9 accuracy there. Since these two parts of the interaction are what we ask it to explain in the two modes, we find that the quality of the explanations strongly correlates with the quality of the algorithm executed by the network. \n Question: How do they measure correlation between the prediction and explanation quality?", "target": "Abstractive"}
{"id": "task462-68f410f9594c4de88bc59488338fa262", "input": "This section introduces our probabilistic model that infers keyword expectation and trains the target model simultaneously. \n Question: What type of classifiers are used?", "target": "Extractive"}
{"id": "task462-8f90c23145ae4c31b2e516094381de1c", "input": "Knowledge Base Question Answering (KBQA) systems answer questions by obtaining information from KB tuples BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 \n Question: What does KBQA abbreviate for", "target": "Extractive"}
{"id": "task462-4aa9c980c1654d78963060c6ea358d76", "input": "In our experiments, the Memory Neural Networks (MemNNs) proposed in babidataset serve as the baselines. \n Question: What word level and character level model baselines are used?", "target": "Abstractive"}
{"id": "task462-156d1ad51ec24d478c417480d0e3ac7d", "input": "Internally, the ASR system maintains a rich hypothesis space in the form of speech lattices or confusion networks (cnets). \n Question: What is a word confusion network?", "target": "Abstractive"}
{"id": "task462-bde59c8bd1584c9aaeee0a7806ee86cc", "input": "In this paper, we have shown that both human and machine translation can alter superficial patterns in data, which requires reconsidering previous findings in cross-lingual transfer learning. \n Question: Does the professional translation or the machine translation introduce the artifacts?", "target": "Yes-no"}
{"id": "task462-3098ff7c1fb84efaa714954eb7ca1541", "input": "The seed lexicon consists of positive and negative predicates. If the predicate of an extracted event is in the seed lexicon and does not involve complex phenomena like negation, we assign the corresponding polarity score ($+1$ for positive events and $-1$ for negative events) to the event. It is a  \n Question: What is the seed lexicon?", "target": "Abstractive"}
{"id": "task462-b77cdecb272340888a546f7bfe4c9949", "input": "In this paper, we introduce a novel policy model to output multiple actions per turn (called multi-act), generating a sequence of tuples and expanding agents' expressive power. Each tuple is defined as $(\\textit {continue}, \\textit {act}, \\textit {slots})$, where continue indicates whether to continue or stop producing new acts, act is an act type (e.g., inform or request), and slots is a set of slots (names) associated with the current act type. Correspondingly, a novel decoder (Figure FIGREF5) is proposed to produce such sequences. Each tuple is generated by a cell called gated Continue Act Slots (gCAS, as in Figure FIGREF7), which is composed of three sequentially connected gated units handling the three components of the tuple. This decoder can generate multi-acts in a double recurrent manner BIBREF18.  \n Question: What is specific to gCAS cell?", "target": "Abstractive"}
{"id": "task462-fa32729dbcfd454f989fb82257258149", "input": "We define a metric called the Semantic Text Exchange Score (STES) that evaluates the overall ability of a model to perform STE, and an adjustable parameter masking (replacement) rate threshold (MRT/RRT) that can be used to control the amount of semantic change. \n Question: Has STES been previously used in the literature to evaluate similar tasks?", "target": "Yes-no"}
{"id": "task462-6e2d00a82c724b2e90706148797dd2ad", "input": "First we are comparing BiLSTMBIBREF14, BiLSTM+CNNBIBREF20, BiLSTM+CRFBIBREF1, BiLSTM+CNN+CRFBIBREF2 models with CNN modelBIBREF0 and Stanford CRF modelBIBREF21. \n Question: Which models are used to solve NER for Nepali?", "target": "Extractive"}
{"id": "task462-402f24780bed484eb6dabb7bde3391bf", "input": "For every input data point and possible target class, LRP delivers one scalar relevance value per input variable, hereby indicating whether the corresponding part of the input is contributing for or against a specific classifier decision, or if this input variable is rather uninvolved and irrelevant to the classification task at all. \n Question: Does the LRP method work in settings that contextualize the words with respect to one another?", "target": "Yes-no"}
{"id": "task462-f1ed4160aa9d462d9aaf398b0adeb62a", "input": "For the future work, we plan to solve the triples with multiple entities as the second entity, which is excluded from problem scope in this paper. The input of QA4IE is a document $D$ with an existing knowledge base $K$ and the output is a set of relation triples $R = \\lbrace e_i, r_{ij}, e_j\\rbrace $ in $D$ where $e_i$ and $e_j$ are two individual entities and $r_{ij}$ is their relation. \n Question: Can this approach model n-ary relations?", "target": "Yes-no"}
{"id": "task462-4c7b5df5994c48558c2bf977ba54341d", "input": "Methodology ::: Corpus-based Approach\nContextual information is informative in the sense that, in general, similar words tend to appear in the same contexts.  In the corpus-based approach, we capture both of these characteristics and generate word embeddings specific to a domain. Methodology ::: Dictionary-based Approach\nIn Turkish, there do not exist well-established sentiment lexicons as in English. In this approach, we made use of the TDK (T\u00fcrk Dil Kurumu - \u201cTurkish Language Institution\u201d) dictionary to obtain word polarities. \n Question: What word-based and dictionary-based feature are used?", "target": "Extractive"}
{"id": "task462-3b7f5c88887e4b629cd0551902fa2962", "input": "The corpus consists of 53 documents, which contain an average number of 156.1 sentences per document, each with 19.55 tokens on average. \n Question: How many documents are in the new corpus?", "target": "Extractive"}
{"id": "task462-23daa65b033c4138aaf31637e628dce5", "input": " We defined a sequence labeling task to extract custom entities from user input. We assumed seven (7) possible entities (see Table TABREF43) to be recognized by the model: topic, subtopic, examination mode and level, question number, intent, as well as the entity other for remaining words in the utterance.   We defined a classification problem to predict the system's next action according to the given user input. We assumed 13 custom actions (see Table TABREF42) that we considered being our labels. In the conversational dataset, each input was automatically labeled by the rule-based system with the corresponding next action and the dialogue-id. Thus, no additional post-labeling was required.  \n Question: How does the IPA label data after interacting with users?", "target": "Abstractive"}
{"id": "task462-48d36d0b966b4681a424d5cb75111f69", "input": "We evaluate the quality of the document embeddings learned by the different variants of CAHAN and the HAN baseline on three of the large-scale document classification datasets introduced by BIBREF14 and used in the original HAN paper BIBREF5. \n Question: Do they compare to other models appart from HAN?", "target": "Yes-no"}
{"id": "task462-ddd0e1fc11b64befa17d6dc960cec1fd", "input": "Baselines. We compare our models against a neural baseline models, hierarchical LSTM (hLSTM), with the attention ablated but with access to the complete context, and a strong, open-sourced feature-rich baseline BIBREF7 . We choose BIBREF7 over other prior works such as BIBREF0 since we do not have access to the dataset or the system used in their papers for replication. BIBREF7 is a logistic regression classifier with features inclusive of bag-of-words representation of the unigrams and thread length, normalised counts of agreements to previous posts, counts of non-lexical reference items such as URLs, and the Coursera forum type in which a thread appeared. We also report aggregated results from a hLSTM model with access only to the last post as context for comparison. Table TABREF17 compares the performance of these baselines against our proposed methods. \n Question: What was the previous state of the art for this task?", "target": "Extractive"}
{"id": "task462-cc00ce8e846d4312ada37a8e5be40e92", "input": "To address this drawback in ROUGE, we propose a new evaluation metric: Critical Information Completeness (CIC). Formally, CIC is a recall of semantic slot information between a candidate summary and a reference summary. CIC is defined as follows:\n\nwhere $V$ stands for a set of delexicalized values in the reference summary, $Count_{match}(v)$ is the number of values co-occurring in the candidate summary and reference summary, and $m$ is the number of values in set $V$. In our experiments, CIC is computed as the arithmetic mean over all the dialog domains to retain the overall performance.\n\nCIC is a suitable complementary metric to ROUGE because it accounts for the most important information within each dialog domain. CIC can be applied to any summarization task with predefined essential entities. \n Question: How does new evaluation metric considers critical informative entities?", "target": "Abstractive"}
{"id": "task462-81aacfd7e186451bb29a1585c2a1a01c", "input": "A higher score indicates better step ordering (with a maximum score of 2). tab:coherencemetrics shows that our personalized models achieve average recipe-level coherence scores of 1.78-1.82, surpassing the baseline at 1.77. On average, human evaluators preferred personalized model outputs to baseline 63% of the time, confirming that personalized attention improves the semantic plausibility of generated recipes. \n Question: What were their results on the new dataset?", "target": "Extractive"}
{"id": "task462-83a2276c60f844a5a0d819b85b5b476a", "input": "To verify our assumption that target encoding and orthogonal regularization help to boost the diversity of generated sequences, we use two metrics, one quantitative and one qualitative, to measure diversity of generation. First, we simply calculate the average unique predictions produced by both INLINEFORM0 and INLINEFORM1 in experiments shown in Section SECREF36 .  In this example there are 29 ground truth phrases. Neither of the models is able to generate all of the keyphrases, but it is obvious that the predictions from INLINEFORM0 all start with \u201ctest\u201d, while predictions from INLINEFORM1 are diverse. \n Question: How is keyphrase diversity measured?", "target": "Extractive"}
{"id": "task462-9e8c793385aa402580b0cb6030616bd6", "input": " A dialog turn from one speaker may not only be a direct response to the other speaker's query, but also likely to be a continuation of his own previous statement. Thus, when modeling turn $k$ in a dialog, we propose to connect the last RNN state of turn $k-2$ directly to the starting RNN state of turn $k$ , instead of letting it to propagate through the RNN for turn $k-1$ . \n Question: How long of dialog history is captured?", "target": "Abstractive"}
{"id": "task462-75348e45afe048af9fac278efca7c310", "input": "We approached the first and second challenges by using a Bayesian approach to learn which terms were associated with events, regardless of whether they are standard language, acronyms, or even a made-up word, so long as they match the events of interest. The third and fourth challenges are approached by using word-pairs, where we extract all the pairs of co-occurring words within each tweet. To find the words most associated with events, we search for the words that achieve the highest number of spikes matching the days of events.  \n Question: How are the keywords associated with events such as protests selected?", "target": "Abstractive"}
{"id": "task462-6771ca4ba0ea455987a63e9a229fa89b", "input": "Results in Table TABREF13 show ten probes. Again, we see the performance of baseline ELMo-transformer and mSynC are similar, with mSynC doing slightly worse on 7 out of 9 tasks. \n Question: For how many probe tasks the shallow-syntax-aware contextual embedding perform better than ELMo\u2019s embedding?", "target": "Extractive"}
{"id": "task462-c4072db42b7843d4b1b796c92bc1008d", "input": "In Sec. \"Materials and Methods\" we discuss our materials and methods, including the dataset we studied, how we preprocessed that data and extracted a `causal' corpus and a corresponding `control' corpus, and the details of the statistical and language analysis tools we studied these corpora with. \n Question: Do they use expert annotations, crowdsourcing, or only automatic methods to analyze the corpora?", "target": "Abstractive"}
{"id": "task462-b5fe3e95716b4643819c32edb1d215be", "input": "The Word2Vec architecture has inspired a great deal of research in the bio/cheminformatics domains. The Word2Vec algorithm has been successfully applied for determining protein classes BIBREF44 and protein-protein interactions (PPI) BIBREF56. \n Question: Is there any concrete example in the paper that shows that this approach had huge impact on drug discovery?", "target": "Yes-no"}
{"id": "task462-1d814df64b6b424cbe623a6e5a9450c1", "input": "One way to analyze the model is to compute model gradients with respect to input features BIBREF26, BIBREF25. Figure FIGREF37 shows that in this particular example, the most important model inputs are verbs possibly associated with the entity butter, in addition to the entity's mentions themselves. It further shows that the model learns to extract shallow clues of identifying actions exerted upon only the entity being tracked, regardless of other entities, by leveraging verb semantics. \n Question: What evidence do they present that the model attends to shallow context clues?", "target": "Abstractive"}
{"id": "task462-d8300ca9c9fe4e47930d0a9705611100", "input": "Using our methodology, we tagged 10,000 Arabic tweet dataset for offensiveness, where offensive tweets account for roughly 19% of the tweets. Further, we labeled tweets as vulgar or hate speech.  \n Question: How many tweets are in the dataset?", "target": "Extractive"}
{"id": "task462-88f5702aa5a64e368131e346b0d5a771", "input": "In HotpotQA, on average we can find 6 candidate chains (2-hop) in a instance, and the human labeled true reasoning chain is unique. A predicted chain is correct if the chain only contains all supporting passages (exact match of passages).\n\nIn MedHop, on average we can find 30 candidate chains (3-hop). For each candidate chain our human annotators labeled whether it is correct or not, and the correct reasoning chain is not unique. A predicted chain is correct if it is one of the chains that human labeled as correct.\n\nThe accuracy is defined as the ratio: The accuracy is defined as the ratio: \n Question: What benchmarks are created?", "target": "Abstractive"}
{"id": "task462-6339b57df75f4241a3bd28500c4da71f", "input": "We first introduce a method that detects bottlenecks in text-games using the overall reward gained and the knowledge graph state. This method freezes the policy used to reach the bottleneck and restarts the training from there on out, additionally conducting a backtracking search to ensure that a sub-optimal policy has not been frozen. The second contribution explore how to leverage knowledge graphs to improve existing exploration algorithms for dealing with combinatorial action-spaces such as Go-Explore BIBREF9.  \n Question: What are the two new strategies?", "target": "Extractive"}
{"id": "task462-b015c763473b41c08b23ed3d2f6493ff", "input": "The Wikipedia dataset consists of articles from English Wikipedia, with quality class labels assigned by the Wikipedia community. \n Question: Which languages do they use?", "target": "Extractive"}
{"id": "task462-f35b322b986240ddb5aa0f65c7077c3b", "input": "We thus try to remove the language-specific information from the representations by centering the representations of sentences in each language so that their average lies at the origin of the vector space. \n Question: Are language-specific and language-neutral components disjunctive?", "target": "Yes-no"}
{"id": "task462-526b17363d184e67a9a36745bd289da9", "input": "After averaging over all concepts, we lose information on the lexical variation that each concept presents but on the other hand one can now investigate which regions show similar geolectal variation, yielding well defined linguistic varieties. Those cells that have similar colors in either figure FIGREF16 or figure FIGREF17 are expected to be ascribed to the same dialect zone. Thus, we can distinguish two main regions or clusters in the maps. The purple background covers most of the map and represents rural regions with small, scattered population. Our analysis shows that this group of cells possesses more specific words in their lexicon. In contrast, the green and yellow cells form a second cluster that is largely concentrated on the center and along the coastline, which correspond to big cities and industrialized areas. In these cells, the use of standard Spanish language is widespread due probably to school education, media, travelers, etc. The character of its vocabulary is more uniform as compared with the purple group. While the purple cluster prefer particular utterances, the lexicon of the urban group includes most of the keywords.  \n Question: What are the characteristics of the rural dialect?", "target": "Abstractive"}
{"id": "task462-0faa22e03c4a4bf28600681e5b4d4da4", "input": "Adding word alignments in parallel sentences results in small, non significant improvements, even if there is some labeled data available in the source language. This difficulty in showing the usefulness of parallel corpora for SRI may be due to the current assumptions about role alignments, which mean that only a small percentage of roles are aligned. Further analyses reveals that annotating small amounts of data can easily outperform the performance gains obtained by adding large unlabeled dataset as well as adding parallel corpora. \n Question: Overall, does having parallel data improve semantic role induction across multiple languages?", "target": "Yes-no"}
{"id": "task462-973741167f95473ca71e9037f10855a0", "input": "Performing the Welch's t-test, both changes after weGAN training are statistically significant at a INLINEFORM0 significance level.  \n Question: Do they evaluate grammaticality of generated text?", "target": "Yes-no"}
{"id": "task462-e1b8c08507c94af49953c96b6bf9c5a4", "input": "We also conduct two human evaluations in order to assess (a) which type of summary participants prefer (we compare extractive and abstractive systems) and (b) how much key information from the document is preserved in the summary (we ask participants to answer questions pertaining to the content in the document by reading system summaries). \n Question: Do they use other evaluation metrics besides ROUGE?", "target": "Yes-no"}
{"id": "task462-1579cdd22a2d49e590d90232a03ee464", "input": "BIBREF6 introduce the KG-A2C, which uses a knowledge graph based state-representation to aid in the section of actions in a combinatorially-sized action-space\u2014specifically they use the knowledge graph to constrain the kinds of entities that can be filled in the blanks in the template action-space. They test their approach on Zork1, showing the combination of the knowledge graph and template action selection resulted in improvements over existing methods. They note that their approach reaches a score of 40 which corresponds to a bottleneck in Zork1 where the player is eaten by a \u201cgrue\u201d (resulting in negative reward) if the player has not first lit a lamp. \n Question: What are the baselines?", "target": "Extractive"}
{"id": "task462-faa053a2645141e6b8dc1d0e9dfe3a0e", "input": "Task: given a caption and a paired image (if used), the goal is to label every token in a caption in BIO scheme (B: beginning, I: inside, O: outside) BIBREF27 .  \n Question: Can named entities in SnapCaptions be discontigious?", "target": "Yes-no"}
{"id": "task462-97d15b9694954c338cc27a1f76d12565", "input": "Conditional Random Fields\nConditional Random Fields (CRF) BIBREF10 are a standard approach when dealing with sequential data in the context of sequence labeling. BiLSTM-CRF\nPrior to the emergence of deep neural language models, BiLSTM-CRF models BIBREF11 had achieved state-of-the-art results for the task of sequence labeling. Multi-Task Learning\nMulti-Task Learning (MTL) BIBREF15 has become popular with the progress in deep learning. BioBERT\nDeep neural language models have recently evolved to a successful method for representing text. In particular, Bidirectional Encoder Representations from Transformers (BERT) outperformed previous state-of-the-art methods by a large margin on various NLP tasks BIBREF17. \n Question: What baseline systems are proposed?", "target": "Extractive"}
{"id": "task462-a12962212269491b90cd7f12b4824623", "input": "For all experiments, the dimensions of word embeddings and recurrent hidden states are both set to 512. \n Question: Do they use the same architecture as LSTM-s and GRUs with just replacing with the LAU unit?", "target": "Yes-no"}
{"id": "task462-f9e0083608d04a198221abed85709a22", "input": "Our first baseline is ROUGE-L BIBREF1 , since it is the most commonly used metric for compression tasks. We compare to the best n-gram-overlap metrics from toutanova2016dataset; We further compare to the negative LM cross-entropy, i.e., the log-probability which is only normalized by sentence length. Our next baseline is perplexity, which corresponds to the exponentiated cross-entropy: Due to its popularity, we also performed initial experiments with BLEU BIBREF17 .  \n Question: Is ROUGE their only baseline?", "target": "Yes-no"}
{"id": "task462-daffce8cb01744f58e8c543927d6f757", "input": "Using DSC loss improves the F1 score by +0.58 for MRPC and +0.73 for QQP. \n Question: What are method improvements of F1 for paraphrase identification?", "target": "Extractive"}
{"id": "task462-5155455d6e4643aaaa52d1292bfc833b", "input": "This results in three additional vectors corresponding to INLINEFORM3 , INLINEFORM4 and INLINEFORM5 difference vectors.\n\nResults \n Question: How are the EAU text spans annotated?", "target": "Abstractive"}
{"id": "task462-e5b8e241f2f1489d903d789d04ec500e", "input": " We then train a paragraph vector model using the Document to Vector (Doc2Vec) framework BIBREF7 on the whole set (13 million) of preprocessed text records, although training on smaller sets (1 million) also produces good results. \n Question: Which text embedding methodologies are used?", "target": "Extractive"}
{"id": "task462-337588b882af49ffa2bd39b0459aa345", "input": "In this section we describe how to evaluate and compare the outcomes of algorithms which assign relevance scores to words (such as LRP or SA) through intrinsic validation. Furthermore, we propose a measure of model explanatory power based on an extrinsic validation procedure. \n Question: Are the document vectors that the authors introduce evaluated in any way other than the new way the authors propose?", "target": "Yes-no"}
{"id": "task462-7a40d6ac207c4b7ba83ea4b55e902bf5", "input": "Table TABREF15 shows a comparison of the results on SimCluster versus K-means algorithm. Here our SimCluster algorithm improves the F1-scores from 0.412 and 0.417 in the two domains to 0.442 and 0.441. The ARI scores also improve from 0.176 and 0.180 to 0.203 and 0.204. \n Question: Do they use the same distance metric for both the SimCluster and K-means algorithm?", "target": "Yes-no"}
{"id": "task462-b9ffb92400e0417584f06d6453bf76ea", "input": "o sufficiently utilize the large dataset $\\mathcal {A}$ and $\\mathcal {M}$, the model is pre-trained on CTC-based ASR task and MT task in the pre-training stage. \n Question: What is the attention module pretrained on?", "target": "Extractive"}
{"id": "task462-442bf5dd48c84e8a888ac20e0c0f550d", "input": "Baselines. As none of the existing KBC methods can solve the OKBC problem, we choose various versions of LiLi as baselines.\n\nSingle: Version of LiLi where we train a single prediction model INLINEFORM0 for all test relations.\n\nSep: We do not transfer (past learned) weights for initializing INLINEFORM0 , i.e., we disable LL.\n\nF-th): Here, we use a fixed prediction threshold 0.5 instead of relation-specific threshold INLINEFORM0 .\n\nBG: The missing or connecting links (when the user does not respond) are filled with \u201c@-RelatedTo-@\" blindly, no guessing mechanism.\n\nw/o PTS: LiLi does not ask for additional clues via past task selection for skillset improvement. \n Question: What baseline is used in the experiments?", "target": "Extractive"}
{"id": "task462-42f2a04286144f92941f6595d640f99d", "input": "Three baseline classification methods: Support Vector Machines (SVM), Adaboost, and Random Forests are adopted to evaluate our extracted features. On each dataset, the employed classifiers are trained with individual feature first, and then with the combination of the two features. \n Question: LDA is an unsupervised method; is this paper introducing an unsupervised approach to spam detection?", "target": "Yes-no"}
{"id": "task462-15f0cf5cbf4a428ba4ae2f95480a3813", "input": "Since the present study focuses on Spanish tweets, all tweets from the English datasets were translated into Spanish.  \n Question: What other languages did they translate the data from?", "target": "Extractive"}
{"id": "task462-56c1f4282349474fbfdc8d57aff53023", "input": "Five attributes, specifying certain details of clinical significance, are defined to characterize the answer types of INLINEFORM4 : (1) the time the patient has been experiencing the symptom, (2) activities that trigger the symptom (to occur or worsen), (3) the extent of seriousness, (4) the frequency occurrence of the symptom, and (5) the location of symptom. For each symptom/attribute, it can take on different linguistic expressions, defined as entities. Note that if the queried symptom or attribute is not mentioned in the dialogue, the groundtruth output is \u201cNo Answer\u201d, as in BIBREF6 . \n Question: What labels do they create on their dataset?", "target": "Extractive"}
{"id": "task462-686735697f584324a1337bf0f6fb4ea2", "input": "Firstly, LastStateRNN is the classic RNN model, where the last state passes through an MLP and then the LR Layer estimates the corresponding probability. In contrast, in the AvgRNN model we consider the average vector of all states that come out of the cells. The AttentionRNN model is the one that it has been presented in BIBREF9. \n Question: What was the baseline?", "target": "Extractive"}
{"id": "task462-82d4483da1e44073a483af1bbc021504", "input": "To discover topics from the collected tweets, we used a topic modeling approach that fuzzy clusters the semantically related words such as assigning \u201cdiabetes\", \u201ccancer\", and \u201cinfluenza\" into a topic that has an overall \u201cdisease\" theme BIBREF44 , BIBREF45 . Among topic models, Latent Dirichlet Allocation (LDA) BIBREF49 is the most popular effective model BIBREF50 , BIBREF19 as studies have shown that LDA is an effective computational linguistics model for discovering topics in a corpus BIBREF51 , BIBREF52 . We used the Mallet implementation of LDA BIBREF49 , BIBREF56 with its default settings to explore opinions in the tweets. \n Question: How were topics of interest about DDEO identified?", "target": "Abstractive"}
{"id": "task462-2e7105f5e2a64aa1ab990e54cde79ab0", "input": "In test batch 4, our system (called FACTOIDS) achieved highest recall score of \u20180.7033\u2019 but low precision of 0.1119, leaving open the question of how could we have better balanced the two measures. \n Question: What was their highest recall score?", "target": "Extractive"}
{"id": "task462-646c1dc2b3b54dfd937099a6297d94cd", "input": "The final code-mixed tweets were forwarded to a group of three annotators who were university students and fluent in both English and Hindi. \n Question: How many annotators tagged each text?", "target": "Extractive"}
{"id": "task462-5a2daf09c1204f79a9d9db7d60888646", "input": "So, we can impose the constraint that our model's representation of the input's syntax cannot contain this context-invariant information. This regularization is strictly preferable to allowing all aspects of word meaning to propagate into the input's syntax representation. Without such a constraint, all inputs could, in principle, be given their own syntactic categories. \n Question: Does having constrained neural units imply word meanings are fixed across different context?", "target": "Yes-no"}
{"id": "task462-d6220a608fb74c77abce5745824257b2", "input": "The reviews we used have been extracted from TripAdvisor and originally proposed in BIBREF10 , BIBREF11 .  \n Question: Where are the hotel reviews from?", "target": "Extractive"}
{"id": "task462-3d50645b6d994bd0a7a3a4d7b59b759e", "input": "The choice of two methods for our empirical study is motivated by the best performance achieved by Logistic Regression in question-question similarity at SemEval 2017 (best system BIBREF37 and second best system BIBREF38 ), and the high performance achieved by neural networks on larger datasets such as SNLI BIBREF13 , BIBREF39 , BIBREF40 , BIBREF41  \n Question: What machine learning and deep learning methods are used for RQE?", "target": "Extractive"}
{"id": "task462-932d542299ac49fd8720162c4a2f3a84", "input": "We evaluate our model on a publicly available dataset, KARA ONE BIBREF17 , composed of multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw). The dataset consists of 14 participants, with each prompt presented 11 times to each individual.  \n Question: How many subjects does the EEG data come from?", "target": "Extractive"}
{"id": "task462-e0a2304026c243e09b7c0488465eece2", "input": "For reliable multi-hop reasoning using OpenIE tuples, we can add inter-tuple connections to the support graph search, controlled by a small number of rules over the OpenIE predicates. Learning such rules for the Science domain is an open problem and potential avenue of future work. \n Question: Can the method answer multi-hop questions?", "target": "Yes-no"}
{"id": "task462-193de252f23841ad9ba0ff836f612a46", "input": "Word2vec representation is far better, advanced and a recent technique which functions by mapping words to a 300 dimensional vector representations. \n Question: What is the dimension of the embeddings?", "target": "Extractive"}
{"id": "task462-1db25bade9644427b79f112afe45f466", "input": "BERT: We implement two BERT-based baselines BIBREF51 for evidence identification. First, we train BERT on each query-policy sentence pair as a binary classification task to identify if the sentence is evidence for the question or not (Bert). We also experiment with a two-stage classifier, where we separately train the model on questions only to predict answerability. At inference time, if the answerable classifier predicts the question is answerable, the evidence identification classifier produces a set of candidate sentences (Bert + Unanswerable).\n\n \n Question: What type of neural model was used?", "target": "Extractive"}
{"id": "task462-38e267f66e854125b445b83cfce546bc", "input": "We compare our approach to two other, the first one uses almost the same tweet data as we use for training, and the second one is the CrowdFlower dataset annotated for emotions.\n\nIn the first one Wang et al. BIBREF21 downloaded over 5M tweets which included one of 131 emotional hashtags based on Parrott's three-level categorization of emotions in seven categories: joy, sadness, anger, love, fear, thankfulness, surprise.  In the second one, the reported results are from a paper by BIBREF33 in which they used maximum entropy classifier with bag of words model to classify various emotional datasets. Here we only report part of their result for CrowdFlower dataset that can be mapped to one of our seven labels. \n Question: What baseline is used?", "target": "Extractive"}
{"id": "task462-8ef6c18d275f4d969cc7eba65218b464", "input": "We conduct our annotation study on Amazon Mechanical Turk, presenting Turkers with Human Intelligence Tasks (henceforth, HITs) consisting of a single conversation between a customer and an agent. In each HIT, we present Turkers with a definition of each dialogue act, as well as a sample annotated dialogue for reference. For each turn in the conversation, we allow Turkers to select as many labels from our taxonomy as required to fully characterize the intent of the turn. Additionally, annotators are asked three questions at the end of each conversation HIT, to which they could respond that they agreed, disagreed, or could not tell: \n Question: How are customer satisfaction, customer frustration and overall problem resolution data collected?", "target": "Abstractive"}
{"id": "task462-bdcda166b4944884b30abb729d1987cf", "input": "There are various possible extensions for this work. For example, using all frames assigned to a phone, rather than using only the middle frame. \n Question: Do they propose any further additions that could be made to improve generalisation to unseen speakers?", "target": "Yes-no"}
{"id": "task462-fd429218caba49e88b1daea689480e75", "input": "We computed Fliess' INLINEFORM0 on the trial set for the five annotators on 21 of the tweets. INLINEFORM1 is .83 for Layer A (OFF vs NOT) indicating high agreement. As to normalization and anonymization, no user metadata or Twitter IDs have been stored, and URLs and Twitter mentions have been substituted to placeholders.  \n Question: How many annotators participated?", "target": "Extractive"}
{"id": "task462-c59aa864964e422da73003a458fb8bee", "input": "The challenge is addressed as follows: given a natural language input sequence describing the scene, such as a piece of a story coming from a transcript, the goal is to infer which action is most likely to happen next. \n Question: Do they literally just treat this as \"predict the next spell that appears in the text\"?", "target": "Yes-no"}
{"id": "task462-7c0532fac9934b739b462e27981fa3a4", "input": "KryptoOracle has been built in the Apache ecosystem and uses Apache Spark. Data structures in Spark are based on resilient distributed datasets (RDD), a read only multi-set of data which can be distributed over a cluster of machines and is fault tolerant.  Spark RDD has the innate capability to recover itself because it stores all execution steps in a lineage graph. In case of any faults in the system, Spark redoes all the previous executions from the built DAG and recovers itself to the previous steady state from any fault such as memory overload. Spark RDDs lie in the core of KryptoOracle and therefore make it easier for it to recover from faults. Moreover, faults like memory overload or system crashes may require for the whole system to hard reboot. However, due to the duplicate copies of the RDDs in Apache Hive and the stored previous state of the machine learning model, KryptoOracle can easily recover to the previous steady state.\n\n \n Question: How is the architecture fault-tolerant?", "target": "Abstractive"}
{"id": "task462-6c2cd2b9e3644853876d5fc760ba2747", "input": " Last, we evaluate our approaches in 9 commonly used text classification datasets. We evaluate our methods on several commonly used datasets whose themes range from sentiment, web-page, science to medical and healthcare. \n Question: What NLP tasks do they consider?", "target": "Abstractive"}
{"id": "task462-db3303370a1a4c3abca1e169010edffc", "input": "Subsequently, an LSTM decoder generates the characters in the output word form using encoder states and an attention mechanism. \n Question: What architecture does the decoder have?", "target": "Extractive"}
