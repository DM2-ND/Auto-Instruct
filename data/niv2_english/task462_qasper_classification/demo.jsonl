{"id": "task462-504181ea19ea4c768012bd7b56dc9907", "input": " BIBREF3 also use bilingual crowd workers, but the studies supporting the use of crowdsourcing for MT evaluation were performed with older MT systems, and their findings may not carry over to the evaluation of contemporary higher-quality neural machine translation (NMT) systems. In addition, the MT developers to which crowd workers were compared are usually not professional translators.  We hypothesise that an evaluation of sentences in isolation, as applied by BIBREF3, precludes raters from detecting translation errors that become apparent only when inter-sentential context is available, and that they will judge MT quality less favourably when evaluating full documents. BIBREF3 used all source texts of the WMT 2017 Chinese-English test set for their experiments, of which only half were originally written in Chinese;  \n Question: What was the weakness in Hassan et al's evaluation design?", "target": "Abstractive"}
{"id": "task462-b4896049e9cf4ec79961f0bc0dc348f1", "input": " Finally, when investigating the relatedness between European vs. non European languages (cf. (En/Fr)$\\rightarrow $Ar), we obtain similar results than those obtained in the monolingual experiment (macro F-score 62.4 vs. 68.0) and best results are achieved by Ar $\\rightarrow $(En/Fr). This shows that there are pragmatic devices in common between both sides and, in a similar way, similar text-based patterns in the narrative way of the ironic tweets. \n Question: Do the authors identify any cultural differences in irony use?", "target": "Yes-no"}
{"id": "task462-73fc070fd4364b0c830449cc8302678a", "input": "We also explored clustering 12742 STRENGTH sentences directly using CLUTO BIBREF19 and Carrot2 Lingo BIBREF20 clustering algorithms.  \n Question: What clustering algorithms were used?", "target": "Extractive"}
