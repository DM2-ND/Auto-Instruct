{"id": "task461-76ad051d980f414092523dc9d4b84e04", "input": "All documents are segmented into paragraphs and processed at the paragraph level (both training and inference); this is acceptable because we observe that most paragraphs are less than 200 characters. The input sequences are segmented by the BERT tokenizer, with the special [CLS] token inserted at the beginning and the special [SEP] token added at the end.", "target": "At what text unit/level were documents processed?"}
{"id": "task461-c70adbe8c8b14e8fb2aaa47d222744fe", "input": "Method ::: Passage Ranking Model\nThe key component of our framework is the Ranker model, which is provided with a question $q$ and $K$ passages $\\mathcal {P} = \\lbrace p_1, p_2 ... p_K\\rbrace $ from a pool of candidates, and outputs a chain of selected passages. Method ::: Cooperative Reasoner\nTo alleviate the noise in the distant supervision signal $\\mathcal {C}$, in addition to the conditional selection, we further propose a cooperative Reasoner model, also implemented with the MatchLSTM architecture (see Appendix SECREF6), to predict the linking entity from the selected passages.", "target": "What are two models' architectures in proposed solution?"}
{"id": "task461-ee9744304e374c82b23a65afad426c5d", "input": "We choose Zulu (ZUL), a Bantoid language. In contrast to the first two, it is strongly prefixing.", "target": "What is an example of a prefixing language?"}
{"id": "task461-ea78f344f2404be4b1ace007f7968443", "input": "In order to derive the discourse features, an entity grid is constructed by feeding the document through an NLP pipeline to identify salient entities. Two flavors of discourse features are created by populating the entity grid with either (i) grammatical relations (GR) or (ii) RST discourse relations (RST).  Denoting a feature as such with INLINEFORM4 , we construct the pooling vector INLINEFORM5 for the char-bigrams, and concatenate INLINEFORM6 to INLINEFORM7 before feeding the resulting vector to the softmax layer.", "target": "How are discourse features incorporated into the model?"}
{"id": "task461-220ba4bab1e04f3a8b25763233a761fd", "input": "Three datasets annotated with emotions are commonly used for the development and evaluation of emotion detection systems, namely the Affective Text dataset, the Fairy Tales dataset, and the ISEAR dataset. A summary is provided in Table TABREF8 , which also shows, in the bottom row, what role each dataset has in our experiments: apart from the development portion of the Affective Text, which we used to develop our models (Section SECREF4 ), all three have been used as benchmarks for our evaluation.", "target": "Which existing benchmarks did they compare to?"}
{"id": "task461-3ca7f9e06bd54473a6575bf09e9e6151", "input": "BERT: We implement two BERT-based baselines BIBREF51 for evidence identification. First, we train BERT on each query-policy sentence pair as a binary classification task to identify if the sentence is evidence for the question or not (Bert). We also experiment with a two-stage classifier, where we separately train the model on questions only to predict answerability. At inference time, if the answerable classifier predicts the question is answerable, the evidence identification classifier produces a set of candidate sentences (Bert + Unanswerable).\n\n", "target": "What type of neural model was used?"}
{"id": "task461-6557468becce4c7790ae90e7f0c54398", "input": "We compare classification and regression approaches and show that classification produces better results than regression but the quality of the results depends on the approach followed to annotate the data labels.", "target": "Did classification models perform better than previous regression one?"}
{"id": "task461-f42beceab3514b41bbaa8f6109f9b108", "input": " Experimentally, on three benchmark datasets for machine translation \u2013 WMT2014, WMT2016 and IWSLT-2014, FlowSeq achieves comparable performance with state-of-the-art non-autoregressive models, and almost constant decoding time w.r.t. the sequence length compared to a typical left-to-right Transformer model, which is super-linear.", "target": "What are three neural machine translation (NMT) benchmark datasets used for evaluation?"}
{"id": "task461-076139f6f640456bb12ce84fb15da926", "input": "As of January 2020, the latest CoVo 2019-06-12 release includes 29 languages. CoVoST is currently built on that release and covers the following 11 languages: French, German, Dutch, Russian, Spanish, Italian, Turkish, Persian, Swedish, Mongolian and Chinese. Common Voice BIBREF10 is a crowdsourcing speech recognition corpus with an open CC0 license. Contributors record voice clips by reading from a bank of donated sentences", "target": "How was the dataset collected?"}
{"id": "task461-bd032a44992a4ce1a09b97facf2e9789", "input": "In the first one, shown in Figure FIGREF12, a special dedicated discriminator is added to the model to control that the latent representation does not contain stylistic information. The second extension of the baseline architecture does not use an adversarial component $D_z$ that is trying to eradicate information on $c$ from component $z$. Instead, the system, shown in Figure FIGREF16 feeds the \"soft\" generated sentence $\\tilde{G}$ into encoder $E$ and checks how close is the representation $E(\\tilde{G} )$ to the original representation $z = E(x)$ in terms of the cosine distance. We further refer to it as shifted autoencoder or SAE. We also study a combination of both approaches described above, shown on Figure FIGREF17.", "target": "What are three new proposed architectures?"}
{"id": "task461-4e711a7f6f8c428d80ec2ce54827a08f", "input": "BERT-ADA BIBREF33 is a domain-adapted BERT-based model proposed for the APC task, which fine-tuned the BERT-BASE model on task-related corpus. This model obtained state-of-the-art accuracy on the Laptops dataset. We build a joint model for the multi-task of ATE and APC based on the BERT-BASE model. After optimizing the model parameters according to the empirical result, the joint model based on BERT-BASE achieved hopeful performance on all three datasets and even surpassed other proposed BERT based improved models on some datasets, such as BERT-PT, AEN-BERT, SDGCN-BERT, and so on.", "target": "What was state of the art on SemEval-2014 task4 Restaurant and Laptop dataset?"}
{"id": "task461-2fb7a80225da4b5585d33ce40736ce6d", "input": "Results in Table TABREF64 show the correctness rates of these scenarios. User correctness score is superior to that of the baseline parser by 7.5% (from 37.1% to 44.6%), while the hybrid approach outscores both with a correctness of 48.7% improving the baseline by 11.6%.", "target": "Which query explanation method was preffered by the users in terms of correctness?"}
{"id": "task461-e8d710962d704093a0081c43b08830e4", "input": "Although MCDN doesn't obtain the highest precision, it increases F1-score by 10.2% and 3% compared with the existing best systems $LS \\cup KLD \\cup CONN$ and $KLD \\cup LS \\cup LS_{inter}$.", "target": "What performance did proposed method achieve, how much better is than previous state-of-the-art?"}
{"id": "task461-bc37e8c38d014185aa526da5a4d6f273", "input": "IDEA\nBIBREF9 Two different BERT models were developed. For Friends, pre-training was done using a sliding window of two utterances to provide dialogue context. Both Next Sentence Prediction (NSP) phase on the complete unlabeled scripts from all 10 seasons of Friends, which are available for download. In addition, the model learned the emotional disposition of each of six main six main characters in Friends (Rachel, Monica, Phoebe, Joey, Chandler and Ross) by adding a special token to represent the speaker. For EmotionPush, pre-training was performed on Twitter data, as it is similar in nature to chat based dialogues. In both cases, special attention was given to the class imbalance issue by applying \u201cweighted balanced warming\u201d on the loss function.", "target": "What model was used by the top team?"}
{"id": "task461-25c3ac6e7d114aa2a04b11fc2683a0c9", "input": "In Table TABREF1, we summarize the quantitative results of the above previous studies. It includes the model basis, feature description, languages classified and the used dataset along with accuracy obtained. The table also lists the overall results of our proposed models (at the top). The languages used by various authors along with their acronyms are English (En), Spanish (Es), French (Fr), German (De), Russian (Ru), Italian (It), Bengali (Ben), Hindi (Hi) and Telegu (Tel).", "target": "Is the performance compared against a baseline model?"}
{"id": "task461-bcd094d2585b4d0b97de51f58290107a", "input": "WikiTableQuestions contains 22,033 questions and is an order of magnitude larger than previous state-of-the-art datasets. Its questions were not designed by predefined templates but were hand crafted by users, demonstrating high linguistic variance.", "target": "How do they gather data for the query explanation problem?"}
{"id": "task461-65dfa1cc0b654c298db0f2aef7f33e22", "input": "We trained word embeddings using either GloVe BIBREF11 or SGNS BIBREF12 on a small or a large corpus.", "target": "What types of word representations are they evaluating?"}
{"id": "task461-516e22e40c7540e9bad6a7f2c2ad2414", "input": "RKS approach proposed in BIBREF21, BIBREF22, explicitly maps data vectors to a space where linear separation is possible. The RKS method provides an approximate kernel function via explicit mapping.", "target": "What is the Random Kitchen Sink approach?"}
{"id": "task461-de9a0629ad4643cf955955b1e2658101", "input": "Both the audio and the transcript are de-identified (by removing the identifying information) with digital zeros and [de-identified] tags, respectively.", "target": "Is the data de-identified?"}
{"id": "task461-248cb6b68a1044be88e32b2509e437d4", "input": "Benchmark Evaluation ::: Classifier Models\nSVM: A linear support vector machine with bag-of-words sentence representations.\n\nMLP: A multi-layer perceptron with USE embeddings BIBREF4 as input.\n\nFastText: A shallow neural network that averages embeddings of n-grams BIBREF5.\n\nCNN: A convolutional neural network with non-static word embeddings initialized with GloVe BIBREF6.\n\nBERT: A neural network that is trained to predict elided words in text and then fine-tuned on our data BIBREF1.\n\nPlatforms: Several platforms exist for the development of task-oriented agents. We consider Google's DialogFlow and Rasa NLU with spacy-sklearn.", "target": "Which classifiers are evaluated?"}
{"id": "task461-c8ad7218cc854cc0a4f794836a594533", "input": "In order to represent individual sentences, we insert external [cls] tokens at the start of each sentence, and each [cls] symbol collects features for the sentence preceding it. We also use interval segment embeddings to distinguish multiple sentences within a document. This way, document representations are learned hierarchically where lower Transformer layers represent adjacent sentences, while higher layers, in combination with self-attention, represent multi-sentence discourse. Position embeddings in the original Bert model have a maximum length of 512; we overcome this limitation by adding more position embeddings that are initialized randomly and fine-tuned with other parameters in the encoder.", "target": "What is novel about their document-level encoder?"}
{"id": "task461-7a795b24787948848119685982805921", "input": "The current state-of-the-art approach BIBREF14 , BIBREF15 uses maximum entropy and CRF models with a combination of language model and hand-crafted features to predict if each character in the hashtag is the beginning of a new word.", "target": "What current state of the art method was used for comparison?"}
{"id": "task461-d6a30deacd6b4a3bbe29cb7a0a2225b6", "input": "We collect a novel dataset of 230K+ recipe texts and 1M+ user interactions (reviews) over 18 years (2000-2018) from Food.com.", "target": "Where do they get the recipes from?"}
{"id": "task461-8470b4c39f224b4b9cc998ad94d0e906", "input": "During the final training process, 500 validation data is used to generate the recurrent neural model, which is 3% of the training data. After finishing the training, the accuracy of the generated model using validation data from the source corpus was 74.40%", "target": "What dataset is used to measure accuracy?"}
{"id": "task461-459c4254d1814440918e7dd6ace279cd", "input": "We download 210 users' all twitter posts who are war veterans and clinically diagnosed with PTSD sufferers as well which resulted a total 12,385 tweets. ", "target": "How many twitter users are surveyed using the clinically validated survey?"}
{"id": "task461-6973c247bd23487a86a9e23794ca24f9", "input": "Our system was ranked second in the competition only 0.3 BLEU points behind the winning team UPC-TALP. The relative low BLEU and high TER scores obtained by all teams are due to out-of-domain data provided in the competition which made the task equally challenging to all participants. The fact that out-of-domain data was provided by the organizers resulted in a challenging but interesting scenario for all participants.", "target": "Does the use of out-of-domain data improve the performance of the method?"}
{"id": "task461-5b09fbf065924444adc9290fdb20cbf1", "input": "The dataset at HASOC 2019 were given in three languages: Hindi, English, and German. Dataset in Hindi and English had three subtasks each, while German had only two subtasks. We participated in all the tasks provided by the organisers and decided to develop a single model that would be language agnostic. We used the same model architecture for all the three languages. The training dataset of Hindi dataset was more balanced than English or German dataset. Hence, the results were around 0.78. As the dataset in German language was highly imbalanced, the results drops to 0.62.", "target": "What are the languages used to test the model?"}
{"id": "task461-cb72a88f94cf4356a9a9c0881f35523d", "input": "In this work, the metrics detailed below are proposed and we evaluate their quality through a human evaluation in subsection SECREF32. In addition to the automatic metrics, we proceeded to a human evaluation. We chose to use the data from our SQuAD-based experiments in order to also to measure the effectiveness of the proposed approach to derive Curiosity-driven QG data from a standard, non-conversational, QA dataset. We randomly sampled 50 samples from the test set. Three professional English speakers were asked to evaluate the questions generated by: humans (i.e. the reference questions), and models trained using pre-training (PT) or (RL), and all combinations of those methods.\n\nBefore submitting the samples for human evaluation, the questions were shuffled. Ratings were collected on a 1-to-5 likert scale, to measure to what extent the generated questions were: answerable by looking at their context; grammatically correct; how much external knowledge is required to answer; relevant to their context; and, semantically sound. The results of the human evaluation are reported in Table TABREF33.", "target": "How they evaluate quality of generated output?"}
{"id": "task461-b4560a52781442fbb06da6160ea9c572", "input": "Which events authors choose to include in their history, which they leave out, and the way the events chosen relate to the march are crucial factors in conveying bias.", "target": "What factors contribute to interpretive biases according to this research?"}
{"id": "task461-f45f6ae2a3884e3b865662d04c4bc35d", "input": "Our Reddit data was gathered using Reddit's public API, collecting the most recent jokes. Every time the scraper ran, it also updated the upvote score of the previously gathered jokes. This data collection occurred every hour through the months of March and April 2019. Since the data was already split into body and punchline sections from Reddit, we created separate datasets containing the body of the joke exclusively and the punchline of the joke exclusively. Additionally, we created a dataset that combined the body and punchline together.\n\nSome sample jokes are shown in Table 1, above. The distribution of joke scores varies wildly, ranging from 0 to 136,354 upvotes. We found that there is a major jump between the 0-200 upvote range and the 200 range and onwards, with only 6% of jokes scoring between 200-20,000. We used this natural divide as the cutoff to decide what qualified as a funny joke, giving us 13884 not-funny jokes and 2025 funny jokes.", "target": "How they evaluate if joke is humorous or not?"}
{"id": "task461-c09734afba924d199d73dea3f3641b8e", "input": "We are using the disaster data from BIBREF5. It contains various dataset including the CrisiLexT6 dataset which contains six crisis events related to English tweets in 2012 and 2013, labeled by relatedness (on-topic and off-topic) of respective crisis. Each crisis event tweets contain almost 10,000 labeled tweets but we are only focused on flood-related tweets thus, we experimented with only two flood event i.e. Queensland flood in Queensland, Australia and Alberta flood in Alberta, Canada and relabeled all on-topic tweets as Related and Off-topic as Unrelated for implicit class labels understanding in this case.", "target": "What dataset did they use?"}
{"id": "task461-cf4c291bf54c4dceb8f32e1384b071b4", "input": "We conduct experiments on WikiSQL BIBREF8 , which provides 87,726 annotated question-SQL pairs over 26,375 web tables. We do our study on SimpleQuestions BIBREF10 , which includes 108,442 simple questions, each of which is accompanied by a subject-relation-object triple. We conduct experiments on SequentialQA BIBREF9 which is derived from the WikiTableQuestions dataset BIBREF19 .", "target": "What datasets are used in this paper?"}
{"id": "task461-e32288ee05374c04b78f9eec4ccebe9c", "input": "We decided to explore the remaining space for improvement on the CBT by testing humans on a random subset of 50 named entity and 50 common noun validation questions that the psr ensemble could not answer correctly.", "target": "How do they show there is space for further improvement?"}
{"id": "task461-b4b07f07bf184eb18268cefa14914bb0", "input": "Meanwhile, attending colleges in the Northeast, West and South regions increases the possibility of posting about sexual harassment (positive coefficients), over the Midwest region. ", "target": "Which geographical regions correlate to the trend?"}
{"id": "task461-6e30f2b7397f456abb150ea771539087", "input": " We also use the final hidden layer of the neural network as a task-specific embedding of the claim, together with the Web evidence. ", "target": "What data is used to build the task-specific embeddings?"}
{"id": "task461-29558fb6287247138ac86a52bbe9ac94", "input": "MTMSN BIBREF4 is the first, and only model so far, that specifically tried to tackle the multi-span questions of DROP. ", "target": "What is the previous model that attempted to tackle multi-span questions as a part of its design?"}
{"id": "task461-8594437797c9447597df80bd3ec30923", "input": "his reduces significantly the needed training time: SBERT can be tuned in less than 20 minutes, while yielding better results than comparable sentence embedding methods.", "target": "How much time takes its training?"}
{"id": "task461-55099a1b76da4a69ae3b05ad0652a516", "input": "We collected all available comments in the stories from Reddit from August 2015. ", "target": "what is the source of the new dataset?"}
{"id": "task461-8d6b22c1aff849299e237a7635517151", "input": "We first use state-of-the-art PDTB taggers for our baseline BIBREF13 , BIBREF12 for the evaluation of the causality prediction of our models ( BIBREF12 requires sentences extracted from the text as its input, so we used our parser to extract sentences from the message).", "target": "What baselines did they consider?"}
{"id": "task461-c8b518e8c1254f449c0e5f2911af8a8a", "input": "However, adversarial misspellings constitute a longstanding real-world problem. Spammers continually bombard email servers, subtly misspelling words in efforts to evade spam detection while preserving the emails' intended meaning BIBREF1 , BIBREF2 .", "target": "Why is the adversarial setting appropriate for misspelling recognition?"}
{"id": "task461-ecddee7330de4832a5e85b361e402d6f", "input": "We extracted 200 sentence pairs from BIBREF3 's dataset and provided each pair with a document context consisting of a preceding and a following sentence, as in the following example.", "target": "What document context was added?"}
{"id": "task461-e32c66891ef843b3aa99222ccf950c91", "input": "With the above hyperparameter setting, the hybrid NER model achieved a F1 score of $0.995$ on synthesized queries and $0.948$ on clinical notes while the i2b2 NER model achieved a F1 score of $0.441$ on synthesized queries and $0.927$ on clinical notes (See Table TABREF23).", "target": "what were their performance results?"}
{"id": "task461-104e4ad82c4b4ce88793e3620bc8e4ae", "input": "Database Construction: we crawled travel information in Beijing from the Web, including Hotel, Attraction, and Restaurant domains (hereafter we name the three domains as HAR domains). Then, we used the metro information of entities in HAR domains to build the metro database. Goal Generation: a multi-domain goal generator was designed based on the database. The relation across domains is captured in two ways. One is to constrain two targets that locate near each other. The other is to use a taxi or metro to commute between two targets in HAR domains mentioned in the context.  Dialogue Collection: before the formal data collection starts, we required the workers to make a small number of dialogues and gave them feedback about the dialogue quality. Then, well-trained workers were paired to converse according to the given goals.  Dialogue Annotation: we used some rules to automatically annotate dialogue acts according to user states, system states, and dialogue histories. ", "target": "How was the dataset collected?"}
{"id": "task461-47ea82e3b90f4d82b891c1bc844cb885", "input": "Our dataset contains tweets about `ObamaCare' in USA collected during march 2010. ", "target": "What dataset of tweets is used?"}
{"id": "task461-2ecb6a0c85d542a2be1f27859518513d", "input": "The second trend is that by using less than 50% of the data available the model tends to overfit the data, as indicated by the consistent increase in the validation loss after about 15 epochs (check dashed lines in right side of Figure FIGREF28 ).", "target": "What experimental results suggest that using less than 50% of the available training examples might result in overfitting?"}
{"id": "task461-637e78b9b4e944b5946e4f36d1601ade", "input": "With regards to complexity and intangibility of ethics and morals, we restrict ourselves to a rather basic implementation of this construct, following the theories of deontological ethics. These ask which choices are morally required, forbidden, or permitted instead of asking which kind of a person we should be or which consequences of our actions are to be preferred. Thus, norms are understood as universal rules of what to do and what not to do. Therefore, we focus on the valuation of social acceptance in single verbs and single verbs with surrounding context information \u2014e.g. trust my friend or trust a machine\u2014 to figure out which of them represent a Do and which tend to be a Don't. ", "target": "How do the authors define deontological ethical reasoning?"}
{"id": "task461-61bbba345b3f4969bcc8a6fdc0e453b1", "input": "The informal setting/environment of social media often encourage multilingual speakers to switch back and forth between languages when speaking or writing. These all resulted in code-mixing and code-switching. Code-mixing refers to the use of linguistic units from different languages in a single utterance or sentence, whereas code-switching refers to the co-occurrence of speech extracts belonging to two different grammatical systemsBIBREF3. This language interchange makes the grammar more complex and thus it becomes tough to handle it by traditional algorithms. Thus the presence of high percentage of code-mixed content in social media text has increased the complexity of the aggression detection task. For example, the dataset provided by the organizers of TRAC-2018 BIBREF0, BIBREF2 is actually a code-mixed dataset.", "target": "What data/studies do the authors provide to support the assertion that the majority of aggressive conversations contain code-mixed languages?"}
{"id": "task461-45b3e091bf7043f4bc19b13b1c73f5e7", "input": "What is the impact of pre-trained representations with less transcribed data? In order to get a better understanding of this, we train acoustic models with different amounts of labeled training data and measure accuracy with and without pre-trained representations (log-mel filterbanks). The pre-trained representations are trained on the full Librispeech corpus and we measure accuracy in terms of WER when decoding with a 4-gram language model. Figure shows that pre-training reduces WER by 32% on nov93dev when only about eight hours of transcribed data is available. Pre-training only on the audio data of WSJ ( WSJ) performs worse compared to the much larger Librispeech ( Libri). This further confirms that pre-training on more data is crucial to good performance.", "target": "Do they explore how much traning data is needed for which magnitude of improvement for WER? "}
{"id": "task461-82cdab0252c94ee3a515385a4ff7f16b", "input": "As shown in Equation ( EQREF6 ), tone prediction sub-network ( INLINEFORM0 ) takes video and pinyin sequence as inputs and predict corresponding tone sequence. Video context vectors INLINEFORM0 and pinyin context vectors INLINEFORM1 are fused when predicting a tone character at each decoder step. The video encoder is the same as in Section SECREF7 and the pinyin encoder is: DISPLAYFORM0 The input video sequence is first fed into the VGG model BIBREF9 to extract visual feature. The output of conv5 of VGG is appended with global average pooling BIBREF10 to get the 512-dim feature vector. Then the 512-dim feature vector is fed into video encoder.", "target": "What visual information characterizes tones?"}
{"id": "task461-c5aad0478c7a46e2a055f975e59a7c6c", "input": "Our model yields the best results on both UAS and LAS metrics of all languages except the Japanese. As for Japanese, our model gives unsatisfactory results because the original treebank was written in Roman phonetic characters instead of hiragana, which is used by both common Japanese writing and our pre-trained embeddings. Despite this, our model overall still gives 1.0% higher average UAS and LAS than the previous best parser, BIAF. Although both BIAF and STACKPTR parsers have achieved relatively high parsing accuracies on the 12 languages and have all UAS higher than 90%, our model achieves state-of-the-art results in all languages for both UAS and LAS. Overall, our model reports more than 1.0% higher average UAS than STACKPTR and 0.3% higher than BIAF.", "target": "What are performance compared to former models?"}
{"id": "task461-2774b1593bc74a21b50f6075fb52b01f", "input": "We train our models on Sentiment140 and Amazon product reviews. Both of these datasets concentrates on sentiment represented by a short text.  For training the softmax model, we divide the text sentiment to two kinds of emotion, positive and negative. And for training the tanh model, we convert the positive and negative emotion to [-1.0, 1.0] continuous sentiment score, while 1.0 means positive and vice versa. ", "target": "Was the introduced LSTM+CNN model trained on annotated data in a supervised fashion?"}
{"id": "task461-a5f2741f4a5c428aa1fd5f580e3921ed", "input": "We then train the sensationalism scorer by classifying sensational and non-sensational headlines using a one-layer CNN with a binary cross entropy loss $L_{\\text{sen}}$. Firstly, 1-D convolution is used to extract word features from the input embeddings of a headline. This is followed by a ReLU activation layer and a max-pooling layer along the time dimension. All features from different channels are concatenated together and projected to the sensationalism score by adding another fully connected layer with sigmoid activation. Binary cross entropy is used to compute the loss $L_{\\text{sen}}$.", "target": "How is sensationalism scorer trained?"}
{"id": "task461-9ff6962c990b40f6a91630aaa5aea580", "input": "We draw our data from news publications, wine reviews, and Reddit, which in addition to large volume, also let us characterize binomials in new ways, and analyze differences in binomial orderings across communities and over time", "target": "What types of various community texts have been investigated for exploring global structure of binomials?"}
{"id": "task461-3fadfe499cc34325bfea2bae343d7ec5", "input": "Topology only embeddings: MMB BIBREF45 , DeepWalk BIBREF1 , LINE BIBREF33 , Node2vec BIBREF46 . ( INLINEFORM1 ) Joint embedding of topology & text: Naive combination, TADW BIBREF5 , CENE BIBREF6 , CANE BIBREF9 , WANE BIBREF10 , DMTE BIBREF34 . ", "target": "Which other embeddings do they compare against?"}
{"id": "task461-4f813da1f53c401581a3d51c71e22870", "input": "A causal attribution dataset is a collection of text pairs that reflect cause-effect relationships proposed by humans (for example, \u201cvirus causes sickness\u201d). These written statements identify the nodes of the network (see also our graph fusion algorithm for dealing with semantically equivalent statements) while cause-effect relationships form the directed edges (\u201cvirus\u201d $\\rightarrow $ \u201csickness\u201d) of the causal attribution network.", "target": "What are causal attribution networks?"}
{"id": "task461-8f8b120a0e62468dbb36b65ee2ff3057", "input": "This study focuses on Switchboard-300, a standard 300-hour English conversational speech recognition task. As a contrast to our best results on Switchboard-300, we also train a seq2seq model on the 2000-hour Switchboard+Fisher data. ", "target": "How much bigger is Switchboard-2000 than Switchboard-300 database?"}
{"id": "task461-f24cb3de2e0c4295bd6f9e7a9f710526", "input": "We pre-trained our model on the Dutch section of the OSCAR corpus, a large multilingual corpus which was obtained by language classification in the Common Crawl corpus BIBREF16.", "target": "What data did they use?"}
{"id": "task461-57e4146f1b054581b9e4f4beac772251", "input": " Since we are interested in the zero-shot capabilities of our representation, we trained our sentiment analysis model only on the english IMDB Large Movie Review dataset and tested it on the chinese ChnSentiCorp dataset and german SB-10K BIBREF24 , BIBREF25 . A natural language inference task consists of two sentences; a premise and a hypothesis which are either contradictions, entailments or neutral. Learning a NLI task takes a certain nuanced understanding of language. Therefore it is of interest whether or not UG-WGAN captures the necessary linguistic features. ", "target": "Did they experiment with tasks other than word problems in math?"}
{"id": "task461-1e36fa8bcb9346ad8c008da4e5d64d65", "input": "The resulting dataset consists of 22,880 users, 41,094 blogs, and 561,003 posts. Table TABREF2 presents additional statistics of our dataset.", "target": "How many users do they look at?"}
{"id": "task461-7e7b0db971cd4d6cbf754638e64a7762", "input": " In almost all genres, DenseNMT models are significantly better than the baselines.", "target": "did they outperform previous methods?"}
{"id": "task461-419e40315e0049a4b170a7a19b8bc96e", "input": "We evaluate the quality of the document embeddings learned by the different variants of CAHAN and the HAN baseline on three of the large-scale document classification datasets introduced by BIBREF14 and used in the original HAN paper BIBREF5.", "target": "Do they compare to other models appart from HAN?"}
{"id": "task461-02a3a40a4c9c4c868daefbb1945060ea", "input": "We identified some limitations during the process, which we describe in this section.\n\nWhen deciding publisher partisanship, the number of people from whom we computed the score was small. For example, de Stentor is estimated to reach 275K readers each day on its official website. Deciding the audience leaning from 55 samples was subject to sampling bias. Besides, the scores differ very little between publishers. None of the publishers had an absolute score higher than 1, meaning that even the most partisan publisher was only slightly partisan. Deciding which publishers we consider as partisan and which not is thus not very reliable.\n\nThe article-level annotation task was not as well-defined as on a crowdsourcing platform. We included the questions as part of an existing survey and didn't want to create much burden to the annotators. Therefore, we did not provide long descriptive text that explained how a person should annotate an article. We thus run under the risk of annotator bias. This is one of the reasons for a low inter-rater agreement.", "target": "What limitations are mentioned?"}
{"id": "task461-30e70244da134379a2f26743b689ff49", "input": "The dataset contains a total of 9710 passages, with an average of 6.24 sentences per passage, 16.16 words per sentence, and an average length of 86 words.", "target": "What is the size of this dataset?"}
{"id": "task461-f2cae67a5622401eb1215a218b4e49a4", "input": "For experiments, we train and evaluate our models in the civil law system of mainland China. We collect and construct a large-scale real-world data set of INLINEFORM0 case documents that the Supreme People's Court of People's Republic of China has made publicly available", "target": "what is the size of the real-world civil case dataset?"}
{"id": "task461-90284e1d81a84cf1b2e25b2af500784c", "input": "Experiments ::: Baselines\nTo comprehensively evaluate our AGDT, we compare the AGDT with several competitive models.", "target": "Is the model evaluated against other Aspect-Based models?"}
{"id": "task461-6d5071c98e9a48879b449fe4cd847808", "input": "A few sample advice rules in English (these are converted to first-order logic format and given as input to our algorithm) are presented in Table TABREF11 .  We modified the work of Odom et al. odomAIME15,odomAAAI15 to learn RDNs in the presence of advice. The key idea is to explicitly represent advice in calculating gradients.", "target": "How do they incorporate human advice?"}
{"id": "task461-d50d57487e4546fd8d7c1cb8d1c02469", "input": "Since BERT has already achieved the state-of-the-art performance of question-answering, in this section we compare our proposed model with state-of-the-art question answering models (i.e. QANet BIBREF39) and BERT-Base BIBREF26. As BERT has two versions: BERT-Base and BERT-Large, due to the lack of computational resource, we can only compare with BERT-Base model instead of BERT-Large.", "target": "What are strong baseline models in specific tasks?"}
{"id": "task461-492e0bd3228741beb04e8d0785ee4802", "input": "We compare our approach to two other, the first one uses almost the same tweet data as we use for training, and the second one is the CrowdFlower dataset annotated for emotions.\n\nIn the first one Wang et al. BIBREF21 downloaded over 5M tweets which included one of 131 emotional hashtags based on Parrott's three-level categorization of emotions in seven categories: joy, sadness, anger, love, fear, thankfulness, surprise.  In the second one, the reported results are from a paper by BIBREF33 in which they used maximum entropy classifier with bag of words model to classify various emotional datasets. Here we only report part of their result for CrowdFlower dataset that can be mapped to one of our seven labels.", "target": "What baseline is used?"}
{"id": "task461-66f4a80e70684c559018d994c2be7f40", "input": "One would expect that training with more data would improve the quality of the embeddings, but we found out with the results obtained with the C3 dataset, that only high-quality data helps.", "target": "What turn out to be more important high volume or high quality data?"}
{"id": "task461-ade8e5df81014a058e995da87e4c4973", "input": "E2ECM BIBREF11: In dialogue policy maker, it adopts a classic classification for skeletal sentence template. In our implement, we construct multiple binary classifications for each act to search the sentence template according to the work proposed by BIBREF11.\n\nCDM BIBREF10: This approach designs a group of classifications (two multi-class classifications and some binary classifications) to model the dialogue policy.", "target": "What are state-of-the-art baselines?"}
{"id": "task461-5132c49bd65c4be490845f4a4c3e613a", "input": "We assess the optimal number of topics that need to be specified for the STM analysis. We follow the recommendations of the original STM paper and focus on exclusivity and semantic coherence measures. Highly frequent words in a given topic that do not appear very often in other topics are viewed as making that topic exclusive.  Following BIBREF8 we generate a set of candidate models ranging between 3 and 50 topics. We then plot the exclusivity and semantic coherence (numbers closer to 0 indicate higher coherence), with a linear regression overlaid (Figure FIGREF3 ). Models above the regression line have a \u201cbetter\u201d exclusivity-semantic coherence trade off. We select the 16-topic model, which has the largest positive residual in the regression fit, and provides higher exclusivity at the same level of semantic coherence. The topic quality is usually evaluated by highest probability words, which is presented in Figure FIGREF4 .", "target": "How are the main international development topics that states raise identified?"}
{"id": "task461-d50f12af061748bbba04f7b015cffb55", "input": " Logistic regression model with character-level n-gram features is presented as a strong baseline for comparison since it was shown very effective.", "target": "What is their baseline?"}
{"id": "task461-cedb2fc1fa0d4016b968163490073880", "input": "The seed lexicon consists of positive and negative predicates. If the predicate of an extracted event is in the seed lexicon and does not involve complex phenomena like negation, we assign the corresponding polarity score ($+1$ for positive events and $-1$ for negative events) to the event. It is a ", "target": "What is the seed lexicon?"}
{"id": "task461-cdb763a93a954b5f82b3ed27276dcb52", "input": "We used four classification algorithms: 1) Logistic regression, which is conventionally used in sentiment classification. Other three algorithms which are relatively new and has shown great results on sentiment classification types of problems are: 2) Na\u00efve Bayes with SVM (NBSVM), 3) Extreme Gradient Boosting (XGBoost) and 4) FastText algorithm with Bidirectional LSTM (FastText-BiLSTM).", "target": "What state of the art models are used in the experiments?"}
{"id": "task461-478f3c4bdaad4e0ebd44b295eb870475", "input": "We use one public dataset Social Honeypot dataset and one self-collected dataset Weibo dataset to validate the effectiveness of our proposed features. Before directly performing the experiments on the employed datasets, we first delete some accounts with few posts in the two employed since the number of tweets is highly indicative of spammers. For the English Honeypot dataset, we remove stopwords, punctuations, non-ASCII words and apply stemming. For the Chinese Weibo dataset, we perform segmentation with \"Jieba\", a Chinese text segmentation tool. After preprocessing steps, the Weibo dataset contains 2197 legitimate users and 802 spammers, and the honeypot dataset contains 2218 legitimate users and 2947 spammers.", "target": "What is the benchmark dataset and is its quality high?"}
{"id": "task461-8a67b0194e4e42b8821f9304806d2a39", "input": "Evaluating FastText, GloVe and word2vec, we show that compared to other word representation learning algorithms, the FastText performs best.", "target": "What do you use to calculate word/sub-word embeddings"}
{"id": "task461-067b4e90d9dd4801b05bd6b6dd9095cf", "input": "To deal with this problem, BIBREF1 introduced a semantically conditioned generation model using Hierarchical Disentangled Self-Attention (HDSA) .", "target": "what semantically conditioned models did they compare with?"}
{"id": "task461-4974357a311a4b94996643ba4cb9ff82", "input": "In this chapter, we apply an ensemble of deep learning and linguistics to tackle both the problem of aspect extraction and subjectivity detection.", "target": "How are aspects identified in aspect extraction?"}
{"id": "task461-b48ee79fd0b249f488a97ff02a10013a", "input": "The crowdworkers were located in the US and hired on the BIBREF22 platform.", "target": "Who are the crowdworkers?"}
{"id": "task461-d3b8719a7d214c2798b4ab52efa38515", "input": "An acoustic word embedding is a function that takes as input a speech segment corresponding to a word, INLINEFORM0 , where each INLINEFORM1 is a vector of frame-level acoustic features, and outputs a fixed-dimensional vector representing the segment, INLINEFORM2 .", "target": "How do they represent input features of their model to train embeddings?"}
{"id": "task461-9bd72766f5c74f9cb8df6602314f6b4f", "input": "Concretely, we selected gold standards that fit our problem definition and were published in the years 2016 to 2019, have at least $(2019 - publication\\ year) \\times 20$ citations, and bucket them according to the answer selection styles as described in Section SECREF4", "target": "What modern MRC gold standards are analyzed?"}
{"id": "task461-9dd59e942e014d1fbf62e4bf01a698bc", "input": "For answer retrieval, a dataset is created by INLINEFORM4 , which gives INLINEFORM5 accuracy and INLINEFORM6 coverage, respectively.", "target": "Do they employ their indexing-based method to create a sample of a QA Wikipedia dataset?"}
{"id": "task461-521e0fa69aca4256978d2f181318f080", "input": "(Information Retrieval). This baseline has been successfully used for related tasks like Question Answering BIBREF39 . We create two versions of this baseline: one with the pool of perspectives INLINEFORM0 and one with the pool of evidences INLINEFORM1 .", "target": "Which machine baselines are used?"}
{"id": "task461-35e80869c2634cb4901003ef8866ec7d", "input": "First, we propose a class of recurrent-like neural networks for NLP tasks that satisfy the differential equation DISPLAYFORM0\n\nwhere DISPLAYFORM0\n\nand where INLINEFORM0 and INLINEFORM1 are learned functions. INLINEFORM2 corresponds to traditional RNNs, with INLINEFORM3 . For INLINEFORM4 , this takes the form of RNN cells with either nested internal memories or dependencies that extend temporally beyond the immediately previous hidden state. ", "target": "What novel class of recurrent-like networks is proposed?"}
{"id": "task461-78dc4d9ccc11482ba4d83d1ebe2a43be", "input": "We also observe that the best combination seems to consist in training our model on the original out-of-context dataset and testing it on the in-context pairs. In this configuration we reach an F-score (0.72) only slightly lower than the one reported in BIBREF3 (0.74), and we record the highest Pearson correlation, 0.3 (which is still not strong, compared to BIBREF3 's best run, 0.75). ", "target": "What were the results of the first experiment?"}
{"id": "task461-4c597fabe0ac4b89ad6a25ddc957ac01", "input": " In this work, we consider words that can indicate the characteristics of the neighbor words as contextual keywords and develop an approach to generate features from the automatically extracted contextual keywords.", "target": "What contextual features are used?"}
{"id": "task461-571c64baf0c545228b2803a3f3f2bd77", "input": "The performance on discriminating between offensive (OFF) and non-offensive (NOT) posts is reported in Table TABREF18 . We can see that all systems perform significantly better than chance, with the neural models being substantially better than the SVM. The CNN outperforms the RNN model, achieving a macro-F1 score of 0.80. The CNN system achieved higher performance in this experiment compared to the BiLSTM, with a macro-F1 score of 0.69. All systems performed better at identifying target and threats (TIN) than untargeted offenses (UNT).", "target": "What is the best performing model?"}
{"id": "task461-5efe834ae88742ee966003443e91eb31", "input": "We evalute Bertram on the WNLaMPro dataset of BIBREF0.", "target": "What is dataset for word probing task?"}
{"id": "task461-6d1b4d7a917b431bbcb47254a48bc4a0", "input": "SVMRank is a modification to SVM that assigns scores to each data point and allows the results to be ranked ( BIBREF26 ). We use SVMRank in the experiments below. ", "target": "what is the supervised model they developed?"}
{"id": "task461-6aaefab78e414d18a84705c9967fd652", "input": "Our semi-supervised approach is quite straightforward: first a model is trained on the training set and then this model is used to predict the labels of the silver data. This silver data is then simply added to our training set, after which the model is retrained. However, an extra step is applied to ensure that the silver data is of reasonable quality.", "target": "What semi-supervised learning is applied?"}
{"id": "task461-6d4a4d8c8fd243a19035c1fcde6d5d38", "input": "Without ELMo (the same setting as 4-th row in Table 3 ), our data settings is the same as qin-EtAl:2017:Long whose performance was state-of-the-art and will be compared directly. We see that even without the pre-trained ELMo encoder, our performance is better, which is mostly attributed to our better sentence pair representations.", "target": "Why does their model do better than prior models?"}
{"id": "task461-b3dab065bd2448c5b413f24290b51807", "input": "Given a set of evaluation documents, each having a known correct label from a closed set of labels (often referred to as the \u201cgold-standard\u201d), and a predicted label for each document from the same set, the document-level accuracy is the proportion of documents that are correctly labeled over the entire evaluation collection. This is the most frequently reported metric and conveys the same information as the error rate, which is simply the proportion of documents that are incorrectly labeled (i.e. INLINEFORM0 ). There are two distinct ways in which results are generally summarized per-language: (1) precision, in which documents are grouped according to their predicted language; and (2) recall, in which documents are grouped according to what language they are actually written in. It is also common practice to report an F-score INLINEFORM0 , which is the harmonic mean of precision and recall.", "target": "what evaluation methods are discussed?"}
{"id": "task461-c70c72a9d3f6417ebfc9ba75ba1deeaa", "input": "Overall, MMM has achieved a new SOTA, i.e., test accuracy of 88.9%, which exceeds the previous best by 16.9%.", "target": "How big are improvements of MMM over state of the art?"}
{"id": "task461-8d7575c0a2664e04b7e1e29bb65edc6e", "input": "To simplify the task, we set up a binary classification: candidates who have been liked or shortlisted are considered part of the hirable class and others part of the not hirable class.", "target": "How is \"hirability\" defined?"}
{"id": "task461-6a180bea2cd24835afceae1abeeb3eec", "input": "The prediction of outcomes of debates is very interesting in our case. Most of the results seem to match with the views of some experts such as the political pundits of the Washington Post. This implies that certain rules that were used to score the candidates in the debates by said-experts were in fact reflected by reading peoples' sentiments expressed over social media. This opens up a wide variety of learning possibilities from users' sentiments on social media, which is sometimes referred to as the wisdom of crowd.", "target": "Who is the crowd in these experiments?"}
{"id": "task461-6cef7fc4385c4f49816f4354795bcca1", "input": "We have collected total 3500 questions from the Internet and other sources such as books of general knowledge questions, history etc. The corpus contains the questions and the classes each question belongs to.", "target": "what datasets did they use?"}
{"id": "task461-34f67341f4b74ae8b38b8908792731ea", "input": " In addition to EIN, we created a model (Emotion-based Model) that uses emotional features only and compare it to two baselines. Our aim is to investigate if the emotional features independently can detect false news. The two baselines of this model are Majority Class baseline (MC) and the Random selection baseline (RAN).", "target": "What is the baseline?"}
{"id": "task461-4a40aebe67954c3bbce07ca7b3ff7367", "input": "This architecture builds upon sogaard2016deep, but is adapted in two ways: first, we add an unsupervised sequence labeling task (Language Modeling), second, we add a low-dimensional embedding layer between tasks in the hierarchy to learn dense representations of label tags.", "target": "What is the unsupervised task in the final layer?"}
{"id": "task461-c85fd39a9d0342cdb8d9662d66f4170d", "input": "In order to benchmark toxic comment detection, The Wikipedia Toxic Comments dataset (which we study in this work) was collected and extracted from Wikipedia Talk pages and featured in a Kaggle competition BIBREF12, BIBREF15. ", "target": "What datasets are used?"}
{"id": "task461-3ece93571fcf459e8cb72ac735c5d5b3", "input": "Finally, we combine our CNN and RNN models using a voting process. For each sentence in the test set, we apply several CNN and RNN models presented in Tables TABREF12 and TABREF14 and predict the class with the most votes. In case of a tie, we pick one of the most frequent classes randomly.", "target": "How does their simple voting scheme work?"}
{"id": "task461-7a527c82c3ab4d8f9d399338b7b7360c", "input": "In this paper we propose two neural embedding models in order to learn continuous concept vectors based on the skip-gram model BIBREF11 . Our first model is the Concept Raw Context model (CRC) which utilizes concept mentions in a large scale KB to jointly learn embeddings of both words and concepts. Our second model is the Concept-Concept Context model (3C) which learns the embeddings of concepts from their conceptual contexts (i.e., contexts containing surrounding concepts only). ", "target": "What are the two neural embedding models?"}
{"id": "task461-eeeeb938de424ac7a28af926ca1926aa", "input": "We then proceed to connect these mentions i) if they co-occur within the same document (we will refer to this as DOC-BASED edges), ii) if the pair of named entity mentions is identical (MATCH edges\u2014these may connect nodes across and within documents), or iii) if they are in the same coreference chain, as predicted by the external coreference system (COREF edges).", "target": "Did they use a relation extraction method to construct the edges in the graph?"}
{"id": "task461-117e93a3d5474cc1a3d1e599e0b3f372", "input": "Health Level Seven Fast Healthcare Interoperability Resources (HL7 FHIR)\nFHIR BIBREF5 is a new open standard for healthcare data developed by the same company that developed HL7v2. Resource Description Framework (RDF)\nRDF is the backbone of the semantic webBIBREF8.", "target": "What do FHIR and RDF stand for?"}
{"id": "task461-c97f02fc073848db80bcf1d576ed667c", "input": "As a starting point, we used the DIP corpus BIBREF37 , a collection of 49 clusters of 100 web pages on educational topics (e.g. bullying, homeschooling, drugs) with a short description of each topic.", "target": "Which collections of web documents are included in the corpus?"}
{"id": "task461-a3a1c63c833247b9957ad26542afc801", "input": "For instance, BIBREF16 and BIBREF17 showed that a hypothesis-only baseline performs better than chance due to cues on their lexical choice and sentence length. Similarly, BIBREF18 showed that NLI models tend to predict entailment for sentence pairs with a high lexical overlap.", "target": "What are examples of these artifacts?"}
{"id": "task461-95ed5b6edd7343f59592f64ebec9d86e", "input": "Originally, the seed dictionaries typically spanned several thousand word pairs BIBREF15 , BIBREF18 , BIBREF19 , but more recent work has shown that CLWEs can be induced with even weaker supervision from small dictionaries spanning several hundred pairs BIBREF20 , identical strings BIBREF21 , or even only shared numerals BIBREF22 .", "target": "How are seed dictionaries obtained by fully unsupervised methods?"}
{"id": "task461-99b8c0c03e41402ca0d7205ee113eb97", "input": "The goal of an language model is to assign meaningful probabilities to a sequence of words. Given a set of tokens $\\mathbf {X}=(x_1,....,x_T)$, where $T$ is the length of a sequence, our task is to estimate the joint conditional probability $P(\\mathbf {X})$ which is\n\nwere $(x_{1}, \\ldots , x_{i-1})$ is the context. An Intrinsic evaluation of the performance of Language Models is perplexity (PPL) which is defined as the inverse probability of the set of the tokens and taking the $T^{th}$ root were $T$ is the number of tokens We propose an approximation of the joint probability as,\n\nThis type of approximations has been previously explored with Bi-directional RNN LM's BIBREF9 but not for deep transformer models. We therefore, define a pseudo-perplexity score from the above approximated joint probability.", "target": "How is pseudo-perplexity defined?"}
{"id": "task461-2fdda8556eef40eb94094d5a0e6e71b5", "input": "To answer RQ2, which is concerned with in-domain ATSC performance, we see in tab:results that for the in-domain training case, our models BERT-ADA Lapt and BERT-ADA Rest achieve performance close to state-of-the-art on the laptops dataset and new state-of-the-art on the restaurants dataset with accuracies of $79.19\\%$ and $87.14\\%$, respectively. In general, the ATSC task generalizes well cross-domain, with about 2-$3\\%$ drop in accuracy compared to in-domain training.", "target": "What are the performance results?"}
{"id": "task461-62fac629a46048bfbc1dd76695d5eb04", "input": "Following prior work BIBREF56 , BIBREF57 , BIBREF17 , we employ the MNCut spectral clustering algorithm BIBREF58 , which has wide applicability in similar NLP tasks which involve high-dimensional feature spaces BIBREF59 , BIBREF60 , BIBREF18 .", "target": "What clustering algorithm is used on top of the VerbNet-specialized representations?"}
{"id": "task461-197260b5183045568daa4b2bd8c7be23", "input": "For MSA, we acquired the diacritized corpus that was used to train the RDI BIBREF7 diacritizer and the Farasa diacritizer BIBREF31. The corpus contains 9.7M tokens with approximately 194K unique surface forms (excluding numbers and punctuation marks). For testing, we used the freely available WikiNews test set BIBREF31, which is composed of 70 MSA WikiNews articles (18,300 tokens) and evenly covers a variety of genres including politics, economics, health, science and technology, sports, arts and culture. For CA, we obtained a large collection of fully diacritized classical texts (2.7M tokens) from a book publisher, and we held-out a small subset of 5,000 sentences (approximately 400k words) for testing.", "target": "what datasets were used?"}
{"id": "task461-c9a1d511fb344dedbe7894122d7ae406", "input": "We work with a dataset composed of 3,206 news articles, each labeled real or fake, with a perfect 50/50 split between 1,603 real and fake articles, respectively.", "target": "What is the size of the dataset?"}
{"id": "task461-ebe536636c92483da3b45775f7baf3fc", "input": "For every gendered character in the dataset, we ask annotators to create a new character with a persona of the opposite gender that is otherwise identical except for referring nouns or pronouns.", "target": "In the targeted data collection approach, what type of data is targetted?"}
{"id": "task461-4b8316c47f104256b84336ced7a5a459", "input": " We show that using external knowledge outside the tweet text (from landing pages of URLs) and user features can significantly improve performance. ", "target": "What external sources of information are used?"}
{"id": "task461-c7ff20b1e09e4fb98d4b3fec48ec8695", "input": "We draw on a recently released corpus of state speeches delivered during the annual UN General Debate that provides the first dataset of textual output from states that is recorded at regular time-series intervals and includes a sample of all countries that deliver speeches BIBREF11 . ", "target": "Which dataset do they use?"}
{"id": "task461-d5410dc68e3e4b24832beb77f39c4f64", "input": "We employ a huge dataset of 45,000 self-reported depressed users introduced in BIBREF46 where a lexicon of depression symptoms consisting of 1500 depression-indicative terms was created with the help of psychologist clinician and employed for collecting self-declared depressed individual's profiles. A subset of 8,770 users (24 million time-stamped tweets) containing 3981 depressed and 4789 control users (that do not show any depressive behavior) were verified by two human judges BIBREF46 .", "target": "How is the data annotated?"}
{"id": "task461-e71bd0d5b8204a7db2906fe62a45d5be", "input": "In this section we present two datasets used in our experiments: The NowThisNews dataset, collected for the purpose of this paper, and The BreakingNews dataset BIBREF4 , publicly available dataset of news articles.\n\ncontains 4090 posts with associated videos from NowThisNews Facebook page collected between 07/2015 and 07/2016. For each post we collected its title and the number of views of the corresponding video, which we consider our popularity metric. Due to a fairly lengthy data collection process, we decided to normalize our data by first grouping posts according to their publication month and then labeling the posts for which the popularity metric exceeds the median monthly value as popular, the remaining part as unpopular.", "target": "Where do they obtain the news videos from?"}
{"id": "task461-d755091537974e1a9aa6f9ac3d097aed", "input": "The third person plural pronoun `they' has no gender in English and most other languages (unlike the third person singular pronouns `he', `she', and `it').  If these sentences are translated into French, then `they' in the first sentence should be translated `elles', as referring to Jane and Susan, and `they' in the second sentence should be translated `ils', as referring to Fred and George. A couple of examples: The word `sie' in German serves as both the formal second person prounoun (always capitalized), the third person feminine singular, and the third person plural. ", "target": "What language do they explore?"}
{"id": "task461-d13413af9591442cb6b2376910c2cdbc", "input": " The method is evaluated on the BERTbase model, which has 12 layers, 12 self-attention heads with a hidden size of 768.", "target": "What BERT model do they test?"}
{"id": "task461-b42a1188efd44d9297142d1637f1affe", "input": "This model consists of two identical RNN encoders with no shared parameters, as well as a standard RNN decoder. For each target sentence, two versions of the source sentence are used: the sequential (standard) version and the linearized parse (lexicalized or unlexicalized).", "target": "What kind of encoders are used for the parsed source sentence?"}
{"id": "task461-1c4c2cde988d47c989c5ee25af7177c5", "input": "Abstract meaning representation BIBREF0 , or AMR for short, allows us to do that with the inclusion of most of the shallow-semantic natural language processing (NLP) tasks that are usually addressed separately, such as named entity recognition, semantic role labeling and co-reference resolution.", "target": "Which subtasks do they evaluate on?"}
{"id": "task461-20aacdd5b81743dfbe6150cedee26eb4", "input": "GE-FL reduces the heavy load of instance annotation and performs well when we provide prior knowledge with no bias. In our experiments, we observe that comparable numbers of labeled features for each class have to be supplied. We randomly select $t \\in [1, 20]$ features from the feature pool for one class, and only one feature for the other. Our methods are also evaluated on datasets with different unbalanced class distributions. We manually construct several movie datasets with class distributions of 1:2, 1:3, 1:4 by randomly removing 50%, 67%, 75% positive documents. Incorporating KL divergence is robust enough to control unbalance both in the dataset and in labeled features while the other three methods are not so competitive.", "target": "How do they define robustness of a model?"}
{"id": "task461-3262c06c9abc40db84d662b3404a8630", "input": "For comparison purpose, we build two baseline systems for each direction: one is use the traditional phrase-based statistical machine translation (SMT), the other one is the NMT system. ", "target": "what was the baseline?"}
{"id": "task461-7c8ed00e6cd84bf0a5d6fb6f02c34fd3", "input": "This decoding approach closely follows Algorithm SECREF7 , but along with soft back pointers, we also compute hard back pointers at each time step. After computing all the relevant quantities like model score, loss etc., we follow the hard backpointers to obtain the best sequence INLINEFORM0 .", "target": "Do they compare partially complete sequences (created during steps of beam search) to gold/target sequences?"}
{"id": "task461-6aaf3790ea6843c1be38ae6dd52cd6a8", "input": "The key idea behind our proposed interactive MRC (iMRC) is to restrict the document context that a model observes at one time. Concretely, we split a supporting document into its component sentences and withhold these sentences from the model. Given a question, the model must issue commands to observe sentences in the withheld set; we equip models with actions such as Ctrl+F (search for token) and stop for searching through partially observed documents. A model searches iteratively, conditioning each command on the input question and the sentences it has observed previously. Thus, our task requires models to `feed themselves' rather than spoon-feeding them with information. This casts MRC as a sequential decision-making problem amenable to reinforcement learning (RL).", "target": "How do they train models in this setup?"}
{"id": "task461-6c7ffa487eee473f9bcffe79d938ad01", "input": "Our resulting model obtains state of the art results on both the VQA dataset and the bAbI-10k text question-answering dataset, proving the framework can be generalized across input domains.", "target": "Does the DMN+ model establish state-of-the-art ?"}
{"id": "task461-9390a250eef84d528eb22c25efa532b9", "input": "To test the difficulty of our dataset, we checked the majority class label and the accuracies of five state-of-the-art NLI models adopting different approaches: BiMPM (Bilateral Multi-Perspective Matching Model; BIBREF31 , BIBREF31 ), ESIM (Enhanced Sequential Inference Model; BIBREF32 , BIBREF32 ), Decomposable Attention Model BIBREF33 , KIM (Knowledge-based Inference Model; BIBREF34 , BIBREF34 ), and BERT (Bidirectional Encoder Representations from Transformers model; BIBREF35 , BIBREF35 ). Regarding BERT, we checked the performance of a model pretrained on Wikipedia and BookCorpus for language modeling and trained with SNLI and MultiNLI.", "target": "What NLI models do they analyze?"}
{"id": "task461-d2cfedd25a294e7c9bfb7391557835e0", "input": "In addition to the majority baseline, we also compare our results with a lexicon-based approach. For experimental results, we report majority baseline for each language where the majority baseline corresponds to a model's accuracy if it always predicts the majority class in the dataset.", "target": "what are the baselines?"}
{"id": "task461-a944388f220f42a693257ee36097b78b", "input": "Our in-house dataset includes manually annotated RE data for 6 languages: English, German, Spanish, Italian, Japanese and Portuguese. It defines 56 entity types (e.g., Person, Organization, Geo-Political Entity, Location, Facility, Time, Event_Violence, etc.) and 53 relation types between the entities (e.g., AgentOf, LocatedAt, PartOf, TimeOf, AffectedBy, etc.). The ACE05 dataset includes manually annotated RE data for 3 languages: English, Arabic and Chinese. It defines 7 entity types (Person, Organization, Geo-Political Entity, Location, Facility, Weapon, Vehicle) and 6 relation types between the entities (Agent-Artifact, General-Affiliation, ORG-Affiliation, Part-Whole, Personal-Social, Physical). the ACE (Automatic Content Extraction) 2005 multilingual dataset BIBREF11.", "target": "What datasets are used?"}
{"id": "task461-b9bfc4bbf02f44d3af4925f8ca49e905", "input": "We evaluated our attention transformations on three language pairs. We use the IWSLT 2014 corpus for De-En, the KFTT corpus for Ja-En BIBREF19 , and the WMT 2016 dataset for Ro-En.", "target": "What are the language pairs explored in this paper?"}
{"id": "task461-42b4288d59b74e1cada760c05dee553d", "input": " We apply an off-the-shelf tool for emotion recognition (the manufacturer cannot be disclosed due to licensing restrictions). It delivers frame-by-frame scores ($\\in [0;100]$) for discrete emotional states of joy, anger and fear. We extract the audio signal for the same sequence as described for facial expressions and apply an off-the-shelf tool for emotion recognition. The software delivers single classification scores for a set of 24 discrete emotions for the entire utterance.", "target": "What are the emotion detection tools used for audio and face input?"}
{"id": "task461-47944ee2d23c4a91a93ab3f5a7effec6", "input": "With BERT, two fully unsupervised tasks are performed. The Masked Language Model and the Next Sentence Prediction (NSP).\n\nFor this study, the NSP is used as a proxy for the relevance of response.", "target": "was bert used?"}
{"id": "task461-6373445db9e84c76900440242e4e87ea", "input": "While the dataset was collected with simulated environments, no structure was imposed on the navigation instructions while crowd-sourcing data. Thus, many instructions in our dataset are ambiguous. Moreover, the order of the behaviors in the instructions is not always the same. For instance, a person said \u201cturn right and advance\u201d to describe part of a route, while another person said \u201cgo straight after turning right\u201d in a similar situation. The high variability present in the natural language descriptions of our dataset makes the problem of decoding instructions into behaviors not trivial. See Appendix A of the supplementary material for additional details on our data collection effort.", "target": "What language is the experiment done in?"}
{"id": "task461-862828193c354048997c644bb2828dfb", "input": "Paraphrase Identification (PI) is the task of determining whether two sentences are paraphrase or not. We have implemented the cross-lingual variant of kernel functions for PI and RE tasks as described in section SECREF3 and measured the accuracy of models by testing them on the parallel data set.", "target": "What classification task was used to evaluate the cross-lingual adaptation method described in this work?"}
{"id": "task461-6eebd8b1f0f643329300663787a2c0f0", "input": "We propose a listening comprehension model for the task defined above, the Attention-based Multi-hop Recurrent Neural Network (AMRNN) framework, and show that this model is able to perform reasonably well for the task. In the proposed approach, the audio of the stories is first transcribed into text by ASR, and the proposed model is developed to process the transcriptions for selecting the correct answer out of 4 choices given the question. ", "target": "What approach does this work propose for the new task?"}
{"id": "task461-79b5eaf9c1eb44f5bf137f6c85583298", "input": "We describe our rules for WikiSQL here. Our rule for KBQA is simple without using a curated mapping dictionary. The pipeline of rules in SequentialQA is similar to that of WikiSQL.", "target": "Are the rules dataset specific?"}
{"id": "task461-0065dbf08ee840498dd27544c1eeba75", "input": "Table TABREF14 shows the results of our main experiments on the 2016 and 2018 test sets for French and German. We use Meteor BIBREF31 as the main metric, as in the WMT tasks BIBREF25 . We compare our transformer baseline to transformer models enriched with image information, as well as to the deliberation models, with or without image information.\n\nWe first note that our multimodal models achieve the state of the art performance for transformer networks (constrained models) on the English-German dataset, as compared to BIBREF30 . Second, our deliberation models lead to significant improvements over this baseline across test sets (average INLINEFORM0 , INLINEFORM1 ).", "target": "What dataset does this approach achieve state of the art results on?"}
{"id": "task461-eabeb0404efa432fbfa49ac66b8ebb53", "input": "In this section, we describe the data augmentation methods we use to increase the amount of training data in order to make our NMT systems suffer less from the low-resourced situation in Japanese INLINEFORM0 Vietnamese translation. Back Translation\nOne of the approaches to leverage the monolingual data is to use a machine translation system to translate those data in order to create a synthetic parallel data. Normally, the monolingual data in the target language is translated, thus the name of the method: Back Translation BIBREF11 . Mix-Source Approach\nAnother data augmentation method considered useful in this low-resourced setting is the mix-source method BIBREF12 .", "target": "what methods were used to reduce data sparsity effects?"}
{"id": "task461-2a5a69f945574cf18484b20483df13a4", "input": "As fig:fit shows, estimated test accuracy is highly correlated with actual test accuracy for various datasets, with worst-case values $\\mu <1\\%$ and $\\sigma <5\\%$ . Note that the number of free parameters is small ($||\\le 6$) compared to the number of points (42\u201349 model-data configurations), demonstrating the appropriateness of the proposed function for modeling the complex error landscape.", "target": "What is proof that proposed functional form approximates well generalization error in practice?"}
{"id": "task461-c934d0b2686749858defd99ba9ebef63", "input": "We incorporate typing information by concatenating to the embedding vector of each input symbol one of three embedding vectors, S, E or R, where S is concatenated to structural elements (opening and closing brackets), E to entity symbols and R to relation symbol", "target": "How are typing hints suggested?"}
{"id": "task461-11a490c52da9452e9683c23422ff67eb", "input": "To do so, we constructed a new dataset with 23,700 queries that are short and unstructured, in the same style made by real users of task-oriented systems. ", "target": "What is the size of this dataset?"}
{"id": "task461-8fac478d43b8429bba3b07c1ac1c2c95", "input": "The baseline system for the SLC task is a very simple logistic regression classifier with default parameters, where we represent the input instances with a single feature: the length of the sentence.  The baseline for the FLC task generates spans and selects one of the 18 techniques randomly. ", "target": "What was the baseline for this task?"}
{"id": "task461-ae825391410149ec8770b0911af775f2", "input": "As far as we know, all existing systems treat this task as a pipeline of two separate subtasks, i.e., event extraction and temporal relation classification, and they also assume that gold events are given when training the relation classifier BIBREF0, BIBREF1, BIBREF2, BIBREF3, BIBREF4, BIBREF5. Specifically, they built end-to-end systems that extract events first and then predict temporal relations between them (Fig. FIGREF1). In these pipeline models, event extraction errors will propagate to the relation classification step and cannot be corrected afterwards. Our first contribution is the proposal of a joint model that extracts both events and temporal relations simultaneously (see Fig. FIGREF1).", "target": "Is this the first paper to propose a joint model for event and temporal relation extraction?"}
{"id": "task461-49069c34e1bf431c8bc0b6498818bdf9", "input": "(2) Different from LEM and DPEMM, AEM uses a generator network to capture the event-related patterns and is able to mine events from different text sources (short and long). Moreover, unlike traditional inference procedure, such as Gibbs sampling used in LEM and DPEMM, AEM could extract the events more efficiently due to the CUDA acceleration;", "target": "What alternative to Gibbs sampling is used?"}
{"id": "task461-e6b1f5d954b642789ff5421b608e9109", "input": "For de-identification tasks, the three metrics we will use to evaluate the performance of our architecture are Precision, Recall and INLINEFORM0 score as defined below.", "target": "What evaluation metrics do they use?"}
{"id": "task461-28f7935d89fe45d28a1f72ab8a675ba5", "input": "For our classification experiments, we use the MH17 Twitter dataset introduced by BIBREF4, a dataset collected in order to study the flow of (dis)information about the MH17 plane crash on Twitter.", "target": "What dataset is used for this study?"}
{"id": "task461-6d63d1056ca74e38858f6e54a0c94502", "input": "We examine a deep-learning approach to sequence labeling using a vanilla recurrent neural network (RNN) with word embeddings, as well as a joint inference, structured prediction approach using Stanford's knowledge base construction framework DeepDive BIBREF1 .", "target": "Which structured prediction approach do they adopt for temporal entity extraction?"}
{"id": "task461-489703bec714400a8f8c165297f3319a", "input": "We use two different unsupervised approaches for word sense disambiguation.  The second, called `dense model', represents synsets and contexts in a dense, low-dimensional space by averaging word embeddings. In the synset embeddings model approach, we follow SenseGram BIBREF14 and apply it to the synsets induced from a graph of synonyms.  We observe that the SenseGram-based approach for word sense disambiguation yields substantially better results in every case (Table TABREF25 ). The primary reason for that is the implicit handling of similar words due to the averaging of dense word vectors for semantically related words. ", "target": "Do the authors offer any hypothesis about why the dense mode outperformed the sparse one?"}
{"id": "task461-dd629f71cc4444dd8aa0ddfe33b93001", "input": "The aim of this section is to validate the applicability of our theoretical results\u2014which state that self-attention can perform convolution\u2014and to examine whether self-attention layers in practice do actually learn to operate like convolutional layers, when being trained on standard image classification tasks. In particular, we study the relationship between self-attention and convolution with quadratic and learned relative positional encodings. We find that for both cases, the attention probabilities learned tend to respect the conditions of Lemma UNKREF15, corroborating our hypothesis. Nevertheless, to validate that our model learns a meaningful classifier we compare it to the standard ResNet18 BIBREF14 on the CIFAR-10 dataset BIBREF15.", "target": "What numerical experiments they perform?"}
{"id": "task461-c5ea4a2bacd44872a59c43f323199cbd", "input": "We sampled all papers published in the Computer Science subcategories of Artificial Intelligence (cs.AI), Machine Learning (cs.LG), Social and Information Networks (cs.SI), Computational Linguistics (cs.CL), Computers and Society (cs.CY), Information Retrieval (cs.IR), and Computer Vision (CS.CV), the Statistics subcategory of Machine Learning (stat.ML), and Social Physics (physics.soc-ph). We filtered for papers in which the title or abstract included at least one of the words \u201cmachine learning\u201d, \u201cclassif*\u201d, or \u201csupervi*\u201d (case insensitive). We then filtered to papers in which the title or abstract included at least \u201ctwitter\u201d or \u201ctweet\u201d (case insensitive), which resulted in 494 papers. We used the same query on Elsevier's Scopus database of peer-reviewed articles, selecting 30 randomly sampled articles, which mostly selected from conference proceedings. One paper from the Scopus sample was corrupted, so only 29 papers were examined.", "target": "How were the machine learning papers from ArXiv sampled?"}
{"id": "task461-ed66698e37b24e3c84751510c983486e", "input": "The BioNLP 2009 Shared Task BIBREF195 was based on the GENIA corpus BIBREF196 which contains PubMed abstracts of articles on transcription factors in human blood cells.", "target": "Which datasets are used in this work?"}
{"id": "task461-4406b3c80a8d4bc2ae1e86878780be51", "input": "JESSI is trained using only the datasets given on the shared task, without using any additional external data.", "target": "What datasets were used?"}
{"id": "task461-7ce024cb72564ff3993949f49a2049e8", "input": "Here we consider five baselines to compare with GraLap: (i) Uniform: assign 3 to all the references assuming equal intensity, (ii) SVR+W: recently proposed Support Vector Regression (SVR) with the feature set mentioned in BIBREF4 , (iii) SVR+O: SVR model with our feature set, (iv) C4.5SSL: C4.5 semi-supervised algorithm with our feature set BIBREF23 , and (v) GLM: the traditional graph-based LP model with our feature set BIBREF9 .", "target": "What are the baselines model?"}
{"id": "task461-a312f3dc59454f6dab4bf2a6ece285fb", "input": "The competition is divided into five subtasks which involve standard classification, ordinal classification and distributional estimation. For a more detailed description see BIBREF0 .", "target": "What were the five English subtasks?"}
{"id": "task461-bbff54fda03e4828bcc375b638db36d5", "input": "The results of multilingual training in which the modeling unit is syllables are presented in Table 5. All error rates are the weighted averages of all evaluated speakers. Here, `+ both' represents the result of training with both JNAS and WSJ corpora. The multilingual training is effective in the speaker-open setting, providing a relative WER improvement of 10%. The JNAS corpus was more helpful than the WSJ corpus because of the similarities between Ainu and Japanese language.", "target": "How big are improvements with multilingual ASR training vs single language training?"}
{"id": "task461-3eaebc38372d4116bfcfac94890c9c5b", "input": "To capture this interesting property, we propose a new tagging scheme consisting of three tags, namely { INLINEFORM0 }.\n\nINLINEFORM0 tag indicates that the current word appears before the pun in the given context.\n\nINLINEFORM0 tag highlights the current word is a pun.\n\nINLINEFORM0 tag indicates that the current word appears after the pun.", "target": "What is the tagging scheme employed?"}
{"id": "task461-15fba80e7a5b4ce59fb25fb8ba8a3a89", "input": "We segment a hashtag into meaningful English phrases. In order to achieve this, we use a dictionary of English words. The sentiment of url: Since almost all the articles are written in well-formatted english, we analyze the sentiment of the first paragraph of the article using Standford Sentiment Analysis tool BIBREF4 . ", "target": "Do the authors report only on English language data?"}
{"id": "task461-c244aa4a1bf44b9395c3f890e5ba1f58", "input": "For the emotion recognition from text, we manually transcribe all utterances of our AMMER study. To exploit existing and available data sets which are larger than the AMMER data set, we develop a transfer learning approach. We use a neural network with an embedding layer (frozen weights, pre-trained on Common Crawl and Wikipedia BIBREF36), a bidirectional LSTM BIBREF37, and two dense layers followed by a soft max output layer. This setup is inspired by BIBREF38. We use a dropout rate of 0.3 in all layers and optimize with Adam BIBREF39 with a learning rate of $10^{-5}$ (These parameters are the same for all further experiments). We build on top of the Keras library with the TensorFlow backend. We consider this setup our baseline model.", "target": "What is the baseline method for the task?"}
{"id": "task461-bd150def376e49a2a98e58f0866d17d1", "input": "Baselines. We use one strong non-DNN baseline, NBSVM (with unigrams or bigrams features) BIBREF23 and six DNN baselines. The first DNN baseline is CNN BIBREF25, which does not handle noisy labels. The other five were designed to handle noisy labels.\n\nThe comparison results are shown in Table TABREF12. From the results, we can make the following observations. (1) Our NetAb model achieves the best ACC and F1 on all datasets except for F1 of negative class on Laptop.", "target": "Is the model evaluated against a CNN baseline?"}
{"id": "task461-476c9087d36248aab9f8c3356428828a", "input": "Most lexical resources for sentiment analysis are in English. To still be able to benefit from these sources, the lexicons in the AffectiveTweets package were translated to Spanish, using the machine translation platform Apertium BIBREF5 ", "target": "How was the training data translated?"}
{"id": "task461-84980666d80b4cdcbb867acb4f972a6d", "input": "This work analyzes the understanding of pretrained language models of factual and commonsense knowledge stored in negated statements. To this end, we introduce the negated LAMA dataset. We construct it by simply inserting negation elements (e.g., \u201cnot\u201d) in LAMA cloze statement (e.g., \u201cThe theory of relativity was not developed by [MASK].\u201d).", "target": "How did they extend LAMA evaluation framework to focus on negation?"}
{"id": "task461-7c466d0b9c244ba8aaad774c75ec3ce1", "input": "Table TABREF18 shows the Spearman correlation values of GM$\\_$KL model evaluated on the benchmark word similarity datasets: SL BIBREF20, WS, WS-R, WS-S BIBREF21, MEN BIBREF22, MC BIBREF23, RG BIBREF24, YP BIBREF25, MTurk-287 and MTurk-771 BIBREF26, BIBREF27, and RW BIBREF28.  Table TABREF19 shows the evaluation results of GM$\\_$KL model on the entailment datasets such as entailment pairs dataset BIBREF29 created from WordNet with both positive and negative labels, a crowdsourced dataset BIBREF30 of 79 semantic relations labelled as entailed or not and annotated distributionally similar nouns dataset BIBREF31", "target": "What are the qualitative experiments performed on benchmark datasets?"}
{"id": "task461-6f53595f6e914e4fa9efd43cde6d69fd", "input": " Each passage was labelled by 3 unique annotators.", "target": "How many annotators were there?"}
{"id": "task461-54d505a4d26144b39069a26423c5b4d4", "input": "We have used a dataset of 14 TDs to conduct our experiments.", "target": "Which dataset(s) do they use?"}
{"id": "task461-4aecd3f8c30c4639a2fdcb2f5a386f58", "input": "The previously mentioned datasets are all in English", "target": "For which languages most of the existing MRC datasets are created?"}
{"id": "task461-75244c194be8494bbb7acf6919439258", "input": "CodeInternational: A tool which can translate code between human languages, powered by Google Translate.\n\n", "target": "Is this auto translation tool based on neural networks?"}
{"id": "task461-2b9d7ca35f9b4f6483efd9508b4bcc38", "input": "Our model got the first position in the German sub-task with a macro F1 score of 0.62.", "target": "What is the performance of the model for the German sub-task A?"}
